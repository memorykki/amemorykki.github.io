<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://memorykki.github.io</id>
    <title>Memorykk&apos;s blog</title>
    <updated>2023-01-01T19:49:40.621Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://memorykki.github.io"/>
    <link rel="self" href="https://memorykki.github.io/atom.xml"/>
    <logo>https://memorykki.github.io/images/avatar.png</logo>
    <icon>https://memorykki.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Memorykk&apos;s blog</rights>
    <entry>
        <title type="html"><![CDATA[Nginx]]></title>
        <id>https://memorykki.github.io/nginx/</id>
        <link href="https://memorykki.github.io/nginx/">
        </link>
        <updated>2021-10-30T02:49:13.000Z</updated>
        <summary type="html"><![CDATA[<p>Nginx 是一个高性能的 HTTP 和反向代理服务器</p>
]]></summary>
        <content type="html"><![CDATA[<p>Nginx 是一个高性能的 HTTP 和反向代理服务器</p>
<!-- more -->
<h2 id="概述">概述</h2>
<p>Nginx (&quot;engine x&quot;) 是一个高性能的 HTTP 和反向代理服务器,特点是占有内存少，并发能力强，事实上 nginx 的并发能力确实在同类型的网页服务器中表现较好，</p>
<p>Nginx 可以作为静态页面的 web 服务器，同时还支持 CGI 协议的动态语言，比如 perl、 php等。但是不支持 java。 Java 程序只能通过与 tomcat 配合完成。 Nginx 专为性能优化而开发，性能是其最重要的考量,实现上非常注重效率 ，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。</p>
<ul>
<li>正向代理</li>
</ul>
<p>Nginx 不仅可以做反向代理，实现负载均衡。还能用作正向代理来进行上网等功能。<br>
正向代理：如果把局域网外的 Internet 想象成一个巨大的资源库，则局域网中的客户端要访问 Internet，则需要通过代理服务器来访问，这种代理服务就称为正向代理。</p>
<ul>
<li>反向代理</li>
</ul>
<p>反向代理，其实客户端对代理是无感知的，因为客户端不需要任何配置就可以访问，我们只需要将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器获取数据后，在返回给客户端，此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器<br>
地址，隐藏了真实服务器 IP 地址。</p>
<ul>
<li>负载均衡</li>
</ul>
<p>客户端发送多个请求到服务器，服务器处理请求，有一些可能要与数据库进行交互，服务器处理完毕后，再将结果返回给客户端。<br>
这种架构模式对于早期的系统相对单一，并发请求相对较少的情况下是比较适合的，成本也低。但是随着信息数量的不断增长，访问量和数据量的飞速增长，以及系统业务的复杂度增加，这种架构会造成服务器相应客户端的请求日益缓慢，并发量特别大的时候，还容易造成服务器直接崩溃。很明显这是由于服务器性能的瓶颈造成的问题，那么如何解决这种情况呢？<br>
我们首先想到的可能是升级服务器的配置，比如提高 CPU 执行频率，加大内存等提高机器的物理性能来解决此问题，但是我们知道摩尔定律的日益失效，硬件的性能提升已经不能满足日益提升的需求了。最明显的一个例子，天猫双十一当天，某个热销商品的瞬时访问量<br>
是极其庞大的，那么类似上面的系统架构，将机器都增加到现有的顶级物理配置，都是不能够满足需求的。那么怎么办呢？<br>
上面的分析我们去掉了增加服务器物理配置来解决问题的办法，也就是说纵向解决问题的办法行不通了，那么横向增加服务器的数量呢？这时候集群的概念产生了，单个服务器解决不了，我们增加服务器的数量，然后将请求分发到各个服务器上，将原先请求集中到单个<br>
服务器上的情况改为将请求分发到多个服务器上，将负载分发到不同的服务器，也就是我们所说的负载均衡</p>
<ul>
<li>动静分离</li>
</ul>
<p>为了加快网站的解析速度，可以把动态页面和静态页面由不同的服务器来解析，加快解析速<br>
度。降低原来单个服务器的压力。</p>
<h2 id="负载均衡">负载均衡</h2>
<ul>
<li>
<p>轮询（默认）<br>
每个请求<strong>按时间顺序</strong>逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。</p>
</li>
<li>
<p>weight<br>
weight 代表权,重默认为 1,权重越高被分配的客户端越多指定轮询几率， weight 和访问比率成正比，用于后端服务器性能不均的情况。</p>
</li>
<li>
<p>least_conn</p>
</li>
</ul>
<p>此策略是指每次将请求分发到当前连接数最少的服务器上，试图转发给相对空闲的服务器以实现负载平衡；</p>
<ul>
<li>
<p>ip_hash<br>
每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 的问题。从本质上说，ip hash算法是一种变相的轮询算法，如果两个ip的初始hash值恰好相同，那么来自这两个ip的请求将永远落在同一台服务器上，这为均衡性埋下了较深隐患。</p>
</li>
<li>
<p>url hash</p>
<p>按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。</p>
</li>
<li>
<p>fair（第三方）<br>
根据后端服务器的响应时间判断负载情况，从中选出负载最轻的机器进行分流。</p>
</li>
</ul>
<h2 id="动静分离">动静分离</h2>
<p>&lt;<img src="https://memorykki.github.io/post-images/Nginx/image-20211030175322817.png" alt="" loading="lazy"></p>
<p>Nginx 动静分离简单来说就是把<strong>动态跟静态请求分开</strong>，不能理解成只是单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用 Nginx处理静态页面， Tomcat 处理动态页面。</p>
<p>实现方案：</p>
<p>一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案；</p>
<p>另外一种方法就是动态跟静态文件混合在一起发布，通过 nginx 来分开。通过 location 指定不同的后缀名实现不同的请求转发。通过 expires 参数设置，可以使浏览器缓存过期时间，减少与服务器之前的请求和流量。</p>
<h2 id="工作原理">工作原理</h2>
<p>Nginx本身做的工作实际很少，当它接到一个HTTP请求时，它仅仅是通过查找配置文件将此次请求映射到一个location block，而此location中所配置的各个指令则会启动不同的模块去完成工作，因此模块可以看做Nginx真正的劳动工作者。通常一个location中的指令会涉及一个handler模块和多个filter模块（当然，多个location可以复用同一个模块）。handler模块负责处理请求，完成响应内容的生成，而filter模块对响应内容进行处理。</p>
<p>模块：</p>
<ul>
<li>Handlers（处理器模块）。此类模块直接处理请求，并进行输出内容和修改headers信息等操作。Handlers处理器模块一般只能有一个。</li>
<li>Filters （过滤器模块）。此类模块主要对其他处理器模块输出的内容进行修改操作，最后由Nginx输出。</li>
<li>Proxies （代理类模块）。此类模块是Nginx的HTTP Upstream之类的模块，这些模块主要与后端一些服务比如FastCGI等进行交互，实现服务代理和负载均衡等功能。</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://images2017.cnblogs.com/blog/1183448/201802/1183448-20180210145015185-1393050434.png" alt="img" loading="lazy"></figure>
<h2 id="进程模型"><strong>进程模型</strong></h2>
<p>Nginx默认采用<strong>多进程</strong>工作方式。</p>
<p>Nginx启动后，会运行一个master进程和多个worker进程。其中master充当整个进程组与用户的交互接口，同时对进程进行监护，管理worker进程来实现重启服务、平滑升级、更换日志文件、配置文件实时生效等功能。worker用来处理基本的网络事件，worker之间是平等的，他们共同竞争来处理来自客户端的请求。</p>
<figure data-type="image" tabindex="2"><img src="https://images2017.cnblogs.com/blog/1183448/201802/1183448-20180210145226654-1347579045.png" alt="img" loading="lazy"></figure>
<h3 id="master管理worker">master：管理worker</h3>
<p>master进程主要用来管理worker进程，具体包括如下4个主要功能：<br>
（1）接收来自外界的信号。<br>
（2）向各worker进程发送信号。<br>
（3）监控woker进程的运行状态。<br>
（4）当woker进程退出后（异常情况下），会自动重新启动新的woker进程。</p>
<h3 id="worker处理请求">worker：处理请求</h3>
<p>多个worker进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个worker进程中处理，一个worker进程，不可能处理其它进程的请求。<strong>worker进程的个数是可以设置的，一般我们会设置与机器cpu核数一致</strong>，这里面的原因与nginx的进程模型以及事件处理模型是分不开的。</p>
<p>Nginx采用<strong>异步非阻塞</strong>的方式来处理网络事件：</p>
<ul>
<li>接收请求：</li>
</ul>
<p>在创建master进程时，先建立需要监听的socket（listenfd），然后从master进程中fork()出多个worker进程，如此一来每个worker进程都可以监听用户请求的socket。一般来说，当一个连接进来后，所有在Worker都会收到通知，但是只有一个进程可以接受这个连接请求，其它的都失败，这是所谓的<strong>惊群现象</strong>。nginx提供了一个accept_mutex（<strong>互斥锁</strong>），有了这把锁之后，同一时刻，就只会有一个进程在accpet连接，这样就不会有惊群问题了。</p>
<ul>
<li>处理请求</li>
</ul>
<p>当一个worker进程在accept这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。</p>
<p>我们可以看到，一个请求，完全由worker进程来处理，而且只在一个worker进程中处理。worker进程之间是平等的，每个进程，处理请求的机会也是一样的。</p>
<h2 id="性能高的原因">性能高的原因</h2>
<p>nginx是以多进程的方式来工作的，当然nginx也是支持多线程的方式的，只是我们主流的方式还是多进程的方式，也是nginx的默认方式。</p>
<ul>
<li>多进程模型</li>
<li>异步非阻塞</li>
</ul>
<h3 id="多进程模型-vs-多线程模型"><strong>多进程模型 VS 多线程模型</strong></h3>
<p>首先，对于每个worker进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销；</p>
<p>其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master进程则很快启动新的worker进程。</p>
<p>apache的常用工作方式（apache也有异步非阻塞版本，但因其与自带某些模块冲突，所以不常用），每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。这对操作系统来说，是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的cpu开销很大，自然性能就上不去了，而这些开销完全是没有意义的。</p>
<h3 id="同步阻塞-vs-异步非阻塞"><strong>同步阻塞 VS 异步非阻塞</strong></h3>
<p>同步阻塞的：处理请求时遇到读写事件，而当读写事件没有准备好时，那就只能等了，等事件准备好了，你再继续吧。cpu空闲下来没人用，cpu利用率自然上不去了，更别谈高并发了。</p>
<p>非阻塞就是，事件虽没有准备好，但不会让你一直在等待，马上返回ErrorAgain，先去处理别的请求，但是需要不时地过来检查一下事件的状态，这种开销也是不小的。</p>
<p>所以有了异步非阻塞的事件处理机制，具体到系统调用就是像select/poll/epoll/kqueue这样的系统调用。**同时监控多个事件，调用他们是阻塞的，但可以设置超时时间，在超时时间之内准备好就返回。**epoll为例，当事件没准备好时，放到epoll里面，事件准备好了，我们就去读写，当读写返回ErrorAgain时，我们将它再次加入到epoll里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在epoll里面等着。</p>
<p>这样，高并发下，<strong>线程虽然只有一个，但是一直在循环切换、处理请求而不停止，切换是没有任何代价</strong>，理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式不需要创建线程，每个请求占用的内存也很少，没有上下文切换。并发数再多也不会导致无谓的资源浪费（上下文切换）。<strong>更多的并发数，只是会占用更多的内存而已。</strong></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTg2NTU0Ng==&amp;mid=407889757&amp;idx=3&amp;sn=cfa8a70a5fd2a674a91076f67808273c&amp;scene=23&amp;srcid=0401aeJQEraSG6uvLj69Hfve#rd">参考文章</a></p>
<h3 id="worker-数量"><strong>worker 数量</strong></h3>
<p>推荐设置<strong>worker的个数 == cpu的核数</strong>，在这里就很容易理解了，更多的worker数，只会导致进程来竞争cpu资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了cpu亲缘性的绑定选项，我们可以将某<strong>一个进程绑定在某一个核</strong>上，这样就不会因为进程的切换带来cache的失效。</p>
<p>对于一个基本的web服务器来说，事件通常有三种类型，网络事件、信号、定时器。从上面的讲解中知道，网络事件通过异步非阻塞可以很好的解决掉。如何处理信号与定时器？</p>
<h3 id="信号处理">信号处理</h3>
<p>首先，信号的处理。对nginx来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重入。关于信号的处理，大家可以学习一些专业书籍，这里不多说。对于nginx来说，如果nginx正在等待事件（epoll_wait时），如果程序收到信号，在信号处理函数处理完后，epoll_wait会返回错误，然后程序可再次进入epoll_wait调用。</p>
<h3 id="定时器">定时器</h3>
<p>由于epoll_wait等函数在调用的时候是可以设置一个超时时间的，所以nginx借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入epoll_wait前，<strong>先从该红黑树里面拿到所有定时器事件的最小时间</strong>，在计算出epoll_wait的超时时间后进入epoll_wait。所以当没有事件产生，也没有中断信号时，epoll_wait会超时，也就是说定时器事件到了。这时nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ElasticSearch]]></title>
        <id>https://memorykki.github.io/ElasticSearch/</id>
        <link href="https://memorykki.github.io/ElasticSearch/">
        </link>
        <updated>2021-10-10T02:49:13.000Z</updated>
        <summary type="html"><![CDATA[<p>Elasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Elasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。</p>
<!-- more -->
<h2 id="核心概念">核心概念</h2>
<pre><code>集群，节点，索引，类型，文档，分片，映射是什么？
</code></pre>
<blockquote>
<p>elasticsearch是面向文档，关系型数据库和elasticsearch客观的对比！一切都是json</p>
</blockquote>
<table>
<thead>
<tr>
<th>Relational DB</th>
<th>Elasticsearch</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据库（database）</td>
<td>索引（indices）</td>
</tr>
<tr>
<td>表（tables）</td>
<td>types</td>
</tr>
<tr>
<td>行（rows）</td>
<td>documents</td>
</tr>
<tr>
<td>字段（columns）</td>
<td>fields</td>
</tr>
</tbody>
</table>
<p>物理设计：</p>
<p>elasticsearch在后台把每个索引划分成多个分片。每个分片可以在集群中的不同服务器间迁移</p>
<p>逻辑设计：</p>
<p>一个索引类型中，抱哈an多个文档，当我们索引一篇文档时，可以通过这样的一个顺序找到它：索引-》类型-》文档id，通过这个组合我们就能索引到某个具体的文档。注意：<code>ID不必是整数，实际上它是一个字符串。</code></p>
<h3 id="文档">文档</h3>
<blockquote>
<p>文档</p>
</blockquote>
<p>就是我们的一条条的记录</p>
<p>之前说elasticsearch是面向文档的,那么就意味着索弓和搜索数据的最小单位是文档, elasticsearch中,文档有几个重要属性:</p>
<ul>
<li>自我包含, - -篇文档同时包含字段和对应的值,也就是同时包含key:value !</li>
<li>可以是层次型的，-一个文档中包含自文档,复杂的逻辑实体就是这么来的! {就是一 个json对象! fastjson进行自动转换!}</li>
<li>灵活的结构,文档不依赖预先定义的模式,我们知道关系型数据库中,要提前定义字段才能使用,在elasticsearch中,对于字段是非常灵活的,有时候,我们可以忽略该字段,或者动态的添加一个新的字段。</li>
</ul>
<p>尽管我们可以随意的新增或者忽略某个字段,但是,每个字段的类型非常重要,比如一一个年龄字段类型,可以是字符串也可以是整形。因为elasticsearch会保存字段和类型之间的映射及其他的设置。这种映射具体到每个映射的每种类型,这也是为什么在elasticsearch中,类型有时候也称为映射类型。</p>
<h3 id="类型">类型</h3>
<blockquote>
<p>类型</p>
</blockquote>
<p>类型是文档的逻辑容器,就像关系型数据库一样,表格是行的容器。类型中对于字段的定 义称为映射,比如name映射为字符串类型。我们说文档是无模式的 ,它们不需要拥有映射中所定义的所有字段,比如新增一个字段,那么elasticsearch是怎么做的呢?elasticsearch会自动的将新字段加入映射,但是这个字段的不确定它是什么类型, elasticsearch就开始猜,如果这个值是18 ,那么elasticsearch会认为它是整形。但是elasticsearch也可能猜不对 ，所以最安全的方式就是提前定义好所需要的映射,这点跟关系型数据库殊途同归了,先定义好字段,然后再使用,别整什么幺蛾子。</p>
<h3 id="索引">索引</h3>
<blockquote>
<p>索引</p>
</blockquote>
<p>就是数据库!</p>
<p>索引是映射类型的容器, elasticsearch中的索引是一个非常大的文档集合。索|存储了映射类型的字段和其他设置。然后它们被存储到了各个分片上了。我们来研究下分片是如何工作的。</p>
<p><strong>物理设计:节点和分片如何工作</strong></p>
<p>一个集群至少有一 个节点,而一个节点就是一-个elasricsearch进程 ,节点可以有多个索引默认的,如果你创建索引,那么索引将会有个5个分片( primary shard ,又称主分片)构成的,每一个主分片会有-一个副本( replica shard ,又称复制分片）</p>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200828224136138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>上图是一个有3个节点的集群,可以看到主分片和对应的复制分片都不会在同-个节点内,这样有利于某个节点挂掉了,数据也不至于丢失。实际上, 一个分片是- -个Lucene索引, -一个包含倒排索引的文件目录,倒排索引的结构使得elasticsearch在不扫描全部文档的情况下,就能告诉你哪些文档包含特定的关键字。不过,等等,倒排索引是什么鬼?</p>
<h3 id="倒排索引">倒排索引</h3>
<blockquote>
<p>倒排索引</p>
</blockquote>
<p>elasticsearch使用的是一种称为倒排索引 |的结构,采用Lucene倒排索作为底层。这种结构适用于快速的全文搜索，一个索引由文<br>
档中所有不重复的列表构成,对于每一个词,都有一个包含它的文档列表。 例如,现在有两个文档，每个文档包含如下内容:</p>
<pre><code class="language-shell">Study every day， good good up to forever  # 文 档1包含的内容
To forever, study every day，good good up  # 文档2包含的内容
12
</code></pre>
<p>为为创建倒排索引,我们首先要将每个文档拆分成独立的词(或称为词条或者tokens) ,然后创建一一个包含所有不重 复的词条的排序列表,然后列出每个词条出现在哪个文档:</p>
<table>
<thead>
<tr>
<th>term</th>
<th>doc_1</th>
<th>doc_2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Study</td>
<td>√</td>
<td>x</td>
</tr>
<tr>
<td>To</td>
<td>x</td>
<td>x</td>
</tr>
<tr>
<td>every</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>forever</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>day</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>study</td>
<td>x</td>
<td>√</td>
</tr>
<tr>
<td>good</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>every</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>to</td>
<td>√</td>
<td>x</td>
</tr>
<tr>
<td>up</td>
<td>√</td>
<td>√</td>
</tr>
</tbody>
</table>
<p>现在，我们试图搜索 to forever，只需要查看包含每个词条的文档</p>
<table>
<thead>
<tr>
<th>term</th>
<th>doc_1</th>
<th>doc_2</th>
</tr>
</thead>
<tbody>
<tr>
<td>to</td>
<td>√</td>
<td>x</td>
</tr>
<tr>
<td>forever</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>total</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>两个文档都匹配,但是第一个文档比第二个匹配程度更高。如果没有别的条件,现在,这两个包含关键字的文档都将返回。<br>
再来看一个示例,比如我们通过博客标签来搜索博客文章。那么倒排索引列表就是这样的一个结构:</p>
<table>
<thead>
<tr>
<th>博客文章(原始数据)</th>
<th>博客文章(原始数据)</th>
<th>索引列表(倒排索引)</th>
<th>索引列表(倒排索引)</th>
</tr>
</thead>
<tbody>
<tr>
<td>博客文章ID</td>
<td>标签</td>
<td>标签</td>
<td>博客文章ID</td>
</tr>
<tr>
<td>1</td>
<td>python</td>
<td>python</td>
<td>1，2，3</td>
</tr>
<tr>
<td>2</td>
<td>python</td>
<td>linux</td>
<td>3，4</td>
</tr>
<tr>
<td>3</td>
<td>linux，python</td>
<td></td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>linux</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>如果要搜索含有python标签的文章,那相对于查找所有原始数据而言，查找倒排索引后的数据将会快的多。只需要查看标签这一栏,然后获取相关的文章ID即可。完全过滤掉无关的所有数据,提高效率!</p>
<p>elasticsearch的索引和Lucene的索引对比</p>
<p>在elasticsearch中，索引(库)这个词被频繁使用,这就是术语的使用。在elasticsearch中 ,索引被分为多个分片,每份分片是-个Lucene的索引。<strong>所以一个elasticsearch索引是由多 个Lucene索引组成的</strong>。别问为什么,谁让elasticsearch使用Lucene作为底层呢!如无特指，说起索引都是指elasticsearch的索引。</p>
<p>接下来的一切操作都在kibana中Dev Tools下的Console里完成。基础操作!</p>
<h3 id="ik分词器">ik分词器</h3>
<blockquote>
<p>什么是IK分词器 ?</p>
</blockquote>
<p>分词:即把一-段中文或者别的划分成一个个的关键字,我们在搜索时候会把自己的信息进行分词,会把数据库中或者索引库中的数据进行分词,然后进行一个匹配操作,默认的中文分词是将每个字看成一个词,比如“我爱狂神”会被分为&quot;我&quot;,“爱”,“狂”,“神” ,这显然是不符合要求的,所以我们需要安装中文分词器ik来解决这个问题。</p>
<p>如果要使用中文,建议使用ik分词器!</p>
<p>IK提供了两个分词算法: <strong>ik_ smart和ik_ max_ word</strong> ,其中ik_ smart为最少切分, ik_ max_ _word为最细粒度划分!一会我们测试!</p>
<p>什么是IK分词器：</p>
<ul>
<li>把一句话分词</li>
<li>如果使用中文：推荐IK分词器</li>
<li>两个分词算法：ik_smart（最少切分），ik_max_word（最细粒度划分）</li>
</ul>
<p><strong>【ik_smart】测试：</strong></p>
<pre><code class="language-json">GET _analyze
{
  &quot;analyzer&quot;: &quot;ik_smart&quot;,
  &quot;text&quot;: &quot;我是社会主义接班人&quot;
}

//输出
{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;我&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 1,
      &quot;type&quot; : &quot;CN_CHAR&quot;,
      &quot;position&quot; : 0
    },
    {
      &quot;token&quot; : &quot;是&quot;,
      &quot;start_offset&quot; : 1,
      &quot;end_offset&quot; : 2,
      &quot;type&quot; : &quot;CN_CHAR&quot;,
      &quot;position&quot; : 1
    },
    {
      &quot;token&quot; : &quot;社会主义&quot;,
      &quot;start_offset&quot; : 2,
      &quot;end_offset&quot; : 6,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 2
    },
    {
      &quot;token&quot; : &quot;接班人&quot;,
      &quot;start_offset&quot; : 6,
      &quot;end_offset&quot; : 9,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 3
    }
  ]
}
123456789101112131415161718192021222324252627282930313233343536373839
</code></pre>
<p><strong>【ik_max_word】测试：</strong></p>
<pre><code class="language-json">GET _analyze
{
  &quot;analyzer&quot;: &quot;ik_max_word&quot;,
  &quot;text&quot;: &quot;我是社会主义接班人&quot;
}
//输出
{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;我&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 1,
      &quot;type&quot; : &quot;CN_CHAR&quot;,
      &quot;position&quot; : 0
    },
    {
      &quot;token&quot; : &quot;是&quot;,
      &quot;start_offset&quot; : 1,
      &quot;end_offset&quot; : 2,
      &quot;type&quot; : &quot;CN_CHAR&quot;,
      &quot;position&quot; : 1
    },
    {
      &quot;token&quot; : &quot;社会主义&quot;,
      &quot;start_offset&quot; : 2,
      &quot;end_offset&quot; : 6,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 2
    },
    {
      &quot;token&quot; : &quot;社会&quot;,
      &quot;start_offset&quot; : 2,
      &quot;end_offset&quot; : 4,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 3
    },
    {
      &quot;token&quot; : &quot;主义&quot;,
      &quot;start_offset&quot; : 4,
      &quot;end_offset&quot; : 6,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 4
    },
    {
      &quot;token&quot; : &quot;接班人&quot;,
      &quot;start_offset&quot; : 6,
      &quot;end_offset&quot; : 9,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 5
    },
    {
      &quot;token&quot; : &quot;接班&quot;,
      &quot;start_offset&quot; : 6,
      &quot;end_offset&quot; : 8,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 6
    },
    {
      &quot;token&quot; : &quot;人&quot;,
      &quot;start_offset&quot; : 8,
      &quot;end_offset&quot; : 9,
      &quot;type&quot; : &quot;CN_CHAR&quot;,
      &quot;position&quot; : 7
    }
  ]
}
</code></pre>
<h2 id="命令使用">命令使用</h2>
<h3 id="31-rest风格说明">3.1 Rest风格说明</h3>
<p>一种软件架构风格，而不是标准。更易于实现缓存等机制</p>
<table>
<thead>
<tr>
<th>method</th>
<th>url地址</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>PUT</td>
<td>localhost:9200/索引名称/类型名称/文档id</td>
<td>创建文档(指定文档id)</td>
</tr>
<tr>
<td>POST</td>
<td>localhost:9200/索引名称/类型名称</td>
<td>创建文档（随机文档id）</td>
</tr>
<tr>
<td>POST</td>
<td>localhost:9200/索引名称/类型名称/文档id/_update</td>
<td>修改文档</td>
</tr>
<tr>
<td>DELETE</td>
<td>localhost:9200/索引名称/类型名称/文档id</td>
<td>删除文档</td>
</tr>
<tr>
<td>GET</td>
<td>localhost:9200/索引名称/类型名称/文档id</td>
<td>通过文档id查询文档</td>
</tr>
<tr>
<td>POST</td>
<td>localhost:9200/索引名称/类型名称/_search</td>
<td>查询所有的数据</td>
</tr>
</tbody>
</table>
<blockquote>
<p>基础测试</p>
</blockquote>
<p>1.创建一个索引</p>
<p>PUT /索引名/类型名(高版本都不写了，都是_doc)/文档id</p>
<p>{请求体}</p>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20200828224224886.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>完成了自动添加了索引！数据也成功的添加了。</p>
<figure data-type="image" tabindex="3"><img src="https://img-blog.csdnimg.cn/20200828224246679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-LdUc6t8b-1598625546984)(C:sers3984ppDataoamingyporaypora-user-images598532887497.png)]" loading="lazy"></figure>
<p>那么name这个字段用不用指定类型呢</p>
<figure data-type="image" tabindex="4"><img src="https://img-blog.csdnimg.cn/20200828224311944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>指定字段的类型properties 就比如sql创表</p>
<p>获得这个规则！可以通过GET请求获得具体的信息</p>
<figure data-type="image" tabindex="5"><img src="https://img-blog.csdnimg.cn/2020082822452110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>如果自己不设置文档字段类型，那么es会自动给默认类型</p>
<figure data-type="image" tabindex="6"><img src="https://img-blog.csdnimg.cn/20200828224539919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-CNGgrrjI-1598625546996)(C:sers3984ppDataoamingyporaypora-user-images598533818617.png)]" loading="lazy"></figure>
<h3 id="32-cat命令">3.2 cat命令</h3>
<p>获取健康值</p>
<figure data-type="image" tabindex="7"><img src="https://img-blog.csdnimg.cn/20200828224607691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>获取所有的信息</p>
<pre><code class="language-shell">GET _cat/indices?v
1
</code></pre>
<figure data-type="image" tabindex="8"><img src="https://img-blog.csdnimg.cn/20200828224623550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-1ZKzwXyB-1598625547001)(C:sers3984ppDataoamingyporaypora-user-images598534090085.png)]" loading="lazy"></figure>
<p>还有很多 可以自动展示 都试试</p>
<blockquote>
<h4 id="修改索引">修改索引</h4>
</blockquote>
<p>1.修改我们可以还是用原来的PUT的命令，根据id来修改</p>
<figure data-type="image" tabindex="9"><img src="https://img-blog.csdnimg.cn/2020082822464153.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-J5lCYCsQ-1598625547003)(C:sers3984esktop件d学习文件598534298931.png)]" loading="lazy"></figure>
<p>但是如果没有填写的字段 会重置为空了 ，相当于java接口传对象修改，如果只是传id的某些字段，那其他没传的值都为空了。</p>
<p>2.还有一种update方法 这种不设置某些值 数据不会丢失</p>
<pre><code class="language-html">POST /test3/_doc/1/_update
{
  &quot;doc&quot;:{
    &quot;name&quot;:&quot;212121&quot;
  }
}

//下面两种都是会将不修改的值清空的

POST /test3/_doc/1
{
    &quot;name&quot;:&quot;212121&quot;
}

POST /test3/_doc/1
{
  &quot;doc&quot;:{
    &quot;name&quot;:&quot;212121&quot;
  }
}
</code></pre>
<figure data-type="image" tabindex="10"><img src="https://img-blog.csdnimg.cn/20200828224703275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<figure data-type="image" tabindex="11"><img src="https://img-blog.csdnimg.cn/20200828224718584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>带doc修改 查询也是带doc的（document）</p>
<blockquote>
<h4 id="删除索引">删除索引</h4>
</blockquote>
<p>关于删除索引或者文档的操作</p>
<figure data-type="image" tabindex="12"><img src="https://img-blog.csdnimg.cn/20200828224733641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>通过DELETE命令实现删除，根据你的请求来判断是删除索引还是删除文档记录</p>
<p>使用RESTFUL的风格是我们ES推荐大家使用的！</p>
<h3 id="33-关于文档的基本操作">3.3 关于文档的基本操作</h3>
<h4 id="查询">查询</h4>
<p>最简单的搜索是GET</p>
<p>搜索功能search</p>
<figure data-type="image" tabindex="13"><img src="https://img-blog.csdnimg.cn/20200828224751861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>这边name是text 所以做了分词的查询 如果是keyword就不会分词搜索了</p>
<blockquote>
<h3 id="复杂操作搜索select排序分页高亮模糊查询精准查询">复杂操作搜索select（排序，分页，高亮，模糊查询，精准查询）</h3>
</blockquote>
<figure data-type="image" tabindex="14"><img src="https://img-blog.csdnimg.cn/20200828224818363.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<pre><code class="language-json">//测试只能一个字段查询
GET lisen/user/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;name&quot;: &quot;李森&quot;
    }
  }
}
</code></pre>
<p>结果过滤，就是只展示列表中某些字段</p>
<figure data-type="image" tabindex="15"><img src="https://img-blog.csdnimg.cn/20200828224837652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>包含</p>
<figure data-type="image" tabindex="16"><img src="https://img-blog.csdnimg.cn/20200828224854820.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>不包含</p>
<figure data-type="image" tabindex="17"><img src="https://img-blog.csdnimg.cn/20200828224913742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>排序</p>
<figure data-type="image" tabindex="18"><img src="https://img-blog.csdnimg.cn/20200828224929501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>分页</p>
<figure data-type="image" tabindex="19"><img src="https://img-blog.csdnimg.cn/20200828224950826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>代码</p>
<pre><code class="language-json">GET lisen/user/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;name&quot;: &quot;李森&quot;
    }
  },
  &quot;sort&quot;:{
    &quot;age&quot;:{
      &quot;order&quot;:&quot;asc&quot;
    }
  },
  &quot;from&quot;: 0,
  &quot;size&quot;: 1
}
</code></pre>
<h4 id="多条件查询">多条件查询</h4>
<blockquote>
<p>布尔值查询</p>
</blockquote>
<p>must（and），所有的条件都要符合</p>
<figure data-type="image" tabindex="20"><img src="https://img-blog.csdnimg.cn/20200828225018246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>should（or）或者的 跟数据库一样</p>
<figure data-type="image" tabindex="21"><img src="https://img-blog.csdnimg.cn/20200828225032999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>must_not（not）<br>
<img src="https://img-blog.csdnimg.cn/20200828225058189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></p>
<p>条件区间<br>
<img src="https://img-blog.csdnimg.cn/2020082822512379.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></p>
<ul>
<li>gt大于</li>
<li>gte大于等于</li>
<li>lte小于</li>
<li>lte小于等于</li>
</ul>
<blockquote>
<p>匹配多个条件（数组）</p>
</blockquote>
<figure data-type="image" tabindex="22"><img src="https://img-blog.csdnimg.cn/20200828225140204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>match没用倒排索引 这边改正一下</p>
<blockquote>
<p>精确查找</p>
</blockquote>
<p>term查询是直接通过倒排索引指定的词条进程精确查找的</p>
<h4 id="关于分词">关于分词</h4>
<ul>
<li>term，直接查询精确的</li>
<li>match，会使用分词器解析！（先分析文档，然后通过分析的文档进行查询）</li>
</ul>
<figure data-type="image" tabindex="23"><img src="https://img-blog.csdnimg.cn/20200828225158338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>默认的是被分词了</p>
<figure data-type="image" tabindex="24"><img src="https://img-blog.csdnimg.cn/20200828225215133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>keyword没有被分词</p>
<p>精确查询多个值</p>
<figure data-type="image" tabindex="25"><img src="https://img-blog.csdnimg.cn/20200828225246611.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>高亮</p>
</blockquote>
<figure data-type="image" tabindex="26"><img src="https://img-blog.csdnimg.cn/20200828225305992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>还能自定义高亮的样式<br>
<img src="https://img-blog.csdnimg.cn/20200828225327239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2VuMDEwNzAxMDc=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></p>
<h2 id="springboot集成">springboot集成</h2>
<h3 id="41-引入依赖包">4.1 引入依赖包</h3>
<p>创建一个springboot的项目 同时勾选上<code>springboot-web</code>的包以及<code>Nosql的elasticsearch</code>的包</p>
<p>如果没有就手动引入</p>
<pre><code class="language-xml">&lt;!--es客户端--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;
    &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;
    &lt;version&gt;7.6.2&lt;/version&gt;
&lt;/dependency&gt;

&lt;!--springboot的elasticsearch服务--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>
<p>注意下spring-boot的parent包内的依赖的es的版本是不是你对应的版本</p>
<p>不是的话就在pom文件下写个properties的版本</p>
<pre><code class="language-xml">&lt;!--这边配置下自己对应的版本--&gt;
&lt;properties&gt;
    &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;elasticsearch.version&gt;7.6.2&lt;/elasticsearch.version&gt;
&lt;/properties&gt;
</code></pre>
<h3 id="42-注入resthighlevelclient-客户端">4.2 注入RestHighLevelClient 客户端</h3>
<pre><code class="language-java">@Configuration
public class ElasticSearchClientConfig {
    @Bean
    public RestHighLevelClient restHighLevelClient(){
        RestHighLevelClient client = new RestHighLevelClient(
                RestClient.builder(new HttpHost(&quot;127.0.0.1&quot;,9200,&quot;http&quot;))
        );
        return client;
    }
}
</code></pre>
<h3 id="43-索引的增-删-是否存在">4.3 索引的增、删、是否存在</h3>
<pre><code class="language-java">//测试索引的创建
@Test
void testCreateIndex() throws IOException {
    //1.创建索引的请求
    CreateIndexRequest request = new CreateIndexRequest(&quot;lisen_index&quot;);
    //2客户端执行请求，请求后获得响应
    CreateIndexResponse response = client.indices().create(request, RequestOptions.DEFAULT);
    System.out.println(response);
}

//测试索引是否存在
@Test
void testExistIndex() throws IOException {
    //1.创建索引的请求
    GetIndexRequest request = new GetIndexRequest(&quot;lisen_index&quot;);
    //2客户端执行请求，请求后获得响应
    boolean exist =  client.indices().exists(request, RequestOptions.DEFAULT);
    System.out.println(&quot;测试索引是否存在-----&quot;+exist);
}

//删除索引
@Test
void testDeleteIndex() throws IOException {
    DeleteIndexRequest request = new DeleteIndexRequest(&quot;lisen_index&quot;);
    AcknowledgedResponse delete = client.indices().delete(request,RequestOptions.DEFAULT);
    System.out.println(&quot;删除索引--------&quot;+delete.isAcknowledged());
}
</code></pre>
<h3 id="44-文档的操作">4.4 文档的操作</h3>
<pre><code class="language-java">//测试添加文档
    @Test
    void testAddDocument() throws IOException {
        User user = new User(&quot;lisen&quot;,27);
        IndexRequest request = new IndexRequest(&quot;lisen_index&quot;);
        request.id(&quot;1&quot;);
        //设置超时时间
        request.timeout(&quot;1s&quot;);
        //将数据放到json字符串
        request.source(JSON.toJSONString(user), XContentType.JSON);
        //发送请求
        IndexResponse response = client.index(request,RequestOptions.DEFAULT);
        System.out.println(&quot;添加文档-------&quot;+response.toString());
        System.out.println(&quot;添加文档-------&quot;+response.status());
//        结果
//        添加文档-------IndexResponse[index=lisen_index,type=_doc,id=1,version=1,result=created,seqNo=0,primaryTerm=1,shards={&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0}]
//        添加文档-------CREATED
    }

    //测试文档是否存在
    @Test
    void testExistDocument() throws IOException {
        //测试文档的 没有index
        GetRequest request= new GetRequest(&quot;lisen_index&quot;,&quot;1&quot;);
        //没有indices()了
        boolean exist = client.exists(request, RequestOptions.DEFAULT);
        System.out.println(&quot;测试文档是否存在-----&quot;+exist);
    }

    //测试获取文档
    @Test
    void testGetDocument() throws IOException {
        GetRequest request= new GetRequest(&quot;lisen_index&quot;,&quot;1&quot;);
        GetResponse response = client.get(request, RequestOptions.DEFAULT);
        System.out.println(&quot;测试获取文档-----&quot;+response.getSourceAsString());
        System.out.println(&quot;测试获取文档-----&quot;+response);

//        结果
//        测试获取文档-----{&quot;age&quot;:27,&quot;name&quot;:&quot;lisen&quot;}
//        测试获取文档-----{&quot;_index&quot;:&quot;lisen_index&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:1,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;found&quot;:true,&quot;_source&quot;:{&quot;age&quot;:27,&quot;name&quot;:&quot;lisen&quot;}}

    }

    //测试修改文档
    @Test
    void testUpdateDocument() throws IOException {
        User user = new User(&quot;李逍遥&quot;, 55);
        //修改是id为1的
        UpdateRequest request= new UpdateRequest(&quot;lisen_index&quot;,&quot;1&quot;);
        request.timeout(&quot;1s&quot;);
        request.doc(JSON.toJSONString(user),XContentType.JSON);

        UpdateResponse response = client.update(request, RequestOptions.DEFAULT);
        System.out.println(&quot;测试修改文档-----&quot;+response);
        System.out.println(&quot;测试修改文档-----&quot;+response.status());

//        结果
//        测试修改文档-----UpdateResponse[index=lisen_index,type=_doc,id=1,version=2,seqNo=1,primaryTerm=1,result=updated,shards=ShardInfo{total=2, successful=1, failures=[]}]
//        测试修改文档-----OK

//        被删除的
//        测试获取文档-----null
//        测试获取文档-----{&quot;_index&quot;:&quot;lisen_index&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;found&quot;:false}
    }


    //测试删除文档
    @Test
    void testDeleteDocument() throws IOException {
        DeleteRequest request= new DeleteRequest(&quot;lisen_index&quot;,&quot;1&quot;);
        request.timeout(&quot;1s&quot;);
        DeleteResponse response = client.delete(request, RequestOptions.DEFAULT);
        System.out.println(&quot;测试删除文档------&quot;+response.status());
    }

    //测试批量添加文档
    @Test
    void testBulkAddDocument() throws IOException {
        ArrayList&lt;User&gt; userlist=new ArrayList&lt;User&gt;();
        userlist.add(new User(&quot;cyx1&quot;,5));
        userlist.add(new User(&quot;cyx2&quot;,6));
        userlist.add(new User(&quot;cyx3&quot;,40));
        userlist.add(new User(&quot;cyx4&quot;,25));
        userlist.add(new User(&quot;cyx5&quot;,15));
        userlist.add(new User(&quot;cyx6&quot;,35));

        //批量操作的Request
        BulkRequest request = new BulkRequest();
        request.timeout(&quot;1s&quot;);

        //批量处理请求
        for (int i = 0; i &lt; userlist.size(); i++) {
            request.add(
                    new IndexRequest(&quot;lisen_index&quot;)
                            .id(&quot;&quot;+(i+1))
                            .source(JSON.toJSONString(userlist.get(i)),XContentType.JSON)
            );
        }
        BulkResponse response = client.bulk(request, RequestOptions.DEFAULT);
        //response.hasFailures()是否是失败的
        System.out.println(&quot;测试批量添加文档-----&quot;+response.hasFailures());

//        结果:false为成功 true为失败
//        测试批量添加文档-----false
    }


    //测试查询文档
    @Test
    void testSearchDocument() throws IOException {
        SearchRequest request = new SearchRequest(&quot;lisen_index&quot;);
        //构建搜索条件
        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
        //设置了高亮
        sourceBuilder.highlighter();
        //term name为cyx1的
        TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(&quot;name&quot;, &quot;cyx1&quot;);
        sourceBuilder.query(termQueryBuilder);
        sourceBuilder.timeout(new TimeValue(60, TimeUnit.SECONDS));

        request.source(sourceBuilder);
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);

        System.out.println(&quot;测试查询文档-----&quot;+JSON.toJSONString(response.getHits()));
        System.out.println(&quot;=====================&quot;);
        for (SearchHit documentFields : response.getHits().getHits()) {
            System.out.println(&quot;测试查询文档--遍历参数--&quot;+documentFields.getSourceAsMap());
        }

//        测试查询文档-----{&quot;fragment&quot;:true,&quot;hits&quot;:[{&quot;fields&quot;:{},&quot;fragment&quot;:false,&quot;highlightFields&quot;:{},&quot;id&quot;:&quot;1&quot;,&quot;matchedQueries&quot;:[],&quot;primaryTerm&quot;:0,&quot;rawSortValues&quot;:[],&quot;score&quot;:1.8413742,&quot;seqNo&quot;:-2,&quot;sortValues&quot;:[],&quot;sourceAsMap&quot;:{&quot;name&quot;:&quot;cyx1&quot;,&quot;age&quot;:5},&quot;sourceAsString&quot;:&quot;{\&quot;age\&quot;:5,\&quot;name\&quot;:\&quot;cyx1\&quot;}&quot;,&quot;sourceRef&quot;:{&quot;fragment&quot;:true},&quot;type&quot;:&quot;_doc&quot;,&quot;version&quot;:-1}],&quot;maxScore&quot;:1.8413742,&quot;totalHits&quot;:{&quot;relation&quot;:&quot;EQUAL_TO&quot;,&quot;value&quot;:1}}
//        =====================
//        测试查询文档--遍历参数--{name=cyx1, age=5}
    }
</code></pre>
<h2 id="原理">原理</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java NIO]]></title>
        <id>https://memorykki.github.io/nio/</id>
        <link href="https://memorykki.github.io/nio/">
        </link>
        <updated>2021-09-15T02:49:13.000Z</updated>
        <summary type="html"><![CDATA[<p>BIO、NIO</p>
]]></summary>
        <content type="html"><![CDATA[<p>BIO、NIO</p>
<!-- more -->
<ul>
<li><a href="https://www.jianshu.com/p/f1df8c28f5ab">聊聊IO多路复用之select、poll、epoll详解</a></li>
<li><a href="https://www.jianshu.com/p/9ebd4fd8c892">聊聊 IO 多路复用</a></li>
<li><a href="https://www.cnblogs.com/IT-CPC/p/10898871.html">同步异步</a></li>
</ul>
<h2 id="bio">BIO</h2>
<p>同步阻塞IO</p>
<figure data-type="image" tabindex="1"><img src="https://memorykki.github.io/post-images/nio/image-20211019211654912.png" alt="" loading="lazy"></figure>
<p>每个连接对应一个线程，改用多线程/线程池执行还是容易达到最大并发量的瓶颈，并且如果请求并没有发送数据，还是占用资源，优点是简单。</p>
<figure data-type="image" tabindex="2"><img src="https://memorykki.github.io/post-images/nio/image-20211019211906044.png" alt="" loading="lazy"></figure>
<h2 id="nio">NIO</h2>
<p>non-block io 同步非阻塞IO</p>
<figure data-type="image" tabindex="3"><img src="https://memorykki.github.io/post-images/nio/image-20211020091910934.png" alt="" loading="lazy"></figure>
<p><strong>三大核心：</strong></p>
<figure data-type="image" tabindex="4"><img src="https://memorykki.github.io/post-images/nio/image-20211020093122273.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="5"><img src="https://memorykki.github.io/post-images/nio/image-20211020093239943.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="6"><img src="https://memorykki.github.io/post-images/nio/image-20211020093356747.png" alt="" loading="lazy"></figure>
<h2 id="aio">AIO</h2>
<figure data-type="image" tabindex="7"><img src="https://memorykki.github.io/post-images/nio/image-20211020093743091.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="8"><img src="https://memorykki.github.io/post-images/nio/image-20211019212933701.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="9"><img src="https://memorykki.github.io/post-images/nio/image-20211019213030854.png" alt="" loading="lazy"></figure>
<p>相对于BIO，NIO在接受请求连接、处理空连接（无数据的请求）设置为非阻塞，这就是高并发的前提。线程模式为一个线程处理多个请求。</p>
<p>以上代码的问题：</p>
<p>1、死循环使得没有请求时CPU占满；</p>
<p>2、十万个连接放到 List ，但是其中仅少量的连接有数据，大量无效循环；</p>
<p>3、如果有上万个事件，每个事件执行时间很长，就会影响后面连接的建立；</p>
<p>解决：</p>
<p>1、先阻塞线程，等到有accept时放行</p>
<p>2、增加有数据的List，循环遍历</p>
<p>3、交给线程池来处理事件，主线程只接收请求连接。（redis没有用多线程，还是单线程，但它会限制每个事件的执行时间，不能太长，虽然它io收发可能是多线程的，但事件处理还是单线程）</p>
<p><strong>selector解决了这两个问题！！</strong></p>
<p>引入selector多路复用器</p>
<figure data-type="image" tabindex="10"><img src="https://memorykki.github.io/post-images/nio/image-20211019215357239.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="11"><img src="https://memorykki.github.io/post-images/nio/image-20211019220954526.png" alt="" loading="lazy"></figure>
<p><strong>过程：</strong></p>
<ul>
<li>
<p>把serversocket 注册到 selector ，并监听 accpet 事件；</p>
</li>
<li>
<p>循环中，无请求时阻塞，让出CPU，有请求时selector.select()放行（<strong>解决1</strong>）</p>
</li>
<li>
<p>循环selector.selectKeys（无数据的连接不会被获取到，也就不会被遍历，<strong>解决2</strong>）</p>
<ul>
<li>
<p>判断如果是accept事件，注册这个serversocket的read事件到selector；</p>
</li>
<li>
<p>如果是read事件，就执行自定义方法；</p>
</li>
</ul>
</li>
<li>
<p>接着循环阻塞。</p>
</li>
</ul>
<figure data-type="image" tabindex="12"><img src="https://memorykki.github.io/post-images/nio/image-20211019220114052.png" alt="" loading="lazy"></figure>
<p><strong>底层：</strong></p>
<p>open()、select()、register()</p>
<p>Selector.open()底层实现基于不同平台，linux下，实际调用返回的是EpollSelector，其中放着一个集合EpollArrayWrapper，保存着channel，最终是native本地方法实现的，操作系统的内核函数epoll_create（创建epoll实例）、epoll_ctl、epoll_wait</p>
<p>所以selector底层就是一个epoll结构体，包含channels集合，监听其中的事件，有事件发生时就放到就绪列表rdlist中。</p>
<p>redis底层也是通过epoll函数实现的。</p>
<p>IO多路复用底层主要用的linux内核函数select、poll、epoll：</p>
<figure data-type="image" tabindex="13"><img src="https://memorykki.github.io/post-images/nio/image-20211019225642893.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="14"><img src="https://memorykki.github.io/post-images/nio/image-20211020075415416.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="15"><img src="https://memorykki.github.io/post-images/nio/image-20211020075503940.png" alt="" loading="lazy"></figure>
<p>netty就是一个处理数据的，底层就是对NIO api的封装，达到百万并发级别，开发者不需要写建立连接等重复代码，而是交给netty框架执行，只需要自定义一些接口实现就行。</p>
<h2 id="netty线程模型">netty线程模型</h2>
<figure data-type="image" tabindex="16"><img src="https://memorykki.github.io/post-images/nio/image-20211020081248835.png" alt="" loading="lazy"></figure>
<p>利用线程池处理事件，但是线程池也是有限的</p>
<p>AIO</p>
<p>NIO 2.0版本   异步非阻塞</p>
<p>一个有效请求对应一个线程，客户端的IO请求都是由OS完成后再通知服务器应开启线程处理。适用于连接数多且时间较长的应用</p>
<h2 id="nio-bio-比较">NIO BIO 比较</h2>
<figure data-type="image" tabindex="17"><img src="https://memorykki.github.io/post-images/nio/image-20211020092729738.png" alt="" loading="lazy"></figure>
<h2 id="适用场景">适用场景</h2>
<figure data-type="image" tabindex="18"><img src="https://memorykki.github.io/post-images/nio/image-20211020091100460.png" alt="" loading="lazy"></figure>
<h1 id=""></h1>
<h2 id="1-nio与io的区别">1. NIO与IO的区别</h2>
<p>NIO：New IO</p>
<blockquote>
<p>1.4就有NIO了，1.7对NIO进行了改进。1.7对NIO的改动，称之为NIO2.NIO在现在企业中使用的比较多。</p>
</blockquote>
<p>NIO的几个概念：</p>
<ul>
<li>缓冲区</li>
<li>通道</li>
<li>选择器</li>
</ul>
<table>
<thead>
<tr>
<th>IO</th>
<th>NIO</th>
</tr>
</thead>
<tbody>
<tr>
<td>面向流</td>
<td>面向缓冲区</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>原来的IO是面向流，是单向传输。</p>
</li>
<li>
<p>NIO是双向的传输。</p>
</li>
</ul>
<hr>
<h2 id="2-缓冲区">2. 缓冲区</h2>
<p>缓冲区（Bufffer）：在JavaNIO中负责数据的存储。缓冲区就是数组。用于存储不同类型的数据。</p>
<p>根据数据的不同，提供了相应类型的缓冲区。（Boolean类型除外，其他的7个基本类型都有）</p>
<p>有：</p>
<p>ByteBuffer ; CharBuffer ; ShortBuffer ; IntBuffer ; LongBuffer ; FloatBuffer ; DoubleBuffer</p>
<p>上述缓冲区的管理方式都几乎一致。通过allocate();获取缓冲区</p>
<p>最常用的就是<strong>ByteBuffer</strong></p>
<h3 id="21-缓冲区的基本属性">2.1. 缓冲区的基本属性</h3>
<ol>
<li>
<p>分配一个指定大小的缓冲区：</p>
<pre><code class="language-java">ByteBuffer byteBuffer = ByteBuffer.allocate(10);//获取一个10字节大小的缓冲区
</code></pre>
</li>
<li>
<p>从缓冲区存取数据的两个核心方法：<br>
<code>get();</code>和<code>put()；</code></p>
<pre><code class="language-java">byteBuffer.put(&quot;abcde&quot;.getBytes());//存5个Byte的数据
byterBuffer.get();
</code></pre>
</li>
<li>
<p>缓冲区的几个核心属性：</p>
<ol>
<li>capacity：容量，表示缓冲区中最大的容量，一旦生命，不得改变！</li>
<li>limit：界限，第一个不应该读取或写入的数据的索引，即位于 limit 后的数据 不可读写。缓冲区的限制不能为负，并且不能大于其容量。</li>
<li>position：位置，表示缓冲区中正在操作数据的位置。（即将要操作的位置，position下的位置是空的）<br>
!-<a href="https://memorykki.github.io/post-images/image-20210130110906569.png"></a></li>
<li>（position &lt;= limit &lt;= capacity)</li>
</ol>
</li>
</ol>
<h3 id="22-flip方法切换读数据模式">2.2. flip方法（切换读数据模式）</h3>
<blockquote>
<p>flip方法：可以切换到读数据模式。</p>
</blockquote>
<p>切换到读取模式的时候，即切换到读模式，则position变为0，limit变为数据最大的位置。</p>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210130111054709.png"></a></p>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210130111300328.png"></a></p>
<hr>
<h3 id="23-读取buffer数据">2.3. 读取Buffer数据</h3>
<pre><code class="language-java">byteBuffer.flip();//切换到读模式
byte[] data = new byte[byteBuffer.limit()];
byteBuffer.get(data);//获取数据
</code></pre>
<p>get完成之后，各个属性的位置变化情况为？</p>
<ul>
<li>position：变为读之前的limit</li>
<li>limit：不变</li>
<li>capacity：不变</li>
</ul>
<hr>
<h3 id="24-buffer常用方法">2.4. Buffer常用方法</h3>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210130111506335.png"></a></p>
<p>rewind：倒带，倒片。可重复读取数据，将position改为0。可以再次读取。</p>
<p>clear：清空，将buffer中的数据清空。将limit变为capacity，但是缓冲区的数据仍然在，数据处在<code>被遗忘</code>状态，只不过是将limit置为capacity，可以再次重新写入。</p>
<p>mark：标记。记录此时的position。</p>
<p>reset：把position恢复到上次mark的位置。</p>
<p>remaining：获取还可以操作的缓冲区的数量（即 limit - position）</p>
<p>hasRemaining：返回一个boolean值，是否还有剩余的位置可以读取</p>
<p>（即 <code>return (limit - position) &gt; 0 ? true : false;</code>）</p>
<p><strong><mark>总结，缓冲区的四个核心属性：</mark></strong></p>
<p>0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity</p>
<hr>
<h2 id="3-直接缓冲区和非直接缓冲区">3. 直接缓冲区和非直接缓冲区</h2>
<ul>
<li>非直接缓存区：通过allocate() 方法分配缓冲区，将缓冲区建立在JVM的内存中。</li>
<li>直接缓冲区：通过allocateDirect() 方法分配缓冲区，将缓冲区直接建立在物理内存中。<br>
可以提高效率</li>
</ul>
<p><strong><mark>直接缓冲区，只有ByteBuffer支持，其他Buffer不支持！</mark></strong></p>
<p><strong>非直接缓存区：</strong></p>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210130163113932.png"></a></p>
<hr>
<p><strong>直接缓存区：</strong></p>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210130163204143.png"></a></p>
<h3 id="31-创建两种缓冲区">3.1. 创建两种缓冲区</h3>
<pre><code class="language-java">ByteBuffer.allocate(1024);//创建非直接缓冲区
ByteBuffer.allocateDirect(1024);//创建直接缓冲区
</code></pre>
<p>如何判断缓冲区是否为直接缓冲区？</p>
<pre><code class="language-java">byteBuffer.isDirect();
//返回一个boolean，true是直接缓冲区，false是非直接缓冲区
</code></pre>
<h2 id="4-通道">4. 通道</h2>
<blockquote>
<p>通道（Channel）：由 java.nio.channels 包定义 的。Channel 表示 IO 源与目标打开的连接。 Channel 类似于传统的“流”。只不过 Channel 本身不能直接访问数据，Channel 只能与 Buffer 进行交互。</p>
</blockquote>
<p>Java 为 Channel 接口提供的最主要实现类如下</p>
<ul>
<li>FileChannel：用于读取、写入、映射和操作文件的通道。</li>
<li>DatagramChannel：通过 UDP 读写网络中的数据通道。</li>
<li>SocketChannel：通过 TCP 读写网络中的数据。</li>
<li>•ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来 的连接都会创建一个 SocketChannel</li>
</ul>
<h3 id="41-获取通道的三种方式">4.1 获取通道的三种方式：</h3>
<ol>
<li>
<p>Java针对支持通道的类提供了getChannel()方法来获取通道</p>
<p>支持通道的类如下：</p>
<ul>
<li>FileInputStream</li>
<li>FileOutputStream</li>
<li>RandomAccessFile</li>
<li>DatagramSocket</li>
<li>Socket</li>
<li>ServerSocket</li>
</ul>
</li>
<li>
<p>获取通道的其他方式是使用 Files 工具类的静态方法 newByteChannel() 获取字节通道。</p>
</li>
<li>
<p>或者通过通道的静态方法 open() 打开并返回指定通道。</p>
</li>
</ol>
<p>（2和3都是JDK1.7以后的NIO2才支持这种方法）</p>
<p>第二种获取DirectBuffer的方式：使用FileChannel的map()方法将文件区域直接映射到内存中来创建。该方法返回 MappedByteBuffer 。</p>
<pre><code class="language-java">MappedByteBuffer mappedByteBuffer = inChannel.map(MapMode.READ_ONLY,0,inChannel.size());
//只读模式 从0 到size
</code></pre>
<hr>
<h3 id="42-通过getchannel获取通道">4.2. 通过getChannel()获取通道</h3>
<pre><code class="language-java">//获取文件流
FileInputStream fileInputStream = new FileInputStream(&quot;1.jpg&quot;);
FileOutputStream fileOutputStream = new FileOutputStream(&quot;2.jpg&quot;);

//获取文件通道
FileChannel fileInputChannel = fileInputStream.openChannel();
FileChannel fileOutputChannel = fileOutputStream.openChannel();

//获取一个Buffer
ByteBuffer byteBuffer = ByteBuffer.allocate(1024);

while((fileInputChannel.read(byteBuffer)) != -1){
	byteBuffer.flip();//切换到读模式
    fileOutputChannel.wirte(byteBuffer);
    byteBuffer.clear();//清空buf
}

//关闭通道
fileOutChannel.close();
fileInputChannel.close();
fileInputStream.close();
fileOutputStream.close();
</code></pre>
<hr>
<h3 id="43-使用channel的open方法类获取通道">4.3. 使用Channel的open()方法类获取通道</h3>
<blockquote>
<p>使用直接缓冲区完成文件的复制（内存映射文件）</p>
</blockquote>
<p>MappedByteBuffer是内存映射文件，道理和<code>ByteBuffer.allocateDirect();</code>一摸一样。</p>
<p>MappedByteBuffer是在物理内存中。</p>
<p>内存映射文件，只有ByteBuffer支持。</p>
<p>映射的字节缓冲区及其表示的文件映射在缓冲区本身被垃圾收集之前保持有效。</p>
<p>映射字节缓冲区的内容可以随时改变，例如，如果该程序或其他程序改变了映射文件的相应区域的内容。  这些变化是否发生以及何时发生，取决于操作系统，因此未指定。</p>
<p>映射字节缓冲区的行为与普通直接字节缓冲区没有区别。</p>
<pre><code class="language-java">FileChannel inChannel = FileChannel.open(Paths.get(&quot;d:/1.mkv&quot;), StandardOpenOption.READ);
FileChannel outChannel = FileChannel.open(Paths.get(&quot;d:/2.mkv&quot;), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE);

//StandardOpenOption是一个枚举类，其中有多个选项，用于设置该通道的作用
//如读取：StandardOpenOption.READ
//如读和写，则可利用Java的不定参数：
//其中：StandardOpenOption.CREATE_NEW的类型意为
// |— 如果存在就报错，如果不存在就新建
//FileChannel.open(&quot;d:/1.mkv&quot;,StandardOpenOption.READ,StandardOpenOption.WRITE);

//可以利用channel.map()获取内存映射文件Buffer
//MappedByteBuffer是内存映射文件，道理和ByteBuffer.allocateDirect();一摸一样
//MappedByteBuffer是在物理内存中。
//内存映射文件，只有ByteBuffer支持！
MappedByteBuffer inMappedBuf = inChannel.map(MapMode.READ_ONLY, 0, inChannel.size());
MappedByteBuffer outMappedBuf = outChannel.map(MapMode.READ_WRITE, 0, inChannel.size());
//MapMode 也是一个选项枚举类
//直接对缓冲区进行数据的读写操作
byte[] dst = new byte[inMappedBuf.limit()];
inMappedBuf.get(dst);
outMappedBuf.put(dst);

//关闭通道
inChannel.close();
outChannel.close();
</code></pre>
<hr>
<h3 id="44-transferfrom和transferto方法">4.4. transferFrom和transferTo方法</h3>
<ul>
<li>transferFrom -&gt;<br>
transferFrom(ReadableByteChannel src,  long position, long count)<br>
从给定的可读字节通道将字节传输到此通道的文件中。</li>
<li>transferTo -&gt;<br>
transferTo(long position, long count, WritableByteChannel target)<br>
将字节从此通道的文件传输到给定的可写字节通道。</li>
</ul>
<pre><code class="language-java">FileChannel inChannel = FileChannel.open(Paths.get(&quot;d:/1.mkv&quot;), StandardOpenOption.READ);
FileChannel outChannel = FileChannel.open(Paths.get(&quot;d:/2.mkv&quot;), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE);

//transferTo与transferFrom是一个效果
//inChannel.transferTo(0, inChannel.size(), outChannel);
//从0开始读取，读取到inChannel.size()位置，输出给outChannel

outChannel.transferFrom(inChannel, 0, inChannel.size());
//从inChannel获取，从第0个开始获取，获取到inChannel.size()大小的位置

//channel.size();  -&gt; 返回此通道文件的当前大小，以字节为单位。
//FileChannel实例的size()方法将返回该实例所关联文件的大小。如:
//long fileSize = channel.size();


inChannel.close();
outChannel.close();
</code></pre>
<hr>
<h2 id="5-分散与聚集">5. 分散与聚集</h2>
<ul>
<li>分散（Scatter）
<ul>
<li>分散读取（Scatter Reads）：将**<mark>通道</mark>中的**数据分散到多个缓冲区中<br>
!-<a href="https://memorykki.github.io/post-images/image-20210131170203627.png"></a></li>
</ul>
</li>
<li>聚集（Gather）
<ul>
<li>聚集写入（Gather Writes）：将多个缓冲区中的数据**聚集到一个<mark>通道</mark>**中<br>
!-<a href="https://memorykki.github.io/post-images/image-20210131170211191.png"></a></li>
</ul>
</li>
</ul>
<p><strong><mark>分散读取/聚集写入，都是按照顺序进行操作的</mark></strong></p>
<h3 id="51-分散读取">5.1. 分散读取</h3>
<pre><code class="language-java">RandomAccessFile raf = new RandomAccessFile(&quot;1.txt&quot;,&quot;rw&quot;);

//1.获取通道
FileChannel fileChannel = raf.getChannel();

//创建一个缓冲区
ByteBuffer byteBuffer1 = ByteBuffer.allocate(100);
ByteBuffer byteBuffer2 = ByteBuffer,allocate(1024);

//通过分散读取进行读取
ByteBuffer[] bufs = {byteBuffer1,byteBuffer2};
fileChannel.read(bufs);

for(ByteBuffer bb : bufs){
    bb.flip();//切换到读模式
}

//输出前100个字节
System.out.println(new String(bufs[0].array(),0,bufs[0].limit()));

//输出后1024个字节
System.out.println(new String(bufs[1].array(),0,bufs[1].limit()));

/* 通过结果我们可以看到，分散读取的确是按照顺序写入的 */
</code></pre>
<h3 id="52-聚集写入">5.2. 聚集写入</h3>
<pre><code class="language-java">RandomAccessFile raf = new RandomAccessFile(&quot;1.txt&quot;,&quot;rw&quot;);
RandomAccessFile raf2 = new RandomAccessFile(&quot;2.txt&quot;,&quot;rw&quot;);

//1.获取通道
FileChannel fileChannel = raf.getChannel();
FileChannel fileChannel2 = raf2.getChannel();

//创建一个缓冲区
ByteBuffer byteBuffer1 = ByteBuffer.allocate(100);
ByteBuffer byteBuffer2 = ByteBuffer,allocate(1024);
ByteBuffer[] bufs = {byteBuffer1,byteBuffer2};

fileChannel.read(bufs);//从文件中读取数据到bufs

for(ByteBuffer bb : bufs){
    bb.flip();//切换到读模式
}

fileChannel2.write(bufs);//写出文件

fileChannel.close();
fileChannel2.close();
raf.close();
raf2.close();

//最终还是会按照顺序进行写入
</code></pre>
<hr>
<h2 id="6-字符集charset编码与解码">6. 字符集（Charset）编码与解码</h2>
<ul>
<li>编码：字符串转换成字节数组的过程</li>
<li>解码：字节数组转换成字符串的过程</li>
</ul>
<p>Java中提供了<code>Charset</code>类(<code>java.nio.charset.Charset</code>)</p>
<pre><code class="language-java">Map&lt;String,Charset&gt; charsets = Charset.availableCharsets();//获取所有支持的编码。（构造从规范字符集名称到字符集对象的有序映射。 ）
</code></pre>
<p>获取编码器和解码器：</p>
<pre><code class="language-java">Charset charset = Charset.fromName(&quot;GBK&quot;);
CharsetEncoder ce = charset.newEncoder();
CharsetDecoder cd = charset.newDecoder();

CharBuffer charBuffer = CharBuffer.allocate(1024);
charBuffer.put(&quot;尚硅谷威武！&quot;);

charBuffer.flip();//切换读模式

//编码
ByteBuffer byteBuffer = ce.encode(charBuffer);
byteBuffer.flip();//切换成读模式
for(int i=0;i&lt;byteBuffer.limit;i++{
    System.out.println(byteBuffer.get);
}

byteBuffer.reset();//重置position指针
//解码
charBuffer = cd.decode(byteBuffer);
</code></pre>
<hr>
<h2 id="7-nio的非阻塞式网络通信">7. NIO的非阻塞式网络通信</h2>
<ul>
<li>传统的 IO 流都是阻塞式的。也就是说，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取或写入，该线程在此期间不 能执行其他任务。因此，在完成网络通信进行 IO 操作时，由于线程会 阻塞，所以服务器端必须为每个客户端都提供一个独立的线程进行处理， 当服务器端需要处理大量客户端时，性能急剧下降。</li>
<li>Java NIO 是非阻塞模式的。当线程从某通道进行读写数据时，若没有数 据可用时，该线程可以进行其他任务。线程通常将非阻塞 IO 的空闲时 间用于在其他通道上执行 IO 操作，所以单独的线程可以管理多个输入 和输出通道。因此，NIO 可以让服务器端使用一个或有限几个线程来同 时处理连接到服务器端的所有客户端。</li>
</ul>
<h3 id="71-选择器">7.1. 选择器</h3>
<blockquote>
<p>选择器（Selector） 是 SelectableChannle 对象的多路复用器，Selector 可 以同时监控多个 SelectableChannel 的 IO 状况，也就是说，利用 Selector 可使一个单独的线程管理多个 Channel。Selector 是非阻塞 IO 的核心。</p>
</blockquote>
<p>SelectableChannle 的结构如下图：</p>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210131183623472.png"></a></p>
<p>**选择器的作用：**当客户端发送的通道的数据完全准备就绪之后，选择器才会将该任务分配到服务端的一个或多个线程上。</p>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210131183726838.png"></a></p>
<p>也就意味着，当客户端的数据未准备就绪，服务端不会处理该任务，就不会占用线程。</p>
<p>更能利用CPU的资源</p>
<hr>
<p>使用NIO完成网络通信的三个核心：</p>
<ul>
<li>
<p>通道：负责连接<br>
<code>java.nio.channels.Channel</code><br>
|—SelectableChannel<br>
|—SocketChannel<br>
|—ServerSocketChannel<br>
|—DatagramChannel</p>
<p>​	|—Pipe.SinkChannel<br>
​	|—Pipe.SourceChannel</p>
</li>
<li>
<p>缓冲区：数据的存取</p>
</li>
<li>
<p>选择器：是SelectableChannel的多路复用器，用于监控SelectableChannel的IO状况</p>
</li>
</ul>
<p>SelectionKey：选择件</p>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210201101349966.png"></a></p>
<hr>
<p><strong>TCP通信：</strong></p>
<pre><code class="language-java">//客户端
@Test
public void client() throws IOException{
    //1. 获取通道
    SocketChannel sChannel = SocketChannel.open(new InetSocketAddress(&quot;127.0.0.1&quot;, 9898));

    //2. 切换非阻塞模式
    sChannel.configureBlocking(false);

    //3. 分配指定大小的缓冲区
    ByteBuffer buf = ByteBuffer.allocate(1024);

    //4. 发送数据给服务端
    Scanner scan = new Scanner(System.in);

    while(scan.hasNext()){
        String str = scan.next();
        buf.put((new Date().toString() + &quot;\n&quot; + str).getBytes());
        buf.flip();
        sChannel.write(buf);
        buf.clear();
    }

    //5. 关闭通道
    sChannel.close();
}

//服务端
@Test
public void server() throws IOException{
    //1. 获取通道
    ServerSocketChannel ssChannel = ServerSocketChannel.open();

    //2. 切换非阻塞模式
    ssChannel.configureBlocking(false);

    //3. 绑定连接
    ssChannel.bind(new InetSocketAddress(9898));

    //4. 获取选择器
    Selector selector = Selector.open();

    //5. 将通道注册到选择器上, 并且指定“监听接收事件”
    ssChannel.register(selector, SelectionKey.OP_ACCEPT);

    //6. 轮询式的获取选择器上已经“准备就绪”的事件
    while(selector.select() &gt; 0){

        //7. 获取当前选择器中所有注册的“选择键(已就绪的监听事件)”
        Iterator&lt;SelectionKey&gt; it = selector.selectedKeys().iterator();

        while(it.hasNext()){
            //8. 获取准备“就绪”的是事件
            SelectionKey sk = it.next();

            //9. 判断具体是什么事件准备就绪
            if(sk.isAcceptable()){
                //10. 若“接收就绪”，获取客户端连接
                SocketChannel sChannel = ssChannel.accept();

                //11. 切换非阻塞模式
                sChannel.configureBlocking(false);

                //12. 将该通道注册到选择器上
                sChannel.register(selector, SelectionKey.OP_READ);
            }else if(sk.isReadable()){
                //13. 获取当前选择器上“读就绪”状态的通道
                SocketChannel sChannel = (SocketChannel) sk.channel();

                //14. 读取数据
                ByteBuffer buf = ByteBuffer.allocate(1024);

                int len = 0;
                while((len = sChannel.read(buf)) &gt; 0 ){
                    buf.flip();
                    System.out.println(new String(buf.array(), 0, len));
                    buf.clear();
                }
            }

            //15. 取消选择键 SelectionKey
            it.remove();
        }
    }
}
</code></pre>
<hr>
<p><strong>UDP通信：</strong></p>
<pre><code class="language-Java">@Test
public void send() throws IOException{
    DatagramChannel dc = DatagramChannel.open();

    dc.configureBlocking(false);

    ByteBuffer buf = ByteBuffer.allocate(1024);

    Scanner scan = new Scanner(System.in);

    while(scan.hasNext()){
        String str = scan.next();
        buf.put((new Date().toString() + &quot;:\n&quot; + str).getBytes());
        buf.flip();
        dc.send(buf, new InetSocketAddress(&quot;127.0.0.1&quot;, 9898));
        buf.clear();
    }

    dc.close();
}

@Test
public void receive() throws IOException{
    DatagramChannel dc = DatagramChannel.open();

    dc.configureBlocking(false);

    dc.bind(new InetSocketAddress(9898));

    Selector selector = Selector.open();

    dc.register(selector, SelectionKey.OP_READ);

    while(selector.select() &gt; 0){
        Iterator&lt;SelectionKey&gt; it = selector.selectedKeys().iterator();

        while(it.hasNext()){
            SelectionKey sk = it.next();

            if(sk.isReadable()){
                ByteBuffer buf = ByteBuffer.allocate(1024);

                dc.receive(buf);
                buf.flip();
                System.out.println(new String(buf.array(), 0, buf.limit()));
                buf.clear();
            }
        }

        it.remove();
    }
}
</code></pre>
<hr>
<h2 id="8-pipe管道">8. Pipe管道</h2>
<blockquote>
<p>管道（Pipe）：Java NIO 管道是==2个<strong>线程</strong>==之间的**<mark>单向数据连接</mark>**。 Pipe有一个source通道和一个sink通道。数据会 被写到sink通道，从source通道读取。</p>
</blockquote>
<p>!-<a href="https://memorykki.github.io/post-images/image-20210201125836374.png"></a></p>
<pre><code class="language-java">@Test
public void test1() throws IOException{
    //1. 获取管道
    Pipe pipe = Pipe.open();

    //2. 将缓冲区中的数据写入管道
    ByteBuffer buf = ByteBuffer.allocate(1024);

    Pipe.SinkChannel sinkChannel = pipe.sink();
    buf.put(&quot;通过单向管道发送数据&quot;.getBytes());
    buf.flip();
    sinkChannel.write(buf);

    //3. 读取缓冲区中的数据
    Pipe.SourceChannel sourceChannel = pipe.source();
    buf.flip();
    int len = sourceChannel.read(buf);
    System.out.println(new String(buf.array(), 0, len));

    sourceChannel.close();
    sinkChannel.close();
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[JUC]]></title>
        <id>https://memorykki.github.io/juc/</id>
        <link href="https://memorykki.github.io/juc/">
        </link>
        <updated>2021-09-10T02:49:13.000Z</updated>
        <summary type="html"><![CDATA[<p>java.util.concurrent</p>
]]></summary>
        <content type="html"><![CDATA[<p>java.util.concurrent</p>
<!-- more -->
<h2 id="1前言">1.前言</h2>
<h3 id="11-什么是juc">1.1 什么是JUC</h3>
<p>JUC —— （java.util.concurrent）是一个包名的缩写，该包下存放的均为多线程相关类</p>
<hr>
<h3 id="12-java中默认有几个线程">1.2 Java中默认有几个线程？</h3>
<p>一共有 2 个</p>
<ul>
<li>Java程序线程</li>
<li>GC回收器线程</li>
</ul>
<hr>
<h2 id="2-线程">2. 线程</h2>
<h3 id="21-线程的六大状态">2.1 线程的六大状态</h3>
<ul>
<li>NEW 新生</li>
<li>RUNNABLE 运行</li>
<li>BLOCKED 阻塞</li>
<li>WAITING 等待</li>
<li>TIMED_WAITING 超时等待</li>
<li>TERMINATED 死亡</li>
</ul>
<hr>
<h3 id="22-wait与sleep的区别">2.2 wait与sleep的区别</h3>
<ol>
<li>
<p>来自不同的类</p>
<p>wait -&gt; Object</p>
<p>sleep -&gt; Thread</p>
</li>
<li>
<p>关于锁的释放</p>
<p>wait：释放锁</p>
<p>sleep：不释放锁、</p>
</li>
<li>
<p>使用的范围不同</p>
<p>wait只能在synchronized中使用</p>
<p>​	为什么？看我原创整理笔记👇</p>
<p>​	<a href="https://blog.mcplugin.cn/p/304">Java线程之为何wait()和notify()必须要用同步块中 - VioletTec's Blog (mcplugin.cn)</a></p>
<p>sleep可以在任意地方使用</p>
</li>
</ol>
<hr>
<h2 id="3-lock锁">3. Lock（锁）</h2>
<p>Lock是一个接口，位于<code>java.util.concurrent.locks</code>包下</p>
<p>其有多个实现类。</p>
<figure data-type="image" tabindex="1"><img src="https://memorykki.github.io/post-images/juc/image-20210107194254581.png" alt="" loading="lazy"></figure>
<h3 id="31-可重入锁reentrantlock">3.1 可重入锁ReentrantLock</h3>
<p>默认情况下，ReentrantLock的构造方法默认是new一个不公平（unfair）锁</p>
<figure data-type="image" tabindex="2"><img src="https://memorykki.github.io/post-images/juc/image-20210107194210736.png" alt="" loading="lazy"></figure>
<p>但是在构造方法中传入一个boolean，即可控制new的锁是否为公平锁</p>
<p>true：公平锁（fair lock）</p>
<p>false：不公平锁（unfair lock）</p>
<p><strong>为什么默认要用非公平锁？</strong></p>
<p>因为公平。因为如果使用公平锁，会有可能导致执行耗时长的线程优先执行，会导致CPU使用效率下降。</p>
<hr>
<h4 id="311-公平锁和非公平锁">3.1.1 公平锁和非公平锁</h4>
<ul>
<li>
<p>公平锁：先来后到（必须是先来的先执行）</p>
</li>
<li>
<p>非公平锁：非前来后到，可插队（根据CPU进行调度）</p>
</li>
</ul>
<hr>
<h3 id="32-lock与synchronized的区别">3.2 Lock与synchronized的区别</h3>
<ul>
<li>Synchronized是内置关键字，Lock是一个类</li>
<li>Synchronized无法判断是否获取到了锁，Lock可判断是否获得到了锁</li>
<li>Synchronized会自动获取和释放锁</li>
<li>Synchronized 线程 1（获得锁，阻塞）、线程2（等待，傻傻的等）；Lock锁就不一定会等待下去</li>
<li>Synchronized 可重入锁，不可以中断的，非公平<br>
Lock ：可重入锁，可以判断锁，非公平（可以自己设置）</li>
<li>Synchronized 适合锁少量的代码同步问题，Lock 适合锁大量的同步代码</li>
</ul>
<hr>
<h3 id="33-虚假唤醒">3.3 虚假唤醒</h3>
<blockquote>
<p>当一个条件满足时，很多线程都被唤醒了，但是只有其中部分是有用的唤醒，其它的唤醒都是无用功<br>
1.比如说买货，如果商品本来没有货物，突然进了一件商品，这是所有的线程都被唤醒了，但是只能一个人买，所以其他人都是假唤醒，获取不到对象的锁</p>
</blockquote>
<p>比如一下代码：</p>
<pre><code class="language-java">package duoxiancheng.bao;

/*
 * 虚假唤醒的解决：
 *  wait要始终保证在while循环当中。
 */
public class LockTest {
    public static void main(String[] args) {
        Clerk clerk = new Clerk();
        Producter producter = new Producter(clerk);
        Customer customer = new Customer(clerk);

        new Thread(producter,&quot;生产者A&quot;).start();
        new Thread(customer,&quot;消费者A&quot;).start();
        new Thread(producter,&quot;生产者B&quot;).start();
        new Thread(customer,&quot;消费者B&quot;).start();
    }
}

// 售货员
class Clerk {
    private int product = 0;

    // 进货
    public synchronized void add() {
        // 产品已满
        while (product &gt;=1) {
            System.out.println(Thread.currentThread().getName() + &quot;: &quot; + &quot;已满！&quot;);
            try {
                this.wait();
            } catch (InterruptedException e) {
            }
        }
        ++product;
        // 该线程从while中出来的时候，是满足条件的
        System.out.println(Thread.currentThread().getName() + &quot;: &quot; +&quot;....................进货成功，剩下&quot;+product);
        this.notifyAll();
    }

    // 卖货
    public synchronized void sale() {
        while (product &lt;=0) {
            System.out.println(Thread.currentThread().getName() + &quot;: &quot; + &quot;没有买到货&quot;);
            try {
                this.wait();
            } catch (InterruptedException e) {
            }
        }
        --product;
        System.out.println(Thread.currentThread().getName() + &quot;:买到了货物，剩下 &quot; + product);
        this.notifyAll();
    }
}

// 生产者
class Producter implements Runnable {
    private Clerk clerk;

    public Producter(Clerk clerk) {
        this.clerk = clerk;
    }

    // 进货
    @Override
    public void run() {
        for(int i = 0; i &lt; 20; ++i) {
            try {
                Thread.sleep(200);
            } catch (InterruptedException e) {
            }
            clerk.add();
        }
    }
}

// 消费者
class Customer implements Runnable {
    private Clerk clerk;
    public Customer(Clerk clerk) {
        this.clerk = clerk;
    }

    // 买货
    @Override
    public void run() {
        for(int i = 0; i &lt; 20; ++i) {
            clerk.sale();
        }
    }
}
</code></pre>
<p>当add了一个产品时，会notifyAll，唤醒所有线程。但是并非所有线程都需要sale一个产品。</p>
<p>如果使用if，若当一个线程执行完if(product &gt;= 1)后跳过wait语句，然后将CPU时间让出。</p>
<p>如果重复以上步骤，有许多线程都出现该问题时，当他们返回CPU现场，获得CPU运行时间的时候，则会继续执行sale方法，导致多个线程同时sale。如果加上while，当一个县城跳过wait时，让出CPU后，再获得CPU时，会跳到while(product &gt;= 0)重新判断一次，防止虚假唤醒</p>
<p>（add同理）</p>
<hr>
<h2 id="4-8锁的现象">4. 8锁的现象</h2>
<blockquote>
<p>意义：加深我们对被锁的物体的理解。</p>
</blockquote>
<p>具体代码：见视频，懒得写了（dog</p>
<hr>
<h2 id="5-callable简单">5. Callable（简单）</h2>
<p>**callable特点：**可以返回内容，可以抛出异常</p>
<pre><code class="language-java">package com.kuang.callable;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.FutureTask;
import java.util.concurrent.locks.ReentrantLock;
/**
* 1、探究原理
* 2、觉自己会用
*/
public class CallableTest {
public static void main(String[] args) throws ExecutionException,InterruptedException {
        // new Thread(new Runnable()).start();
        // new Thread(new FutureTask&lt;V&gt;()).start();
        // new Thread(new FutureTask&lt;V&gt;( Callable )).start();
        new Thread().start(); // 怎么启动Callable
        MyThread thread = new MyThread();
        FutureTask futureTask = new FutureTask(thread); // 适配类
        new Thread(futureTask,&quot;A&quot;).start();
        new Thread(futureTask,&quot;B&quot;).start(); // 结果会被缓存，效率高
        Integer o = (Integer) futureTask.get(); //这个get 方法可能会产生阻塞！把他放到
        最后
        // 或者使用异步通信来处理！
        System.out.println(o);
    }
}
class MyThread implements Callable&lt;Integer&gt; {
    @Override
    public Integer call() {
        System.out.println(&quot;call()&quot;); // 会打印几个call
        // 耗时的操作
        return 1024;
    }
}
</code></pre>
<hr>
<h2 id="6-常用的辅助类必会">6. 常用的辅助类(必会)</h2>
<h3 id="61-countdownlatch">6.1 CountDownLatch</h3>
<blockquote>
<p>允许一个或多个线程等待直到在其他线程中执行的一组操作完成的同步辅助</p>
</blockquote>
<pre><code class="language-java">CountDownLatch coutnDownLatch = new CoutDownLatch(6);//减法计数器
for(int i=1;i&lt;=6;i++){
    new Thread(()-&gt;{
		System.out.println(Thread.currentThread().getName + &quot;go out&quot;);
        coutDownLatch.countDown();//计数器-1
    },String.valueof(i)).start();
}
coutDownLatch.await();//等待计数器归零
System.out.println(&quot;Close the Door!&quot;);
</code></pre>
<p>CountDownLath常用方法：</p>
<ul>
<li>new CoutDownLatch(int count);//构造方法，用于初始化计数器的值</li>
<li>countDown();//计数器-1</li>
<li>await();//等待线程直到计数器为0为止</li>
</ul>
<hr>
<h3 id="62-cyclicbarrier">6.2 CyclicBarrier</h3>
<blockquote>
<p>允许一组线程全部等待达到彼此共同的屏障点的同步辅助。</p>
</blockquote>
<p>可以理解为：加法计数器</p>
<pre><code class="language-java">//集齐七颗龙珠召唤神龙。
CyclicBarrier cyclicbarrier = new CyclicBarrier(7,()-&gt;{
    System.out.println(&quot;召唤神龙成功！&quot;);
});
for(int i=1;i&lt;=7;i++){
    new Thread(()-&gt;{
        System.out.println(&quot;收集了&quot;+i+&quot;颗龙珠&quot;);
    },String.valueof(i)).start();
}
cyclicBarrier.await();//等待计数器变成7时
</code></pre>
<blockquote>
<p>CyclicBarrier和CountDownLatch的区别就是：<br>
CountDownLatch不可以重置计数。</p>
</blockquote>
<figure data-type="image" tabindex="3"><img src="https://memorykki.github.io/post-images/juc/image-20210129164950688.png" alt="" loading="lazy"></figure>
<blockquote>
<p>如果想要重置计数，可以使用CyclicBarrier。</p>
</blockquote>
<figure data-type="image" tabindex="4"><img src="https://memorykki.github.io/post-images/juc/image-20210129165010297.png" alt="" loading="lazy"></figure>
<hr>
<h3 id="63-semaphore">6.3 Semaphore</h3>
<p>Semaphore：信号量</p>
<blockquote>
<p>一个计数信号量，在概念上，信号量维持一组许可证。如果有必要，每个acquire()都会阻塞，直到许可证可用，然后才能使用它</p>
</blockquote>
<pre><code class="language-java">//acquire() 得到
//release() 释放
package com.kuang.add;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
public class SemaphoreDemo {
    public static void main(String[] args) {
        // 线程数量：停车位! 限流！
        Semaphore semaphore = new Semaphore(3);
        for (int i = 1; i &lt;=6 ; i++) {
            new Thread(()-&gt;{
                // acquire() 得到
                try {
                    semaphore.acquire();
                    System.out.println(Thread.currentThread().getName()+&quot;抢到车
                    位&quot;);
                    TimeUnit.SECONDS.sleep(2);
                    System.out.println(Thread.currentThread().getName()+&quot;离开车
                    位&quot;);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                } finally {
                    semaphore.release(); // release() 释放
                }
            },String.valueOf(i)).start();
        }
    }
}
</code></pre>
<p><strong>原理：</strong></p>
<p>semaphore.acquire() 获得，假设如果已经满了，等待，等待被释放为止！</p>
<p>semaphore.release(); 释放，会将当前的信号量释放 + 1，然后唤醒等待的线程！</p>
<hr>
<h3 id="64-exchanger笔者补充">6.4 Exchanger（笔者补充）</h3>
<blockquote>
<p>允许两个线程在集合点交换对象，并且在多个管道设计中很有用。</p>
</blockquote>
<p>简单来说，Exchanger的.exchange(Object obj)方法，是一个阻塞的交换点。</p>
<p>当两个线程都在.exchange()阻塞时，进行交换。</p>
<pre><code class="language-java"> class FillAndEmpty {
   Exchanger&lt;DataBuffer&gt; exchanger = new Exchanger&lt;&gt;();
   
   //下面是两个装数据的实体类
   DataBuffer initialEmptyBuffer = ... a made-up type
   DataBuffer initialFullBuffer = ...
	
   //FillingLoop ： 不断向DataBuffer中添加数据，并交换给EmptyingLoop
   class FillingLoop implements Runnable {
     public void run() {
       DataBuffer currentBuffer = initialEmptyBuffer;
       try {
         while (currentBuffer != null) {
           addToBuffer(currentBuffer);//从currentBuffer里取出一个数据
           if (currentBuffer.isFull()){
               //如果是装满了，就交还给EmptyingLoop类进行消费
	           currentBuffer = exchanger.exchange(currentBuffer);
           }
         }
       } catch (InterruptedException ex) { ... handle ... }
     }
   }

   //EmptyingLoop ： 不断从DataBuffer中取出数据，并交换给FillingLoop
   class EmptyingLoop implements Runnable {
     public void run() {
       DataBuffer currentBuffer = initialFullBuffer;
       try {
         while (currentBuffer != null) {
           takeFromBuffer(currentBuffer);
           if (currentBuffer.isEmpty()){
               //如果消费完了，则交换buffer给FillingLoop进行生产
             currentBuffer = exchanger.exchange(currentBuffer);
           }
         }
       } catch (InterruptedException ex) { ... handle ...}
     }
   }
   
   void takeFromBuffer(DataBuffer dataBuffer){
       //.....
       dataBuffer.take();
       //......
   }
   void addToBuffer(DataBuffer dataBuffer){
       //.....
       dataBuffer.add(xxxxx);
       //......
   }
   void start() {
     new Thread(new FillingLoop()).start();
     new Thread(new EmptyingLoop()).start();
   }
 }
</code></pre>
<hr>
<h2 id="7-读写锁">7. 读写锁</h2>
<ul>
<li>独占锁（写锁） 一次只能被一个线程占有</li>
<li>共享锁（读锁） 多个线程可以同时占有</li>
<li>ReadWriteLock</li>
<li>读-读 可以共存！</li>
<li>读-写 不能共存！</li>
<li>写-写 不能共存！</li>
</ul>
<hr>
<h2 id="8-阻塞队列">8. 阻塞队列</h2>
<p>BlockingQueue：阻塞队列</p>
<figure data-type="image" tabindex="5"><img src="https://memorykki.github.io/post-images/juc/image-20210123122529714.png" alt="" loading="lazy"></figure>
<p>SynchronousQueue 同步队列：</p>
<p>和其他的BlockingQueue不一样。</p>
<p>没有容量， 进去一个元素，必须等待取出来之后，才能再往里面放一个元素！</p>
<p>只有 put()、take()两个方法</p>
<hr>
<h2 id="9-线程池重点">9. 线程池（重点）</h2>
<blockquote>
<p>池化技术</p>
</blockquote>
<p>程序的运行，本质：占用系统的资源！优化资源的使用=》池化技术</p>
<p>线程池、连接池、内存池、对象池</p>
<p>创建和销毁线程需要从用户态内陷到内核态进行操作，是一个耗时的操作。</p>
<p>线程池：三大方法、7大参数、4种拒绝策略</p>
<p>池化技术：池化技术简单点来说，就是提前保存大量的资源，以备不时之需。</p>
<p>↓↓↓为了方便理解线程池，我插入一些题外话，方便深入理解线程池↓↓↓：</p>
<hr>
<p>什么是线程？</p>
<p>线程是调度CPU的最小单元，也叫做轻量级进程LWP（Light Weight Process）<br>
Java中有两种线程模型：</p>
<ol>
<li>用户级线程（ULT）（Uer Level Thread）</li>
<li>内核级线程（KLT）（Kernel Level Thread）</li>
</ol>
<ul>
<li><strong>ULT</strong>：用户程序实现，不依赖操作系统核心，应用提供创建，同步，调度和管理线程的函数来控制用户线程，不需要<strong>用户态</strong>/<strong>内核态</strong>的切换，速度快，内核对<strong>ULT</strong>无感知，线程阻塞则进程（包括它的所有线程）阻塞。</li>
<li><strong>KLT</strong>：系统内核管理线程（KLT），内核保存线程的状态和上下文的信息，线程阻塞不会引起进程阻塞。在处理器系统上多线程在多处理器上并行运行。现成的创建，调度和管理由内核完成，效率比UTL慢，比进程操作快。</li>
</ul>
<hr>
<p>市面上绝大多数的Java虚拟机都是使用的<strong>KLT</strong>，及<strong>系统内核管理线程</strong>。<br>
文字描述比较抽象，我们来画一个图描述一下<strong>ULT</strong>和<strong>KLT</strong>的区别</p>
<hr>
<figure data-type="image" tabindex="6"><img src="https://memorykki.github.io/post-images/juc/5e6e1c2ae83c3a1e3af7acc8.jpg" alt="" loading="lazy"></figure>
<hr>
<p>JVM由于在用户空间，无权使用内核空间，只能调用系统开放的API（如：Linux开放的p_thread函数）去操作线程，在映射到底层的CPU上，由于调度API需要提高权限，所以会把自身状态<strong>陷入</strong>到内核态来取得权限。<br>
用户所有的线程都会存放在线程表中，由内核统一的调度和维护。<br>
这就是为什么会进行<strong>用户态/内核态</strong>状态切换的原因。</p>
<hr>
<figure data-type="image" tabindex="7"><img src="https://memorykki.github.io/post-images/juc/5e6e1f10e83c3a1e3af91fdb.jpg" alt="" loading="lazy"></figure>
<hr>
<p>根据上图，JVM创建和执行线程可以列为一下这么几个步骤</p>
<ol>
<li>线程会使用库调度器</li>
<li>之后陷入到内核空间</li>
<li>创建内核线程</li>
<li>内核中的线程会被维护到线程表中</li>
<li>由操作系统调度程序去调度</li>
<li>CPU会根据调度算法分配时间</li>
<li>把没有执行完的线程写入给你高速内存区（SSP）</li>
</ol>
<blockquote>
<p>内核空间中有一个高速内存区SSP（程序任务运行状态段）是用于存储还没有执行完成，但是被分配的时间已经用完的线程中的数据，，等待下一次被分配到了时间后，就把保存在SSP里的上下文信息加载到CPU的缓存。</p>
</blockquote>
<p>综上所述，线程是一个稀缺资源，他的创建和销毁是一个相对偏重且消耗资源的操作，而Java线程依赖于内核进程，创建线程需要进行操作系统状态切换，为避免过度消耗，我们要设法重用线程执行多个任务。</p>
<hr>
<p>线程池就是一个线程缓存，负责对线程进行统一分配，调度与监控</p>
<p>他的优点有很多，最突出的优点就是：</p>
<ul>
<li>重用存在的线程，减少线程的创建，消亡所用开销，提升性能</li>
<li>提高响应速度。当任务到达时，可以不需要等待线程的创建就可以立即执行</li>
<li>提高线程的可管理性，可统一分配，调度和监控。</li>
</ul>
<p>那么线程池是如何把线程统一分配调度与监控的呢？</p>
<hr>
<p><img src="https://memorykki.github.io/post-images/juc/5e6e2193e83c3a1e3afa3244.jpg" alt="" loading="lazy"><br>
【画图太累了，我就用的网图，图片来源视频：https://www.bilibili.com/video/av88030891 】</p>
<p><img src="https://memorykki.github.io/post-images/juc/5e6e21e3e83c3a1e3afa4efb.jpg" alt="" loading="lazy"><br>
【画图太累了，我就用的网图，图片来源：https://blog.csdn.net/lchq1995/article/details/85230399 】</p>
<hr>
<p>我们在NEW一个最基本的线程池的时候，会传入这么一下几个参数：<br>
<strong>corePoolSize</strong>：线程池核心线程数量<br>
<strong>maximumPoolSize</strong>：线程池最大线程数两<br>
<strong>keepAliveTime</strong>：空闲线程存活时间</p>
<ul>
<li><strong>corePoolSize</strong>顾名思义就是最大核心线程数量，是线程池可以同时执行的线程数量。</li>
<li><strong>maximumPoolSize</strong>，既然有corePoolSize，那么如果corePoolSize满了怎么办呢？这时候就会用到一个队列，叫<strong>阻塞队列</strong>（<strong>Block Queue</strong>）<br>
什么是<strong>BlockQueue</strong>？它有什么特点？<br>
既然是Queue，那么久满足队列模型的（<strong>FIFO</strong>）原则，一端放入，另一端取出。（First In First Out）<br>
阻塞队列有一个特点就是：在任意时刻，不管并发量有多高，永远只有一个线程能进行队列的如对或出队，所以BlockQueue是一个线程安全的队列。<br>
并且如果队列满了，只能进行出队操作，所有入队操作必须等待，也就是阻塞。<br>
如果队列为空，那么就只能进行入队操作，所有出队操作必须等待，也就是阻塞。<br>
一旦线程池的线程量满了，那么新被execute进来的线程，就会被存储进BlockQueue，BlockQueue的大小就是maximumPoolSize - corePoolSize的大小。</li>
</ul>
<hr>
<p>线程池和五种状态：</p>
<ul>
<li><strong>Running</strong>：能接受新的execute以及处理已添加的任务</li>
<li><strong>Shutdown</strong>：不接收任何新的execute，可以处理已添加的任务</li>
<li><strong>Stop</strong>：不接受任何新的execute，不处理已添加的任务</li>
<li><strong>Tidying</strong>：所有任务已经终止，ctl记录的任务数量为0.</li>
<li><strong>Termiated</strong>：线程池彻底终止，则线程池转换为Terminated状态。</li>
</ul>
<hr>
<figure data-type="image" tabindex="8"><img src="https://memorykki.github.io/post-images/juc/5e6e2524e83c3a1e3afba883.jpg" alt="" loading="lazy"></figure>
<hr>
<p>那么这么多线程池状态和这么多线程的信息，是如何保存的呢？<br>
这里线程池内部用到了一个32字节的Integer类型来记录线程池的状态和线程数量信息。</p>
<hr>
<figure data-type="image" tabindex="9"><img src="https://memorykki.github.io/post-images/juc/5e6e2927e83c3a1e3afd5d34.jpg" alt="" loading="lazy"></figure>
<hr>
<p>这个Integer类型的高3未二进制用来表示线程池的状态，后29为用来表示线程的数量。<br>
线程池定义了这么几个数字作为线程的状态</p>
<p>RUNNING = -1<br>
SHUTDOWN = 0<br>
STOP = 1<br>
TIDYING = 2<br>
TERMINATED = 3</p>
<p>并且所有数字都想做移位29位。<br>
&lt;&lt;  COUNT_BITS（COUNT_BITS=29)</p>
<p><strong>最终</strong>会得到高三位为：<br>
<strong>RUNNING</strong> = 111<br>
<strong>SHUTDOWN</strong> = 000<br>
<strong>STOP</strong> = 001<br>
<strong>TIDYING</strong> = 010<br>
<strong>TERMINATED</strong> = 011</p>
<hr>
<p>他是怎么得到的呢？</p>
<p>我们来回顾一下基础</p>
<p>拿-1来举个例子<br>
众所周知，<br>
1在32位Integer的类型中二进制为：<br>
0000 0000 0000 0000 0000 0000 0000 0001<br>
那么-1就应该去1的反位后+1，并且再加上一个符号位1000<br>
则，-1就应该为：<br>
1000 1111 1111 1111 1111 1111 1111 1111<br>
那么-1向左移位29位，低位补0，那么则<br>
-1&lt;&lt;29 等于<br>
1110 0000 0000 0000 0000 0000 0000 0000<br>
所以高三位为111</p>
<hr>
<p>所以这就是RUNNING的高三为为111的由来<br>
后面的29位用于存储线程的数量。</p>
<hr>
<p>这种应用基本数据高效存储的思想可以用于存储一些记录，有点就是不用去多个变量的读取，提升速度。</p>
<hr>
<p>具体线程池的实现可以百度搜索<strong>JAVA线程池的实现</strong>，在这里只是浅谈一下线程池的好处以及浅层原理。</p>
<hr>
<p>↑↑↑以上为插入的题外内容，方便对线程池的理解。转载自本人原创博客。↑↑↑</p>
<p>原文地址：https://blog.mcplugin.cn/p/225</p>
<hr>
<p>根据阿里巴巴的《Java开发手册》中</p>
<figure data-type="image" tabindex="10"><img src="https://memorykki.github.io/post-images/juc/image-20210123150117054.png" alt="" loading="lazy"></figure>
<p>不允许使用Executors去创建线程/线程池，而是使用ThreadPoolExecutor</p>
<p>为什么会发生OOM？因额外FixedThreadPool、SingleThreadPool的允许请求的最大队列长度，以及CachedThreadPool允许创建的最大线程数量为Integer.MAXVALUE，Integer最大值为21亿多（<code>2147483647</code>），若发生意外，则容易在线程池的队列中发生OOM</p>
<p>禁止以下列形式创建线程/线程池❌↓</p>
<pre><code class="language-java">ExecutorService threadPool1 = Executors.newSingleThreadExecutor();// 单个线程
ExecutorService threadPool2 = Executors.newFixedThreadPool(5); // 创建一个固定的线程池的大小
ExecutorService threadPool3 = Executors.newCachedThreadPool(); // 可伸缩的，遇强则强，遇弱则弱

</code></pre>
<p>通过查看Executors.newXxxThreadPool();的方法，我们可以看到它底层都使用了ThreadPoolExecutor类。</p>
<figure data-type="image" tabindex="11"><img src="https://memorykki.github.io/post-images/juc/image-20210123151334625.png" alt="" loading="lazy"></figure>
<p>我们可以看到CachedThreadPool的最大线程数量的确为Interger的最大值，即<code>2147483647</code></p>
<p>即，Executors的本质还是使用了ThreadPoolExecutor();</p>
<p>我们接着来看ThreadPoolExecutor()的构造方法：</p>
<figure data-type="image" tabindex="12"><img src="https://memorykki.github.io/post-images/juc/image-20210123151506816.png" alt="" loading="lazy"></figure>
<p>最大线程应如何定义？</p>
<ul>
<li>CPU 密集型  -&gt;  有几个核心就是可以同时并行几个
<ul>
<li>将最大线程可以定义为CPU的核心数量</li>
<li>获取CPU核心数量: <code>Runtime.getRuntime().availableProcess();</code></li>
</ul>
</li>
<li>IO    密集型  -&gt;  判断你的程序中十分耗IO的线程有多少个。至要大于这个数字就可以了！一般是耗IO线程数量的2倍</li>
</ul>
<hr>
<h2 id="10-四大函数式接口重点-必须掌握">10. 四大函数式接口（重点、必须掌握）</h2>
<p>新时代程序员特色社会主义编程技能：</p>
<ol>
<li>lambda表达式</li>
<li>链式编程</li>
<li>函数式接口</li>
<li>流式计算</li>
</ol>
<blockquote>
<p>函数式接口：只有一个方法的接口。这个接口有一个注解：@FunctionalInterface</p>
</blockquote>
<p>学习目的：简化编程模型，在新版本的底层大量应用。</p>
<p>如：list.foreatch(消费者类型的函数式接口);</p>
<p><strong>四大原生的函数式接口：</strong></p>
<ol>
<li>Consumer</li>
<li>Function</li>
<li>Predicate</li>
<li>Supplier</li>
</ol>
<hr>
<p>就记到这里，下面都没在手动记笔记了...懒了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL]]></title>
        <id>https://memorykki.github.io/Mysql/</id>
        <link href="https://memorykki.github.io/Mysql/">
        </link>
        <updated>2021-08-27T02:23:12.000Z</updated>
        <summary type="html"><![CDATA[<p>Mysql高级</p>
]]></summary>
        <content type="html"><![CDATA[<p>Mysql高级</p>
<!-- more -->
<p>索引是帮助MySQL高效获取数据的排好序的数据结构，将无序的数据编程有序的查询。</p>
<ul>
<li>二叉树：单链</li>
<li>红黑树：二叉平衡树，提高一倍。旋转成本，树的高度可能会很高，效率很差</li>
<li>Hash表：</li>
<li>B-Tree：改进红黑树，使高度尽可能小，节点横向扩展，一个存储更多的节点 。</li>
</ul>
<p>索引数据分布在叶子、非叶子中，索引不重复，索引从左至右递增。</p>
<figure data-type="image" tabindex="1"><img src="https://memorykki.github.io/post-images/Mysql/image-20210825202125656.png" alt="" loading="lazy"></figure>
<ul>
<li>B+Tree：叶子节点指针连接，非叶子节点不存储数据，叶子节点包含所有索引，非叶子节点包含一部分索引，冗余索引（只需要冗余主键），数据放在叶子节点。</li>
</ul>
<p>双向链表便于大于、小于的索引查找</p>
<figure data-type="image" tabindex="2"><img src="https://memorykki.github.io/post-images/Mysql/image-20210825202004251.png" alt="" loading="lazy"></figure>
<p>树的高度越小越好，一个节点上存储更多索引（16KB），所以从磁盘一次加载的数据多，二分查找速度快</p>
<p>B+树非叶子节点不存储数据，也就能放更多的索引，存储量大，B树节点有数据就不行。所以决定树的高度：非叶子节点中能放的索引数量。</p>
<p>三层即可存储2000w+索引，在mysql启动时将上层索引加载到内存中，索引取数据只需要大约一次IO。</p>
<figure data-type="image" tabindex="3"><img src="https://memorykki.github.io/post-images/Mysql/image-20210825203138095.png" alt="" loading="lazy"></figure>
<p>.frm 表结构</p>
<p>.MYD 表数据</p>
<p>.MYI 表索引</p>
<p>myisam查询过程：条件是否是索引，是则遍历myiB+树，根据地址找到myd磁盘地址。</p>
<p>.frm 表结构</p>
<p>.ibd 数据+索引，表数据也是按照B+组织</p>
<p>innodb推荐建立主键：自增主键：</p>
<ul>
<li>聚集索引：叶节点包含了完整的数据记录，数据和索引聚集在一起，innodb主键索引。</li>
</ul>
<p>innodb非主键索引（回表、二次）：叶子节点存储主键值：一致性和节省存储空间。</p>
<ul>
<li>非聚集索引：索引和数据分开存储。（myisam）效率不如聚集，回表：二次索引（拿到索引再去磁盘遍历索引）</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://memorykki.github.io/post-images/Mysql/image-20210825204526349.png" alt="" loading="lazy"></figure>
<p>必须要建主键？Mysql会使用主键构建B+树，没有主键Mysql会找一个不重复的列来构建，找不到就生成一个隐藏列rowid来构建，所以节省空间。</p>
<p>为什么不能用UUID？UUID非整型非自增，比较大小效率慢，所以推荐整型。B+范围查找只需两次，非自增只向后插，不用分页，分裂次数少，维护B+结构成本小，推荐自增。</p>
<p>索引就是目录，用空间换时间，B+的节点就是16KB的页，查找就是遍历页上的有序数据，一个页上有多个节点，但是页越来越长，形成链表效率会降低，所以构建目录的目录，即树。</p>
<p>mysql运行时间长之后，缓存加载到内存越来越大，</p>
<ul>
<li>Hash索引：</li>
</ul>
<p>一次计算。不支持范围查询，hash冲突问题</p>
<ul>
<li>联合索引：</li>
</ul>
<p>多个字段，按先后顺序，谁先排好序就以谁的排序为准，后面的忽略。</p>
<figure data-type="image" tabindex="5"><img src="https://memorykki.github.io/post-images/Mysql/image-20210825211720812.png" alt="" loading="lazy"></figure>
<p>建立联合索引需要重新构建B+树，所以会和主键索引构建的B+树大量冗余数据，资源浪费，所以可以使联合索引树叶子节点不存储数据，只存储主键，然后回表，是个折中的办法。</p>
<figure data-type="image" tabindex="6"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826090232766.png" alt="" loading="lazy"></figure>
<p>主键索引：</p>
<figure data-type="image" tabindex="7"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826094000399.png" alt="" loading="lazy"></figure>
<p>select * from t where b&gt;1;(bcd联合索引)</p>
<h3 id="选择索引">选择索引</h3>
<p>利用联合索引可以找到数据，但是查找的是*全部数据，叶子节点只有bcd三列数据，所以需要7次回表查出全部数据，全表扫描走主键只需要四次，更快，所以实际执行的是全表扫描，非联合索引。</p>
<ul>
<li>
<p>如果是select b,c,d,a；那么也能走联合索引，因为想要的四个字段都在一棵树上，叶子是主键a</p>
</li>
<li>
<p>select b;联合索引的叶子能够找出满足要求的数据，就不用走全表扫描。</p>
</li>
<li>
<p>同样地，如果联合索引回表次数&lt;全表扫描加载页次数，也不用走全扫描。</p>
</li>
<li>
<p>select b from t;没有where条件也能走联合索引，主键索引和联合索引都能找到所需数据，并且如果二者的页数相等，但是因为联合索引叶子节点存储主键，主键索引叶子存储数据，所以联合索引的一页比主键索引的一页存储的数据更多，虽然不完整，只有bcd，但是查询我们只要b就够了，所以不走主键。</p>
</li>
<li>
<p>select * from t order by b,c,d;走bcd不用排序但需要回表八次，全表扫描需要四次，需要排序。因为只有8条数据，排序很快，所以走的是全表扫描。如果select b不需要回表，走的是联合索引。</p>
</li>
</ul>
<p>最左前缀法则：查询从最左前列开始并且二不能跳过中间。也就是联合索引触发必须从包含第一个列的条件开始。</p>
<p>原理：比如age=30，如果不考虑name是否有序而使用age索引的前提是前面的name相同，那么就应该理解为同一列的节点的age是有序的，但是不是有序的，需要全表扫描（从左向右逐个检查）。判断索引是否会用到的原则，当前列是否有序。</p>
<p>Mysql中类型不匹配时，字符转数字，数字型字符会自动类型转成数字，非数字型字符会转成0，即：</p>
<p>select 'a'=0;  true</p>
<p>select 'b12'=0;  true</p>
<p>select '1'=0; false</p>
<p>隐式的类型转换可能会导致索引失效。</p>
<p>Mysql5.8之前仅支持升序索引，5.7支持语法层面的降序，但是索引组织仍然是升序的。</p>
<h3 id="b树-b树">B树 B+树</h3>
<p>B树每个节点都存储key和data，所有节点组成这棵树，并且叶子节点指针为null。</p>
<figure data-type="image" tabindex="8"><img src="https://memorykki.github.io/post-images/Mysql/20170920132504569-1630027523648.png" alt="" loading="lazy"></figure>
<p>B+树只有叶子节点存储data，叶子节点包含了这棵树的所有键值，叶子节点不存储指针。</p>
<figure data-type="image" tabindex="9"><img src="https://memorykki.github.io/post-images/Mysql/20170920132523536.png" alt="" loading="lazy"></figure>
<p>MySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指<br>
针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。</p>
<figure data-type="image" tabindex="10"><img src="https://memorykki.github.io/post-images/Mysql/image-20210827092732211.png" alt="" loading="lazy"></figure>
<p>原因有很多，最主要的是这棵树矮胖，呵呵。一般来说，索引很大，往往以索引文件的形式存储的磁盘上，索引查找时产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的时间复杂度。树高度越小，I/O次数越少。</p>
<p>那为什么是B+树而不是B树呢，因为它内节点不存储data，这样一个节点就可以存储更多的key。</p>
<h3 id="索引类型"><strong>索引类型</strong></h3>
<figure data-type="image" tabindex="11"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826161730787.png" alt="" loading="lazy"></figure>
<p>普通可重复，唯一不可重复，唯一可以有null，可以有多个，主键不能有null，且只能有一个。全文索引like关键字。</p>
<h3 id="聚簇-非聚簇"><strong>聚簇 非聚簇</strong></h3>
<figure data-type="image" tabindex="12"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826163500181.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="13"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826163536434.png" alt="" loading="lazy"></figure>
<p>innodb仅叶子节点将索引数据放在一起。</p>
<p>表中除了主键索引的其他索引都是辅助索引，其叶子节点存储地十主键，类非聚簇索引，但存储的不是行地址，而是主键，所以非聚簇索引都是辅助索引。</p>
<h3 id="索引数据结构"><strong>索引数据结构</strong></h3>
<figure data-type="image" tabindex="14"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826164046114.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="15"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826164125546.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="16"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826164153227.png" alt="" loading="lazy"></figure>
<h3 id="索引的设计原则"><strong>索引的设计原则</strong></h3>
<figure data-type="image" tabindex="17"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826164315221-1629967396390.png" alt="" loading="lazy"></figure>
<ul>
<li>适合索引的是where</li>
<li>使用前缀段短索引</li>
<li>不能在更新频繁、数据区分度小、重复度高、数据量小的列建索引</li>
</ul>
<h3 id="锁"><strong>锁</strong></h3>
<p>&lt;<img src="https://memorykki.github.io/post-images/Mysql/image-20210826164836207.png" alt="" loading="lazy"></p>
<p>&lt;<img src="https://memorykki.github.io/post-images/Mysql/image-20210826165201233.png" alt="" loading="lazy"></p>
<p>&lt;<img src="https://memorykki.github.io/post-images/Mysql/image-20210826165417928.png" alt="" loading="lazy"></p>
<p>&lt;<img src="https://memorykki.github.io/post-images/Mysql/image-20210826170133849.png" alt="" loading="lazy"></p>
<p>1 4 5 7，间隙锁锁住234，临建锁锁住1234.</p>
<p>行锁页锁会出现死锁。</p>
<p>临建锁next key = 记录锁record + 间隙锁gap，三者都属于行锁。</p>
<p>意向锁提高了加锁效率。</p>
<p>innodb默认采用行锁，myisam默认采用表锁。</p>
<h3 id="执行计划"><strong>执行计划</strong></h3>
<ul>
<li>
<p>id：select的序号，顺序增长，越大优先级越高；</p>
</li>
<li>
<p>select_type：</p>
<ul>
<li>simple：简单查询，不包含子查询和union；</li>
<li>primary：复杂查询中最外层的select；</li>
<li>subquery：select中的子查询；</li>
<li>derived：from中的子查询。</li>
</ul>
</li>
<li>
<p>table：正在访问哪个表</p>
<ul>
<li>from有子查询时，drivenN，N代表id=N的查询</li>
<li>有union时，union1,2，表示参与union的id</li>
</ul>
</li>
<li>
<p>type：关联类型或访问类型，即MySQL决定<strong>如何查找表中的行</strong>，查找数据行记录的大概范围。<br>
依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL<br>
一般来说，得保证查询达到range级别，最好达到ref</p>
<ul>
<li>NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可以单独查找索引来完成，不需要在执行时访问表</li>
<li>const：<strong>通过索引一次命中；</strong></li>
<li>system：表中只有一行记录；</li>
<li>eq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多<strong>只会返回一条</strong>符合条件的记录。<strong>唯一性索引扫描</strong></li>
</ul>
<pre><code class="language-sql">explain select * from film_actor left join film on film_actor.film_id = film.id;
</code></pre>
<ul>
<li>ref：相比 eq_ref，<strong>非唯一性索引扫描</strong>，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行。  <strong>和eq_ref都需要回表</strong></li>
<li>range：<strong>范围扫描</strong>，通常出现在 in(), between ,&gt; ,&lt;, &gt;= 等操作中。使用一个索引来检索给定范围的行。</li>
<li>index：<strong>扫描全索引就能拿到结果</strong>，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这种通常比ALL快一些</li>
<li>ALL：即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了</li>
</ul>
</li>
<li>
<p>key：显示mysql<strong>实际采用哪个索引</strong>来优化对该表的访问。<br>
如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用<br>
force index、ignore index ；</p>
</li>
<li>
<p>possible_keys：显示查询<strong>可能使用哪些索引</strong>来查找。<br>
explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引对此查询帮助不大，选择了全表查询。<br>
如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提高查询性能，然后用 explain 查看效果  。</p>
</li>
<li>
<p>key_len：mysql在<strong>索引里使用的字节数</strong>，通过这个值可以算出具体使用了索引中的哪些列，特别是联合索引。</p>
<p>索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引</p>
</li>
<li>
<p>ref：在key列记录的索引中，表查找值所<strong>用到的列或常量</strong>，常见的有：const（等值查询），字段名（例：film.id）</p>
</li>
<li>
<p>rows：mysql估计要读取并检测的行数，越少越好，注意这个不是结果集里的行数</p>
</li>
<li>
<p>filtered：读取rows行，返回x行，x/rows 返回百分比</p>
</li>
<li>
<p>Extra：额外信息</p>
<ul>
<li>Using index：使用<strong>覆盖</strong>索引 ，性能高</li>
</ul>
<p>覆盖索引定义：mysql执行计划explain结果里的key有使用索引，如果select后面查询的字段都可以从这个索引的树中获取，这种情况一般可以说是用到了覆盖索引，extra里一般都有using index；覆盖索引一般针对的是辅助索引，整个查询结果只通过辅助索引就能拿到结果，不需要通过辅助索引树找到主键，再通过主键去主键索引树里获取其它字段值。</p>
<pre><code class="language-sql">explain select film_id from film_actor where film_id = 1;
</code></pre>
<ul>
<li>Using where：使用 where 语句来处理结果，和是否读取索引无关</li>
</ul>
<pre><code class="language-sql">explain select * from actor where name = 'a';
</code></pre>
<ul>
<li>Using index condition：查询的列<strong>不完全</strong>被索引覆盖，where条件中是一个前导列的范围；</li>
</ul>
<pre><code class="language-sql">explain select * from film_actor where film_id &gt; 1;
</code></pre>
<ul>
<li>Using temporary：排序、分组时等，mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化</li>
</ul>
<pre><code class="language-sql">explain select distinct name from film;
</code></pre>
<ul>
<li>
<p>Using filesort：将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一般也是要考虑使用索引来优化的</p>
</li>
<li>
<p>Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段是</p>
</li>
</ul>
<pre><code class="language-sql">explain select min(id) from film;
</code></pre>
</li>
</ul>
<h3 id="事务"><strong>事务</strong></h3>
<p>原子性、一致性、隔离性、持久性。</p>
<figure data-type="image" tabindex="18"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826205502058.png" alt="" loading="lazy"></figure>
<p>一致性：事务之前id唯一，不能操作之后不唯一了。</p>
<figure data-type="image" tabindex="19"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826205907242.png" alt="" loading="lazy"></figure>
<p>readview只针对查询操作，如果在此期间其他事务插入了新数据，就会导致之幻读。配合间隙锁解决。</p>
<figure data-type="image" tabindex="20"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826210528829.png" alt="" loading="lazy"></figure>
<h3 id="主从同步">主从同步</h3>
<figure data-type="image" tabindex="21"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826211212487.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="22"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826211235973.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="23"><img src="https://memorykki.github.io/post-images/Mysql/image-20210827101814389.png" alt="" loading="lazy"></figure>
<hr>
<h3 id="buffer-pool">Buffer Pool</h3>
<p>客户端 - server（连接器 - 查询缓存 - 分析器 - 优化器 - 执行器） - 存储引擎（查询结果，返回结果集）</p>
<p>Buffer Pool 128MB：查找的结果从磁盘复制到BufferPool，先从pool查，修改先改pool，离散的页数组：</p>
<ul>
<li>
<p>free list：记录空闲的页，便于插入</p>
</li>
<li>
<p>flush list：记录脏页，便于后台线程寻找</p>
</li>
<li>
<p>lru list：pool占满时，最近最久未使用的页被淘汰，头部是新的，尾部是旧的</p>
</li>
</ul>
<p>对于全表扫描，大量数据会将pool换掉。Innodb将lru分为热点区5/8，冷数据区3/8，优先淘汰冷数据区，两次访问到数据的间隔&gt;1s，表示一个正常的频率，转移冷数据-&gt;热数据，全表扫描时间隔小于1s，就不会发生冷替换热的情况。</p>
<hr>
<h3 id="redo-log">redo log</h3>
<p>脏页刷新丢失：</p>
<p>1、修改pool产生脏页</p>
<p>2、生成逻辑redo log（mysql内存中脏页持久化到磁盘需要，挂掉之后重新执行redo恢复） -&gt; log buffer</p>
<p>3、redo log 持久化（当事务提交时）</p>
<p>4、bin log 持久化</p>
<p>5、undo log</p>
<p>6、修改成功</p>
<p>redo(log file0,log file1)file满检查点：将log file持久化到磁盘。logfile太小持久化频繁，太大恢复启动慢。</p>
<p>**持久化机制：**不持久化、立即持久、立即刷新到OS缓存而不立即持久化</p>
<figure data-type="image" tabindex="24"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826145638599.png" alt="" loading="lazy"></figure>
<p>执行事务生成bin log</p>
<table>
<thead>
<tr>
<th>redo</th>
<th>bin log</th>
</tr>
</thead>
<tbody>
<tr>
<td>innodb</td>
<td>mysql级别</td>
</tr>
<tr>
<td>物理的、记录一页的某个位置的数据修改，速度快</td>
<td>逻辑的sql语句，慢。用于主从复制</td>
</tr>
</tbody>
</table>
<p>undo log：反向日志，记录修改之前的数据，回滚使用，实现事物的隔离级别。</p>
<h3 id="double-write"><strong>double write</strong></h3>
<p>（OS中）</p>
<p>innodb数据页16KB，OS页4KB分四次写入。解决问题：如果中途挂了，就不清楚是写入了还是没写入，即没有原子性，所以出现双写缓存。</p>
<p>innodb写入双写缓存即认为成功，并产生redo log，成功之后log失效</p>
<p>刷新一次先写入双写缓存，再写入表空间两部完成。如果第一步挂了 ，这时候可以使用redo log恢复，如果第二部挂了，可以重新从双写缓存拿到完整的数据重新写。</p>
<h3 id="change-buffer"><strong>Change Buffer</strong></h3>
<p>插入缓冲区：写操作更新数据页、索引页，数据页由日志优化，change优化索引页。</p>
<p>储存在buffer pool（索引页+数据页）中，占25%。存储修改的信息（update语句），修改时数据页被更新，但索引页不更新，暂时存在change buffer中，等到下次查询走索引调用的时候将磁盘中的索引调到pool中，再和change中对应的update整合，拿到正确的索引页，即延迟更新机制，使update效率变高了。</p>
<h3 id="可重复读"><strong>可重复读</strong></h3>
<p>开启两个事务，a先读取，b修改此值，a再次读取，结果仍为之前的旧值。</p>
<p>undo实现：每个事务通过链表readview记录和它同时存在的活跃的事务，以及它们的undo log，在第二次查询时，按照记录的undolog，判断生成的事务是否在readview里，若在则执行undolog，这样就可以还原之前的值，然后输出。</p>
<figure data-type="image" tabindex="25"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826155926540.png" alt="" loading="lazy"></figure>
<p>可重复读在第一次读时生成readview，后面使用同一个readview；读已提交在别的事务提交之后发生更新，判断undo链发现不在readview里，就直接取最新值。也就是说，在别的事务提交之后，查找的数据还不是最新的，而是事务启动时候的，读取已提交却可以拿到最新数据，所以可重复读相对更严。</p>
<p>隔离级别越来越严，越难读到最新值，效率也就越低。</p>
<figure data-type="image" tabindex="26"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826161145756.png" alt="" loading="lazy"></figure>
<h3 id="视图"><strong>视图</strong></h3>
<p>视图（View）是一种虚拟存在的表。视图并不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的。通俗的讲，视图就是一条SELECT语句执行后返回的结果集。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。</p>
<p><strong>能够对数据进行修改，但是只能修改一张表中的数据。</strong></p>
<p>视图相对于普通的表的优势主要包括以下几项。</p>
<ul>
<li>简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。</li>
<li>安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。</li>
<li>数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。</li>
</ul>
<h3 id="存储过程"><strong>存储过程</strong></h3>
<p>存储过程和函数是 事先经过编译并存储在数据库中的一段 <strong>SQL 语句的集合</strong>，调用存储过程和函数可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。</p>
<ul>
<li>
<p>函数 ： 是一个有返回值的过程 ；</p>
</li>
<li>
<p>过程 ： 是一个没有返回值的函数 ；</p>
</li>
</ul>
<h3 id="触发器"><strong>触发器</strong></h3>
<p>触发器是与表有关的数据库对象，指在 insert/update/delete 之前或之后，触发并执行触发器中定义的SQL语句集合。触发器的这种特性可以协助应用在数据库端确保数据的完整性 , 日志记录 , 数据校验等操作 。</p>
<h3 id="体系结构"><strong>体系结构</strong></h3>
<figure data-type="image" tabindex="27"><img src="https://memorykki.github.io/post-images/Mysql/image-20210827093733982.png" alt="" loading="lazy"></figure>
<p>整个MySQL Server由以下组成<br>
Connection Pool : 连接池组件<br>
Management Services &amp; Utilities : 管理服务和工具组件<br>
SQL Interface : SQL接口组件<br>
Parser : 查询分析器组件<br>
Optimizer : 优化器组件<br>
Caches &amp; Buffers : 缓冲池组件<br>
Pluggable Storage Engines : 存储引擎<br>
File System : 文件系统</p>
<p>连接层、服务层、引擎层、存储层。</p>
<p>MySQL提供了插件式的存储引擎架构，存储引擎是基于表的，而不是基于库的。</p>
<h3 id="myisam-innodb"><strong>MyISAM InnoDB</strong></h3>
<figure data-type="image" tabindex="28"><img src="https://memorykki.github.io/post-images/Mysql/image-20210826211329868.png" alt="" loading="lazy"></figure>
<p>MYISAM适合查询，InnoDB适合写。</p>
<table>
<thead>
<tr>
<th>特点</th>
<th>InnoDB</th>
<th>MyISAM</th>
<th>MEMORY</th>
<th>MERGE</th>
<th>NDB</th>
</tr>
</thead>
<tbody>
<tr>
<td>存储限制</td>
<td>64TB</td>
<td>有</td>
<td>有</td>
<td>没有</td>
<td>有</td>
</tr>
<tr>
<td>事务安全</td>
<td><mark>支持</mark></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>锁机制</td>
<td><mark>行锁(适合高并发)</mark></td>
<td><mark>表锁</mark></td>
<td>表锁</td>
<td>表锁</td>
<td>行锁</td>
</tr>
<tr>
<td>B树索引</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>哈希索引</td>
<td></td>
<td></td>
<td>支持</td>
<td></td>
<td></td>
</tr>
<tr>
<td>全文索引</td>
<td>支持(5.6版本之后)</td>
<td>支持</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>集群索引</td>
<td>支持</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据索引</td>
<td>支持</td>
<td></td>
<td>支持</td>
<td></td>
<td>支持</td>
</tr>
<tr>
<td>索引缓存</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>数据可压缩</td>
<td></td>
<td>支持</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>空间使用</td>
<td>高</td>
<td>低</td>
<td>N/A</td>
<td>低</td>
<td>低</td>
</tr>
<tr>
<td>内存使用</td>
<td>高</td>
<td>低</td>
<td>中等</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>批量插入速度</td>
<td>低</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>高</td>
</tr>
<tr>
<td>支持外键</td>
<td><mark>支持</mark></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>外键</strong></p>
<ul>
<li>
<p>MySQL支持外键的存储引擎只有InnoDB ， 在创建外键的时候， 要求父表必须有对应的索引 ， 子表在创建外键的时候， 也会自动的创建对应的索引。</p>
</li>
<li>
<p>MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。</p>
</li>
</ul>
<p><strong>存储方式</strong></p>
<ul>
<li>InnoDB 存储表和索引有以下两种方式 ：</li>
</ul>
<p>①. 使用共享表空间存储， 这种方式创建的表的表结构保存在.frm文件中， 数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。</p>
<p>MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。</p>
<p>②. 使用多表空间存储， 这种方式创建的表的表结构仍然存在 .frm 文件中，但是每个表的数据和索引单独保存在 .ibd 中。</p>
<p>MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。</p>
<ul>
<li>
<p>每个MyISAM在磁盘上存储成3个文件，其文件名都和表名相同，但拓展名分别是 ：</p>
<p>.frm (存储表定义)；</p>
<p>.MYD(MYData , 存储数据)；</p>
<p>.MYI(MYIndex , 存储索引)；</p>
</li>
</ul>
<p><strong>选择</strong></p>
<ul>
<li>InnoDB : 是Mysql的默认存储引擎，用于事务处理应用程序，支持外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询意外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。InnoDB存储引擎除了有效的降低由于删除和更新导致的锁定， 还可以确保事务的完整提交和回滚，对于类似于计费系统或者财务系统等对数据准确性要求比较高的系统，InnoDB是最合适的选择。</li>
<li>MyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。</li>
</ul>
<h3 id="避免索引失效"><strong>避免索引失效</strong></h3>
<p>1).  全值匹配 ，对索引中所有列都指定具体值。</p>
<p>改情况下，索引生效，执行效率高。</p>
<pre><code class="language-sql">explain select * from tb_seller where name='小米科技' and status='1' and address='北京市'\G;
</code></pre>
<p>2). 最左前缀法则</p>
<p>如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始，并且不跳过索引中的列。</p>
<p>匹配最左前缀法则，走索引：</p>
<p>违法最左前缀法则 ， 索引失效：</p>
<p>如果符合最左法则，但是出现跳跃某一列，只有最左列索引生效：</p>
<p>3). 范围查询右边的列，不能使用索引 。</p>
<figure data-type="image" tabindex="29"><img src="https://memorykki.github.io/post-images/Mysql/image-20210827095442175.png" alt="" loading="lazy"></figure>
<p>根据前面的两个字段name ， status 查询是走索引的， 但是最后一个条件address 没有用到索引。</p>
<p>4). 不要在索引列上进行运算操作， 索引将失效。</p>
<p>5). 字符串不加单引号，造成索引失效。</p>
<figure data-type="image" tabindex="30"><img src="https://memorykki.github.io/post-images/Mysql/image-20210827095508302.png" alt="" loading="lazy"></figure>
<p>由于，在查询是，没有对字符串加单引号，MySQL的查询优化器，会自动的进行类型转换，造成索引失效。</p>
<p>6). 尽量使用覆盖索引，避免select *</p>
<p>尽量使用覆盖索引（只访问索引的查询（索引列完全包含查询列）），减少select * 。</p>
<p>如果查询列，超出索引列，也会降低性能。</p>
<p>7). 用or分割开的条件， 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。</p>
<p>示例，name字段是索引列 ， 而createtime不是索引列，中间是or进行连接是不走索引的 ：</p>
<pre><code class="language-sql">explain select * from tb_seller where name='黑马程序员' or createtime = '2088-01-01 12:00:00'\G;	
</code></pre>
<figure data-type="image" tabindex="31"><img src="https://memorykki.github.io/post-images/Mysql/image-20210827095641389.png" alt="" loading="lazy"></figure>
<p>8).  以%开头的Like模糊查询，索引失效。</p>
<p>解决：不使用select *，使用覆盖索引select</p>
<p>9). 如果MySQL评估使用索引比全表更慢，则不使用索引。</p>
<p>10). is  NULL ， is NOT NULL  <font color='red'>有时</font>索引失效。</p>
<p>11). in 走索引， not in 索引失效。</p>
<p>12). 单列索引和复合索引。</p>
<p>尽量使用复合索引，而少使用单列索引 。</p>
<h3 id="sql优化"><strong>SQL优化</strong></h3>
<ul>
<li>
<p>大量插入</p>
<ul>
<li>按照主键插入</li>
<li>关闭唯一校验</li>
</ul>
<p>导入数据前执行 SET UNIQUE_CHECKS=0，关闭唯一性校验，在导入结束后执行SET UNIQUE_CHECKS=1，</p>
<ul>
<li>手动提交事务</li>
</ul>
<p>导入前执行 SET AUTOCOMMIT=0，关闭自动提交，导入结束后再执行 SET AUTOCOMMIT=1，打开自动提交</p>
</li>
<li>
<p>insert语句</p>
<ul>
<li>尽量使用一条语句包含多个值插入</li>
<li>开一个事务插入</li>
<li>有序插入</li>
</ul>
</li>
<li>
<p>分页</p>
<p>一般分页查询时，通过创建覆盖索引能够比较好地提高性能。一个常见又非常头疼的问题就是 limit 2000000,10  ，此时需要MySQL排序前2000010 记录，仅仅返回2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。</p>
<ul>
<li>
<p>先查主键，在根据主键查全部记录</p>
<figure data-type="image" tabindex="32"><img src="https://memorykki.github.io/post-images/Mysql/image-20210827100841455.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。</p>
<figure data-type="image" tabindex="33"><img src="https://memorykki.github.io/post-images/Mysql/image-20210827100919667.png" alt="" loading="lazy"></figure>
</li>
</ul>
</li>
<li>
<p>使用SQL提示</p>
<ul>
<li>USE INDEX：提供希望MySQL去参考的索引列表，就可以让MySQL不再考虑其他可用的索引。</li>
<li>IGNORE INDEX：忽略一个或者多个索引</li>
<li>FORCE INDEX：强制MySQL使用一个特定的索引</li>
</ul>
</li>
</ul>
<h3 id="mvcc">MVCC</h3>
<p>https://www.cnblogs.com/xuwc/p/13873611.html</p>
<h4 id="什么是mvcc">什么是MVCC</h4>
<blockquote>
<p>MVCC<br>
<code>MVCC</code>，全称<code>Multi-Version Concurrency Control</code>，即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。</p>
<p>多版本控制: 指的是一种提高并发的技术。最早的数据库系统，只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，<strong>只有写写之间相互阻塞，其他三种操作都可以并行</strong>，这样大幅度提高了InnoDB的并发度。在内部实现中，与Postgres在数据行上实现多版本不同，InnoDB是在undolog中实现的，通过undolog可以找回数据的历史版本。找回的数据历史版本可以提供给用户读(按照隔离级别的定义，有些读请求只能看到比较老的数据版本)，也可以在回滚的时候覆盖数据页上的数据。在InnoDB内部中，会记录一个全局的活跃读写事务数组，其主要用来判断事务的可见性。<br>
MVCC是一种多版本并发控制机制。</p>
</blockquote>
<p>MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读</p>
<hr>
<h4 id="当前读-快照读">当前读 快照读</h4>
<p>在学习MVCC多版本并发控制之前，我们必须先了解一下，什么是MySQL InnoDB下的<code>当前读</code>和<code>快照读</code>?</p>
<ul>
<li>当前读<br>
像select lock in share mode(<code>共享锁</code>), select for update ; update, insert ,delete(<code>排他锁</code>)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁</li>
<li>快照读<br>
像<code>不加锁</code>的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本</li>
</ul>
<p>说白了MVCC就是为了实现读-写冲突不加锁，而这个读指的就是<code>快照读</code>, 而非当前读，当前读实际上是一种加锁的操作，是悲观锁的实现</p>
<hr>
<h4 id="当前读-快照读-mvcc的关系">当前读 快照读 MVCC的关系</h4>
<ul>
<li>准确的说，MVCC多版本并发控制指的是 “维持一个数据的多个版本，使得读写操作没有冲突” 这么一个概念。仅仅是一个理想概念</li>
<li>而在MySQL中，实现这么一个MVCC理想概念，我们就需要MySQL提供具体的功能去实现它，而快照读就是MySQL为我们实现MVCC理想模型的其中一个具体非阻塞读功能。而相对而言，当前读就是悲观锁的具体功能实现</li>
<li>要说的再细致一些，快照读本身也是一个抽象概念，再深入研究。MVCC模型在MySQL中的具体实现则是由 <code>3个隐式字段</code>，<code>undo日志</code> ，<code>Read View</code> 等去完成的，具体可以看下面的MVCC实现原理</li>
</ul>
<hr>
<h4 id="mvcc能解决什么问题">MVCC能解决什么问题</h4>
<p>数据库并发场景有三种，分别为：</p>
<ul>
<li><code>读-读</code>：不存在任何问题，也不需要并发控制</li>
<li><code>读-写</code>：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读</li>
<li><code>写-写</code>：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失</li>
</ul>
<p>备注：第1类丢失更新：事务A撤销时，把已经提交的事务B的更新数据覆盖了；第2类丢失更新：事务A覆盖事务B已经提交的数据，造成事务B所做的操作丢失</p>
<p>MVCC带来的好处是？<br>
多版本并发控制（MVCC）是一种用来解决<code>读-写冲突</code>的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题</p>
<ul>
<li>在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能</li>
<li>同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题</li>
</ul>
<p>小结一下咯<br>
总之，MVCC就是因为大牛们，不满意只让数据库采用悲观锁这样性能不佳的形式去解决读-写冲突问题，而提出的解决方案，所以在数据库中，因为有了MVCC，所以我们可以形成两个组合：</p>
<ul>
<li><code>MVCC + 悲观锁</code><br>
MVCC解决读写冲突，悲观锁解决写写冲突</li>
<li><code>MVCC + 乐观锁</code><br>
MVCC解决读写冲突，乐观锁解决写写冲突</li>
</ul>
<p>这种组合的方式就可以最大程度的提高数据库并发性能，并解决读写冲突，和写写冲突导致的问题</p>
<h3 id="mvcc的实现原理">MVCC的实现原理</h3>
<hr>
<p>MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决<code>读写冲突</code>，它的实现原理主要是依赖记录中的 <code>3个隐式字段</code>，<code>undo日志</code> ，<code>Read View</code> 来实现的。所以我们先来看看这个三个point的概念</p>
<h4 id="隐式字段">隐式字段</h4>
<p>每行记录除了我们自定义的字段外，还有数据库隐式定义的<code>DB_TRX_ID</code>,<code>DB_ROLL_PTR</code>,<code>DB_ROW_ID</code>等字段</p>
<ul>
<li><code>DB_TRX_ID</code><br>
6byte，最近修改(<code>修改/插入</code>)事务ID：记录创建这条记录/最后一次修改该记录的事务ID</li>
<li><code>DB_ROLL_PTR</code><br>
7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里）</li>
<li><code>DB_ROW_ID</code><br>
6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以<code>DB_ROW_ID</code>产生一个聚簇索引</li>
<li>实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20190313213705258.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NuYWlsTWFubg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
如上图，<code>DB_ROW_ID</code>是数据库默认为该行记录生成的唯一隐式主键，<code>DB_TRX_ID</code>是当前操作该记录的事务ID,而<code>DB_ROLL_PTR</code>是一个回滚指针，用于配合undo日志，指向上一个旧版本</p>
<hr>
<h4 id="undo-log">undo log</h4>
<p>undo log主要分为两种：</p>
<ul>
<li>insert undo log<br>
代表事务在<code>insert</code>新记录时产生的<code>undo log</code>, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃</li>
<li>update undo log<br>
事务在进行<code>update</code>或<code>delete</code>时产生的<code>undo log</code>; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被<code>purge</code>线程统一清除</li>
</ul>
<blockquote>
<p>purge</p>
<ul>
<li>从前面的分析可以看出，为了实现InnoDB的MVCC机制，更新或者删除操作都只是设置一下老记录的deleted_bit，并不真正将过时的记录删除。</li>
<li>为了节省磁盘空间，InnoDB有专门的purge线程来清理deleted_bit为true的记录。为了不影响MVCC的正常工作，purge线程自己也维护了一个read view（这个read view相当于系统中最老活跃事务的read view）;如果某个记录的deleted_bit为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。</li>
</ul>
</blockquote>
<p>对MVCC有帮助的实质是<code>update undo log</code> ，<code>undo log</code>实际上就是存在<code>rollback segment</code>中旧记录链，它的执行流程如下：</p>
<p>一、 比如一个有个事务插入persion表插入了一条新记录，记录如下，<code>name</code>为Jerry, <code>age</code>为24岁，<code>隐式主键</code>是1，<code>事务ID</code>和<code>回滚指针</code>，我们假设为NULL</p>
<figure data-type="image" tabindex="34"><img src="https://img-blog.csdnimg.cn/20190313213836406.png" alt="img" loading="lazy"></figure>
<p>二、 现在来了一个<code>事务1</code>对该记录的<code>name</code>做出了修改，改为Tom</p>
<ul>
<li>在<code>事务1</code>修改该行(记录)数据时，数据库会先对该行加<code>排他锁</code></li>
<li>然后把该行数据拷贝到<code>undo log</code>中，作为旧记录，既在<code>undo log</code>中有当前行的拷贝副本</li>
<li>拷贝完毕后，修改该行<code>name</code>为Tom，并且修改隐藏字段的事务ID为当前<code>事务1</code>的ID, 我们默认从<code>1</code>开始，之后递增，回滚指针指向拷贝到<code>undo log</code>的副本记录，既表示我的上一个版本就是它</li>
<li>事务提交后，释放锁</li>
</ul>
<figure data-type="image" tabindex="35"><img src="https://img-blog.csdnimg.cn/20190313220441831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NuYWlsTWFubg==,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<p>三、 又来了个<code>事务2</code>修改<code>person表</code>的同一个记录，将<code>age</code>修改为30岁</p>
<ul>
<li>在<code>事务2</code>修改该行数据时，数据库也先为该行加锁</li>
<li>然后把该行数据拷贝到<code>undo log</code>中，作为旧记录，发现该行记录已经有<code>undo log</code>了，那么最新的旧数据作为链表的表头，插在该行记录的<code>undo log</code>最前面</li>
<li>修改该行<code>age</code>为30岁，并且修改隐藏字段的事务ID为当前<code>事务2</code>的ID, 那就是<code>2</code>，回滚指针指向刚刚拷贝到<code>undo log</code>的副本记录</li>
<li>事务提交，释放锁</li>
</ul>
<figure data-type="image" tabindex="36"><img src="https://img-blog.csdnimg.cn/20190313220528630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NuYWlsTWFubg==,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<p>从上面，我们就可以看出，不同事务或者相同事务的对同一记录的修改，会导致该记录的<code>undo log</code>成为一条记录版本线性表，既链表，<code>undo log</code>的链首就是最新的旧记录，链尾就是最早的旧记录（当然就像之前说的该undo log的节点可能是会purge线程清除掉，向图中的第一条insert undo log，其实在事务提交之后可能就被删除丢失了，不过这里为了演示，所以还放在这里）</p>
<hr>
<h4 id="read-view">Read View</h4>
<p>什么是Read View?</p>
<p>什么是Read View，说白了Read View就是事务进行<code>快照读</code>操作的时候生产的<code>读视图</code>(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)</p>
<p>所以我们知道 <code>Read View</code>主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个<code>Read View</code>读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的<code>undo log</code>里面的某个版本的数据。</p>
<pre><code>Read View`遵循一个可见性算法，主要是将`要被修改的数据`的最新记录中的`DB_TRX_ID`（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果`DB_TRX_ID`跟Read View的属性做了某些比较，不符合可见性，那就通过`DB_ROLL_PTR`回滚指针去取出`Undo Log`中的`DB_TRX_ID`再比较，即遍历链表的`DB_TRX_ID`（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的`DB_TRX_ID`, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新`老版本
</code></pre>
<p>那么这个判断条件是什么呢？即<code>changes_visible</code>方法（不完全哈，但能看出大致逻辑），该方法展示了我们拿DB_TRX_ID去跟Read View某些属性进行怎么样的比较</p>
<p>在展示之前，我先简化一下Read View，我们可以把Read View简单的理解成有三个全局属性</p>
<blockquote>
<ul>
<li><code>trx_list</code>（名字我随便取的）<br>
一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID</li>
<li><code>up_limit_id</code><br>
记录trx_list列表中事务ID最小的ID</li>
<li><code>low_limit_id</code><br>
ReadView生成时刻系统尚未分配的下一个事务ID，也就是<code>目前已出现过的事务ID的最大值+1</code></li>
</ul>
</blockquote>
<ul>
<li>首先比较<code>DB_TRX_ID &lt; up_limit_id</code>, 如果小于，则当前事务能看到<code>DB_TRX_ID</code> 所在的记录，如果大于等于进入下一个判断</li>
<li>接下来判断 <code>DB_TRX_ID 大于等于 low_limit_id</code> , 如果大于等于则代表<code>DB_TRX_ID</code> 所在的记录在<code>Read View</code>生成后才出现的，那对当前事务肯定不可见，如果小于则进入下一个判断</li>
<li>判断<code>DB_TRX_ID</code> 是否在活跃事务之中，<code>trx_list.contains(DB_TRX_ID)</code>，如果在，则代表我<code>Read View</code>生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的；如果不在，则说明，你这个事务在<code>Read View</code>生成之前就已经Commit了，你修改的结果，我当前事务是能看见的</li>
</ul>
<hr>
<h4 id="整体流程">整体流程</h4>
<p>我们在了解了<code>隐式字段</code>，<code>undo log</code>， 以及<code>Read View</code>的概念之后，就可以来看看MVCC实现的整体流程是怎么样了</p>
<p>整体的流程是怎么样的呢？我们可以模拟一下</p>
<ul>
<li>当<code>事务2</code>对某行数据执行了<code>快照读</code>，数据库为该行数据生成一个<code>Read View</code>读视图，假设当前事务ID为<code>2</code>，此时还有<code>事务1</code>和<code>事务3</code>在活跃中，<code>事务4</code>在<code>事务2</code>快照读前一刻提交更新了，所以Read View记录了系统当前活跃事务1，3的ID，维护在一个列表上，假设我们称为<code>trx_list</code></li>
</ul>
<table>
<thead>
<tr>
<th>事务1</th>
<th>事务2</th>
<th>事务3</th>
<th>事务4</th>
</tr>
</thead>
<tbody>
<tr>
<td>事务开始</td>
<td>事务开始</td>
<td>事务开始</td>
<td>事务开始</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>修改且已提交</td>
</tr>
<tr>
<td>进行中</td>
<td>快照读</td>
<td>进行中</td>
<td></td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td></td>
</tr>
</tbody>
</table>
<p>Read View不仅仅会通过一个列表<code>trx_list</code>来维护<code>事务2</code>执行<code>快照读</code>那刻系统正活跃的事务ID，还会有两个属性<code>up_limit_id</code>（记录trx_list列表中事务ID最小的ID），<code>low_limit_id</code>(记录trx_list列表中事务ID最大的ID，也有人说快照读那刻系统尚未分配的下一个事务ID也就是<code>目前已出现过的事务ID的最大值+1</code>，我更倾向于后者 <a href="https://www.zhihu.com/question/66320138/answer/241418502">&gt;&gt;&gt;资料传送门 | 呵呵一笑百媚生的回答</a>) ；所以在这里例子中<code>up_limit_id</code>就是1，<code>low_limit_id</code>就是4 + 1 = 5，trx_list集合的值是1,3，<code>Read View</code>如下图</p>
<figure data-type="image" tabindex="37"><img src="https://img-blog.csdnimg.cn/20190313224045780.png" alt="img" loading="lazy"></figure>
<p>我们的例子中，只有<code>事务4</code>修改过该行记录，并在<code>事务2</code>执行<code>快照读</code>前，就提交了事务，所以当前该行当前数据的<code>undo log</code>如下图所示；我们的事务2在快照读该行记录的时候，就会拿该行记录的<code>DB_TRX_ID</code>去跟<code>up_limit_id</code>,<code>low_limit_id</code>和<code>活跃事务ID列表(trx_list)</code>进行比较，判断当前<code>事务2</code>能看到该记录的版本是哪个。</p>
<figure data-type="image" tabindex="38"><img src="https://img-blog.csdnimg.cn/2019031322511052.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NuYWlsTWFubg==,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<p>所以先拿该记录<code>DB_TRX_ID</code>字段记录的事务ID <code>4</code>去跟<code>Read View</code>的的<code>up_limit_id</code>比较，看<code>4</code>是否小于<code>up_limit_id</code>(1)，所以不符合条件，继续判断 <code>4</code> 是否大于等于 <code>low_limit_id</code>(5)，也不符合条件，最后判断<code>4</code>是否处于<code>trx_list</code>中的活跃事务, 最后发现事务ID为<code>4</code>的事务不在当前活跃事务列表中, 符合可见性条件，所以<code>事务4</code>修改后提交的最新结果对<code>事务2</code>快照读时是可见的，所以<code>事务2</code>能读到的最新数据记录是<code>事务4</code>所提交的版本，而事务4提交的版本也是全局角度上最新的版本</p>
<figure data-type="image" tabindex="39"><img src="https://img-blog.csdnimg.cn/20190911224215559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RJTElHRU5UMjAz,size_16,color_FFFFFF,t_70" alt="# 此处有图片 4" loading="lazy"></figure>
<figure data-type="image" tabindex="40"><img src="https://img-blog.csdnimg.cn/20190314141320189.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NuYWlsTWFubg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>也正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同</p>
<h4 id="rc-rr">RC RR</h4>
<p>正是<code>Read View</code>生成时机的不同，从而造成RC,RR级别下<strong>快照读</strong>的结果的不同</p>
<ul>
<li>在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View, 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见；</li>
<li>即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见</li>
<li>而在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因</li>
</ul>
<p><strong>总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL_黑马]]></title>
        <id>https://memorykki.github.io/Mysql-hm/</id>
        <link href="https://memorykki.github.io/Mysql-hm/">
        </link>
        <updated>2021-08-26T13:23:12.000Z</updated>
        <summary type="html"><![CDATA[<p>Mysql黑马程序员</p>
]]></summary>
        <content type="html"><![CDATA[<p>Mysql黑马程序员</p>
<!-- more -->
<h1 id="mysql高级-day01">Mysql高级-day01</h1>
<h3 id="mysql高级课程简介">MySQL高级课程简介</h3>
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th>Day01</th>
<th>Day02</th>
<th>Day03</th>
<th>Day04</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td>Linux系统安装MySQL</td>
<td>体系结构</td>
<td>应用优化</td>
<td>MySQL 常用工具</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td>索引</td>
<td>存储引擎</td>
<td>查询缓存优化</td>
<td>MySQL 日志</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td>视图</td>
<td>优化SQL步骤</td>
<td>内存管理及优化</td>
<td>MySQL 主从复制</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td>存储过程和函数</td>
<td>索引使用</td>
<td>MySQL锁问题</td>
<td>综合案例</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>触发器</td>
<td>SQL优化</td>
<td>常用SQL技巧</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="1-linux-系统安装mysql">1. Linux 系统安装MySQL</h3>
<h4 id="11-下载linux-安装包">1.1 下载Linux 安装包</h4>
<pre><code>https://dev.mysql.com/downloads/mysql/5.7.html#downloads
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555661091565.png" alt="" loading="lazy"></figure>
<h4 id="12-安装mysql">1.2 安装MySQL</h4>
<pre><code>1). 卸载 centos 中预安装的 mysql
	
	rpm -qa | grep -i mysql
	
	rpm -e mysql-libs-5.1.71-1.el6.x86_64 --nodeps
	
2). 上传 mysql 的安装包
	
	alt + p -------&gt; put  E:/test/MySQL-5.6.22-1.el6.i686.rpm-bundle.tar

3). 解压 mysql 的安装包 
	
	mkdir mysql
	
	tar -xvf MySQL-5.6.22-1.el6.i686.rpm-bundle.tar -C /root/mysql
	
4). 安装依赖包 
	
	yum -y install libaio.so.1 libgcc_s.so.1 libstdc++.so.6 libncurses.so.5 --setopt=protected_multilib=false
	
	yum  update libstdc++-4.4.7-4.el6.x86_64
	
5). 安装 mysql-client
	
	rpm -ivh MySQL-client-5.6.22-1.el6.i686.rpm

6). 安装 mysql-server
	
	rpm -ivh MySQL-server-5.6.22-1.el6.i686.rpm
	
</code></pre>
<h4 id="13-启动-mysql-服务">1.3 启动 MySQL 服务</h4>
<pre><code class="language-SQL">service mysql start

service mysql stop

service mysql status

service mysql restart
</code></pre>
<h4 id="14-登录mysql">1.4 登录MySQL</h4>
<pre><code>mysql 安装完成之后, 会自动生成一个随机的密码, 并且保存在一个密码文件中 : /root/.mysql_secret

mysql -u root -p 

登录之后, 修改密码 :

set password = password('itcast');

授权远程访问 : 

grant all privileges on *.* to 'root' @'%' identified by 'itcast';
flush privileges;

</code></pre>
<h3 id="2-索引">2. 索引</h3>
<h4 id="21-索引概述">2.1 索引概述</h4>
<p>MySQL官方对索引的定义为：索引（index）是帮助MySQL高效获取数据的数据结构（有序）。在数据之外，数据库系统还维护者满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据， 这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。如下面的<mark>示意图</mark>所示 :</p>
<figure data-type="image" tabindex="2"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555902055367.png" alt="" loading="lazy"></figure>
<p>左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找快速获取到相应数据。</p>
<p>一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。索引是数据库中用来提高性能的最常用的工具。</p>
<h4 id="22-索引优势劣势">2.2 索引优势劣势</h4>
<p>优势</p>
<p>1） 类似于书籍的目录索引，提高数据检索的效率，降低数据库的IO成本。</p>
<p>2） 通过索引列对数据进行排序，降低数据排序的成本，降低CPU的消耗。</p>
<p>劣势</p>
<p>1） 实际上索引也是一张表，该表中保存了主键与索引字段，并指向实体类的记录，所以索引列也是要占用空间的。</p>
<p>2） 虽然索引大大提高了查询效率，同时却也降低更新表的速度，如对表进行INSERT、UPDATE、DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。</p>
<h4 id="23-索引结构">2.3 索引结构</h4>
<p>索引是在MySQL的存储引擎层中实现的，而不是在服务器层实现的。所以每种存储引擎的索引都不一定完全相同，也不是所有的存储引擎都支持所有的索引类型的。MySQL目前提供了以下4种索引：</p>
<ul>
<li>BTREE 索引 ： 最常见的索引类型，大部分索引都支持 B 树索引。</li>
<li>HASH 索引：只有Memory引擎支持 ， 使用场景简单 。</li>
<li>R-tree 索引（空间索引）：空间索引是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少，不做特别介绍。</li>
<li>Full-text （全文索引） ：全文索引也是MyISAM的一个特殊索引类型，主要用于全文索引，InnoDB从Mysql5.6版本开始支持全文索引。</li>
</ul>
<center><b>MyISAM、InnoDB、Memory三种存储引擎对各种索引类型的支持</b></center>
<table>
<thead>
<tr>
<th>索引</th>
<th>InnoDB引擎</th>
<th>MyISAM引擎</th>
<th>Memory引擎</th>
</tr>
</thead>
<tbody>
<tr>
<td>BTREE索引</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>HASH 索引</td>
<td>不支持</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>R-tree 索引</td>
<td>不支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>Full-text</td>
<td>5.6版本之后支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
</tbody>
</table>
<p>我们平常所说的索引，如果没有特别指明，都是指B+树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引、复合索引、前缀索引、唯一索引默认都是使用 B+tree 索引，统称为 索引。</p>
<h5 id="231-btree-结构">2.3.1 BTREE 结构</h5>
<p>BTree又叫多路平衡搜索树，一颗m叉的BTree特性如下：</p>
<ul>
<li>树中每个节点最多包含m个孩子。</li>
<li>除根节点与叶子节点外，每个节点至少有[ceil(m/2)]个孩子。</li>
<li>若根节点不是叶子节点，则至少有两个孩子。</li>
<li>所有的叶子节点都在同一层。</li>
<li>每个非叶子节点由n个key与n+1个指针组成，其中[ceil(m/2)-1] &lt;= n &lt;= m-1</li>
</ul>
<p>以5叉BTree为例，key的数量：公式推导[ceil(m/2)-1] &lt;= n &lt;= m-1。所以 2 &lt;= n &lt;=4 。当n&gt;4时，中间节点分裂到父节点，两边节点分裂。</p>
<p>插入 C N G A H E K Q M F W L T Z D P R X Y S 数据为例。</p>
<p>演变过程如下：</p>
<p>1). 插入前4个字母 C N G A</p>
<figure data-type="image" tabindex="3"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555944126588.png" alt="" loading="lazy"></figure>
<p>2). 插入H，n&gt;4，中间元素G字母向上分裂到新的节点</p>
<figure data-type="image" tabindex="4"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555944549825.png" alt="" loading="lazy"></figure>
<p>3). 插入E，K，Q不需要分裂</p>
<figure data-type="image" tabindex="5"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555944596893.png" alt="" loading="lazy"></figure>
<p>4). 插入M，中间元素M字母向上分裂到父节点G</p>
<figure data-type="image" tabindex="6"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555944652560.png" alt="" loading="lazy"></figure>
<p>5). 插入F，W，L，T不需要分裂</p>
<figure data-type="image" tabindex="7"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555944686928.png" alt="" loading="lazy"></figure>
<p>6). 插入Z，中间元素T向上分裂到父节点中</p>
<figure data-type="image" tabindex="8"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555944713486.png" alt="" loading="lazy"></figure>
<p>7). 插入D，中间元素D向上分裂到父节点中。然后插入P，R，X，Y不需要分裂</p>
<figure data-type="image" tabindex="9"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555944749984.png" alt="" loading="lazy"></figure>
<p>8). 最后插入S，NPQR节点n&gt;5，中间节点Q向上分裂，但分裂后父节点DGMT的n&gt;5，中间节点M向上分裂</p>
<figure data-type="image" tabindex="10"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555944848294.png" alt="" loading="lazy"></figure>
<p>到此，该BTREE树就已经构建完成了， BTREE树 和 二叉树 相比， 查询数据的效率更高， 因为对于相同的数据量来说，BTREE的层级结构比二叉树小，因此搜索速度快。</p>
<h5 id="233-btree-结构">2.3.3 B+TREE 结构</h5>
<p>B+Tree为BTree的变种，B+Tree与BTree的区别为：</p>
<p>1). n叉B+Tree最多含有n个key，而BTree最多含有n-1个key。</p>
<p>2). B+Tree的叶子节点保存所有的key信息，依key大小顺序排列。</p>
<p>3). 所有的非叶子节点都可以看作是key的索引部分。</p>
<figure data-type="image" tabindex="11"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/00001.jpg" alt="" loading="lazy"></figure>
<p>由于B+Tree只有叶子节点保存key信息，查询任何key都要从root走到叶子。所以B+Tree的查询效率更加稳定。</p>
<h5 id="233-mysql中的btree">2.3.3 MySQL中的B+Tree</h5>
<p>MySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。</p>
<p>MySQL中的 B+Tree 索引结构示意图:</p>
<figure data-type="image" tabindex="12"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1555906287178.png" alt="" loading="lazy"></figure>
<h4 id="24-索引分类">2.4 索引分类</h4>
<p>1） 单值索引 ：即一个索引只包含单个列，一个表可以有多个单列索引</p>
<p>2） 唯一索引 ：索引列的值必须唯一，但允许有空值</p>
<p>3） 复合索引 ：即一个索引包含多个列</p>
<h4 id="25-索引语法">2.5 索引语法</h4>
<p>索引在创建表的时候，可以同时创建， 也可以随时增加新的索引。</p>
<p>准备环境:</p>
<pre><code class="language-SQL">create database demo_01 default charset=utf8mb4;

use demo_01;

CREATE TABLE `city` (
  `city_id` int(11) NOT NULL AUTO_INCREMENT,
  `city_name` varchar(50) NOT NULL,
  `country_id` int(11) NOT NULL,
  PRIMARY KEY (`city_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

CREATE TABLE `country` (
  `country_id` int(11) NOT NULL AUTO_INCREMENT,
  `country_name` varchar(100) NOT NULL,
  PRIMARY KEY (`country_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;


insert into `city` (`city_id`, `city_name`, `country_id`) values(1,'西安',1);
insert into `city` (`city_id`, `city_name`, `country_id`) values(2,'NewYork',2);
insert into `city` (`city_id`, `city_name`, `country_id`) values(3,'北京',1);
insert into `city` (`city_id`, `city_name`, `country_id`) values(4,'上海',1);

insert into `country` (`country_id`, `country_name`) values(1,'China');
insert into `country` (`country_id`, `country_name`) values(2,'America');
insert into `country` (`country_id`, `country_name`) values(3,'Japan');
insert into `country` (`country_id`, `country_name`) values(4,'UK');
</code></pre>
<h5 id="251-创建索引">2.5.1 创建索引</h5>
<p>语法 ：</p>
<pre><code class="language-sql">CREATE 	[UNIQUE|FULLTEXT|SPATIAL]  INDEX index_name 
[USING  index_type]
ON tbl_name(index_col_name,...)


index_col_name : column_name[(length)][ASC | DESC]
</code></pre>
<p>示例 ： 为city表中的city_name字段创建索引 ；</p>
<p><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1551438009843.png" alt="" loading="lazy">    ​</p>
<p>​</p>
<h5 id="252-查看索引">2.5.2 查看索引</h5>
<p>语法：</p>
<pre><code>show index  from  table_name;
</code></pre>
<p>示例：查看city表中的索引信息；</p>
<figure data-type="image" tabindex="13"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1551440511890.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="14"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1551440544483.png" alt="" loading="lazy"></figure>
<h5 id="253-删除索引">2.5.3 删除索引</h5>
<p>语法 ：</p>
<pre><code>DROP  INDEX  index_name  ON  tbl_name;
</code></pre>
<p>示例 ： 想要删除city表上的索引idx_city_name，可以操作如下：</p>
<figure data-type="image" tabindex="15"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1551438238293.png" alt="" loading="lazy"></figure>
<h5 id="254-alter命令">2.5.4 ALTER命令</h5>
<pre><code>1). alter  table  tb_name  add  primary  key(column_list); 	该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL	2). alter  table  tb_name  add  unique index_name(column_list);		这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）	3). alter  table  tb_name  add  index index_name(column_list);	添加普通索引， 索引值可以出现多次。	4). alter  table  tb_name  add  fulltext  index_name(column_list);		该语句指定了索引为FULLTEXT， 用于全文索引	
</code></pre>
<h4 id="26-索引设计原则">2.6 索引设计原则</h4>
<p>​	索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率，更高效的使用索引。</p>
<ul>
<li>
<p>对查询频次较高，且数据量比较大的表建立索引。</p>
</li>
<li>
<p>索引字段的选择，最佳候选列应当从where子句的条件中提取，如果where子句中的组合比较多，那么应当挑选最常用、过滤效果最好的列的组合。</p>
</li>
<li>
<p>使用唯一索引，区分度越高，使用索引的效率越高。</p>
</li>
<li>
<p>索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价自然也就水涨船高。对于插入、更新、删除等DML操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低DML操作的效率，增加相应操作的时间消耗。另外索引过多的话，MySQL也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但无疑提高了选择的代价。</p>
</li>
<li>
<p>使用短索引，索引创建之后也是使用硬盘来存储的，因此提升索引访问的I/O效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升MySQL访问索引的I/O效率。</p>
</li>
<li>
<p>利用最左前缀，N个列组合而成的组合索引，那么相当于是创建了N个索引，如果查询时where子句中使用了组成该索引的前几个字段，那么这条查询SQL可以利用组合索引来提升查询效率。</p>
<pre><code>创建复合索引:	CREATE INDEX idx_name_email_status ON tb_seller(NAME,email,STATUS);就相当于	对name 创建索引 ;	对name , email 创建了索引 ;	对name , email, status 创建了索引 ;
</code></pre>
</li>
</ul>
<h3 id="3-视图">3. 视图</h3>
<h4 id="31-视图概述">3.1 视图概述</h4>
<p>​	视图（View）是一种虚拟存在的表。视图并不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的。通俗的讲，视图就是一条SELECT语句执行后返回的结果集。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。</p>
<p>视图相对于普通的表的优势主要包括以下几项。</p>
<ul>
<li>简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。</li>
<li>安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。</li>
<li>数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。</li>
</ul>
<h4 id="32-创建或者修改视图">3.2 创建或者修改视图</h4>
<p>创建视图的语法为：</p>
<pre><code class="language-sql">CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION]
</code></pre>
<p>修改视图的语法为：</p>
<pre><code class="language-sql">ALTER [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION]
</code></pre>
<pre><code>选项 : 	WITH [CASCADED | LOCAL] CHECK OPTION 决定了是否允许更新数据使记录不再满足视图的条件。		LOCAL ： 只要满足本视图的条件就可以更新。	CASCADED ： 必须满足所有针对该视图的所有视图的条件才可以更新。 默认值.
</code></pre>
<p>示例 , 创建city_country_view视图 , 执行如下SQL :</p>
<pre><code class="language-sql">create or replace view city_country_view as select t.*,c.country_name from country c , city t where c.country_id = t.country_id;
</code></pre>
<p>查询视图 :</p>
<figure data-type="image" tabindex="16"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1551503428635.png" alt="" loading="lazy"></figure>
<h4 id="33-查看视图">3.3 查看视图</h4>
<p>​	从 MySQL 5.1 版本开始，使用 SHOW TABLES 命令的时候不仅显示表的名字，同时也会显示视图的名字，而不存在单独显示视图的 SHOW VIEWS 命令。</p>
<figure data-type="image" tabindex="17"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1551537565159.png" alt="" loading="lazy"></figure>
<p>同样，在使用 SHOW TABLE STATUS 命令的时候，不但可以显示表的信息，同时也可以显示视图的信息。</p>
<figure data-type="image" tabindex="18"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1551537646323.png" alt="" loading="lazy"></figure>
<p>如果需要查询某个视图的定义，可以使用 SHOW CREATE VIEW 命令进行查看 ：</p>
<figure data-type="image" tabindex="19"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1551588962944.png" alt="" loading="lazy"></figure>
<h4 id="34-删除视图">3.4 删除视图</h4>
<p>语法 :</p>
<pre><code class="language-sql">DROP VIEW [IF EXISTS] view_name [, view_name] ...[RESTRICT | CASCADE]	
</code></pre>
<p>示例 , 删除视图city_country_view :</p>
<pre><code>DROP VIEW city_country_view ;
</code></pre>
<h3 id="4-存储过程和函数">4. 存储过程和函数</h3>
<h4 id="41-存储过程和函数概述">4.1 存储过程和函数概述</h4>
<p>​	存储过程和函数是  事先经过编译并存储在数据库中的一段 SQL 语句的集合，调用存储过程和函数可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。</p>
<p>​	存储过程和函数的区别在于函数必须有返回值，而存储过程没有。</p>
<p>​	函数 ： 是一个有返回值的过程 ；</p>
<p>​	过程 ： 是一个没有返回值的函数 ；</p>
<h4 id="42-创建存储过程">4.2 创建存储过程</h4>
<pre><code class="language-sql">CREATE PROCEDURE procedure_name ([proc_parameter[,...]])begin	-- SQL语句end ;
</code></pre>
<p>示例 ：</p>
<pre><code class="language-sql">delimiter $create procedure pro_test1()begin	select 'Hello Mysql' ;end$delimiter ;
</code></pre>
<p><strong><font color="red">知识小贴士</font></strong></p>
<p>DELIMITER</p>
<p>​	该关键字用来声明SQL语句的分隔符 , 告诉 MySQL 解释器，该段命令是否已经结束了，mysql是否可以执行了。默认情况下，delimiter是分号;。在命令行客户端中，如果有一行命令以分号结束，那么回车后，mysql将会执行该命令。</p>
<h4 id="43-调用存储过程">4.3 调用存储过程</h4>
<pre><code class="language-sql">call procedure_name() ;	
</code></pre>
<h4 id="44-查看存储过程">4.4 查看存储过程</h4>
<pre><code class="language-sql">-- 查询db_name数据库中的所有的存储过程select name from mysql.proc where db='db_name';-- 查询存储过程的状态信息show procedure status;-- 查询某个存储过程的定义show create procedure test.pro_test1 \G;
</code></pre>
<h4 id="45-删除存储过程">4.5 删除存储过程</h4>
<pre><code class="language-sql">DROP PROCEDURE  [IF EXISTS] sp_name ；
</code></pre>
<h4 id="46-语法">4.6 语法</h4>
<p>存储过程是可以编程的，意味着可以使用变量，表达式，控制结构 ， 来完成比较复杂的功能。</p>
<h5 id="461-变量">4.6.1 变量</h5>
<ul>
<li>
<p>DECLARE</p>
<p>通过 DECLARE 可以定义一个局部变量，该变量的作用范围只能在 BEGIN…END 块中。</p>
</li>
</ul>
<pre><code class="language-sql">DECLARE var_name[,...] type [DEFAULT value]
</code></pre>
<p>示例 :</p>
<pre><code class="language-sql"> delimiter $ create procedure pro_test2()  begin  	declare num int default 5; 	select num+ 10;  end$ delimiter ; 
</code></pre>
<ul>
<li>SET</li>
</ul>
<p>直接赋值使用 SET，可以赋常量或者赋表达式，具体语法如下：</p>
<pre><code>  SET var_name = expr [, var_name = expr] ...
</code></pre>
<p>示例 :</p>
<pre><code class="language-sql">  DELIMITER $    CREATE  PROCEDURE pro_test3()  BEGIN  	DECLARE NAME VARCHAR(20);  	SET NAME = 'MYSQL';  	SELECT NAME ;  END$    DELIMITER ;
</code></pre>
<p>也可以通过select ... into 方式进行赋值操作 :</p>
<pre><code class="language-SQL">DELIMITER $CREATE  PROCEDURE pro_test5()BEGIN	declare  countnum int;	select count(*) into countnum from city;	select countnum;END$DELIMITER ;
</code></pre>
<h5 id="462-if条件判断">4.6.2 if条件判断</h5>
<p>语法结构 :</p>
<pre><code class="language-sql">if search_condition then statement_list	[elseif search_condition then statement_list] ...		[else statement_list]	end if;
</code></pre>
<p>需求：</p>
<pre><code>根据定义的身高变量，判定当前身高的所属的身材类型 	180 及以上 ----------&gt; 身材高挑	170 - 180  ---------&gt; 标准身材	170 以下  ----------&gt; 一般身材
</code></pre>
<p>示例 :</p>
<pre><code class="language-sql">delimiter $create procedure pro_test6()begin  declare  height  int  default  175;   declare  description  varchar(50);    if  height &gt;= 180  then    set description = '身材高挑';  elseif height &gt;= 170 and height &lt; 180  then    set description = '标准身材';  else    set description = '一般身材';  end if;    select description ;end$delimiter ;
</code></pre>
<p>调用结果为 :</p>
<figure data-type="image" tabindex="20"><img src="https://memorykki.github.io/post-images/MySQL_hm/1/1552057035580.png" alt="" loading="lazy"></figure>
<h5 id="463-传递参数">4.6.3 传递参数</h5>
<p>语法格式 :</p>
<pre><code>create procedure procedure_name([in/out/inout] 参数名   参数类型)...IN :   该参数可以作为输入，也就是需要调用方传入值 , 默认OUT:   该参数作为输出，也就是该参数可以作为返回值INOUT: 既可以作为输入参数，也可以作为输出参数
</code></pre>
<p><strong>IN - 输入</strong></p>
<p>需求 :</p>
<pre><code>根据定义的身高变量，判定当前身高的所属的身材类型 
</code></pre>
<p>示例  :</p>
<pre><code class="language-sql">delimiter $create procedure pro_test5(in height int)begin    declare description varchar(50) default '';  if height &gt;= 180 then    set description='身材高挑';  elseif height &gt;= 170 and height &lt; 180 then    set description='标准身材';  else    set description='一般身材';  end if;  select concat('身高 ', height , '对应的身材类型为:',description);end$delimiter ;
</code></pre>
<p><strong>OUT-输出</strong></p>
<p>需求 :</p>
<pre><code>根据传入的身高变量，获取当前身高的所属的身材类型  
</code></pre>
<p>示例:</p>
<pre><code class="language-SQL">create procedure pro_test5(in height int , out description varchar(100))begin  if height &gt;= 180 then    set description='身材高挑';  elseif height &gt;= 170 and height &lt; 180 then    set description='标准身材';  else    set description='一般身材';  end if;end$	
</code></pre>
<p>调用:</p>
<pre><code>call pro_test5(168, @description)$select @description$
</code></pre>
<p><font color='red'><strong>小知识</strong> </font></p>
<p>@description :  这种变量要在变量名称前面加上“@”符号，叫做用户会话变量，代表整个会话过程他都是有作用的，这个类似于全局变量一样。</p>
<p>@@global.sort_buffer_size : 这种在变量前加上 &quot;@@&quot; 符号, 叫做 系统变量</p>
<h5 id="464-case结构">4.6.4 case结构</h5>
<p>语法结构 :</p>
<pre><code class="language-SQL">方式一 : CASE case_value  WHEN when_value THEN statement_list    [WHEN when_value THEN statement_list] ...    [ELSE statement_list]  END CASE;方式二 : CASE  WHEN search_condition THEN statement_list    [WHEN search_condition THEN statement_list] ...    [ELSE statement_list]  END CASE;
</code></pre>
<p>需求:</p>
<pre><code>给定一个月份, 然后计算出所在的季度
</code></pre>
<p>示例  :</p>
<pre><code class="language-sql">delimiter $create procedure pro_test9(month int)begin  declare result varchar(20);  case     when month &gt;= 1 and month &lt;=3 then       set result = '第一季度';    when month &gt;= 4 and month &lt;=6 then       set result = '第二季度';    when month &gt;= 7 and month &lt;=9 then       set result = '第三季度';    when month &gt;= 10 and month &lt;=12 then       set result = '第四季度';  end case;    select concat('您输入的月份为 :', month , ' , 该月份为 : ' , result) as content ;  end$delimiter ;
</code></pre>
<h5 id="465-while循环">4.6.5 while循环</h5>
<p>语法结构:</p>
<pre><code class="language-sql">while search_condition do	statement_list	end while;
</code></pre>
<p>需求:</p>
<pre><code>计算从1加到n的值
</code></pre>
<p>示例  :</p>
<pre><code class="language-sql">delimiter $create procedure pro_test8(n int)begin  declare total int default 0;  declare num int default 1;  while num&lt;=n do    set total = total + num;	set num = num + 1;  end while;  select total;end$delimiter ;
</code></pre>
<h5 id="466-repeat结构">4.6.6 repeat结构</h5>
<p>有条件的循环控制语句, 当满足条件的时候退出循环 。while 是满足条件才执行，repeat 是满足条件就退出循环。</p>
<p>语法结构 :</p>
<pre><code class="language-SQL">REPEAT  statement_list  UNTIL search_conditionEND REPEAT;
</code></pre>
<p>需求:</p>
<pre><code>计算从1加到n的值
</code></pre>
<p>示例  :</p>
<pre><code class="language-sql">delimiter $create procedure pro_test10(n int)begin  declare total int default 0;    repeat     set total = total + n;    set n = n - 1;    until n=0    end repeat;    select total ;  end$delimiter ;
</code></pre>
<h5 id="467-loop语句">4.6.7 loop语句</h5>
<p>LOOP 实现简单的循环，退出循环的条件需要使用其他的语句定义，通常可以使用 LEAVE 语句实现，具体语法如下：</p>
<pre><code class="language-sql">[begin_label:] LOOP  statement_listEND LOOP [end_label]
</code></pre>
<p>如果不在 statement_list 中增加退出循环的语句，那么 LOOP 语句可以用来实现简单的死循环。</p>
<h5 id="468-leave语句">4.6.8 leave语句</h5>
<p>用来从标注的流程构造中退出，通常和 BEGIN ... END 或者循环一起使用。下面是一个使用 LOOP 和 LEAVE 的简单例子 , 退出循环：</p>
<pre><code class="language-SQL">delimiter $CREATE PROCEDURE pro_test11(n int)BEGIN  declare total int default 0;    ins: LOOP        IF n &lt;= 0 then      leave ins;    END IF;        set total = total + n;    set n = n - 1;  	  END LOOP ins;    select total;END$delimiter ;
</code></pre>
<h5 id="469-游标光标">4.6.9 游标/光标</h5>
<p>游标是用来存储查询结果集的数据类型 , 在存储过程和函数中可以使用光标对结果集进行循环的处理。光标的使用包括光标的声明、OPEN、FETCH 和 CLOSE，其语法分别如下。</p>
<p>声明光标：</p>
<pre><code class="language-sql">DECLARE cursor_name CURSOR FOR select_statement ;
</code></pre>
<p>OPEN 光标：</p>
<pre><code class="language-sql">OPEN cursor_name ;
</code></pre>
<p>FETCH 光标：</p>
<pre><code class="language-sql">FETCH cursor_name INTO var_name [, var_name] ...
</code></pre>
<p>CLOSE 光标：</p>
<pre><code class="language-sql">CLOSE cursor_name ;
</code></pre>
<p>示例 :</p>
<p>初始化脚本:</p>
<pre><code class="language-sql">create table emp(  id int(11) not null auto_increment ,  name varchar(50) not null comment '姓名',  age int(11) comment '年龄',  salary int(11) comment '薪水',  primary key(`id`))engine=innodb default charset=utf8 ;insert into emp(id,name,age,salary) values(null,'金毛狮王',55,3800),(null,'白眉鹰王',60,4000),(null,'青翼蝠王',38,2800),(null,'紫衫龙王',42,1800);
</code></pre>
<pre><code class="language-SQL">-- 查询emp表中数据, 并逐行获取进行展示create procedure pro_test11()begin  declare e_id int(11);  declare e_name varchar(50);  declare e_age int(11);  declare e_salary int(11);  declare emp_result cursor for select * from emp;    open emp_result;    fetch emp_result into e_id,e_name,e_age,e_salary;  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);    fetch emp_result into e_id,e_name,e_age,e_salary;  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);    fetch emp_result into e_id,e_name,e_age,e_salary;  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);    fetch emp_result into e_id,e_name,e_age,e_salary;  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);    fetch emp_result into e_id,e_name,e_age,e_salary;  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);    close emp_result;end$
</code></pre>
<p>通过循环结构 , 获取游标中的数据 :</p>
<pre><code class="language-sql">DELIMITER $create procedure pro_test12()begin  DECLARE id int(11);  DECLARE name varchar(50);  DECLARE age int(11);  DECLARE salary int(11);  DECLARE has_data int default 1;    DECLARE emp_result CURSOR FOR select * from emp;  DECLARE EXIT HANDLER FOR NOT FOUND set has_data = 0;    open emp_result;    repeat    fetch emp_result into id , name , age , salary;    select concat('id为',id, ', name 为' ,name , ', age为 ' ,age , ', 薪水为: ', salary);    until has_data = 0  end repeat;    close emp_result;end$DELIMITER ; 
</code></pre>
<h4 id="47-存储函数">4.7 存储函数</h4>
<p>语法结构:</p>
<pre><code>CREATE FUNCTION function_name([param type ... ]) RETURNS type BEGIN	...END;
</code></pre>
<p>案例 :</p>
<p>定义一个存储过程, 请求满足条件的总记录数 ;</p>
<pre><code class="language-SQL">delimiter $create function count_city(countryId int)returns intbegin  declare cnum int ;    select count(*) into cnum from city where country_id = countryId;    return cnum;end$delimiter ;
</code></pre>
<p>调用:</p>
<pre><code>select count_city(1);select count_city(2);
</code></pre>
<h3 id="5-触发器">5. 触发器</h3>
<h4 id="51-介绍">5.1 介绍</h4>
<p>触发器是与表有关的数据库对象，指在 insert/update/delete 之前或之后，触发并执行触发器中定义的SQL语句集合。触发器的这种特性可以协助应用在数据库端确保数据的完整性 , 日志记录 , 数据校验等操作 。</p>
<p>使用别名 OLD 和 NEW 来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。</p>
<table>
<thead>
<tr>
<th>触发器类型</th>
<th>NEW 和 OLD的使用</th>
</tr>
</thead>
<tbody>
<tr>
<td>INSERT 型触发器</td>
<td>NEW 表示将要或者已经新增的数据</td>
</tr>
<tr>
<td>UPDATE 型触发器</td>
<td>OLD 表示修改之前的数据 , NEW 表示将要或已经修改后的数据</td>
</tr>
<tr>
<td>DELETE 型触发器</td>
<td>OLD 表示将要或者已经删除的数据</td>
</tr>
</tbody>
</table>
<h4 id="52-创建触发器">5.2 创建触发器</h4>
<p>语法结构 :</p>
<pre><code class="language-sql">create trigger trigger_name before/after insert/update/deleteon tbl_name [ for each row ]  -- 行级触发器begin	trigger_stmt ;end;
</code></pre>
<p>示例</p>
<p>需求</p>
<pre><code>通过触发器记录 emp 表的数据变更日志 , 包含增加, 修改 , 删除 ;
</code></pre>
<p>首先创建一张日志表 :</p>
<pre><code class="language-sql">create table emp_logs(  id int(11) not null auto_increment,  operation varchar(20) not null comment '操作类型, insert/update/delete',  operate_time datetime not null comment '操作时间',  operate_id int(11) not null comment '操作表的ID',  operate_params varchar(500) comment '操作参数',  primary key(`id`))engine=innodb default charset=utf8;
</code></pre>
<p>创建 insert 型触发器，完成插入数据时的日志记录 :</p>
<pre><code class="language-sql">DELIMITER $create trigger emp_logs_insert_triggerafter insert on emp for each row begin  insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'insert',now(),new.id,concat('插入后(id:',new.id,', name:',new.name,', age:',new.age,', salary:',new.salary,')'));	end $DELIMITER ;
</code></pre>
<p>创建 update 型触发器，完成更新数据时的日志记录 :</p>
<pre><code class="language-sql">DELIMITER $create trigger emp_logs_update_triggerafter update on emp for each row begin  insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'update',now(),new.id,concat('修改前(id:',old.id,', name:',old.name,', age:',old.age,', salary:',old.salary,') , 修改后(id',new.id, 'name:',new.name,', age:',new.age,', salary:',new.salary,')'));                                                                      end $DELIMITER ;
</code></pre>
<p>创建delete 行的触发器 , 完成删除数据时的日志记录 :</p>
<pre><code class="language-sql">DELIMITER $create trigger emp_logs_delete_triggerafter delete on emp for each row begin  insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'delete',now(),old.id,concat('删除前(id:',old.id,', name:',old.name,', age:',old.age,', salary:',old.salary,')'));                                                                      end $DELIMITER ;
</code></pre>
<p>测试：</p>
<pre><code class="language-sql">insert into emp(id,name,age,salary) values(null, '光明左使',30,3500);insert into emp(id,name,age,salary) values(null, '光明右使',33,3200);update emp set age = 39 where id = 3;delete from emp where id = 5;
</code></pre>
<h4 id="53-删除触发器">5.3 删除触发器</h4>
<p>语法结构 :</p>
<pre><code>drop trigger [schema_name.]trigger_name
</code></pre>
<p>如果没有指定 schema_name，默认为当前数据库 。</p>
<h4 id="54-查看触发器">5.4 查看触发器</h4>
<p>可以通过执行 SHOW TRIGGERS 命令查看触发器的状态、语法等信息。</p>
<p>语法结构 ：</p>
<pre><code>show triggers ；
</code></pre>
<h1 id="mysql高级-day02">Mysql高级-day02</h1>
<h3 id="1-mysql的体系结构概览">1. Mysql的体系结构概览</h3>
<figure data-type="image" tabindex="21"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/000001.jpg" alt="" loading="lazy"></figure>
<p>整个MySQL Server由以下组成</p>
<ul>
<li>Connection Pool : 连接池组件</li>
<li>Management Services &amp; Utilities : 管理服务和工具组件</li>
<li>SQL Interface : SQL接口组件</li>
<li>Parser : 查询分析器组件</li>
<li>Optimizer : 优化器组件</li>
<li>Caches &amp; Buffers : 缓冲池组件</li>
<li>Pluggable Storage Engines : 存储引擎</li>
<li>File System : 文件系统</li>
</ul>
<p>1） 连接层</p>
<p>最上层是一些客户端和链接服务，包含本地sock 通信和大多数基于客户端/服务端工具实现的类似于 TCP/IP的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。</p>
<p>2） 服务层</p>
<p>第二层架构主要完成大多数的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化，部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如 过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询的顺序，是否利用索引等， 最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存，如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。</p>
<p>3） 引擎层</p>
<p>存储引擎层， 存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API和存储引擎进行通信。不同的存储引擎具有不同的功能，这样我们可以根据自己的需要，来选取合适的存储引擎。</p>
<p>4）存储层</p>
<p>数据存储层， 主要是将数据存储在文件系统之上，并完成与存储引擎的交互。</p>
<p>和其他数据库相比，MySQL有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎上，插件式的存储引擎架构，将查询处理和其他的系统任务以及数据的存储提取分离。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。</p>
<h3 id="2-存储引擎">2. 存储引擎</h3>
<h4 id="21-存储引擎概述">2.1 存储引擎概述</h4>
<p>​	和大多数的数据库不同, MySQL中有一个存储引擎的概念, 针对不同的存储需求可以选择最优的存储引擎。</p>
<p>​	存储引擎就是存储数据，建立索引，更新查询数据等等技术的实现方式 。存储引擎是基于表的，而不是基于库的。所以存储引擎也可被称为表类型。</p>
<p>​	Oracle，SqlServer等数据库只有一种存储引擎。MySQL提供了插件式的存储引擎架构。所以MySQL存在多种存储引擎，可以根据需要使用相应引擎，或者编写存储引擎。</p>
<p>​	MySQL5.0支持的存储引擎包含 ： InnoDB 、MyISAM 、BDB、MEMORY、MERGE、EXAMPLE、NDB Cluster、ARCHIVE、CSV、BLACKHOLE、FEDERATED等，其中InnoDB和BDB提供事务安全表，其他存储引擎是非事务安全表。</p>
<p>可以通过指定 show engines ， 来查询当前数据库支持的存储引擎 ：</p>
<figure data-type="image" tabindex="22"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1551186043529.png" alt="" loading="lazy"></figure>
<p>创建新表时如果不指定存储引擎，那么系统就会使用默认的存储引擎，MySQL5.5之前的默认存储引擎是MyISAM，5.5之后就改为了InnoDB。</p>
<p>查看Mysql数据库默认的存储引擎 ， 指令 ：</p>
<pre><code> show variables like '%storage_engine%' ； 
</code></pre>
<figure data-type="image" tabindex="23"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556086372754.png" alt="" loading="lazy"></figure>
<h4 id="22-各种存储引擎特性">2.2 各种存储引擎特性</h4>
<p>下面重点介绍几种常用的存储引擎， 并对比各个存储引擎之间的区别， 如下表所示 ：</p>
<table>
<thead>
<tr>
<th>特点</th>
<th>InnoDB</th>
<th>MyISAM</th>
<th>MEMORY</th>
<th>MERGE</th>
<th>NDB</th>
</tr>
</thead>
<tbody>
<tr>
<td>存储限制</td>
<td>64TB</td>
<td>有</td>
<td>有</td>
<td>没有</td>
<td>有</td>
</tr>
<tr>
<td>事务安全</td>
<td><mark>支持</mark></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>锁机制</td>
<td><mark>行锁(适合高并发)</mark></td>
<td><mark>表锁</mark></td>
<td>表锁</td>
<td>表锁</td>
<td>行锁</td>
</tr>
<tr>
<td>B树索引</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>哈希索引</td>
<td></td>
<td></td>
<td>支持</td>
<td></td>
<td></td>
</tr>
<tr>
<td>全文索引</td>
<td>支持(5.6版本之后)</td>
<td>支持</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>集群索引</td>
<td>支持</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据索引</td>
<td>支持</td>
<td></td>
<td>支持</td>
<td></td>
<td>支持</td>
</tr>
<tr>
<td>索引缓存</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>数据可压缩</td>
<td></td>
<td>支持</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>空间使用</td>
<td>高</td>
<td>低</td>
<td>N/A</td>
<td>低</td>
<td>低</td>
</tr>
<tr>
<td>内存使用</td>
<td>高</td>
<td>低</td>
<td>中等</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>批量插入速度</td>
<td>低</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>高</td>
</tr>
<tr>
<td>支持外键</td>
<td><mark>支持</mark></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>下面我们将重点介绍最长使用的两种存储引擎： InnoDB、MyISAM ， 另外两种 MEMORY、MERGE ， 了解即可。</p>
<h5 id="221-innodb">2.2.1 InnoDB</h5>
<p>​	InnoDB存储引擎是Mysql的默认存储引擎。InnoDB存储引擎提供了具有提交、回滚、崩溃恢复能力的事务安全。但是对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引。</p>
<p>InnoDB存储引擎不同于其他存储引擎的特点 ：</p>
<p><strong>事务控制</strong></p>
<pre><code>create table goods_innodb(
	id int NOT NULL AUTO_INCREMENT,
	name varchar(20) NOT NULL,
    primary key(id)
)ENGINE=innodb DEFAULT CHARSET=utf8;
</code></pre>
<pre><code>start transaction;

insert into goods_innodb(id,name)values(null,'Meta20');

commit;
</code></pre>
<figure data-type="image" tabindex="24"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556075130115.png" alt="" loading="lazy"></figure>
<p>测试，发现在InnoDB中是存在事务的 ；</p>
<p><strong>外键约束</strong></p>
<p>​	MySQL支持外键的存储引擎只有InnoDB ， 在创建外键的时候， 要求父表必须有对应的索引 ， 子表在创建外键的时候， 也会自动的创建对应的索引。</p>
<p>​	下面两张表中 ， country_innodb是父表 ， country_id为主键索引，city_innodb表是子表，country_id字段为外键，对应于country_innodb表的主键country_id 。</p>
<pre><code class="language-sql">create table country_innodb(
	country_id int NOT NULL AUTO_INCREMENT,
    country_name varchar(100) NOT NULL,
    primary key(country_id)
)ENGINE=InnoDB DEFAULT CHARSET=utf8;


create table city_innodb(
	city_id int NOT NULL AUTO_INCREMENT,
    city_name varchar(50) NOT NULL,
    country_id int NOT NULL,
    primary key(city_id),
    key idx_fk_country_id(country_id),
    CONSTRAINT `fk_city_country` FOREIGN KEY(country_id) REFERENCES country_innodb(country_id) ON DELETE RESTRICT ON UPDATE CASCADE
)ENGINE=InnoDB DEFAULT CHARSET=utf8;



insert into country_innodb values(null,'China'),(null,'America'),(null,'Japan');
insert into city_innodb values(null,'Xian',1),(null,'NewYork',2),(null,'BeiJing',1);

</code></pre>
<p>在创建索引时， 可以指定在删除、更新父表时，对子表进行的相应操作，包括 RESTRICT、CASCADE、SET NULL 和 NO ACTION。</p>
<p>RESTRICT和NO ACTION相同， 是指限制在子表有关联记录的情况下， 父表不能更新；</p>
<p>CASCADE表示父表在更新或者删除时，更新或者删除子表对应的记录；</p>
<p>SET NULL 则表示父表在更新或者删除的时候，子表的对应字段被SET NULL 。</p>
<p>针对上面创建的两个表， 子表的外键指定是ON DELETE RESTRICT ON UPDATE CASCADE 方式的， 那么在主表删除记录的时候， 如果子表有对应记录， 则不允许删除， 主表在更新记录的时候， 如果子表有对应记录， 则子表对应更新 。</p>
<p>表中数据如下图所示 ：</p>
<figure data-type="image" tabindex="25"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556087540767.png" alt="" loading="lazy"></figure>
<p>外键信息可以使用如下两种方式查看 ：</p>
<pre><code>show create table city_innodb ;
</code></pre>
<figure data-type="image" tabindex="26"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556087611295.png" alt="" loading="lazy"></figure>
<p>删除country_id为1 的country数据：</p>
<pre><code> delete from country_innodb where country_id = 1;
</code></pre>
<figure data-type="image" tabindex="27"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556087719145.png" alt="" loading="lazy"></figure>
<p>更新主表country表的字段 country_id :</p>
<pre><code>update country_innodb set country_id = 100 where country_id = 1;
</code></pre>
<figure data-type="image" tabindex="28"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556087759615.png" alt="" loading="lazy"></figure>
<p>更新后， 子表的数据信息为 ：</p>
<figure data-type="image" tabindex="29"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556087793738.png" alt="" loading="lazy"></figure>
<p><strong>存储方式</strong></p>
<p>InnoDB 存储表和索引有以下两种方式 ：</p>
<p>①. 使用共享表空间存储， 这种方式创建的表的表结构保存在.frm文件中， 数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。</p>
<p>②. 使用多表空间存储， 这种方式创建的表的表结构仍然存在 .frm 文件中，但是每个表的数据和索引单独保存在 .ibd 中。</p>
<figure data-type="image" tabindex="30"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556075336630.png" alt="" loading="lazy"></figure>
<h5 id="222-myisam">2.2.2 MyISAM</h5>
<p>​	MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。有以下两个比较重要的特点：</p>
<p><strong>不支持事务</strong></p>
<pre><code class="language-sql">create table goods_myisam(
	id int NOT NULL AUTO_INCREMENT,
	name varchar(20) NOT NULL,
    primary key(id)
)ENGINE=myisam DEFAULT CHARSET=utf8;
</code></pre>
<figure data-type="image" tabindex="31"><img src="E:/%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-02/%E6%96%87%E6%A1%A3/images/MySQL_hm/2/1551347590309.png" alt="1551347590309" loading="lazy"></figure>
<p>通过测试，我们发现，在MyISAM存储引擎中，是没有事务控制的 ；</p>
<p><strong>文件存储方式</strong></p>
<p>每个MyISAM在磁盘上存储成3个文件，其文件名都和表名相同，但拓展名分别是 ：</p>
<p>.frm (存储表定义)；</p>
<p>.MYD(MYData , 存储数据)；</p>
<p>.MYI(MYIndex , 存储索引)；</p>
<figure data-type="image" tabindex="32"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556075073836.png" alt="" loading="lazy"></figure>
<h5 id="223-memory">2.2.3 MEMORY</h5>
<p>​	Memory存储引擎将表的数据存放在内存中。每个MEMORY表实际对应一个磁盘文件，格式是.frm ，该文件中只存储表的结构，而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。MEMORY 类型的表访问非常地快，因为他的数据是存放在内存中的，并且默认使用HASH索引 ， 但是服务一旦关闭，表中的数据就会丢失。</p>
<h5 id="224-merge">2.2.4 MERGE</h5>
<p>​	MERGE存储引擎是一组MyISAM表的组合，这些MyISAM表必须结构完全相同，MERGE表本身并没有存储数据，对MERGE类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部的MyISAM表进行的。</p>
<p>​	对于MERGE类型表的插入操作，是通过INSERT_METHOD子句定义插入的表，可以有3个不同的值，使用FIRST 或 LAST 值使得插入操作被相应地作用在第一或者最后一个表上，不定义这个子句或者定义为NO，表示不能对这个MERGE表执行插入操作。</p>
<p>​	可以对MERGE表进行DROP操作，但是这个操作只是删除MERGE表的定义，对内部的表是没有任何影响的。</p>
<figure data-type="image" tabindex="33"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556076359503.png" alt="" loading="lazy"></figure>
<p>下面是一个创建和使用MERGE表的示例 ：</p>
<p>1）. 创建3个测试表 order_1990, order_1991, order_all , 其中order_all是前两个表的MERGE表 ：</p>
<pre><code class="language-sql">create table order_1990(	order_id int ,	order_money double(10,2),	order_address varchar(50),	primary key (order_id))engine = myisam default charset=utf8;create table order_1991(	order_id int ,	order_money double(10,2),	order_address varchar(50),	primary key (order_id))engine = myisam default charset=utf8;create table order_all(	order_id int ,	order_money double(10,2),	order_address varchar(50),	primary key (order_id))engine = merge union = (order_1990,order_1991) INSERT_METHOD=LAST default charset=utf8;
</code></pre>
<p>2）. 分别向两张表中插入记录</p>
<pre><code class="language-sql">insert into order_1990 values(1,100.0,'北京');insert into order_1990 values(2,100.0,'上海');insert into order_1991 values(10,200.0,'北京');insert into order_1991 values(11,200.0,'上海');
</code></pre>
<p>3）. 查询3张表中的数据。</p>
<p>order_1990中的数据 ：</p>
<figure data-type="image" tabindex="34"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1551408083254.png" alt="" loading="lazy"></figure>
<p>order_1991中的数据 ：</p>
<figure data-type="image" tabindex="35"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1551408133323.png" alt="" loading="lazy"></figure>
<p>order_all中的数据 ：</p>
<figure data-type="image" tabindex="36"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1551408216185.png" alt="" loading="lazy"></figure>
<p>​</p>
<p>4）. 往order_all中插入一条记录 ，由于在MERGE表定义时，INSERT_METHOD 选择的是LAST，那么插入的数据会想最后一张表中插入。</p>
<pre><code class="language-sql">insert into order_all values(100,10000.0,'西安')；
</code></pre>
<figure data-type="image" tabindex="37"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1551408519889.png" alt="" loading="lazy"></figure>
<h4 id="23-存储引擎的选择">2.3 存储引擎的选择</h4>
<p>​	在选择存储引擎时，应该根据应用系统的特点选择合适的存储引擎。对于复杂的应用系统，还可以根据实际情况选择多种存储引擎进行组合。以下是几种常用的存储引擎的使用环境。</p>
<ul>
<li>InnoDB : 是Mysql的默认存储引擎，用于事务处理应用程序，支持外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询意外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。InnoDB存储引擎除了有效的降低由于删除和更新导致的锁定， 还可以确保事务的完整提交和回滚，对于类似于计费系统或者财务系统等对数据准确性要求比较高的系统，InnoDB是最合适的选择。</li>
<li>MyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。</li>
<li>MEMORY：将所有数据保存在RAM中，在需要快速定位记录和其他类似数据环境下，可以提供几块的访问。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，其次是要确保表的数据可以恢复，数据库异常终止后表中的数据是可以恢复的。MEMORY表通常用于更新不太频繁的小表，用以快速得到访问结果。</li>
<li>MERGE：用于将一系列等同的MyISAM表以逻辑方式组合在一起，并作为一个对象引用他们。MERGE表的优点在于可以突破对单个MyISAM表的大小限制，并且通过将不同的表分布在多个磁盘上，可以有效的改善MERGE表的访问效率。这对于存储诸如数据仓储等VLDB环境十分合适。</li>
</ul>
<h3 id="3-优化sql步骤">3. 优化SQL步骤</h3>
<p>在应用的的开发过程中，由于初期数据量小，开发人员写 SQL 语句时更重视功能上的实现，但是当应用系统正式上线后，随着生产数据量的急剧增长，很多 SQL 语句开始逐渐显露出性能问题，对生产的影响也越来越大，此时这些有问题的 SQL 语句就成为整个系统性能的瓶颈，因此我们必须要对它们进行优化，本章将详细介绍在 MySQL 中优化 SQL 语句的方法。</p>
<p>当面对一个有 SQL 性能问题的数据库时，我们应该从何处入手来进行系统的分析，使得能够尽快定位问题 SQL 并尽快解决问题。</p>
<h4 id="31-查看sql执行频率">3.1 查看SQL执行频率</h4>
<p>MySQL 客户端连接成功后，通过 show [session|global] status 命令可以提供服务器状态信息。show [session|global] status 可以根据需要加上参数“session”或者“global”来显示 session 级（当前连接）的计结果和 global 级（自数据库上次启动至今）的统计结果。如果不写，默认使用参数是“session”。</p>
<p>下面的命令显示了当前 session 中所有统计参数的值：</p>
<pre><code>show status like 'Com_______';
</code></pre>
<figure data-type="image" tabindex="38"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552487172501.png" alt="" loading="lazy"></figure>
<pre><code>show status like 'Innodb_rows_%';
</code></pre>
<figure data-type="image" tabindex="39"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552487245859.png" alt="" loading="lazy"></figure>
<p>Com_xxx 表示每个 xxx 语句执行的次数，我们通常比较关心的是以下几个统计参数。</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Com_select</td>
<td>执行 select 操作的次数，一次查询只累加 1。</td>
</tr>
<tr>
<td style="text-align:left">Com_insert</td>
<td>执行 INSERT 操作的次数，对于批量插入的 INSERT 操作，只累加一次。</td>
</tr>
<tr>
<td style="text-align:left">Com_update</td>
<td>执行 UPDATE 操作的次数。</td>
</tr>
<tr>
<td style="text-align:left">Com_delete</td>
<td>执行 DELETE 操作的次数。</td>
</tr>
<tr>
<td style="text-align:left">Innodb_rows_read</td>
<td>select 查询返回的行数。</td>
</tr>
<tr>
<td style="text-align:left">Innodb_rows_inserted</td>
<td>执行 INSERT 操作插入的行数。</td>
</tr>
<tr>
<td style="text-align:left">Innodb_rows_updated</td>
<td>执行 UPDATE 操作更新的行数。</td>
</tr>
<tr>
<td style="text-align:left">Innodb_rows_deleted</td>
<td>执行 DELETE 操作删除的行数。</td>
</tr>
<tr>
<td style="text-align:left">Connections</td>
<td>试图连接 MySQL 服务器的次数。</td>
</tr>
<tr>
<td style="text-align:left">Uptime</td>
<td>服务器工作时间。</td>
</tr>
<tr>
<td style="text-align:left">Slow_queries</td>
<td>慢查询的次数。</td>
</tr>
</tbody>
</table>
<p>Com_***      :  这些参数对于所有存储引擎的表操作都会进行累计。</p>
<p>Innodb_*** :  这几个参数只是针对InnoDB 存储引擎的，累加的算法也略有不同。</p>
<h4 id="32-定位低效率执行sql">3.2 定位低效率执行SQL</h4>
<p>可以通过以下两种方式定位执行效率较低的 SQL 语句。</p>
<ul>
<li>慢查询日志 : 通过慢查询日志定位那些执行效率较低的 SQL 语句，用--log-slow-queries[=file_name]选项启动时，mysqld 写一个包含所有执行时间超过 long_query_time 秒的 SQL 语句的日志文件。具体可以查看本书第 26 章中日志管理的相关部分。</li>
<li>show processlist  : 慢查询日志在查询结束以后才纪录，所以在应用反映执行效率出现问题的时候查询慢查询日志并不能定位问题，可以使用show processlist命令查看当前MySQL在进行的线程，包括线程的状态、是否锁表等，可以实时地查看 SQL 的执行情况，同时对一些锁表操作进行优化。</li>
</ul>
<figure data-type="image" tabindex="40"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556098544349.png" alt="" loading="lazy"></figure>
<pre><code>1） id列，用户登录mysql时，系统分配的&quot;connection_id&quot;，可以使用函数connection_id()查看2） user列，显示当前用户。如果不是root，这个命令就只显示用户权限范围的sql语句3） host列，显示这个语句是从哪个ip的哪个端口上发的，可以用来跟踪出现问题语句的用户4） db列，显示这个进程目前连接的是哪个数据库5） command列，显示当前连接的执行的命令，一般取值为休眠（sleep），查询（query），连接（connect）等6） time列，显示这个状态持续的时间，单位是秒7） state列，显示使用当前连接的sql语句的状态，很重要的列。state描述的是语句执行中的某一个状态。一个sql语句，以查询为例，可能需要经过copying to tmp table、sorting result、sending data等状态才可以完成8） info列，显示这个sql语句，是判断问题语句的一个重要依据
</code></pre>
<h4 id="33-explain分析执行计划">3.3 explain分析执行计划</h4>
<p>通过以上步骤查询到效率低的 SQL 语句后，可以通过 EXPLAIN或者 DESC命令获取 MySQL如何执行 SELECT 语句的信息，包括在 SELECT 语句执行过程中表如何连接和连接的顺序。</p>
<p>查询SQL语句的执行计划 ：</p>
<pre><code class="language-sql">explain  select * from tb_item where id = 1;
</code></pre>
<figure data-type="image" tabindex="41"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552487489859.png" alt="" loading="lazy"></figure>
<pre><code class="language-sql">explain  select * from tb_item where title = '阿尔卡特 (OT-979) 冰川白 联通3G手机3';
</code></pre>
<figure data-type="image" tabindex="42"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552487526919.png" alt="" loading="lazy"></figure>
<table>
<thead>
<tr>
<th>字段</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。</td>
</tr>
<tr>
<td>select_type</td>
<td>表示 SELECT 的类型，常见的取值有 SIMPLE（简单表，即不使用表连接或者子查询）、PRIMARY（主查询，即外层的查询）、UNION（UNION 中的第二个或者后面的查询语句）、SUBQUERY（子查询中的第一个 SELECT）等</td>
</tr>
<tr>
<td>table</td>
<td>输出结果集的表</td>
</tr>
<tr>
<td>type</td>
<td>表示表的连接类型，性能由好到差的连接类型为( system  ---&gt;  const  -----&gt;  eq_ref  ------&gt;  ref  -------&gt;  ref_or_null----&gt;  index_merge  ---&gt;  index_subquery  -----&gt;  range  -----&gt;  index  ------&gt; all )</td>
</tr>
<tr>
<td>possible_keys</td>
<td>表示查询时，可能使用的索引</td>
</tr>
<tr>
<td>key</td>
<td>表示实际使用的索引</td>
</tr>
<tr>
<td>key_len</td>
<td>索引字段的长度</td>
</tr>
<tr>
<td>rows</td>
<td>扫描行的数量</td>
</tr>
<tr>
<td>extra</td>
<td>执行情况的说明和描述</td>
</tr>
</tbody>
</table>
<h5 id="331-环境准备">3.3.1 环境准备</h5>
<figure data-type="image" tabindex="43"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556122799330.png" alt="" loading="lazy"></figure>
<pre><code class="language-sql">CREATE TABLE `t_role` (  `id` varchar(32) NOT NULL,  `role_name` varchar(255) DEFAULT NULL,  `role_code` varchar(255) DEFAULT NULL,  `description` varchar(255) DEFAULT NULL,  PRIMARY KEY (`id`),  UNIQUE KEY `unique_role_name` (`role_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `t_user` (  `id` varchar(32) NOT NULL,  `username` varchar(45) NOT NULL,  `password` varchar(96) NOT NULL,  `name` varchar(45) NOT NULL,  PRIMARY KEY (`id`),  UNIQUE KEY `unique_user_username` (`username`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `user_role` (  `id` int(11) NOT NULL auto_increment ,  `user_id` varchar(32) DEFAULT NULL,  `role_id` varchar(32) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `fk_ur_user_id` (`user_id`),  KEY `fk_ur_role_id` (`role_id`),  CONSTRAINT `fk_ur_role_id` FOREIGN KEY (`role_id`) REFERENCES `t_role` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION,  CONSTRAINT `fk_ur_user_id` FOREIGN KEY (`user_id`) REFERENCES `t_user` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into `t_user` (`id`, `username`, `password`, `name`) values('1','super','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','超级管理员');insert into `t_user` (`id`, `username`, `password`, `name`) values('2','admin','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','系统管理员');insert into `t_user` (`id`, `username`, `password`, `name`) values('3','itcast','$2a$10$8qmaHgUFUAmPR5pOuWhYWOr291WJYjHelUlYn07k5ELF8ZCrW0Cui','test02');insert into `t_user` (`id`, `username`, `password`, `name`) values('4','stu1','$2a$10$pLtt2KDAFpwTWLjNsmTEi.oU1yOZyIn9XkziK/y/spH5rftCpUMZa','学生1');insert into `t_user` (`id`, `username`, `password`, `name`) values('5','stu2','$2a$10$nxPKkYSez7uz2YQYUnwhR.z57km3yqKn3Hr/p1FR6ZKgc18u.Tvqm','学生2');insert into `t_user` (`id`, `username`, `password`, `name`) values('6','t1','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','老师1');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('5','学生','student','学生');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('7','老师','teacher','老师');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('8','教学管理员','teachmanager','教学管理员');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('9','管理员','admin','管理员');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('10','超级管理员','super','超级管理员');INSERT INTO user_role(id,user_id,role_id) VALUES(NULL, '1', '5'),(NULL, '1', '7'),(NULL, '2', '8'),(NULL, '3', '9'),(NULL, '4', '8'),(NULL, '5', '10') ;
</code></pre>
<h5 id="332-explain-之-id">3.3.2 explain 之 id</h5>
<p>id 字段是 select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。id 情况有三种 ：</p>
<p>1） id 相同表示加载表的顺序是从上到下。</p>
<pre><code>explain select * from t_role r, t_user u, user_role ur where r.id = ur.role_id and u.id = ur.user_id ;
</code></pre>
<figure data-type="image" tabindex="44"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556102471304.png" alt="" loading="lazy"></figure>
<p>2） id 不同id值越大，优先级越高，越先被执行。</p>
<pre><code class="language-SQL">EXPLAIN SELECT * FROM t_role WHERE id = (SELECT role_id FROM user_role WHERE user_id = (SELECT id FROM t_user WHERE username = 'stu1'))
</code></pre>
<figure data-type="image" tabindex="45"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556103009534.png" alt="" loading="lazy"></figure>
<p>3） id 有相同，也有不同，同时存在。id相同的可以认为是一组，从上往下顺序执行；在所有的组中，id的值越大，优先级越高，越先执行。</p>
<pre><code class="language-sql">EXPLAIN SELECT * FROM t_role r , (SELECT * FROM user_role ur WHERE ur.`user_id` = '2') a WHERE r.id = a.role_id ; 
</code></pre>
<figure data-type="image" tabindex="46"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556103294182.png" alt="" loading="lazy"></figure>
<h5 id="333-explain-之-select_type">3.3.3 explain 之 select_type</h5>
<p>表示 SELECT 的类型，常见的取值，如下表所示：</p>
<table>
<thead>
<tr>
<th>select_type</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>SIMPLE</td>
<td>简单的select查询，查询中不包含子查询或者UNION</td>
</tr>
<tr>
<td>PRIMARY</td>
<td>查询中若包含任何复杂的子查询，最外层查询标记为该标识</td>
</tr>
<tr>
<td>SUBQUERY</td>
<td>在SELECT 或 WHERE 列表中包含了子查询</td>
</tr>
<tr>
<td>DERIVED</td>
<td>在FROM 列表中包含的子查询，被标记为 DERIVED（衍生） MYSQL会递归执行这些子查询，把结果放在临时表中</td>
</tr>
<tr>
<td>UNION</td>
<td>若第二个SELECT出现在UNION之后，则标记为UNION ； 若UNION包含在FROM子句的子查询中，外层SELECT将被标记为 ： DERIVED</td>
</tr>
<tr>
<td>UNION RESULT</td>
<td>从UNION表获取结果的SELECT</td>
</tr>
</tbody>
</table>
<h5 id="334-explain-之-table">3.3.4 explain 之 table</h5>
<p>展示这一行的数据是关于哪一张表的</p>
<h5 id="335-explain-之-type">3.3.5 explain 之 type</h5>
<p>type 显示的是访问类型，是较为重要的一个指标，可取值为：</p>
<table>
<thead>
<tr>
<th>type</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>NULL</td>
<td>MySQL不访问任何表，索引，直接返回结果</td>
</tr>
<tr>
<td>system</td>
<td>表只有一行记录(等于系统表)，这是const类型的特例，一般不会出现</td>
</tr>
<tr>
<td>const</td>
<td>表示通过索引一次就找到了，const 用于比较primary key 或者 unique 索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL 就能将该查询转换为一个常亮。const于将 &quot;主键&quot; 或 &quot;唯一&quot; 索引的所有部分与常量值进行比较</td>
</tr>
<tr>
<td>eq_ref</td>
<td>类似ref，区别在于使用的是唯一索引，使用主键的关联查询，关联查询出的记录只有一条。常见于主键或唯一索引扫描</td>
</tr>
<tr>
<td>ref</td>
<td>非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，返回所有匹配某个单独值的所有行（多个）</td>
</tr>
<tr>
<td>range</td>
<td>只检索给定返回的行，使用一个索引来选择行。 where 之后出现 between ， &lt; , &gt; , in 等操作。</td>
</tr>
<tr>
<td>index</td>
<td>index 与 ALL的区别为  index 类型只是遍历了索引树， 通常比ALL 快， ALL 是遍历数据文件。</td>
</tr>
<tr>
<td>all</td>
<td>将遍历全表以找到匹配的行</td>
</tr>
</tbody>
</table>
<p>结果值从最好到最坏以此是：</p>
<pre><code>NULL &gt; system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALLsystem &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL
</code></pre>
<p><mark>一般来说， 我们需要保证查询至少达到 range 级别， 最好达到ref 。</mark></p>
<h5 id="336-explain-之-key">3.3.6 explain 之  key</h5>
<pre><code>possible_keys : 显示可能应用在这张表的索引， 一个或多个。 key ： 实际使用的索引， 如果为NULL， 则没有使用索引。key_len : 表示索引中使用的字节数， 该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下， 长度越短越好 。
</code></pre>
<h5 id="337-explain-之-rows">3.3.7 explain 之 rows</h5>
<p>扫描行的数量。</p>
<h5 id="338-explain-之-extra">3.3.8 explain 之 extra</h5>
<p>其他的额外的执行计划信息，在该列展示 。</p>
<table>
<thead>
<tr>
<th>extra</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>using  filesort</td>
<td>说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取， 称为 “文件排序”, 效率低。</td>
</tr>
<tr>
<td>using  temporary</td>
<td>使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于 order by 和 group by； 效率低</td>
</tr>
<tr>
<td>using  index</td>
<td>表示相应的select操作使用了覆盖索引， 避免访问表的数据行， 效率不错。</td>
</tr>
</tbody>
</table>
<h4 id="34-show-profile分析sql">3.4 show profile分析SQL</h4>
<p>Mysql从5.0.37版本开始增加了对 show profiles 和 show profile 语句的支持。show profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪里去了。</p>
<p>通过 have_profiling 参数，能够看到当前MySQL是否支持profile：</p>
<figure data-type="image" tabindex="47"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552488401999.png" alt="" loading="lazy"></figure>
<p>默认profiling是关闭的，可以通过set语句在Session级别开启profiling：</p>
<figure data-type="image" tabindex="48"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552488372405.png" alt="" loading="lazy"></figure>
<pre><code class="language-sql">set profiling=1; //开启profiling 开关；
</code></pre>
<p>通过profile，我们能够更清楚地了解SQL执行的过程。</p>
<p>首先，我们可以执行一系列的操作，如下图所示：</p>
<pre><code class="language-sql">show databases;use db01;show tables;select * from tb_item where id &lt; 5;select count(*) from tb_item;
</code></pre>
<p>执行完上述命令之后，再执行show profiles 指令， 来查看SQL语句执行的耗时：</p>
<figure data-type="image" tabindex="49"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552489017940.png" alt="" loading="lazy"></figure>
<p>通过show  profile for  query  query_id 语句可以查看到该SQL执行过程中每个线程的状态和消耗的时间：</p>
<figure data-type="image" tabindex="50"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552489053763.png" alt="" loading="lazy"></figure>
<pre><code class="language-tex">TIP ：	Sending data 状态表示MySQL线程开始访问数据行并把结果返回给客户端，而不仅仅是返回个客户端。由于在Sending data状态下，MySQL线程往往需要做大量的磁盘读取操作，所以经常是整各查询中耗时最长的状态。
</code></pre>
<p>在获取到最消耗时间的线程状态后，MySQL支持进一步选择all、cpu、block io 、context switch、page faults等明细类型类查看MySQL在使用什么资源上耗费了过高的时间。例如，选择查看CPU的耗费时间  ：</p>
<figure data-type="image" tabindex="51"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552489671119.png" alt="" loading="lazy"></figure>
<table>
<thead>
<tr>
<th>字段</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>Status</td>
<td>sql 语句执行的状态</td>
</tr>
<tr>
<td>Duration</td>
<td>sql 执行过程中每一个步骤的耗时</td>
</tr>
<tr>
<td>CPU_user</td>
<td>当前用户占有的cpu</td>
</tr>
<tr>
<td>CPU_system</td>
<td>系统占有的cpu</td>
</tr>
</tbody>
</table>
<h4 id="35-trace分析优化器执行计划">3.5 trace分析优化器执行计划</h4>
<p>MySQL5.6提供了对SQL的跟踪trace, 通过trace文件能够进一步了解为什么优化器选择A计划, 而不是选择B计划。</p>
<p>打开trace ， 设置格式为 JSON，并设置trace最大能够使用的内存大小，避免解析过程中因为默认内存过小而不能够完整展示。</p>
<pre><code class="language-sql">SET optimizer_trace=&quot;enabled=on&quot;,end_markers_in_json=on;set optimizer_trace_max_mem_size=1000000;
</code></pre>
<p>执行SQL语句 ：</p>
<pre><code class="language-sql">select * from tb_item where id &lt; 4;
</code></pre>
<p>最后， 检查information_schema.optimizer_trace就可以知道MySQL是如何执行SQL的 ：</p>
<pre><code class="language-sql">select * from information_schema.optimizer_trace\G;
</code></pre>
<pre><code class="language-json">*************************** 1. row ***************************QUERY: select * from tb_item where id &lt; 4TRACE: {  &quot;steps&quot;: [    {      &quot;join_preparation&quot;: {        &quot;select#&quot;: 1,        &quot;steps&quot;: [          {            &quot;expanded_query&quot;: &quot;/* select#1 */ select `tb_item`.`id` AS `id`,`tb_item`.`title` AS `title`,`tb_item`.`price` AS `price`,`tb_item`.`num` AS `num`,`tb_item`.`categoryid` AS `categoryid`,`tb_item`.`status` AS `status`,`tb_item`.`sellerid` AS `sellerid`,`tb_item`.`createtime` AS `createtime`,`tb_item`.`updatetime` AS `updatetime` from `tb_item` where (`tb_item`.`id` &lt; 4)&quot;          }        ] /* steps */      } /* join_preparation */    },    {      &quot;join_optimization&quot;: {        &quot;select#&quot;: 1,        &quot;steps&quot;: [          {            &quot;condition_processing&quot;: {              &quot;condition&quot;: &quot;WHERE&quot;,              &quot;original_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot;,              &quot;steps&quot;: [                {                  &quot;transformation&quot;: &quot;equality_propagation&quot;,                  &quot;resulting_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot;                },                {                  &quot;transformation&quot;: &quot;constant_propagation&quot;,                  &quot;resulting_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot;                },                {                  &quot;transformation&quot;: &quot;trivial_condition_removal&quot;,                  &quot;resulting_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot;                }              ] /* steps */            } /* condition_processing */          },          {            &quot;table_dependencies&quot;: [              {                &quot;table&quot;: &quot;`tb_item`&quot;,                &quot;row_may_be_null&quot;: false,                &quot;map_bit&quot;: 0,                &quot;depends_on_map_bits&quot;: [                ] /* depends_on_map_bits */              }            ] /* table_dependencies */          },          {            &quot;ref_optimizer_key_uses&quot;: [            ] /* ref_optimizer_key_uses */          },          {            &quot;rows_estimation&quot;: [              {                &quot;table&quot;: &quot;`tb_item`&quot;,                &quot;range_analysis&quot;: {                  &quot;table_scan&quot;: {                    &quot;rows&quot;: 9816098,                    &quot;cost&quot;: 2.04e6                  } /* table_scan */,                  &quot;potential_range_indices&quot;: [                    {                      &quot;index&quot;: &quot;PRIMARY&quot;,                      &quot;usable&quot;: true,                      &quot;key_parts&quot;: [                        &quot;id&quot;                      ] /* key_parts */                    }                  ] /* potential_range_indices */,                  &quot;setup_range_conditions&quot;: [                  ] /* setup_range_conditions */,                  &quot;group_index_range&quot;: {                    &quot;chosen&quot;: false,                    &quot;cause&quot;: &quot;not_group_by_or_distinct&quot;                  } /* group_index_range */,                  &quot;analyzing_range_alternatives&quot;: {                    &quot;range_scan_alternatives&quot;: [                      {                        &quot;index&quot;: &quot;PRIMARY&quot;,                        &quot;ranges&quot;: [                          &quot;id &lt; 4&quot;                        ] /* ranges */,                        &quot;index_dives_for_eq_ranges&quot;: true,                        &quot;rowid_ordered&quot;: true,                        &quot;using_mrr&quot;: false,                        &quot;index_only&quot;: false,                        &quot;rows&quot;: 3,                        &quot;cost&quot;: 1.6154,                        &quot;chosen&quot;: true                      }                    ] /* range_scan_alternatives */,                    &quot;analyzing_roworder_intersect&quot;: {                      &quot;usable&quot;: false,                      &quot;cause&quot;: &quot;too_few_roworder_scans&quot;                    } /* analyzing_roworder_intersect */                  } /* analyzing_range_alternatives */,                  &quot;chosen_range_access_summary&quot;: {                    &quot;range_access_plan&quot;: {                      &quot;type&quot;: &quot;range_scan&quot;,                      &quot;index&quot;: &quot;PRIMARY&quot;,                      &quot;rows&quot;: 3,                      &quot;ranges&quot;: [                        &quot;id &lt; 4&quot;                      ] /* ranges */                    } /* range_access_plan */,                    &quot;rows_for_plan&quot;: 3,                    &quot;cost_for_plan&quot;: 1.6154,                    &quot;chosen&quot;: true                  } /* chosen_range_access_summary */                } /* range_analysis */              }            ] /* rows_estimation */          },          {            &quot;considered_execution_plans&quot;: [              {                &quot;plan_prefix&quot;: [                ] /* plan_prefix */,                &quot;table&quot;: &quot;`tb_item`&quot;,                &quot;best_access_path&quot;: {                  &quot;considered_access_paths&quot;: [                    {                      &quot;access_type&quot;: &quot;range&quot;,                      &quot;rows&quot;: 3,                      &quot;cost&quot;: 2.2154,                      &quot;chosen&quot;: true                    }                  ] /* considered_access_paths */                } /* best_access_path */,                &quot;cost_for_plan&quot;: 2.2154,                &quot;rows_for_plan&quot;: 3,                &quot;chosen&quot;: true              }            ] /* considered_execution_plans */          },          {            &quot;attaching_conditions_to_tables&quot;: {              &quot;original_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot;,              &quot;attached_conditions_computation&quot;: [              ] /* attached_conditions_computation */,              &quot;attached_conditions_summary&quot;: [                {                  &quot;table&quot;: &quot;`tb_item`&quot;,                  &quot;attached&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot;                }              ] /* attached_conditions_summary */            } /* attaching_conditions_to_tables */          },          {            &quot;refine_plan&quot;: [              {                &quot;table&quot;: &quot;`tb_item`&quot;,                &quot;access_type&quot;: &quot;range&quot;              }            ] /* refine_plan */          }        ] /* steps */      } /* join_optimization */    },    {      &quot;join_execution&quot;: {        &quot;select#&quot;: 1,        &quot;steps&quot;: [        ] /* steps */      } /* join_execution */    }  ] /* steps */}
</code></pre>
<h3 id="4-索引的使用">4. 索引的使用</h3>
<p>索引是数据库优化最常用也是最重要的手段之一, 通过索引通常可以帮助用户解决大多数的MySQL的性能优化问题。</p>
<h4 id="41-验证索引提升查询效率">4.1 验证索引提升查询效率</h4>
<p>在我们准备的表结构tb_item 中， 一共存储了 300 万记录；</p>
<p>A. 根据ID查询</p>
<pre><code>select * from tb_item where id = 1999\G;
</code></pre>
<figure data-type="image" tabindex="52"><img src="E:/%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-02/%E6%96%87%E6%A1%A3/images/MySQL_hm/2/1553261992653.png" alt="1553261992653" loading="lazy"></figure>
<p>查询速度很快， 接近0s ， 主要的原因是因为id为主键， 有索引；</p>
<figure data-type="image" tabindex="53"><img src="E:/%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-02/%E6%96%87%E6%A1%A3/images/MySQL_hm/2/1553262044466.png" alt="1553262044466" loading="lazy"></figure>
<p>2). 根据 title 进行精确查询</p>
<pre><code class="language-SQL">select * from tb_item where title = 'iphoneX 移动3G 32G941'\G; 
</code></pre>
<figure data-type="image" tabindex="54"><img src="E:/%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-02/%E6%96%87%E6%A1%A3/images/MySQL_hm/2/1553262215900.png" alt="1553262215900" loading="lazy"></figure>
<p>查看SQL语句的执行计划 ：</p>
<figure data-type="image" tabindex="55"><img src="E:/%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-02/%E6%96%87%E6%A1%A3/images/MySQL_hm/2/1553262469785.png" alt="1553262469785" loading="lazy"></figure>
<p>处理方案 ， 针对title字段， 创建索引 ：</p>
<pre><code class="language-SQL">create index idx_item_title on tb_item(title);
</code></pre>
<figure data-type="image" tabindex="56"><img src="E:/%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-02/%E6%96%87%E6%A1%A3/images/MySQL_hm/2/1553263229523.png" alt="1553263229523" loading="lazy"></figure>
<p>索引创建完成之后，再次进行查询 ：</p>
<figure data-type="image" tabindex="57"><img src="E:/%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-02/%E6%96%87%E6%A1%A3/images/MySQL_hm/2/1553263302706.png" alt="1553263302706" loading="lazy"></figure>
<p>通过explain ， 查看执行计划，执行SQL时使用了刚才创建的索引</p>
<figure data-type="image" tabindex="58"><img src="E:/%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-02/%E6%96%87%E6%A1%A3/images/MySQL_hm/2/1553263355262.png" alt="1553263355262" loading="lazy"></figure>
<h4 id="42-索引的使用">4.2 索引的使用</h4>
<h5 id="421-准备环境">4.2.1 准备环境</h5>
<pre><code class="language-sql">create table `tb_seller` (	`sellerid` varchar (100),	`name` varchar (100),	`nickname` varchar (50),	`password` varchar (60),	`status` varchar (1),	`address` varchar (100),	`createtime` datetime,    primary key(`sellerid`))engine=innodb default charset=utf8mb4; insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('alibaba','阿里巴巴','阿里小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('baidu','百度科技有限公司','百度小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('huawei','华为科技有限公司','华为小店','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('itcast','传智播客教育科技有限公司','传智播客','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('itheima','黑马程序员','黑马程序员','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('luoji','罗技科技有限公司','罗技小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('oppo','OPPO科技有限公司','OPPO官方旗舰店','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('ourpalm','掌趣科技股份有限公司','掌趣小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('qiandu','千度科技','千度小店','e10adc3949ba59abbe56e057f20f883e','2','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('sina','新浪科技有限公司','新浪官方旗舰店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('xiaomi','小米科技','小米官方旗舰店','e10adc3949ba59abbe56e057f20f883e','1','西安市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('yijia','宜家家居','宜家家居旗舰店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');create index idx_seller_name_sta_addr on tb_seller(name,status,address);
</code></pre>
<h5 id="422-避免索引失效">4.2.2 避免索引失效</h5>
<p>1).  全值匹配 ，对索引中所有列都指定具体值。</p>
<p>改情况下，索引生效，执行效率高。</p>
<pre><code class="language-sql">explain select * from tb_seller where name='小米科技' and status='1' and address='北京市'\G;
</code></pre>
<figure data-type="image" tabindex="59"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556170997921.png" alt="" loading="lazy"></figure>
<p>2). 最左前缀法则</p>
<p>如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始，并且不跳过索引中的列。</p>
<p>匹配最左前缀法则，走索引：</p>
<figure data-type="image" tabindex="60"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556171348995.png" alt="" loading="lazy"></figure>
<p>违法最左前缀法则 ， 索引失效：</p>
<figure data-type="image" tabindex="61"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556171428140.png" alt="" loading="lazy"></figure>
<p>如果符合最左法则，但是出现跳跃某一列，只有最左列索引生效：</p>
<figure data-type="image" tabindex="62"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556171662203.png" alt="" loading="lazy"></figure>
<p>3). 范围查询右边的列，不能使用索引 。</p>
<figure data-type="image" tabindex="63"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556172256791.png" alt="" loading="lazy"></figure>
<p>根据前面的两个字段name ， status 查询是走索引的， 但是最后一个条件address 没有用到索引。</p>
<p>4). 不要在索引列上进行运算操作， 索引将失效。</p>
<figure data-type="image" tabindex="64"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556172813715.png" alt="" loading="lazy"></figure>
<p>5). 字符串不加单引号，造成索引失效。</p>
<figure data-type="image" tabindex="65"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556172967493.png" alt="" loading="lazy"></figure>
<p>由于，在查询是，没有对字符串加单引号，MySQL的查询优化器，会自动的进行类型转换，造成索引失效。</p>
<p>6). 尽量使用覆盖索引，避免select *</p>
<p>尽量使用覆盖索引（只访问索引的查询（索引列完全包含查询列）），减少select * 。</p>
<figure data-type="image" tabindex="66"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556173928299.png" alt="" loading="lazy"></figure>
<p>如果查询列，超出索引列，也会降低性能。</p>
<figure data-type="image" tabindex="67"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556173986068.png" alt="" loading="lazy"></figure>
<pre><code>TIP : 	    using index ：使用覆盖索引的时候就会出现    using where：在查找使用索引的情况下，需要回表去查询所需的数据    using index condition：查找使用了索引，但是需要回表查询数据    using index ; using where：查找使用了索引，但是需要的数据都在索引列中能找到，所以不需要回表查询数据
</code></pre>
<p>7). 用or分割开的条件， 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。</p>
<p>示例，name字段是索引列 ， 而createtime不是索引列，中间是or进行连接是不走索引的 ：</p>
<pre><code class="language-sql">explain select * from tb_seller where name='黑马程序员' or createtime = '2088-01-01 12:00:00'\G;	
</code></pre>
<figure data-type="image" tabindex="68"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556174994440.png" alt="" loading="lazy"></figure>
<p>8).  以%开头的Like模糊查询，索引失效。</p>
<p>如果仅仅是尾部模糊匹配，索引不会失效。如果是头部模糊匹配，索引失效。</p>
<figure data-type="image" tabindex="69"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556175114369.png" alt="" loading="lazy"></figure>
<p>解决方案 ：</p>
<p>通过覆盖索引来解决</p>
<figure data-type="image" tabindex="70"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556247686483.png" alt="" loading="lazy"></figure>
<p>9). 如果MySQL评估使用索引比全表更慢，则不使用索引。</p>
<figure data-type="image" tabindex="71"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556175445210.png" alt="" loading="lazy"></figure>
<p>10). is  NULL ， is NOT NULL  <font color='red'>有时</font>索引失效。</p>
<figure data-type="image" tabindex="72"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556180634889.png" alt="" loading="lazy"></figure>
<p>11). in 走索引， not in 索引失效。</p>
<figure data-type="image" tabindex="73"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556249602732.png" alt="" loading="lazy"></figure>
<p>12). 单列索引和复合索引。</p>
<p>尽量使用复合索引，而少使用单列索引 。</p>
<p>创建复合索引</p>
<pre><code>create index idx_name_sta_address on tb_seller(name, status, address);就相当于创建了三个索引 ： 	name	name + status	name + status + address
</code></pre>
<p>创建单列索引</p>
<pre><code>create index idx_seller_name on tb_seller(name);create index idx_seller_status on tb_seller(status);create index idx_seller_address on tb_seller(address);
</code></pre>
<p>数据库会选择一个最优的索引（辨识度最高索引）来使用，并不会使用全部索引 。</p>
<h4 id="43-查看索引使用情况">4.3 查看索引使用情况</h4>
<pre><code class="language-sql">show status like 'Handler_read%';	show global status like 'Handler_read%';	
</code></pre>
<figure data-type="image" tabindex="74"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1552885364563.png" alt="" loading="lazy"></figure>
<pre><code>Handler_read_first：索引中第一条被读的次数。如果较高，表示服务器正执行大量全索引扫描（这个值越低越好）。Handler_read_key：如果索引正在工作，这个值代表一个行被索引值读的次数，如果值越低，表示索引得到的性能改善不高，因为索引不经常使用（这个值越高越好）。Handler_read_next ：按照键顺序读下一行的请求数。如果你用范围约束或如果执行索引扫描来查询索引列，该值增加。Handler_read_prev：按照键顺序读前一行的请求数。该读方法主要用于优化ORDER BY ... DESC。Handler_read_rnd ：根据固定位置读一行的请求数。如果你正执行大量查询并需要对结果进行排序该值较高。你可能使用了大量需要MySQL扫描整个表的查询或你的连接没有正确使用键。这个值较高，意味着运行效率低，应该建立索引来补救。Handler_read_rnd_next：在数据文件中读下一行的请求数。如果你正进行大量的表扫描，该值较高。通常说明你的表索引不正确或写入的查询没有利用索引。
</code></pre>
<h3 id="5-sql优化">5. SQL优化</h3>
<h4 id="51-大批量插入数据">5.1 大批量插入数据</h4>
<p>环境准备 ：</p>
<pre><code class="language-sql">CREATE TABLE `tb_user_2` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `username` varchar(45) NOT NULL,  `password` varchar(96) NOT NULL,  `name` varchar(45) NOT NULL,  `birthday` datetime DEFAULT NULL,  `sex` char(1) DEFAULT NULL,  `email` varchar(45) DEFAULT NULL,  `phone` varchar(45) DEFAULT NULL,  `qq` varchar(32) DEFAULT NULL,  `status` varchar(32) NOT NULL COMMENT '用户状态',  `create_time` datetime NOT NULL,  `update_time` datetime DEFAULT NULL,  PRIMARY KEY (`id`),  UNIQUE KEY `unique_user_username` (`username`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 ;
</code></pre>
<p>当使用load 命令导入数据的时候，适当的设置可以提高导入的效率。</p>
<figure data-type="image" tabindex="75"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556269346488.png" alt="" loading="lazy"></figure>
<p>对于 InnoDB 类型的表，有以下几种方式可以提高导入的效率：</p>
<p>1） 主键顺序插入</p>
<p>因为InnoDB类型的表是按照主键的顺序保存的，所以将导入的数据按照主键的顺序排列，可以有效的提高导入数据的效率。如果InnoDB表没有主键，那么系统会自动默认创建一个内部列作为主键，所以如果可以给表创建一个主键，将可以利用这点，来提高导入数据的效率。</p>
<pre><code>脚本文件介绍 :	sql1.log  ----&gt; 主键有序	sql2.log  ----&gt; 主键无序
</code></pre>
<p>插入ID顺序排列数据：</p>
<figure data-type="image" tabindex="76"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1555771750567.png" alt="" loading="lazy"></figure>
<p>插入ID无序排列数据：</p>
<figure data-type="image" tabindex="77"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1555771959734.png" alt="" loading="lazy"></figure>
<p>2） 关闭唯一性校验</p>
<p>在导入数据前执行 SET UNIQUE_CHECKS=0，关闭唯一性校验，在导入结束后执行SET UNIQUE_CHECKS=1，恢复唯一性校验，可以提高导入的效率。</p>
<figure data-type="image" tabindex="78"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1555772132736.png" alt="" loading="lazy"></figure>
<p>3） 手动提交事务</p>
<p>如果应用使用自动提交的方式，建议在导入前执行 SET AUTOCOMMIT=0，关闭自动提交，导入结束后再执行 SET AUTOCOMMIT=1，打开自动提交，也可以提高导入的效率。</p>
<figure data-type="image" tabindex="79"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1555772351208.png" alt="" loading="lazy"></figure>
<h4 id="52-优化insert语句">5.2 优化insert语句</h4>
<p>当进行数据的insert操作的时候，可以考虑采用以下几种优化方案。</p>
<ul>
<li>
<p>如果需要同时对一张表插入很多行数据时，应该尽量使用多个值表的insert语句，这种方式将大大的缩减客户端与数据库之间的连接、关闭等消耗。使得效率比分开执行的单个insert语句快。</p>
<p>示例， 原始方式为：</p>
<pre><code class="language-sql">insert into tb_test values(1,'Tom');insert into tb_test values(2,'Cat');insert into tb_test values(3,'Jerry');
</code></pre>
<p>优化后的方案为 ：</p>
<pre><code class="language-sql">insert into tb_test values(1,'Tom'),(2,'Cat')，(3,'Jerry');
</code></pre>
</li>
<li>
<p>在事务中进行数据插入。</p>
<pre><code class="language-sql">start transaction;insert into tb_test values(1,'Tom');insert into tb_test values(2,'Cat');insert into tb_test values(3,'Jerry');commit;
</code></pre>
</li>
<li>
<p>数据有序插入</p>
<pre><code class="language-sql">insert into tb_test values(4,'Tim');insert into tb_test values(1,'Tom');insert into tb_test values(3,'Jerry');insert into tb_test values(5,'Rose');insert into tb_test values(2,'Cat');
</code></pre>
<p>优化后</p>
<pre><code class="language-sql">insert into tb_test values(1,'Tom');insert into tb_test values(2,'Cat');insert into tb_test values(3,'Jerry');insert into tb_test values(4,'Tim');insert into tb_test values(5,'Rose');
</code></pre>
</li>
</ul>
<h4 id="53-优化order-by语句">5.3 优化order by语句</h4>
<h5 id="531-环境准备">5.3.1 环境准备</h5>
<pre><code class="language-SQL">CREATE TABLE `emp` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `name` varchar(100) NOT NULL,  `age` int(3) NOT NULL,  `salary` int(11) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4;insert into `emp` (`id`, `name`, `age`, `salary`) values('1','Tom','25','2300');insert into `emp` (`id`, `name`, `age`, `salary`) values('2','Jerry','30','3500');insert into `emp` (`id`, `name`, `age`, `salary`) values('3','Luci','25','2800');insert into `emp` (`id`, `name`, `age`, `salary`) values('4','Jay','36','3500');insert into `emp` (`id`, `name`, `age`, `salary`) values('5','Tom2','21','2200');insert into `emp` (`id`, `name`, `age`, `salary`) values('6','Jerry2','31','3300');insert into `emp` (`id`, `name`, `age`, `salary`) values('7','Luci2','26','2700');insert into `emp` (`id`, `name`, `age`, `salary`) values('8','Jay2','33','3500');insert into `emp` (`id`, `name`, `age`, `salary`) values('9','Tom3','23','2400');insert into `emp` (`id`, `name`, `age`, `salary`) values('10','Jerry3','32','3100');insert into `emp` (`id`, `name`, `age`, `salary`) values('11','Luci3','26','2900');insert into `emp` (`id`, `name`, `age`, `salary`) values('12','Jay3','37','4500');create index idx_emp_age_salary on emp(age,salary);
</code></pre>
<h5 id="532-两种排序方式">5.3.2 两种排序方式</h5>
<p>1). 第一种是通过对返回数据进行排序，也就是通常说的 filesort 排序，所有不是通过索引直接返回排序结果的排序都叫 FileSort 排序。</p>
<figure data-type="image" tabindex="80"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556335817763.png" alt="" loading="lazy"></figure>
<p>2). 第二种通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。</p>
<figure data-type="image" tabindex="81"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556335866539.png" alt="" loading="lazy"></figure>
<p>多字段排序</p>
<figure data-type="image" tabindex="82"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556336352061.png" alt="" loading="lazy"></figure>
<p>了解了MySQL的排序方式，优化目标就清晰了：尽量减少额外的排序，通过索引直接返回有序数据。where 条件和Order by 使用相同的索引，并且Order By 的顺序和索引顺序相同， 并且Order  by 的字段都是升序，或者都是降序。否则肯定需要额外的操作，这样就会出现FileSort。</p>
<h5 id="533-filesort-的优化">5.3.3 Filesort 的优化</h5>
<p>通过创建合适的索引，能够减少 Filesort 的出现，但是在某些情况下，条件限制不能让Filesort消失，那就需要加快 Filesort的排序操作。对于Filesort ， MySQL 有两种排序算法：</p>
<p>1） 两次扫描算法 ：MySQL4.1 之前，使用该方式排序。首先根据条件取出排序字段和行指针信息，然后在排序区 sort buffer 中排序，如果sort buffer不够，则在临时表 temporary table 中存储排序结果。完成排序之后，再根据行指针回表读取记录，该操作可能会导致大量随机I/O操作。</p>
<p>2）一次扫描算法：一次性取出满足条件的所有字段，然后在排序区 sort  buffer 中排序后直接输出结果集。排序时内存开销较大，但是排序效率比两次扫描算法要高。</p>
<p>MySQL 通过比较系统变量 max_length_for_sort_data 的大小和Query语句取出的字段总大小， 来判定是否那种排序算法，如果max_length_for_sort_data 更大，那么使用第二种优化之后的算法；否则使用第一种。</p>
<p>可以适当提高 sort_buffer_size  和 max_length_for_sort_data  系统变量，来增大排序区的大小，提高排序的效率。</p>
<figure data-type="image" tabindex="83"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556338367593.png" alt="" loading="lazy"></figure>
<h4 id="54-优化group-by-语句">5.4 优化group by 语句</h4>
<p>由于GROUP BY 实际上也同样会进行排序操作，而且与ORDER BY 相比，GROUP BY 主要只是多了排序之后的分组操作。当然，如果在分组的时候还使用了其他的一些聚合函数，那么还需要一些聚合函数的计算。所以，在GROUP BY 的实现过程中，与 ORDER BY 一样也可以利用到索引。</p>
<p>如果查询包含 group by 但是用户想要避免排序结果的消耗， 则可以执行order by null 禁止排序。如下 ：</p>
<pre><code class="language-SQL">drop index idx_emp_age_salary on emp;explain select age,count(*) from emp group by age;
</code></pre>
<figure data-type="image" tabindex="84"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556339573979.png" alt="" loading="lazy"></figure>
<p>优化后</p>
<pre><code class="language-sql">explain select age,count(*) from emp group by age order by null;
</code></pre>
<figure data-type="image" tabindex="85"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556339633161.png" alt="" loading="lazy"></figure>
<p>从上面的例子可以看出，第一个SQL语句需要进行&quot;filesort&quot;，而第二个SQL由于order  by  null 不需要进行 &quot;filesort&quot;， 而上文提过Filesort往往非常耗费时间。</p>
<p>创建索引 ：</p>
<pre><code class="language-SQL">create index idx_emp_age_salary on emp(age,salary)；
</code></pre>
<figure data-type="image" tabindex="86"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556339688158.png" alt="" loading="lazy"></figure>
<h4 id="55-优化嵌套查询">5.5 优化嵌套查询</h4>
<p>Mysql4.1版本之后，开始支持SQL的子查询。这个技术可以使用SELECT语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的SQL操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询是可以被更高效的连接（JOIN）替代。</p>
<p>示例 ，查找有角色的所有的用户信息 :</p>
<pre><code class="language-SQL"> explain select * from t_user where id in (select user_id from user_role );
</code></pre>
<p>执行计划为 :</p>
<figure data-type="image" tabindex="87"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556359399199.png" alt="" loading="lazy"></figure>
<p>优化后 :</p>
<pre><code class="language-SQL">explain select * from t_user u , user_role ur where u.id = ur.user_id;
</code></pre>
<figure data-type="image" tabindex="88"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556359482142.png" alt="" loading="lazy"></figure>
<p>连接(Join)查询之所以更有效率一些 ，是因为MySQL不需要在内存中创建临时表来完成这个逻辑上需要两个步骤的查询工作。</p>
<h4 id="56-优化or条件">5.6 优化OR条件</h4>
<p>对于包含OR的查询子句，如果要利用索引，则OR之间的每个条件列都必须用到索引 ， 而且不能使用到复合索引； 如果没有索引，则应该考虑增加索引。</p>
<p>获取 emp 表中的所有的索引 ：</p>
<figure data-type="image" tabindex="89"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556354464657.png" alt="" loading="lazy"></figure>
<p>示例 ：</p>
<pre><code class="language-SQL">explain select * from emp where id = 1 or age = 30;
</code></pre>
<figure data-type="image" tabindex="90"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556354887509.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="91"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556354920964.png" alt="" loading="lazy"></figure>
<p>建议使用 union 替换 or ：</p>
<figure data-type="image" tabindex="92"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556355027728.png" alt="" loading="lazy"></figure>
<p>我们来比较下重要指标，发现主要差别是 type 和 ref 这两项</p>
<p>type 显示的是访问类型，是较为重要的一个指标，结果值从好到坏依次是：</p>
<pre><code>system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null  &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL
</code></pre>
<p>UNION 语句的 type 值为 ref，OR 语句的 type 值为 range，可以看到这是一个很明显的差距</p>
<p>UNION 语句的 ref 值为 const，OR 语句的 type 值为 null，const 表示是常量值引用，非常快</p>
<p>这两项的差距就说明了 UNION 要优于 OR 。</p>
<h4 id="57-优化分页查询">5.7 优化分页查询</h4>
<p>一般分页查询时，通过创建覆盖索引能够比较好地提高性能。一个常见又非常头疼的问题就是 limit 2000000,10  ，此时需要MySQL排序前2000010 记录，仅仅返回2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。</p>
<figure data-type="image" tabindex="93"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556361314783.png" alt="" loading="lazy"></figure>
<h5 id="571-优化思路一">5.7.1 优化思路一</h5>
<p>在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。</p>
<figure data-type="image" tabindex="94"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556416102800.png" alt="" loading="lazy"></figure>
<h5 id="572-优化思路二">5.7.2 优化思路二</h5>
<p>该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。</p>
<figure data-type="image" tabindex="95"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556363928151.png" alt="" loading="lazy"></figure>
<h4 id="58-使用sql提示">5.8 使用SQL提示</h4>
<p>SQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。</p>
<h5 id="581-use-index">5.8.1 USE INDEX</h5>
<p>在查询语句中表名的后面，添加 use index 来提供希望MySQL去参考的索引列表，就可以让MySQL不再考虑其他可用的索引。</p>
<pre><code>create index idx_seller_name on tb_seller(name);
</code></pre>
<figure data-type="image" tabindex="96"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556370971576.png" alt="" loading="lazy"></figure>
<h5 id="582-ignore-index">5.8.2 IGNORE INDEX</h5>
<p>如果用户只是单纯的想让MySQL忽略一个或者多个索引，则可以使用 ignore index 作为 hint 。</p>
<pre><code> explain select * from tb_seller ignore index(idx_seller_name) where name = '小米科技';
</code></pre>
<figure data-type="image" tabindex="97"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556371004594.png" alt="" loading="lazy"></figure>
<h5 id="583-force-index">5.8.3 FORCE INDEX</h5>
<p>为强制MySQL使用一个特定的索引，可在查询中使用 force index 作为hint 。</p>
<pre><code class="language-SQL">create index idx_seller_address on tb_seller(address);
</code></pre>
<figure data-type="image" tabindex="98"><img src="https://memorykki.github.io/post-images/MySQL_hm/2/1556371355788.png" alt="" loading="lazy"></figure>
<h1 id="mysql高级-day03">Mysql高级-day03</h1>
<h3 id="1-应用优化">1. 应用优化</h3>
<p>前面章节，我们介绍了很多数据库的优化措施。但是在实际生产环境中，由于数据库本身的性能局限，就必须要对前台的应用进行一些优化，来降低数据库的访问压力。</p>
<h4 id="11-使用连接池">1.1 使用连接池</h4>
<p>对于访问数据库来说，建立连接的代价是比较昂贵的，因为我们频繁的创建关闭连接，是比较耗费资源的，我们有必要建立 数据库连接池，以提高访问的性能。</p>
<h4 id="12-减少对mysql的访问">1.2 减少对MySQL的访问</h4>
<h5 id="121-避免对数据进行重复检索">1.2.1 避免对数据进行重复检索</h5>
<p>在编写应用代码时，需要能够理清对数据库的访问逻辑。能够一次连接就获取到结果的，就不用两次连接，这样可以大大减少对数据库无用的重复请求。</p>
<p>比如 ，需要获取书籍的id 和name字段 ， 则查询如下：</p>
<pre><code> select id , name from tb_book;
</code></pre>
<p>之后，在业务逻辑中有需要获取到书籍状态信息， 则查询如下：</p>
<pre><code>select id , status from tb_book;
</code></pre>
<p>这样，就需要向数据库提交两次请求，数据库就要做两次查询操作。其实完全可以用一条SQL语句得到想要的结果。</p>
<pre><code>select id, name , status from tb_book;
</code></pre>
<h5 id="122-增加cache层">1.2.2 增加cache层</h5>
<p>在应用中，我们可以在应用中增加 缓存 层来达到减轻数据库负担的目的。缓存层有很多种，也有很多实现方式，只要能达到降低数据库的负担又能满足应用需求就可以。</p>
<p>因此可以部分数据从数据库中抽取出来放到应用端以文本方式存储， 或者使用框架(Mybatis, Hibernate)提供的一级缓存/二级缓存，或者使用redis数据库来缓存数据 。</p>
<h4 id="13-负载均衡">1.3 负载均衡</h4>
<p>负载均衡是应用中使用非常普遍的一种优化方法，它的机制就是利用某种均衡算法，将固定的负载量分布到不同的服务器上， 以此来降低单台服务器的负载，达到优化的效果。</p>
<h5 id="131-利用mysql复制分流查询">1.3.1 利用MySQL复制分流查询</h5>
<p>通过MySQL的主从复制，实现读写分离，使增删改操作走主节点，查询操作走从节点，从而可以降低单台服务器的读写压力。</p>
<figure data-type="image" tabindex="99"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1.jpg" alt="" loading="lazy"></figure>
<h5 id="132-采用分布式数据库架构">1.3.2 采用分布式数据库架构</h5>
<p>分布式数据库架构适合大数据量、负载高的情况，它有良好的拓展性和高可用性。通过在多台服务器之间分布数据，可以实现在多台服务器之间的负载均衡，提高访问效率。</p>
<h3 id="2-mysql中查询缓存优化">2. Mysql中查询缓存优化</h3>
<h4 id="21-概述">2.1 概述</h4>
<p>开启Mysql的查询缓存，当执行完全相同的SQL语句的时候，服务器就会直接从缓存中读取结果，当数据被修改，之前的缓存会失效，修改比较频繁的表不适合做查询缓存。</p>
<h4 id="22-操作流程">2.2 操作流程</h4>
<figure data-type="image" tabindex="100"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/20180919131632347.png" alt="" loading="lazy"></figure>
<ol>
<li>客户端发送一条查询给服务器；</li>
<li>服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段；</li>
<li>服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划；</li>
<li>MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询；</li>
<li>将结果返回给客户端。</li>
</ol>
<h4 id="23-查询缓存配置">2.3 查询缓存配置</h4>
<ol>
<li>
<p>查看当前的MySQL数据库是否支持查询缓存：</p>
<pre><code class="language-SQL">SHOW VARIABLES LIKE 'have_query_cache';	
</code></pre>
<figure data-type="image" tabindex="101"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1555249929012.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>查看当前MySQL是否开启了查询缓存 ：</p>
<pre><code class="language-SQL">SHOW VARIABLES LIKE 'query_cache_type';
</code></pre>
<figure data-type="image" tabindex="102"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1555250015377.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>查看查询缓存的占用大小 ：</p>
<pre><code class="language-SQL">SHOW VARIABLES LIKE 'query_cache_size';
</code></pre>
<figure data-type="image" tabindex="103"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1555250142451.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>查看查询缓存的状态变量：</p>
<pre><code class="language-SQL">SHOW STATUS LIKE 'Qcache%';
</code></pre>
<figure data-type="image" tabindex="104"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1555250443958.png" alt="" loading="lazy"></figure>
<p>各个变量的含义如下：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qcache_free_blocks</td>
<td>查询缓存中的可用内存块数</td>
</tr>
<tr>
<td>Qcache_free_memory</td>
<td>查询缓存的可用内存量</td>
</tr>
<tr>
<td>Qcache_hits</td>
<td>查询缓存命中数</td>
</tr>
<tr>
<td>Qcache_inserts</td>
<td>添加到查询缓存的查询数</td>
</tr>
<tr>
<td>Qcache_lowmen_prunes</td>
<td>由于内存不足而从查询缓存中删除的查询数</td>
</tr>
<tr>
<td>Qcache_not_cached</td>
<td>非缓存查询的数量（由于 query_cache_type 设置而无法缓存或未缓存）</td>
</tr>
<tr>
<td>Qcache_queries_in_cache</td>
<td>查询缓存中注册的查询数</td>
</tr>
<tr>
<td>Qcache_total_blocks</td>
<td>查询缓存中的块总数</td>
</tr>
</tbody>
</table>
</li>
</ol>
<h4 id="24-开启查询缓存">2.4 开启查询缓存</h4>
<p>MySQL的查询缓存默认是关闭的，需要手动配置参数 query_cache_type ， 来开启查询缓存。query_cache_type 该参数的可取值有三个 ：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>OFF 或 0</td>
<td>查询缓存功能关闭</td>
</tr>
<tr>
<td>ON 或 1</td>
<td>查询缓存功能打开，SELECT的结果符合缓存条件即会缓存，否则，不予缓存，显式指定 SQL_NO_CACHE，不予缓存</td>
</tr>
<tr>
<td>DEMAND 或 2</td>
<td>查询缓存功能按需进行，显式指定 SQL_CACHE 的SELECT语句才会缓存；其它均不予缓存</td>
</tr>
</tbody>
</table>
<p>在 /usr/my.cnf 配置中，增加以下配置 ：</p>
<figure data-type="image" tabindex="105"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1555251383805.png" alt="" loading="lazy"></figure>
<p>配置完毕之后，重启服务既可生效 ；</p>
<p>然后就可以在命令行执行SQL语句进行验证 ，执行一条比较耗时的SQL语句，然后再多执行几次，查看后面几次的执行时间；获取通过查看查询缓存的缓存命中数，来判定是否走查询缓存。</p>
<h4 id="25-查询缓存select选项">2.5 查询缓存SELECT选项</h4>
<p>可以在SELECT语句中指定两个与查询缓存相关的选项 ：</p>
<p>SQL_CACHE : 如果查询结果是可缓存的，并且 query_cache_type 系统变量的值为ON或 DEMAND ，则缓存查询结果 。</p>
<p>SQL_NO_CACHE : 服务器不使用查询缓存。它既不检查查询缓存，也不检查结果是否已缓存，也不缓存查询结果。</p>
<p>例子：</p>
<pre><code class="language-SQL">SELECT SQL_CACHE id, name FROM customer;
SELECT SQL_NO_CACHE id, name FROM customer;
</code></pre>
<p>​</p>
<h4 id="26-查询缓存失效的情况">2.6 查询缓存失效的情况</h4>
<p>1） SQL 语句不一致的情况， 要想命中查询缓存，查询的SQL语句必须一致。</p>
<pre><code class="language-SQL">SQL1 : select count(*) from tb_item;
SQL2 : Select count(*) from tb_item;
</code></pre>
<p>2） 当查询语句中有一些不确定的时，则不会缓存。如 ： now() , current_date() , curdate() , curtime() , rand() , uuid() , user() , database() 。</p>
<pre><code class="language-SQL">SQL1 : select * from tb_item where updatetime &lt; now() limit 1;
SQL2 : select user();
SQL3 : select database();
</code></pre>
<p>3） 不使用任何表查询语句。</p>
<pre><code class="language-SQL">select 'A';
</code></pre>
<p>4）  查询 mysql， information_schema或  performance_schema 数据库中的表时，不会走查询缓存。</p>
<pre><code class="language-SQL">select * from information_schema.engines;
</code></pre>
<p>5） 在存储的函数，触发器或事件的主体内执行的查询。</p>
<p>6） 如果表更改，则使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除。这包括使用<code>MERGE</code>映射到已更改表的表的查询。一个表可以被许多类型的语句，如被改变 INSERT， UPDATE， DELETE， TRUNCATE TABLE， ALTER TABLE， DROP TABLE，或 DROP DATABASE 。</p>
<h3 id="3-mysql内存管理及优化">3. Mysql内存管理及优化</h3>
<h4 id="31-内存优化原则">3.1 内存优化原则</h4>
<p>1） 将尽量多的内存分配给MySQL做缓存，但要给操作系统和其他程序预留足够内存。</p>
<p>2） MyISAM 存储引擎的数据文件读取依赖于操作系统自身的IO缓存，因此，如果有MyISAM表，就要预留更多的内存给操作系统做IO缓存。</p>
<p>3） 排序区、连接区等缓存是分配给每个数据库会话（session）专用的，其默认值的设置要根据最大连接数合理分配，如果设置太大，不但浪费资源，而且在并发连接较高时会导致物理内存耗尽。</p>
<h4 id="32-myisam-内存优化">3.2 MyISAM 内存优化</h4>
<p>myisam存储引擎使用 key_buffer 缓存索引块，加速myisam索引的读写速度。对于myisam表的数据块，mysql没有特别的缓存机制，完全依赖于操作系统的IO缓存。</p>
<h5 id="key_buffer_size">key_buffer_size</h5>
<p>key_buffer_size决定MyISAM索引块缓存区的大小，直接影响到MyISAM表的存取效率。可以在MySQL参数文件中设置key_buffer_size的值，对于一般MyISAM数据库，建议至少将1/4可用内存分配给key_buffer_size。</p>
<p>在/usr/my.cnf 中做如下配置：</p>
<pre><code>key_buffer_size=512M
</code></pre>
<h5 id="read_buffer_size">read_buffer_size</h5>
<p>如果需要经常顺序扫描myisam表，可以通过增大read_buffer_size的值来改善性能。但需要注意的是read_buffer_size是每个session独占的，如果默认值设置太大，就会造成内存浪费。</p>
<h5 id="read_rnd_buffer_size">read_rnd_buffer_size</h5>
<p>对于需要做排序的myisam表的查询，如带有order by子句的sql，适当增加 read_rnd_buffer_size 的值，可以改善此类的sql性能。但需要注意的是 read_rnd_buffer_size 是每个session独占的，如果默认值设置太大，就会造成内存浪费。</p>
<h4 id="33-innodb-内存优化">3.3 InnoDB 内存优化</h4>
<p>innodb用一块内存区做IO缓存池，该缓存池不仅用来缓存innodb的索引块，而且也用来缓存innodb的数据块。</p>
<h5 id="innodb_buffer_pool_size">innodb_buffer_pool_size</h5>
<p>该变量决定了 innodb 存储引擎表数据和索引数据的最大缓存区大小。在保证操作系统及其他程序有足够内存可用的情况下，innodb_buffer_pool_size 的值越大，缓存命中率越高，访问InnoDB表需要的磁盘I/O 就越少，性能也就越高。</p>
<pre><code>innodb_buffer_pool_size=512M
</code></pre>
<h5 id="innodb_log_buffer_size">innodb_log_buffer_size</h5>
<p>决定了innodb重做日志缓存的大小，对于可能产生大量更新记录的大事务，增加innodb_log_buffer_size的大小，可以避免innodb在事务提交前就执行不必要的日志写入磁盘操作。</p>
<pre><code>innodb_log_buffer_size=10M
</code></pre>
<h3 id="4-mysql并发参数调整">4. Mysql并发参数调整</h3>
<p>从实现上来说，MySQL Server 是多线程结构，包括后台线程和客户服务线程。多线程可以有效利用服务器资源，提高数据库的并发性能。在Mysql中，控制并发连接和线程的主要参数包括 max_connections、back_log、thread_cache_size、table_open_cahce。</p>
<h4 id="41-max_connections">4.1 max_connections</h4>
<p>采用max_connections 控制允许连接到MySQL数据库的最大数量，默认值是 151。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接数已达到允许最大值而失败，这是可以考虑增大max_connections 的值。</p>
<p>Mysql 最大可支持的连接数，取决于很多因素，包括给定操作系统平台的线程库的质量、内存大小、每个连接的负荷、CPU的处理速度，期望的响应时间等。在Linux 平台下，性能好的服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。</p>
<h4 id="42-back_log">4.2 back_log</h4>
<p>back_log 参数控制MySQL监听TCP端口时设置的积压请求栈大小。如果MySql的连接数达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源，将会报错。5.6.6 版本之前默认值为 50 ， 之后的版本默认为 50 + （max_connections / 5）， 但最大不超过900。</p>
<p>如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大back_log 的值。</p>
<h4 id="43-table_open_cache">4.3 table_open_cache</h4>
<p>该参数用来控制所有SQL语句执行线程可打开表缓存的数量， 而在执行SQL语句时，每一个SQL执行线程至少要打开 1 个表缓存。该参数的值应该根据设置的最大连接数 max_connections 以及每个连接执行关联查询中涉及的表的最大数量来设定 ：</p>
<p>​	max_connections x N ；</p>
<h4 id="44-thread_cache_size">4.4 thread_cache_size</h4>
<p>为了加快连接数据库的速度，MySQL 会缓存一定数量的客户服务线程以备重用，通过参数 thread_cache_size 可控制 MySQL 缓存客户服务线程的数量。</p>
<h4 id="45-innodb_lock_wait_timeout">4.5 innodb_lock_wait_timeout</h4>
<p>该参数是用来设置InnoDB 事务等待行锁的时间，默认值是50ms ， 可以根据需要进行动态设置。对于需要快速反馈的业务系统来说，可以将行锁的等待时间调小，以避免事务长时间挂起； 对于后台运行的批量处理程序来说， 可以将行锁的等待时间调大， 以避免发生大的回滚操作。</p>
<h3 id="5-mysql锁问题">5. Mysql锁问题</h3>
<h4 id="51-锁概述">5.1 锁概述</h4>
<p>锁是计算机协调多个进程或线程并发访问某一资源的机制（避免争抢）。</p>
<p>在数据库中，除传统的计算资源（如 CPU、RAM、I/O 等）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。</p>
<h4 id="52-锁分类">5.2 锁分类</h4>
<p>从对数据操作的粒度分 ：</p>
<p>1） 表锁：操作时，会锁定整个表。</p>
<p>2） 行锁：操作时，会锁定当前操作行。</p>
<p>从对数据操作的类型分：</p>
<p>1） 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响。</p>
<p>2） 写锁（排它锁）：当前操作没有完成之前，它会阻断其他写锁和读锁。</p>
<h4 id="53-mysql-锁">5.3 Mysql 锁</h4>
<p>相对其他数据库而言，MySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。下表中罗列出了各存储引擎对锁的支持情况：</p>
<table>
<thead>
<tr>
<th>存储引擎</th>
<th>表级锁</th>
<th>行级锁</th>
<th>页面锁</th>
</tr>
</thead>
<tbody>
<tr>
<td>MyISAM</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>InnoDB</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>MEMORY</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>BDB</td>
<td>支持</td>
<td>不支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
<p>MySQL这3种锁的特性可大致归纳如下 ：</p>
<table>
<thead>
<tr>
<th>锁类型</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>表级锁</td>
<td>偏向MyISAM 存储引擎，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。</td>
</tr>
<tr>
<td>行级锁</td>
<td>偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。</td>
</tr>
<tr>
<td>页面锁</td>
<td>开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。</td>
</tr>
</tbody>
</table>
<p>从上述特点可见，很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理（OLTP）系统。</p>
<h4 id="52-myisam-表锁">5.2 MyISAM 表锁</h4>
<p>MyISAM 存储引擎只支持表锁，这也是MySQL开始几个版本中唯一支持的锁类型。</p>
<h5 id="521-如何加表锁">5.2.1 如何加表锁</h5>
<p>MyISAM 在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。</p>
<p>显示加表锁语法：</p>
<pre><code class="language-SQL">加读锁 ： lock table table_name read;

加写锁 ： lock table table_name write；
</code></pre>
<h5 id="522-读锁案例">5.2.2 读锁案例</h5>
<p>准备环境</p>
<pre><code class="language-SQL">create database demo_03 default charset=utf8mb4;

use demo_03;

CREATE TABLE `tb_book` (
  `id` INT(11) auto_increment,
  `name` VARCHAR(50) DEFAULT NULL,
  `publish_time` DATE DEFAULT NULL,
  `status` CHAR(1) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=myisam DEFAULT CHARSET=utf8 ;

INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,'java编程思想','2088-08-01','1');
INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,'solr编程思想','2088-08-08','0');



CREATE TABLE `tb_user` (
  `id` INT(11) auto_increment,
  `name` VARCHAR(50) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=myisam DEFAULT CHARSET=utf8 ;

INSERT INTO tb_user (id, name) VALUES(NULL,'令狐冲');
INSERT INTO tb_user (id, name) VALUES(NULL,'田伯光');

</code></pre>
<p>客户端 一 ：</p>
<p>1）获得tb_book 表的读锁</p>
<pre><code>lock table tb_book read;
</code></pre>
<p>2） 执行查询操作</p>
<pre><code>select * from tb_book;
</code></pre>
<figure data-type="image" tabindex="106"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553906896564.png" alt="" loading="lazy"></figure>
<p>可以正常执行 ， 查询出数据。</p>
<p>客户端 二 ：</p>
<p>3） 执行查询操作</p>
<pre><code>select * from tb_book;
</code></pre>
<figure data-type="image" tabindex="107"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553907044500.png" alt="" loading="lazy"></figure>
<p>客户端 一 ：</p>
<p>4）查询未锁定的表</p>
<pre><code>select name from tb_seller;
</code></pre>
<figure data-type="image" tabindex="108"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553908913515.png" alt="" loading="lazy"></figure>
<p>客户端 二 ：</p>
<p>5）查询未锁定的表</p>
<pre><code>select name from tb_seller;
</code></pre>
<figure data-type="image" tabindex="109"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553908973840.png" alt="" loading="lazy"></figure>
<p>可以正常查询出未锁定的表；</p>
<p>客户端 一 ：</p>
<p>6） 执行插入操作</p>
<pre><code>insert into tb_book values(null,'Mysql高级','2088-01-01','1');
</code></pre>
<figure data-type="image" tabindex="110"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553907198462.png" alt="" loading="lazy"></figure>
<p>执行插入， 直接报错 ， 由于当前tb_book 获得的是 读锁， 不能执行更新操作。</p>
<p>客户端 二 ：</p>
<p>7） 执行插入操作</p>
<pre><code>insert into tb_book values(null,'Mysql高级','2088-01-01','1');
</code></pre>
<figure data-type="image" tabindex="111"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553907403957.png" alt="" loading="lazy"></figure>
<p>当在客户端一中释放锁指令 unlock tables  后 ， 客户端二中的 inesrt 语句 ， 立即执行 ；</p>
<h5 id="523-写锁案例">5.2.3 写锁案例</h5>
<p>客户端 一 :</p>
<p>1）获得tb_book 表的写锁</p>
<pre><code>lock table tb_book write ;
</code></pre>
<p>2）执行查询操作</p>
<pre><code>select * from tb_book ;
</code></pre>
<figure data-type="image" tabindex="112"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553907849829.png" alt="" loading="lazy"></figure>
<p>查询操作执行成功；</p>
<p>3）执行更新操作</p>
<pre><code>update tb_book set name = 'java编程思想（第二版）' where id = 1;
</code></pre>
<figure data-type="image" tabindex="113"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553907875221.png" alt="" loading="lazy"></figure>
<p>更新操作执行成功 ；</p>
<p>客户端 二 :</p>
<p>4）执行查询操作</p>
<pre><code>select * from tb_book ;
</code></pre>
<figure data-type="image" tabindex="114"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553908019755.png" alt="" loading="lazy"></figure>
<p>当在客户端一中释放锁指令 unlock tables  后 ， 客户端二中的 select 语句 ， 立即执行 ；</p>
<figure data-type="image" tabindex="115"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1553908131373.png" alt="" loading="lazy"></figure>
<h5 id="524-结论">5.2.4 结论</h5>
<p>锁模式的相互兼容性如表中所示：</p>
<figure data-type="image" tabindex="116"><img src="../../../../../../%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-03/%E6%96%87%E6%A1%A3//images/MySQL_hm/3/1553905621992.png" alt="1553905621992" loading="lazy"></figure>
<p>由上表可见：</p>
<p>​	1） 对MyISAM 表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；</p>
<p>​	2） 对MyISAM 表的写操作，则会阻塞其他用户对同一表的读和写操作；</p>
<p>​	简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁，则既会阻塞读，又会阻塞写。</p>
<p>此外，MyISAM 的读写锁调度是写优先，这也是MyISAM不适合做写为主的表的存储引擎的原因。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。</p>
<h5 id="525-查看锁的争用情况">5.2.5 查看锁的争用情况</h5>
<pre><code>show open tables；
</code></pre>
<figure data-type="image" tabindex="117"><img src="../../../../../../%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-03/%E6%96%87%E6%A1%A3//images/MySQL_hm/3/1556443073322.png" alt="1556443073322" loading="lazy"></figure>
<p>In_user : 表当前被查询使用的次数。如果该数为零，则表是打开的，但是当前没有被使用。</p>
<p>Name_locked：表名称是否被锁定。名称锁定用于取消表或对表进行重命名等操作。</p>
<pre><code>show status like 'Table_locks%';
</code></pre>
<figure data-type="image" tabindex="118"><img src="../../../../../../%E6%95%99%E5%AD%A6%E8%B5%84%E6%96%99/%E8%AF%BE%E7%A8%8B%E8%90%A5%E9%94%80/Mysql%E9%AB%98%E7%BA%A7/%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99/day-03/%E6%96%87%E6%A1%A3//images/MySQL_hm/3/1556443170082.png" alt="1556443170082" loading="lazy"></figure>
<p>Table_locks_immediate ： 指的是能够立即获得表级锁的次数，每立即获取锁，值加1。</p>
<p>Table_locks_waited ： 指的是不能立即获取表级锁而需要等待的次数，每等待一次，该值加1，此值高说明存在着较为严重的表级锁争用情况。</p>
<h4 id="53-innodb-行锁">5.3 InnoDB 行锁</h4>
<h5 id="531-行锁介绍">5.3.1 行锁介绍</h5>
<p>行锁特点 ：偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。</p>
<p>InnoDB 与 MyISAM 的最大不同有两点：一是支持事务；二是 采用了行级锁。</p>
<h5 id="532-背景知识">5.3.2 背景知识</h5>
<p><strong>事务及其ACID属性</strong></p>
<p>事务是由一组SQL语句组成的逻辑处理单元。</p>
<p>事务具有以下4个特性，简称为事务ACID属性。</p>
<table>
<thead>
<tr>
<th>ACID属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>原子性（Atomicity）</td>
<td>事务是一个原子操作单元，其对数据的修改，要么全部成功，要么全部失败。</td>
</tr>
<tr>
<td>一致性（Consistent）</td>
<td>在事务开始和完成时，数据都必须保持一致状态。</td>
</tr>
<tr>
<td>隔离性（Isolation）</td>
<td>数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的 “独立” 环境下运行。</td>
</tr>
<tr>
<td>持久性（Durable）</td>
<td>事务完成之后，对于数据的修改是永久的。</td>
</tr>
</tbody>
</table>
<p><strong>并发事务处理带来的问题</strong></p>
<table>
<thead>
<tr>
<th>问题</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>丢失更新（Lost Update）</td>
<td>当两个或多个事务选择同一行，最初的事务修改的值，会被后面的事务修改的值覆盖。</td>
</tr>
<tr>
<td>脏读（Dirty Reads）</td>
<td>当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。</td>
</tr>
<tr>
<td>不可重复读（Non-Repeatable Reads）</td>
<td>一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现和以前读出的数据不一致。</td>
</tr>
<tr>
<td>幻读（Phantom Reads）</td>
<td>一个事务按照相同的查询条件重新读取以前查询过的数据，却发现其他事务插入了满足其查询条件的新数据。</td>
</tr>
</tbody>
</table>
<p><strong>事务隔离级别</strong></p>
<p>为了解决上述提到的事务并发问题，数据库提供一定的事务隔离机制来解决这个问题。数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使用事务在一定程度上“串行化” 进行，这显然与“并发” 是矛盾的。</p>
<p>数据库的隔离级别有4个，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏写、脏读、不可重复读、幻读这几类问题。</p>
<table>
<thead>
<tr>
<th>隔离级别</th>
<th>丢失更新</th>
<th>脏读</th>
<th>不可重复读</th>
<th>幻读</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read uncommitted</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>Read committed</td>
<td>×</td>
<td>×</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>Repeatable read（默认）</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>√</td>
</tr>
<tr>
<td>Serializable</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
</tbody>
</table>
<p>备注 ： √  代表可能出现 ， × 代表不会出现 。</p>
<p>Mysql 的数据库的默认隔离级别为 Repeatable read ， 查看方式：</p>
<pre><code>show variables like 'tx_isolation';
</code></pre>
<figure data-type="image" tabindex="119"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554331600009.png" alt="" loading="lazy"></figure>
<h5 id="533-innodb-的行锁模式">5.3.3 InnoDB 的行锁模式</h5>
<p>InnoDB  实现了以下两种类型的行锁。</p>
<ul>
<li>共享锁（S）：又称为读锁，简称S锁，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。</li>
<li>排他锁（X）：又称为写锁，简称X锁，排他锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行读取和修改。</li>
</ul>
<p>对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；</p>
<p>对于普通SELECT语句，InnoDB不会加任何锁；</p>
<p>可以通过以下语句显示给记录集加共享锁或排他锁 。</p>
<pre><code>共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE

排他锁（X) ：SELECT * FROM table_name WHERE ... FOR UPDATE
</code></pre>
<h5 id="534-案例准备工作">5.3.4 案例准备工作</h5>
<pre><code class="language-sql">create table test_innodb_lock(
	id int(11),
	name varchar(16),
	sex varchar(1)
)engine = innodb default charset=utf8;

insert into test_innodb_lock values(1,'100','1');
insert into test_innodb_lock values(3,'3','1');
insert into test_innodb_lock values(4,'400','0');
insert into test_innodb_lock values(5,'500','1');
insert into test_innodb_lock values(6,'600','0');
insert into test_innodb_lock values(7,'700','0');
insert into test_innodb_lock values(8,'800','1');
insert into test_innodb_lock values(9,'900','1');
insert into test_innodb_lock values(1,'200','0');

create index idx_test_innodb_lock_id on test_innodb_lock(id);
create index idx_test_innodb_lock_name on test_innodb_lock(name);
</code></pre>
<h5 id="535-行锁基本演示">5.3.5 行锁基本演示</h5>
<table>
<thead>
<tr>
<th>Session-1</th>
<th>Session-2</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554354601867.png" alt="" loading="lazy">  关闭自动提交功能</td>
<td></td>
</tr>
<tr>
<td><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554354717336.png" alt="" loading="lazy"> 可以正常的查询出全部的数据</td>
<td></td>
</tr>
<tr>
<td><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554354832708.png" alt="" loading="lazy">获取id为3的数据 ；</td>
<td></td>
</tr>
<tr>
<td><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554382905352.png" alt="" loading="lazy"> 更新id为3 的数据， 出于等待状态</td>
<td></td>
</tr>
<tr>
<td><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554383044542.png" alt="" loading="lazy"> 解除阻塞，更新正常进行</td>
<td></td>
</tr>
<tr>
<td>以上， 操作的都是同一行的数据，接下来，演示不同行的数据 ：</td>
<td></td>
</tr>
<tr>
<td><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554385236768.png" alt="" loading="lazy"> 由于与Session-1 操作不是同一行，获取当前行锁，执行更新；</td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="536-无索引行锁升级为表锁">5.3.6 无索引行锁升级为表锁</h5>
<p>如果不通过索引条件检索数据，那么InnoDB将对表中的所有记录加锁，实际效果跟表锁一样。</p>
<p>查看当前表的索引 ： show  index  from test_innodb_lock ;</p>
<figure data-type="image" tabindex="120"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554385956215.png" alt="" loading="lazy"></figure>
<table>
<thead>
<tr>
<th>Session-1</th>
<th>Session-2</th>
</tr>
</thead>
<tbody>
<tr>
<td>关闭事务的自动提交<img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554386312524.png" alt="" loading="lazy"></td>
<td></td>
</tr>
<tr>
<td>执行更新语句 ：<img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554386685610.png" alt="" loading="lazy"></td>
<td></td>
</tr>
<tr>
<td>提交事务：<img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554386750004.png" alt="" loading="lazy"></td>
<td></td>
</tr>
<tr>
<td></td>
<td>执行提交操作 ：<img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554386804807.png" alt="" loading="lazy"></td>
</tr>
</tbody>
</table>
<p>由于 执行更新时 ， name字段本来为varchar类型， 我们是作为数组类型使用，存在类型转换，索引失效，最终行锁变为表锁 ；</p>
<h5 id="537-间隙锁危害">5.3.7 间隙锁危害</h5>
<p>当我们用范围条件，而不是使用相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据进行加锁； 对于键值在条件范围内但并不存在的记录，叫做 &quot;间隙（GAP）&quot; ， InnoDB也会对这个 &quot;间隙&quot; 加锁，这种锁机制就是所谓的 间隙锁（Next-Key锁） 。</p>
<p>示例 ：</p>
<table>
<thead>
<tr>
<th>Session-1</th>
<th>Session-2</th>
</tr>
</thead>
<tbody>
<tr>
<td>关闭事务自动提交 <img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554387994533.png" alt="" loading="lazy"></td>
<td></td>
</tr>
<tr>
<td>根据id范围更新数据<img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554388492478.png" alt="" loading="lazy"></td>
<td></td>
</tr>
<tr>
<td></td>
<td>插入id为2的记录， 出于阻塞状态<img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554388515936.png" alt="" loading="lazy"></td>
</tr>
<tr>
<td>提交事务 ；<img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554388149305.png" alt="" loading="lazy"></td>
<td></td>
</tr>
<tr>
<td></td>
<td>解除阻塞 ， 执行插入操作 ：<img src="https://memorykki.github.io/post-images/MySQL_hm/3/1554388548562.png" alt="" loading="lazy"></td>
</tr>
<tr>
<td></td>
<td>提交事务 ：</td>
</tr>
</tbody>
</table>
<h5 id="538-innodb-行锁争用情况">5.3.8 InnoDB 行锁争用情况</h5>
<pre><code class="language-sql">show  status like 'innodb_row_lock%';
</code></pre>
<figure data-type="image" tabindex="121"><img src="https://memorykki.github.io/post-images/MySQL_hm/3/1556455943670.png" alt="" loading="lazy"></figure>
<pre><code>Innodb_row_lock_current_waits: 当前正在等待锁定的数量

Innodb_row_lock_time: 从系统启动到现在锁定总时间长度

Innodb_row_lock_time_avg:每次等待所花平均时长

Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间

Innodb_row_lock_waits: 系统启动后到现在总共等待的次数


当等待的次数很高，而且每次等待的时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。

</code></pre>
<h5 id="539-总结">5.3.9 总结</h5>
<p>InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面带来了性能损耗可能比表锁会更高一些，但是在整体并发处理能力方面要远远由于MyISAM的表锁的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势。</p>
<p>但是，InnoDB的行级锁同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。</p>
<p>优化建议：</p>
<ul>
<li>尽可能让所有数据检索都能通过索引来完成，避免无索引行锁升级为表锁。</li>
<li>合理设计索引，尽量缩小锁的范围</li>
<li>尽可能减少索引条件，及索引范围，避免间隙锁</li>
<li>尽量控制事务大小，减少锁定资源量和时间长度</li>
<li>尽可使用低级别事务隔离（但是需要业务层面满足需求）</li>
</ul>
<h3 id="6-常用sql技巧">6. 常用SQL技巧</h3>
<h4 id="61-sql执行顺序">6.1 SQL执行顺序</h4>
<p>编写顺序</p>
<pre><code class="language-SQL">SELECT DISTINCT
	&lt;select list&gt;
FROM
	&lt;left_table&gt; &lt;join_type&gt;
JOIN
	&lt;right_table&gt; ON &lt;join_condition&gt;
WHERE
	&lt;where_condition&gt;
GROUP BY
	&lt;group_by_list&gt;
HAVING
	&lt;having_condition&gt;
ORDER BY
	&lt;order_by_condition&gt;
LIMIT
	&lt;limit_params&gt;
</code></pre>
<p>执行顺序</p>
<pre><code class="language-sql">FROM	&lt;left_table&gt;

ON 		&lt;join_condition&gt;

&lt;join_type&gt;		JOIN	&lt;right_table&gt;

WHERE		&lt;where_condition&gt;

GROUP BY 	&lt;group_by_list&gt;

HAVING		&lt;having_condition&gt;

SELECT DISTINCT		&lt;select list&gt;

ORDER BY	&lt;order_by_condition&gt;

LIMIT		&lt;limit_params&gt;
</code></pre>
<h4 id="62-正则表达式使用">6.2 正则表达式使用</h4>
<p>正则表达式（Regular Expression）是指一个用来描述或者匹配一系列符合某个句法规则的字符串的单个字符串。</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>^</td>
<td>在字符串开始处进行匹配</td>
</tr>
<tr>
<td>$</td>
<td>在字符串末尾处进行匹配</td>
</tr>
<tr>
<td>.</td>
<td>匹配任意单个字符, 包括换行符</td>
</tr>
<tr>
<td>[...]</td>
<td>匹配出括号内的任意字符</td>
</tr>
<tr>
<td>[^...]</td>
<td>匹配不出括号内的任意字符</td>
</tr>
<tr>
<td>a*</td>
<td>匹配零个或者多个a(包括空串)</td>
</tr>
<tr>
<td>a+</td>
<td>匹配一个或者多个a(不包括空串)</td>
</tr>
<tr>
<td>a?</td>
<td>匹配零个或者一个a</td>
</tr>
<tr>
<td>a1|a2</td>
<td>匹配a1或a2</td>
</tr>
<tr>
<td>a(m)</td>
<td>匹配m个a</td>
</tr>
<tr>
<td>a(m,)</td>
<td>至少匹配m个a</td>
</tr>
<tr>
<td>a(m,n)</td>
<td>匹配m个a 到 n个a</td>
</tr>
<tr>
<td>a(,n)</td>
<td>匹配0到n个a</td>
</tr>
<tr>
<td>(...)</td>
<td>将模式元素组成单一元素</td>
</tr>
</tbody>
</table>
<pre><code>select * from emp where name regexp '^T';

select * from emp where name regexp '2$';

select * from emp where name regexp '[uvw]';
</code></pre>
<h4 id="63-mysql-常用函数">6.3 MySQL 常用函数</h4>
<p>数字函数</p>
<table>
<thead>
<tr>
<th>函数名称</th>
<th>作 用</th>
</tr>
</thead>
<tbody>
<tr>
<td>ABS</td>
<td>求绝对值</td>
</tr>
<tr>
<td>SQRT</td>
<td>求二次方根</td>
</tr>
<tr>
<td>MOD</td>
<td>求余数</td>
</tr>
<tr>
<td>CEIL 和 CEILING</td>
<td>两个函数功能相同，都是返回不小于参数的最小整数，即向上取整</td>
</tr>
<tr>
<td>FLOOR</td>
<td>向下取整，返回值转化为一个BIGINT</td>
</tr>
<tr>
<td>RAND</td>
<td>生成一个0~1之间的随机数，传入整数参数是，用来产生重复序列</td>
</tr>
<tr>
<td>ROUND</td>
<td>对所传参数进行四舍五入</td>
</tr>
<tr>
<td>SIGN</td>
<td>返回参数的符号</td>
</tr>
<tr>
<td>POW 和 POWER</td>
<td>两个函数的功能相同，都是所传参数的次方的结果值</td>
</tr>
<tr>
<td>SIN</td>
<td>求正弦值</td>
</tr>
<tr>
<td>ASIN</td>
<td>求反正弦值，与函数 SIN 互为反函数</td>
</tr>
<tr>
<td>COS</td>
<td>求余弦值</td>
</tr>
<tr>
<td>ACOS</td>
<td>求反余弦值，与函数 COS 互为反函数</td>
</tr>
<tr>
<td>TAN</td>
<td>求正切值</td>
</tr>
<tr>
<td>ATAN</td>
<td>求反正切值，与函数 TAN 互为反函数</td>
</tr>
<tr>
<td>COT</td>
<td>求余切值</td>
</tr>
</tbody>
</table>
<p>字符串函数</p>
<table>
<thead>
<tr>
<th>函数名称</th>
<th>作 用</th>
</tr>
</thead>
<tbody>
<tr>
<td>LENGTH</td>
<td>计算字符串长度函数，返回字符串的字节长度</td>
</tr>
<tr>
<td>CONCAT</td>
<td>合并字符串函数，返回结果为连接参数产生的字符串，参数可以使一个或多个</td>
</tr>
<tr>
<td>INSERT</td>
<td>替换字符串函数</td>
</tr>
<tr>
<td>LOWER</td>
<td>将字符串中的字母转换为小写</td>
</tr>
<tr>
<td>UPPER</td>
<td>将字符串中的字母转换为大写</td>
</tr>
<tr>
<td>LEFT</td>
<td>从左侧字截取符串，返回字符串左边的若干个字符</td>
</tr>
<tr>
<td>RIGHT</td>
<td>从右侧字截取符串，返回字符串右边的若干个字符</td>
</tr>
<tr>
<td>TRIM</td>
<td>删除字符串左右两侧的空格</td>
</tr>
<tr>
<td>REPLACE</td>
<td>字符串替换函数，返回替换后的新字符串</td>
</tr>
<tr>
<td>SUBSTRING</td>
<td>截取字符串，返回从指定位置开始的指定长度的字符换</td>
</tr>
<tr>
<td>REVERSE</td>
<td>字符串反转（逆序）函数，返回与原始字符串顺序相反的字符串</td>
</tr>
</tbody>
</table>
<p>日期函数</p>
<table>
<thead>
<tr>
<th>函数名称</th>
<th>作 用</th>
</tr>
</thead>
<tbody>
<tr>
<td>CURDATE 和 CURRENT_DATE</td>
<td>两个函数作用相同，返回当前系统的日期值</td>
</tr>
<tr>
<td>CURTIME 和 CURRENT_TIME</td>
<td>两个函数作用相同，返回当前系统的时间值</td>
</tr>
<tr>
<td>NOW 和  SYSDATE</td>
<td>两个函数作用相同，返回当前系统的日期和时间值</td>
</tr>
<tr>
<td>MONTH</td>
<td>获取指定日期中的月份</td>
</tr>
<tr>
<td>MONTHNAME</td>
<td>获取指定日期中的月份英文名称</td>
</tr>
<tr>
<td>DAYNAME</td>
<td>获取指定曰期对应的星期几的英文名称</td>
</tr>
<tr>
<td>DAYOFWEEK</td>
<td>获取指定日期对应的一周的索引位置值</td>
</tr>
<tr>
<td>WEEK</td>
<td>获取指定日期是一年中的第几周，返回值的范围是否为 0〜52 或 1〜53</td>
</tr>
<tr>
<td>DAYOFYEAR</td>
<td>获取指定曰期是一年中的第几天，返回值范围是1~366</td>
</tr>
<tr>
<td>DAYOFMONTH</td>
<td>获取指定日期是一个月中是第几天，返回值范围是1~31</td>
</tr>
<tr>
<td>YEAR</td>
<td>获取年份，返回值范围是 1970〜2069</td>
</tr>
<tr>
<td>TIME_TO_SEC</td>
<td>将时间参数转换为秒数</td>
</tr>
<tr>
<td>SEC_TO_TIME</td>
<td>将秒数转换为时间，与TIME_TO_SEC 互为反函数</td>
</tr>
<tr>
<td>DATE_ADD 和 ADDDATE</td>
<td>两个函数功能相同，都是向日期添加指定的时间间隔</td>
</tr>
<tr>
<td>DATE_SUB 和 SUBDATE</td>
<td>两个函数功能相同，都是向日期减去指定的时间间隔</td>
</tr>
<tr>
<td>ADDTIME</td>
<td>时间加法运算，在原始时间上添加指定的时间</td>
</tr>
<tr>
<td>SUBTIME</td>
<td>时间减法运算，在原始时间上减去指定的时间</td>
</tr>
<tr>
<td>DATEDIFF</td>
<td>获取两个日期之间间隔，返回参数 1 减去参数 2 的值</td>
</tr>
<tr>
<td>DATE_FORMAT</td>
<td>格式化指定的日期，根据参数返回指定格式的值</td>
</tr>
<tr>
<td>WEEKDAY</td>
<td>获取指定日期在一周内的对应的工作日索引</td>
</tr>
</tbody>
</table>
<p>聚合函数</p>
<table>
<thead>
<tr>
<th>函数名称</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>MAX</td>
<td>查询指定列的最大值</td>
</tr>
<tr>
<td>MIN</td>
<td>查询指定列的最小值</td>
</tr>
<tr>
<td>COUNT</td>
<td>统计查询结果的行数</td>
</tr>
<tr>
<td>SUM</td>
<td>求和，返回指定列的总和</td>
</tr>
<tr>
<td>AVG</td>
<td>求平均值，返回指定列数据的平均值</td>
</tr>
</tbody>
</table>
<h1 id="mysql高级-day04">Mysql高级-day04</h1>
<h3 id="1-mysql中常用工具">1. MySql中常用工具</h3>
<h4 id="11-mysql">1.1 mysql</h4>
<p>该mysql不是指mysql服务，而是指mysql的客户端工具。</p>
<p>语法 ：</p>
<pre><code>mysql [options] [database]
</code></pre>
<h5 id="111-连接选项">1.1.1 连接选项</h5>
<pre><code>参数 ： 
	-u, --user=name			指定用户名
	-p, --password[=name]	指定密码
	-h, --host=name			指定服务器IP或域名
	-P, --port=#			指定连接端口

示例 ：
	mysql -h 127.0.0.1 -P 3306 -u root -p
	
	mysql -h127.0.0.1 -P3306 -uroot -p2143
	
</code></pre>
<h5 id="112-执行选项">1.1.2 执行选项</h5>
<pre><code>-e, --execute=name		执行SQL语句并退出
</code></pre>
<p>此选项可以在Mysql客户端执行SQL语句，而不用连接到MySQL数据库再执行，对于一些批处理脚本，这种方式尤其方便。</p>
<pre><code>示例：
	mysql -uroot -p2143 db01 -e &quot;select * from tb_book&quot;;
</code></pre>
<figure data-type="image" tabindex="122"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555325632715.png" alt="" loading="lazy"></figure>
<h4 id="12-mysqladmin">1.2 mysqladmin</h4>
<p>mysqladmin 是一个执行管理操作的客户端程序。可以用它来检查服务器的配置和当前状态、创建并删除数据库等。</p>
<p>可以通过 ： mysqladmin --help  指令查看帮助文档</p>
<figure data-type="image" tabindex="123"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555326108697.png" alt="" loading="lazy"></figure>
<pre><code>示例 ：
	mysqladmin -uroot -p2143 create 'test01';  
	mysqladmin -uroot -p2143 drop 'test01';
	mysqladmin -uroot -p2143 version;
	
</code></pre>
<h4 id="13-mysqlbinlog">1.3 mysqlbinlog</h4>
<p>由于服务器生成的二进制日志文件以二进制格式保存，所以如果想要检查这些文本的文本格式，就会使用到mysqlbinlog 日志管理工具。</p>
<p>语法 ：</p>
<pre><code>mysqlbinlog [options]  log-files1 log-files2 ...

选项：
	
	-d, --database=name : 指定数据库名称，只列出指定的数据库相关操作。
	
	-o, --offset=# : 忽略掉日志中的前n行命令。
	
	-r,--result-file=name : 将输出的文本格式日志输出到指定文件。
	
	-s, --short-form : 显示简单格式， 省略掉一些信息。
	
	--start-datatime=date1  --stop-datetime=date2 : 指定日期间隔内的所有日志。
	
	--start-position=pos1 --stop-position=pos2 : 指定位置间隔内的所有日志。
</code></pre>
<h4 id="14-mysqldump">1.4 mysqldump</h4>
<p>mysqldump 客户端工具用来备份数据库或在不同数据库之间进行数据迁移。备份内容包含创建表，及插入表的SQL语句。</p>
<p>语法 ：</p>
<pre><code>mysqldump [options] db_name [tables]

mysqldump [options] --database/-B db1 [db2 db3...]

mysqldump [options] --all-databases/-A
</code></pre>
<h5 id="141-连接选项">1.4.1 连接选项</h5>
<pre><code>参数 ： 
	-u, --user=name			指定用户名
	-p, --password[=name]	指定密码
	-h, --host=name			指定服务器IP或域名
	-P, --port=#			指定连接端口
</code></pre>
<h5 id="142-输出内容选项">1.4.2 输出内容选项</h5>
<pre><code>参数：
	--add-drop-database		在每个数据库创建语句前加上 Drop database 语句
	--add-drop-table		在每个表创建语句前加上 Drop table 语句 , 默认开启 ; 不开启 (--skip-add-drop-table)
	
	-n, --no-create-db		不包含数据库的创建语句
	-t, --no-create-info	不包含数据表的创建语句
	-d --no-data			不包含数据
	
	 -T, --tab=name			自动生成两个文件：一个.sql文件，创建表结构的语句；
	 						一个.txt文件，数据文件，相当于select into outfile  
</code></pre>
<pre><code>示例 ： 
	mysqldump -uroot -p2143 db01 tb_book --add-drop-database --add-drop-table &gt; a
	
	mysqldump -uroot -p2143 -T /tmp test city
</code></pre>
<figure data-type="image" tabindex="124"><img src="C:%5CUsers%5CAdministrator%5CAppData%5CRoaming%5CTypora%5Ctypora-user-imagess%5C1555501806693.png" alt="1555501806693" loading="lazy"></figure>
<h4 id="15-mysqlimportsource">1.5 mysqlimport/source</h4>
<p>mysqlimport 是客户端数据导入工具，用来导入mysqldump 加 -T 参数后导出的文本文件。</p>
<p>语法：</p>
<pre><code>mysqlimport [options]  db_name  textfile1  [textfile2...]
</code></pre>
<p>示例：</p>
<pre><code>mysqlimport -uroot -p2143 test /tmp/city.txt
</code></pre>
<p>如果需要导入sql文件,可以使用mysql中的source 指令 :</p>
<pre><code>source /root/tb_book.sql
</code></pre>
<h4 id="16-mysqlshow">1.6 mysqlshow</h4>
<p>mysqlshow 客户端对象查找工具，用来很快地查找存在哪些数据库、数据库中的表、表中的列或者索引。</p>
<p>语法：</p>
<pre><code>mysqlshow [options] [db_name [table_name [col_name]]]
</code></pre>
<p>参数：</p>
<pre><code>--count		显示数据库及表的统计信息（数据库，表 均可以不指定）-i			显示指定数据库或者指定表的状态信息
</code></pre>
<p>示例：</p>
<pre><code>#查询每个数据库的表的数量及表中记录的数量mysqlshow -uroot -p2143 --count#查询test库中每个表中的字段书，及行数mysqlshow -uroot -p2143 test --count#查询test库中book表的详细情况mysqlshow -uroot -p2143 test book --count
</code></pre>
<h3 id="2-mysql-日志">2. Mysql 日志</h3>
<p>在任何一种数据库中，都会有各种各样的日志，记录着数据库工作的方方面面，以帮助数据库管理员追踪数据库曾经发生过的各种事件。MySQL 也不例外，在 MySQL 中，有 4 种不同的日志，分别是错误日志、二进制日志（BINLOG 日志）、查询日志和慢查询日志，这些日志记录着数据库在不同方面的踪迹。</p>
<h4 id="21-错误日志">2.1 错误日志</h4>
<p>错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，可以首先查看此日志。</p>
<p>该日志是默认开启的 ， 默认存放目录为 mysql 的数据目录（var/lib/mysql）, 默认的日志文件名为  hostname.err（hostname是主机名）。</p>
<p>查看日志位置指令 ：</p>
<pre><code class="language-sql">show variables like 'log_error%';
</code></pre>
<figure data-type="image" tabindex="125"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1553993244446.png" alt="" loading="lazy"></figure>
<p>查看日志内容 ：</p>
<pre><code class="language-shell">tail -f /var/lib/mysql/xaxh-server.err
</code></pre>
<figure data-type="image" tabindex="126"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1553993537874.png" alt="" loading="lazy"></figure>
<h4 id="22-二进制日志">2.2 二进制日志</h4>
<h5 id="221概述">2.2.1概述</h5>
<p>二进制日志（BINLOG）记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句，但是不包括数据查询语句。此日志对于灾难时的数据恢复起着极其重要的作用，MySQL的主从复制， 就是通过该binlog实现的。</p>
<p>二进制日志，默认情况下是没有开启的，需要到MySQL的配置文件中开启，并配置MySQL日志的格式。</p>
<p>配置文件位置 : /usr/my.cnf</p>
<p>日志存放位置 : 配置时，给定了文件名但是没有指定路径，日志默认写入Mysql的数据目录。</p>
<pre><code>#配置开启binlog日志， 日志的文件前缀为 mysqlbin -----&gt; 生成的文件名如 : mysqlbin.000001,mysqlbin.000002
log_bin=mysqlbin

#配置二进制日志的格式
binlog_format=STATEMENT

</code></pre>
<h5 id="222-日志格式">2.2.2 日志格式</h5>
<p><strong>STATEMENT</strong></p>
<p>该日志格式在日志文件中记录的都是SQL语句（statement），每一条对数据进行修改的SQL都会记录在日志文件中，通过Mysql提供的mysqlbinlog工具，可以清晰的查看到每条语句的文本。主从复制的时候，从库（slave）会将日志解析为原文本，并在从库重新执行一次。</p>
<p><strong>ROW</strong></p>
<p>该日志格式在日志文件中记录的是每一行的数据变更，而不是记录SQL语句。比如，执行SQL语句 ： update tb_book set status='1' , 如果是STATEMENT 日志格式，在日志中会记录一行SQL文件； 如果是ROW，由于是对全表进行更新，也就是每一行记录都会发生变更，ROW 格式的日志中会记录每一行的数据变更。</p>
<p><strong>MIXED</strong></p>
<p>这是目前MySQL默认的日志格式，即混合了STATEMENT 和 ROW两种格式。默认情况下采用STATEMENT，但是在一些特殊情况下采用ROW来进行记录。MIXED 格式能尽量利用两种模式的优点，而避开他们的缺点。</p>
<h5 id="223-日志读取">2.2.3 日志读取</h5>
<p>由于日志以二进制方式存储，不能直接读取，需要用mysqlbinlog工具来查看，语法如下 ：</p>
<pre><code>mysqlbinlog log-file；
</code></pre>
<p><strong>查看STATEMENT格式日志</strong></p>
<p>执行插入语句 ：</p>
<pre><code class="language-SQL">insert into tb_book values(null,'Lucene','2088-05-01','0');
</code></pre>
<p>查看日志文件 ：</p>
<figure data-type="image" tabindex="127"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554079717375.png" alt="" loading="lazy"></figure>
<p>mysqlbin.index : 该文件是日志索引文件 ， 记录日志的文件名；</p>
<p>mysqlbing.000001 ：日志文件</p>
<p>查看日志内容 ：</p>
<pre><code>mysqlbinlog mysqlbing.000001；
</code></pre>
<figure data-type="image" tabindex="128"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554080016778.png" alt="" loading="lazy"></figure>
<p><strong>查看ROW格式日志</strong></p>
<p>配置 :</p>
<pre><code>#配置开启binlog日志， 日志的文件前缀为 mysqlbin -----&gt; 生成的文件名如 : mysqlbin.000001,mysqlbin.000002
log_bin=mysqlbin

#配置二进制日志的格式
binlog_format=ROW

</code></pre>
<p>插入数据 :</p>
<pre><code class="language-sql">insert into tb_book values(null,'SpringCloud实战','2088-05-05','0');
</code></pre>
<p>如果日志格式是 ROW , 直接查看数据 , 是查看不懂的 ; 可以在mysqlbinlog 后面加上参数 -vv</p>
<pre><code class="language-SQL">mysqlbinlog -vv mysqlbin.000002 
</code></pre>
<figure data-type="image" tabindex="129"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554095452022.png" alt="" loading="lazy"></figure>
<h5 id="224-日志删除">2.2.4 日志删除</h5>
<p>对于比较繁忙的系统，由于每天生成日志量大 ，这些日志如果长时间不清楚，将会占用大量的磁盘空间。下面我们将会讲解几种删除日志的常见方法 ：</p>
<p><strong>方式一</strong></p>
<p>通过 Reset Master 指令删除全部 binlog 日志，删除之后，日志编号，将从 xxxx.000001重新开始 。</p>
<p>查询之前 ，先查询下日志文件 ：</p>
<figure data-type="image" tabindex="130"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554118609489.png" alt="" loading="lazy"></figure>
<p>执行删除日志指令：</p>
<pre><code>Reset Master
</code></pre>
<p>执行之后， 查看日志文件 ：</p>
<figure data-type="image" tabindex="131"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554118675264.png" alt="" loading="lazy"></figure>
<p><strong>方式二</strong></p>
<p>执行指令 <code> purge  master logs to 'mysqlbin.******'</code> ，该命令将删除  <code> ******</code> 编号之前的所有日志。</p>
<p><strong>方式三</strong></p>
<p>执行指令 <code> purge master logs before 'yyyy-mm-dd hh24:mi:ss'</code> ，该命令将删除日志为 &quot;yyyy-mm-dd hh24:mi:ss&quot; 之前产生的所有日志 。</p>
<p><strong>方式四</strong></p>
<p>设置参数 --expire_logs_days=# ，此参数的含义是设置日志的过期天数， 过了指定的天数后日志将会被自动删除，这样将有利于减少DBA 管理日志的工作量。</p>
<p>配置如下 ：</p>
<figure data-type="image" tabindex="132"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554125506938.png" alt="" loading="lazy"></figure>
<h4 id="23-查询日志">2.3 查询日志</h4>
<p>查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的SQL语句。</p>
<p>默认情况下， 查询日志是未开启的。如果需要开启查询日志，可以设置以下配置 ：</p>
<pre><code>#该选项用来开启查询日志 ， 可选值 ： 0 或者 1 ； 0 代表关闭， 1 代表开启 
general_log=1

#设置日志的文件名 ， 如果没有指定， 默认的文件名为 host_name.log 
general_log_file=file_name

</code></pre>
<p>在 mysql 的配置文件 /usr/my.cnf 中配置如下内容 ：</p>
<figure data-type="image" tabindex="133"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554128184632.png" alt="" loading="lazy"></figure>
<p>配置完毕之后，在数据库执行以下操作 ：</p>
<pre><code>select * from tb_book;
select * from tb_book where id = 1;
update tb_book set name = 'lucene入门指南' where id = 5;
select * from tb_book where id &lt; 8;

</code></pre>
<p>执行完毕之后， 再次来查询日志文件 ：</p>
<figure data-type="image" tabindex="134"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554128089851.png" alt="" loading="lazy"></figure>
<h4 id="24-慢查询日志">2.4 慢查询日志</h4>
<p>慢查询日志记录了所有执行时间超过参数 long_query_time 设置值并且扫描记录数不小于 min_examined_row_limit 的所有的SQL语句的日志。long_query_time 默认为 10 秒，最小为 0， 精度可以到微秒。</p>
<h5 id="241-文件位置和格式">2.4.1 文件位置和格式</h5>
<p>慢查询日志默认是关闭的 。可以通过两个参数来控制慢查询日志 ：</p>
<pre><code># 该参数用来控制慢查询日志是否开启， 可取值： 1 和 0 ， 1 代表开启， 0 代表关闭slow_query_log=1 # 该参数用来指定慢查询日志的文件名slow_query_log_file=slow_query.log# 该选项用来配置查询的时间限制， 超过这个时间将认为值慢查询， 将需要进行日志记录， 默认10slong_query_time=10
</code></pre>
<h5 id="242-日志的读取">2.4.2 日志的读取</h5>
<p>和错误日志、查询日志一样，慢查询日志记录的格式也是纯文本，可以被直接读取。</p>
<p>1） 查询long_query_time 的值。</p>
<figure data-type="image" tabindex="135"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554130333472.png" alt="" loading="lazy"></figure>
<p>2） 执行查询操作</p>
<pre><code class="language-sql">select id, title,price,num ,status from tb_item where id = 1;
</code></pre>
<figure data-type="image" tabindex="136"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554130448709.png" alt="" loading="lazy"></figure>
<p>由于该语句执行时间很短，为0s ， 所以不会记录在慢查询日志中。</p>
<pre><code>select * from tb_item where title like '%阿尔卡特 (OT-927) 炭黑 联通3G手机 双卡双待165454%' ;
</code></pre>
<figure data-type="image" tabindex="137"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554130532577.png" alt="" loading="lazy"></figure>
<p>该SQL语句 ， 执行时长为 26.77s ，超过10s ， 所以会记录在慢查询日志文件中。</p>
<p>3） 查看慢查询日志文件</p>
<p>直接通过cat 指令查询该日志文件 ：</p>
<figure data-type="image" tabindex="138"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554130669360.png" alt="" loading="lazy"></figure>
<p>如果慢查询日志内容很多， 直接查看文件，比较麻烦， 这个时候可以借助于mysql自带的 mysqldumpslow 工具， 来对慢查询日志进行分类汇总。</p>
<figure data-type="image" tabindex="139"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554130856485.png" alt="" loading="lazy"></figure>
<h3 id="3-mysql复制">3. Mysql复制</h3>
<h4 id="31-复制概述">3.1 复制概述</h4>
<p>复制是指将主数据库的DDL 和 DML 操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。</p>
<p>MySQL支持一台主库同时向多台从库进行复制， 从库同时也可以作为其他从服务器的主库，实现链状复制。</p>
<h4 id="32-复制原理">3.2 复制原理</h4>
<p>MySQL 的主从复制原理如下。</p>
<figure data-type="image" tabindex="140"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1.jpg" alt="" loading="lazy"></figure>
<p>从上层来看，复制分成三步：</p>
<ul>
<li>
<p>Master 主库在事务提交时，会把数据变更作为时间 Events 记录在二进制日志文件 Binlog 中。</p>
</li>
<li>
<p>主库推送二进制日志文件 Binlog 中的日志事件到从库的中继日志 Relay Log 。</p>
</li>
<li>
<p>slave重做中继日志中的事件，将改变反映它自己的数据。</p>
</li>
</ul>
<h4 id="33-复制优势">3.3 复制优势</h4>
<p>MySQL 复制的有点主要包含以下三个方面：</p>
<ul>
<li>
<p>主库出现问题，可以快速切换到从库提供服务。</p>
</li>
<li>
<p>可以在从库上执行查询操作，从主库中更新，实现读写分离，降低主库的访问压力。</p>
</li>
<li>
<p>可以在从库中执行备份，以避免备份期间影响主库的服务。</p>
</li>
</ul>
<h4 id="34-搭建步骤">3.4 搭建步骤</h4>
<h5 id="341-master">3.4.1 master</h5>
<p>1） 在master 的配置文件（/usr/my.cnf）中，配置如下内容：</p>
<pre><code class="language-properties">#mysql 服务ID,保证整个集群环境中唯一
server-id=1

#mysql binlog 日志的存储路径和文件名
log-bin=/var/lib/mysql/mysqlbin

#错误日志,默认已经开启
#log-err

#mysql的安装目录
#basedir

#mysql的临时目录
#tmpdir

#mysql的数据存放目录
#datadir

#是否只读,1 代表只读, 0 代表读写
read-only=0

#忽略的数据, 指不需要同步的数据库
binlog-ignore-db=mysql

#指定同步的数据库
#binlog-do-db=db01
</code></pre>
<p>2） 执行完毕之后，需要重启Mysql：</p>
<pre><code class="language-sql">service mysql restart ；
</code></pre>
<p>3） 创建同步数据的账户，并且进行授权操作：</p>
<pre><code class="language-sql">grant replication slave on *.* to 'itcast'@'192.168.192.131' identified by 'itcast';	

flush privileges;
</code></pre>
<p>4） 查看master状态：</p>
<pre><code class="language-sql">show master status;
</code></pre>
<figure data-type="image" tabindex="141"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554477759735.png" alt="" loading="lazy"></figure>
<p>字段含义：</p>
<pre><code>File : 从哪个日志文件开始推送日志文件 Position ： 从哪个位置开始推送日志Binlog_Ignore_DB : 指定不需要同步的数据库
</code></pre>
<h5 id="342-slave">3.4.2 slave</h5>
<p>1） 在 slave 端配置文件中，配置如下内容：</p>
<pre><code class="language-properties">#mysql服务端ID,唯一
server-id=2

#指定binlog日志
log-bin=/var/lib/mysql/mysqlbin
</code></pre>
<p>2）  执行完毕之后，需要重启Mysql：</p>
<pre><code>service mysql restart；
</code></pre>
<p>3） 执行如下指令 ：</p>
<pre><code class="language-sql">change master to master_host= '192.168.192.130', master_user='itcast', master_password='itcast', master_log_file='mysqlbin.000001', master_log_pos=413;
</code></pre>
<p>指定当前从库对应的主库的IP地址，用户名，密码，从哪个日志文件开始的那个位置开始同步推送日志。</p>
<p>4） 开启同步操作</p>
<pre><code>start slave;

show slave status;
</code></pre>
<figure data-type="image" tabindex="142"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554479387365.png" alt="" loading="lazy"></figure>
<p>5） 停止同步操作</p>
<pre><code>stop slave;
</code></pre>
<h5 id="343-验证同步操作">3.4.3 验证同步操作</h5>
<p>1） 在主库中创建数据库，创建表，并插入数据 ：</p>
<pre><code class="language-sql">create database db01;

user db01;

create table user(
	id int(11) not null auto_increment,
	name varchar(50) not null,
	sex varchar(1),
	primary key (id)
)engine=innodb default charset=utf8;

insert into user(id,name,sex) values(null,'Tom','1');
insert into user(id,name,sex) values(null,'Trigger','0');
insert into user(id,name,sex) values(null,'Dawn','1');
</code></pre>
<p>2） 在从库中查询数据，进行验证 ：</p>
<p>在从库中，可以查看到刚才创建的数据库：</p>
<figure data-type="image" tabindex="143"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554544658640.png" alt="" loading="lazy"></figure>
<p>在该数据库中，查询user表中的数据：</p>
<figure data-type="image" tabindex="144"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1554544679538.png" alt="" loading="lazy"></figure>
<h3 id="4-综合案例">4. 综合案例</h3>
<h4 id="41-需求分析">4.1 需求分析</h4>
<p>在业务系统中，需要记录当前业务系统的访问日志，该访问日志包含：操作人，操作时间，访问类，访问方法，请求参数，请求结果，请求结果类型，请求时长 等信息。记录详细的系统访问日志，主要便于对系统中的用户请求进行追踪，并且在系统 的管理后台可以查看到用户的访问记录。</p>
<p>记录系统中的日志信息，可以通过Spring 框架的AOP来实现。具体的请求处理流程，如下：</p>
<figure data-type="image" tabindex="145"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555075760661.png" alt="" loading="lazy"></figure>
<h4 id="42-搭建案例环境">4.2 搭建案例环境</h4>
<h5 id="421-数据库表">4.2.1 数据库表</h5>
<pre><code class="language-sql">CREATE DATABASE mysql_demo DEFAULT CHARACTER SET utf8mb4 ；

CREATE TABLE `brand` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) DEFAULT NULL COMMENT '品牌名称',
  `first_char` varchar(1) DEFAULT NULL COMMENT '品牌首字母',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;



CREATE TABLE `item` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '商品id',
  `title` varchar(100) NOT NULL COMMENT '商品标题',
  `price` double(10,2) NOT NULL COMMENT '商品价格，单位为：元',
  `num` int(10) NOT NULL COMMENT '库存数量',
  `categoryid` bigint(10) NOT NULL COMMENT '所属类目，叶子类目',
  `status` varchar(1) DEFAULT NULL COMMENT '商品状态，1-正常，2-下架，3-删除',
  `sellerid` varchar(50) DEFAULT NULL COMMENT '商家ID',
  `createtime` datetime DEFAULT NULL COMMENT '创建时间',
  `updatetime` datetime DEFAULT NULL COMMENT '更新时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='商品表';



CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `username` varchar(45) NOT NULL,
  `password` varchar(96) NOT NULL,
  `name` varchar(45) NOT NULL,
  `birthday` datetime DEFAULT NULL,
  `sex` char(1) DEFAULT NULL,
  `email` varchar(45) DEFAULT NULL,
  `phone` varchar(45) DEFAULT NULL,
  `qq` varchar(32) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;


CREATE TABLE `operation_log` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'ID',
  `operate_class` varchar(200) DEFAULT NULL COMMENT '操作类',
  `operate_method` varchar(200) DEFAULT NULL COMMENT '操作方法',
  `return_class` varchar(200) DEFAULT NULL COMMENT '返回值类型',
  `operate_user` varchar(20) DEFAULT NULL COMMENT '操作用户',
  `operate_time` varchar(20) DEFAULT NULL COMMENT '操作时间',
  `param_and_value` varchar(500) DEFAULT NULL COMMENT '请求参数名及参数值',
  `cost_time` bigint(20) DEFAULT NULL COMMENT '执行方法耗时, 单位 ms',
  `return_value` varchar(200) DEFAULT NULL COMMENT '返回值',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4;

</code></pre>
<h5 id="422-pomxml">4.2.2 pom.xml</h5>
<pre><code class="language-xml">&lt;properties&gt;
  &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
  &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;
  &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;

  &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
  &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
  &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
  &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt;
  &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt;
  &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt;
  &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt;
&lt;/properties&gt;

&lt;dependencies&gt; &lt;!-- spring --&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.aspectj&lt;/groupId&gt;
    &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;
    &lt;version&gt;1.6.8&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
    &lt;artifactId&gt;lombok&lt;/artifactId&gt;
    &lt;version&gt;1.16.16&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.springframework&lt;/groupId&gt;
    &lt;artifactId&gt;spring-context&lt;/artifactId&gt;
    &lt;version&gt;${spring.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.springframework&lt;/groupId&gt;
    &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt;
    &lt;version&gt;${spring.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.springframework&lt;/groupId&gt;
    &lt;artifactId&gt;spring-orm&lt;/artifactId&gt;
    &lt;version&gt;${spring.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.springframework&lt;/groupId&gt;
    &lt;artifactId&gt;spring-test&lt;/artifactId&gt;
    &lt;version&gt;${spring.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.springframework&lt;/groupId&gt;
    &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;
    &lt;version&gt;${spring.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.springframework&lt;/groupId&gt;
    &lt;artifactId&gt;spring-tx&lt;/artifactId&gt;
    &lt;version&gt;${spring.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;junit&lt;/groupId&gt;
    &lt;artifactId&gt;junit&lt;/artifactId&gt;
    &lt;version&gt;4.12&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;javax.servlet&lt;/groupId&gt;
    &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt;
    &lt;version&gt;3.1.0&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt;
    &lt;artifactId&gt;jsp-api&lt;/artifactId&gt;
    &lt;version&gt;2.0&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
  &lt;/dependency&gt;


  &lt;dependency&gt;
    &lt;groupId&gt;log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j&lt;/artifactId&gt;
    &lt;version&gt;${log4j.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.mybatis&lt;/groupId&gt;
    &lt;artifactId&gt;mybatis&lt;/artifactId&gt;
    &lt;version&gt;${mybatis.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.mybatis&lt;/groupId&gt;
    &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;
    &lt;version&gt;1.3.0&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;c3p0&lt;/groupId&gt;
    &lt;artifactId&gt;c3p0&lt;/artifactId&gt;
    &lt;version&gt;0.9.1.2&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;5.1.5&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
    &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;
    &lt;version&gt;2.9.0&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
    &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;
    &lt;version&gt;2.9.0&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
    &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;
    &lt;version&gt;2.9.0&lt;/version&gt;
  &lt;/dependency&gt;
&lt;/dependencies&gt;




 &lt;build&gt;
   &lt;plugins&gt;
     &lt;plugin&gt;
       &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt;
       &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt;
       &lt;version&gt;2.2&lt;/version&gt;
       &lt;configuration&gt;
         &lt;port&gt;8080&lt;/port&gt;
         &lt;path&gt;/&lt;/path&gt;
         &lt;uriEncoding&gt;utf-8&lt;/uriEncoding&gt;
       &lt;/configuration&gt;
     &lt;/plugin&gt;
   &lt;/plugins&gt;
 &lt;/build&gt;
</code></pre>
<h5 id="423-webxml">4.2.3 web.xml</h5>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot;
       xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot;
       version=&quot;2.5&quot;&gt;

    &lt;!-- 解决post乱码 --&gt;
    &lt;filter&gt;
        &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;
        &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;
        &lt;init-param&gt;
            &lt;param-name&gt;encoding&lt;/param-name&gt;
            &lt;param-value&gt;utf-8&lt;/param-value&gt;
        &lt;/init-param&gt;
        &lt;init-param&gt;
            &lt;param-name&gt;forceEncoding&lt;/param-name&gt;
            &lt;param-value&gt;true&lt;/param-value&gt;
        &lt;/init-param&gt;
    &lt;/filter&gt;
    &lt;filter-mapping&gt;
        &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;
        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
    &lt;/filter-mapping&gt;

    &lt;context-param&gt;
        &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;
        &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;
    &lt;/context-param&gt;
    &lt;listener&gt;
        &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;
    &lt;/listener&gt;


    &lt;servlet&gt;
        &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt;
        &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;
        &lt;!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载--&gt;
        &lt;init-param&gt;
            &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;
            &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt;
        &lt;/init-param&gt;
    &lt;/servlet&gt;
    &lt;servlet-mapping&gt;
        &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt;
        &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;
    &lt;/servlet-mapping&gt;

    &lt;welcome-file-list&gt;
      &lt;welcome-file&gt;log-datalist.html&lt;/welcome-file&gt;
    &lt;/welcome-file-list&gt;
&lt;/web-app&gt;
</code></pre>
<h5 id="424-dbproperties">4.2.4 db.properties</h5>
<pre><code class="language-properties">jdbc.driver=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://192.168.142.128:3306/mysql_demo
jdbc.username=root
jdbc.password=itcast
</code></pre>
<h5 id="425-applicationcontextxml">4.2.5 applicationContext.xml</h5>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;
       xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;
       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;
	   xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
                           http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd
                            http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd
                            http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;

    &lt;!-- 加载配置文件 --&gt;
    &lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt;

    &lt;!-- 配置 spring 创建容器时要扫描的包 --&gt;
    &lt;context:component-scan base-package=&quot;cn.itcast&quot;&gt;
        &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;&gt;	
        &lt;/context:exclude-filter&gt;
    &lt;/context:component-scan&gt;

    &lt;!-- 配置 MyBatis 的 Session 工厂 --&gt;
    &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt;
        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;
        &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;cn.itcast.pojo&quot;/&gt;
     &lt;/bean&gt;

    &lt;!-- 配置数据源 --&gt;
    &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt;
        &lt;property name=&quot;driverClass&quot; value=&quot;${jdbc.driver}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;jdbcUrl&quot; value=&quot;${jdbc.url}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;user&quot; value=&quot;${jdbc.username}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;password&quot; value=&quot;${jdbc.password}&quot;&gt;&lt;/property&gt;
    &lt;/bean&gt;

    &lt;!-- 配置 Mapper 扫描器 --&gt;
    &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;
        &lt;property name=&quot;basePackage&quot; value=&quot;cn.itcast.mapper&quot;/&gt;
    &lt;/bean&gt;

    &lt;!-- 配置事务管理器 --&gt;
    &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;
        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;
    &lt;/bean&gt;

    &lt;!-- 配置事务的注解驱动 --&gt;
    &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;&gt;&lt;/tx:annotation-driven&gt;
&lt;/beans&gt;
</code></pre>
<h5 id="426-springmvcxml">4.2.6 springmvc.xml</h5>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
       xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot;
       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;
       xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans
            http://www.springframework.org/schema/beans/spring-beans.xsd
            http://www.springframework.org/schema/mvc
            http://www.springframework.org/schema/mvc/spring-mvc.xsd
            http://www.springframework.org/schema/aop
            http://www.springframework.org/schema/aop/spring-aop.xsd
            http://www.springframework.org/schema/context
            http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;

    &lt;context:component-scan base-package=&quot;cn.itcast.controller&quot;&gt;&lt;/context:component-scan&gt;

    &lt;mvc:annotation-driven&gt;&lt;/mvc:annotation-driven&gt;

    &lt;aop:aspectj-autoproxy /&gt;

&lt;/beans&gt;
</code></pre>
<h5 id="427-导入基础工程">4.2.7 导入基础工程</h5>
<figure data-type="image" tabindex="146"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555076434270.png" alt="" loading="lazy"></figure>
<h4 id="43-通过aop记录操作日志">4.3 通过AOP记录操作日志</h4>
<h5 id="431-自定义注解">4.3.1 自定义注解</h5>
<p>通过自定义注解，来标示方法需不需要进行记录日志，如果该方法在访问时需要记录日志，则在该方法上标示该注解既可。</p>
<pre><code class="language-java">@Inherited
@Documented
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface OperateLog {
}
</code></pre>
<h5 id="432-定义通知类">4.3.2 定义通知类</h5>
<pre><code class="language-java">@Component
@Aspect
public class OperateAdvice {
   
   private static Logger log = Logger.getLogger(OperateAdvice.class);
   
   @Autowired
   private OperationLogService operationLogService;
   

   @Around(&quot;execution(* cn.itcast.controller.*.*(..)) &amp;&amp; @annotation(operateLog)&quot;)
   public Object insertLogAround(ProceedingJoinPoint pjp , OperateLog operateLog) throws Throwable{
      System.out.println(&quot; ************************ 记录日志 [start]  ****************************** &quot;);
      
      OperationLog op = new OperationLog();
      
      DateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);

      op.setOperateTime(sdf.format(new Date()));
      op.setOperateUser(DataUtils.getRandStr(8));
      
      op.setOperateClass(pjp.getTarget().getClass().getName());
      op.setOperateMethod(pjp.getSignature().getName());
      
      //获取方法调用时传递的参数
      Object[] args = pjp.getArgs();
      op.setParamAndValue(Arrays.toString(args));

      long start_time = System.currentTimeMillis();

      //放行
      Object object = pjp.proceed();

      long end_time = System.currentTimeMillis();
      op.setCostTime(end_time - start_time);

      if(object != null){
         op.setReturnClass(object.getClass().getName());
         op.setReturnValue(object.toString());
      }else{
         op.setReturnClass(&quot;java.lang.Object&quot;);
         op.setParamAndValue(&quot;void&quot;);
      }

      log.error(JsonUtils.obj2JsonString(op));

      operationLogService.insert(op);
      
      System.out.println(&quot; ************************** 记录日志 [end]  *************************** &quot;);
      
      return object;
   }
   
}
</code></pre>
<h5 id="433-方法上加注解">4.3.3 方法上加注解</h5>
<p>在需要记录日志的方法上加上注解@OperateLog。</p>
<pre><code class="language-java">@OperateLog
@RequestMapping(&quot;/insert&quot;)
public Result insert(@RequestBody Brand brand){
    try {
        brandService.insert(brand);
        return new Result(true,&quot;操作成功&quot;);
    } catch (Exception e) {
        e.printStackTrace();
        return new Result(false,&quot;操作失败&quot;);
    }
}
</code></pre>
<h4 id="44-日志查询后端代码实现">4.4 日志查询后端代码实现</h4>
<h5 id="441-mapper接口">4.4.1 Mapper接口</h5>
<pre><code class="language-java">public interface OperationLogMapper {

    public void insert(OperationLog operationLog);

    public List&lt;OperationLog&gt; selectListByCondition(Map dataMap);

    public Long countByCondition(Map dataMap);

}
</code></pre>
<h5 id="442-mapperxml-映射配置文件">4.4.2 Mapper.xml 映射配置文件</h5>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt;
&lt;mapper namespace=&quot;cn.itcast.mapper.OperationLogMapper&quot; &gt;

    &lt;insert id=&quot;insert&quot; parameterType=&quot;operationLog&quot;&gt;
        INSERT INTO operation_log(id,return_value,return_class,operate_user,operate_time,param_and_value,
        operate_class,operate_method,cost_time)
      VALUES(NULL,#{returnValue},#{returnClass},#{operateUser},#{operateTime},#{paramAndValue},
        #{operateClass},#{operateMethod},#{costTime})
    &lt;/insert&gt;

    &lt;select id=&quot;selectListByCondition&quot; parameterType=&quot;map&quot; resultType=&quot;operationLog&quot;&gt;
      select
        id ,
        operate_class as operateClass ,
        operate_method as operateMethod,
        return_class as returnClass,
        operate_user as operateUser,
        operate_time as operateTime,
        param_and_value as paramAndValue,
        cost_time as costTime,
        return_value as returnValue
      from operation_log
      &lt;include refid=&quot;oplog_where&quot;/&gt;
      limit #{start},#{size}
    &lt;/select&gt;


    &lt;select id=&quot;countByCondition&quot; resultType=&quot;long&quot; parameterType=&quot;map&quot;&gt;
        select count(*) from operation_log
        &lt;include refid=&quot;oplog_where&quot;/&gt;
    &lt;/select&gt;


    &lt;sql id=&quot;oplog_where&quot;&gt;
        &lt;where&gt;
            &lt;if test=&quot;operateClass != null and operateClass != '' &quot;&gt;
                and operate_class = #{operateClass}
            &lt;/if&gt;
            &lt;if test=&quot;operateMethod != null and operateMethod != '' &quot;&gt;
                and operate_method = #{operateMethod}
            &lt;/if&gt;
            &lt;if test=&quot;returnClass != null and returnClass != '' &quot;&gt;
                and return_class = #{returnClass}
            &lt;/if&gt;
            &lt;if test=&quot;costTime != null&quot;&gt;
                and cost_time =  #{costTime}
            &lt;/if&gt;
        &lt;/where&gt;
    &lt;/sql&gt;

&lt;/mapper&gt;
</code></pre>
<h5 id="443-service">4.4.3 Service</h5>
<pre><code class="language-java">@Service
@Transactional
public class OperationLogService {

    //private static Logger logger = Logger.getLogger(OperationLogService.class);

    @Autowired
    private OperationLogMapper operationLogMapper;

    //插入数据
    public void insert(OperationLog operationLog){
        operationLogMapper.insert(operationLog);
    }

    //根据条件查询
    public PageResult selectListByCondition(Map dataMap, Integer pageNum , Integer pageSize){

       if(paramMap ==null){
            paramMap = new HashMap();
        }
        paramMap.put(&quot;start&quot; , (pageNum-1)*rows);
        paramMap.put(&quot;rows&quot;,rows);

        Object costTime = paramMap.get(&quot;costTime&quot;);
        if(costTime != null){
            if(&quot;&quot;.equals(costTime.toString())){
                paramMap.put(&quot;costTime&quot;,null);
            }else{
                paramMap.put(&quot;costTime&quot;,new Long(paramMap.get(&quot;costTime&quot;).toString()));
            }
        }

        System.out.println(dataMap);


        long countStart = System.currentTimeMillis();
        Long count = operationLogMapper.countByCondition(dataMap);
        long countEnd = System.currentTimeMillis();
        System.out.println(&quot;Count Cost Time : &quot; + (countEnd-countStart)+&quot; ms&quot;);


        List&lt;OperationLog&gt; list = operationLogMapper.selectListByCondition(dataMap);
        long queryEnd = System.currentTimeMillis();
        System.out.println(&quot;Query Cost Time : &quot; + (queryEnd-countEnd)+&quot; ms&quot;);


        return new PageResult(count,list);

    }

}
</code></pre>
<h5 id="444-controller">4.4.4 Controller</h5>
<pre><code class="language-java">@RestController
@RequestMapping(&quot;/operationLog&quot;)
public class OperationLogController {

    @Autowired
    private OperationLogService operationLogService;

    @RequestMapping(&quot;/findList&quot;)
    public PageResult findList(@RequestBody Map dataMap, Integer pageNum , Integer pageSize){
        PageResult page = operationLogService.selectListByCondition(dataMap, pageNum, pageSize);
        return page;
    }

}
</code></pre>
<h4 id="45-日志查询前端代码实现">4.5 日志查询前端代码实现</h4>
<p>前端代码使用 BootStrap + AdminLTE 进行布局， 使用Vuejs 进行视图层展示。</p>
<h5 id="451-js">4.5.1 js</h5>
<pre><code class="language-html">&lt;script&gt;
   var vm = new Vue({
       el: '#app',
       data: {
           dataList:[],
           searchEntity:{
               operateClass:'',
               operateMethod:'',
               returnClass:'',
               costTime:''
           },

           page: 1,  //显示的是哪一页
           pageSize: 10, //每一页显示的数据条数
           total: 150, //记录总数
           maxPage:8  //最大页数
       },
       methods: {
           pageHandler: function (page) {
               this.page = page;
               this.search();
           },

           search: function () {
               var _this = this;
               this.showLoading();
               axios.post('/operationLog/findList.do?pageNum=' + _this.page + &quot;&amp;pageSize=&quot; + _this.pageSize, _this.searchEntity).then(function (response) {
                   if (response) {
                       _this.dataList = response.data.dataList;
                       _this.total = response.data.total;
                       _this.hideLoading();
                   }
               })
           },

           showLoading: function () {
               $('#loadingModal').modal({backdrop: 'static', keyboard: false});
           },

           hideLoading: function () {
               $('#loadingModal').modal('hide');
           },
       },

       created:function(){
           this.pageHandler(1);
       }
   });

&lt;/script&gt;
</code></pre>
<h5 id="452-列表数据展示">4.5.2 列表数据展示</h5>
<pre><code class="language-html">&lt;tr v-for=&quot;item in dataList&quot;&gt;
    &lt;td&gt;&lt;input name=&quot;ids&quot; type=&quot;checkbox&quot;&gt;&lt;/td&gt;
    &lt;td&gt;{{item.id}}&lt;/td&gt;
    &lt;td&gt;{{item.operateClass}}&lt;/td&gt;
    &lt;td&gt;{{item.operateMethod}}&lt;/td&gt;
    &lt;td&gt;{{item.returnClass}}&lt;/td&gt;
    &lt;td&gt;{{item.returnValue}}&lt;/td&gt;
    &lt;td&gt;{{item.operateUser}}&lt;/td&gt;
    &lt;td&gt;{{item.operateTime}}&lt;/td&gt;
    &lt;td&gt;{{item.costTime}}&lt;/td&gt;
    &lt;td class=&quot;text-center&quot;&gt;
        &lt;button type=&quot;button&quot; class=&quot;btn bg-olive btn-xs&quot;&gt;详情&lt;/button&gt;
        &lt;button type=&quot;button&quot; class=&quot;btn bg-olive btn-xs&quot;&gt;删除&lt;/button&gt;
    &lt;/td&gt;
&lt;/tr&gt;
</code></pre>
<p>4.5.3 分页插件</p>
<pre><code class="language-html">&lt;div class=&quot;wrap&quot; id=&quot;wrap&quot;&gt;
    &lt;zpagenav v-bind:page=&quot;page&quot; v-bind:page-size=&quot;pageSize&quot; v-bind:total=&quot;total&quot;
              v-bind:max-page=&quot;maxPage&quot;  v-on:pagehandler=&quot;pageHandler&quot;&gt;
    &lt;/zpagenav&gt;
&lt;/div&gt;
</code></pre>
<h4 id="46-联调测试">4.6 联调测试</h4>
<p>可以通过postman来访问业务系统，再查看数据库中的日志信息，验证能不能将用户的访问日志记录下来。</p>
<figure data-type="image" tabindex="147"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555077276426.png" alt="" loading="lazy"></figure>
<h4 id="47-分析性能问题">4.7 分析性能问题</h4>
<p>系统中用户访问日志的数据量，随着时间的推移，这张表的数据量会越来越大，因此我们需要根据业务需求，来对日志查询模块的性能进行优化。</p>
<p>1） 分页查询优化</p>
<p>由于在进行日志查询时，是进行分页查询，那也就意味着，在查看时，至少需要查询两次：</p>
<p>A. 查询符合条件的总记录数。--&gt; count 操作</p>
<p>B. 查询符合条件的列表数据。--&gt; 分页查询 limit 操作</p>
<p>通常来说，count() 都需要扫描大量的行（意味着需要访问大量的数据）才能获得精确的结果，因此是很难对该SQL进行优化操作的。如果需要对count进行优化，可以采用另外一种思路，可以增加汇总表，或者redis缓存来专门记录该表对应的记录数，这样的话，就可以很轻松的实现汇总数据的查询，而且效率很高，但是这种统计并不能保证百分之百的准确 。对于数据库的操作，“快速、精确、实现简单”，三者永远只能满足其二，必须舍掉其中一个。</p>
<p>2） 条件查询优化</p>
<p>针对于条件查询,需要对查询条件,及排序字段建立索引。</p>
<p>3） 读写分离</p>
<p>通过主从复制集群，来完成读写分离，使写操作走主节点， 而读操作，走从节点。</p>
<p>4） MySQL服务器优化</p>
<p>5） 应用优化</p>
<h4 id="48-性能优化-分页">4.8 性能优化 - 分页</h4>
<h5 id="481-优化count">4.8.1 优化count</h5>
<p>创建一张表用来记录日志表的总数据量：</p>
<pre><code class="language-SQL">create table log_counter(
	logcount bigint not null
)engine = innodb default CHARSET = utf8;
</code></pre>
<p>在每次插入数据之后，更新该表 ：</p>
<pre><code class="language-xml">&lt;update id=&quot;updateLogCounter&quot; &gt;
    update log_counter set logcount = logcount + 1
&lt;/update&gt;
</code></pre>
<p>在进行分页查询时, 获取总记录数，从该表中查询既可。</p>
<pre><code class="language-xml">&lt;select id=&quot;countLogFromCounter&quot; resultType=&quot;long&quot;&gt;    select logcount from log_counter limit 1&lt;/select&gt;
</code></pre>
<h5 id="482-优化-limit">4.8.2 优化 limit</h5>
<p>在进行分页时，一般通过创建覆盖索引，能够比较好的提高性能。一个非常常见，而又非常头疼的分页场景就是 &quot;limit 1000000,10&quot; ，此时MySQL需要搜索出前1000010 条记录后，仅仅需要返回第 1000001 到 1000010 条记录，前1000000 记录会被抛弃，查询代价非常大。</p>
<figure data-type="image" tabindex="148"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555081714638.png" alt="" loading="lazy"></figure>
<p>当点击比较靠后的页码时，就会出现这个问题，查询效率非常慢。</p>
<p>优化SQL：</p>
<pre><code class="language-sql">select * from operation_log limit 3000000 , 10;
</code></pre>
<p>将上述SQL优化为 :</p>
<pre><code class="language-SQL">select * from operation_log t , (select id from operation_log order by id limit 3000000,10) b where t.id = b.id ;
</code></pre>
<pre><code class="language-xml">&lt;select id=&quot;selectListByCondition&quot; parameterType=&quot;map&quot; resultType=&quot;operationLog&quot;&gt;
  select
    id ,
    operate_class as operateClass ,
    operate_method as operateMethod,
    return_class as returnClass,
    operate_user as operateUser,
    operate_time as operateTime,
    param_and_value as paramAndValue,
    cost_time as costTime,
    return_value as returnValue
  from operation_log t,
    
  (select id from operation_log 
  &lt;where&gt;
    &lt;include refid=&quot;oplog_where&quot;/&gt;
  &lt;/where&gt;
  order by id limit #{start},#{rows}) b  where t.id = b.id  
&lt;/select&gt;
</code></pre>
<h4 id="49-性能优化-索引">4.9 性能优化 - 索引</h4>
<figure data-type="image" tabindex="149"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555152703824.png" alt="" loading="lazy"></figure>
<p>当根据操作人进行查询时， 查询的效率很低，耗时比较长。原因就是因为在创建数据库表结构时，并没有针对于 操作人 字段建立索引。</p>
<pre><code class="language-SQL">CREATE INDEX idx_user_method_return_cost ON operation_log(operate_user,operate_method,return_class,cost_time);
</code></pre>
<p>同上 ， 为了查询效率高，我们也需要对 操作方法、返回值类型、操作耗时 等字段进行创建索引，以提高查询效率。</p>
<pre><code class="language-SQL">CREATE INDEX idx_optlog_method_return_cost ON operation_log(operate_method,return_class,cost_time);

CREATE INDEX idx_optlog_return_cost ON operation_log(return_class,cost_time);

CREATE INDEX idx_optlog_cost ON operation_log(cost_time);

</code></pre>
<h4 id="410-性能优化-排序">4.10 性能优化 - 排序</h4>
<p>在查询数据时，如果业务需求中需要我们对结果内容进行了排序处理 , 这个时候,我们还需要对排序的字段建立适当的索引, 来提高排序的效率 。</p>
<h4 id="411-性能优化-读写分离">4.11 性能优化 - 读写分离</h4>
<h5 id="4111-概述">4.11.1 概述</h5>
<p>在Mysql主从复制的基础上，可以使用读写分离来降低单台Mysql节点的压力，从而来提高访问效率，读写分离的架构如下：</p>
<figure data-type="image" tabindex="150"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555235426739.png" alt="" loading="lazy"></figure>
<p>对于读写分离的实现，可以通过Spring AOP 来进行动态的切换数据源，进行操作 ：</p>
<h5 id="4112-实现方式">4.11.2 实现方式</h5>
<p>db.properties</p>
<pre><code class="language-properties">jdbc.write.driver=com.mysql.jdbc.Driver
jdbc.write.url=jdbc:mysql://192.168.142.128:3306/mysql_demo
jdbc.write.username=root
jdbc.write.password=itcast

jdbc.read.driver=com.mysql.jdbc.Driver
jdbc.read.url=jdbc:mysql://192.168.142.129:3306/mysql_demo
jdbc.read.username=root
jdbc.read.password=itcast
</code></pre>
<p>applicationContext-datasource.xml</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;
       xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;
       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;
       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
        http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd
        http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd
        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;


    &lt;!-- 配置数据源 - Read --&gt;
    &lt;bean id=&quot;readDataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot; destroy-method=&quot;close&quot;  lazy-init=&quot;true&quot;&gt;
        &lt;property name=&quot;driverClass&quot; value=&quot;${jdbc.read.driver}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;jdbcUrl&quot; value=&quot;${jdbc.read.url}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;user&quot; value=&quot;${jdbc.read.username}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;password&quot; value=&quot;${jdbc.read.password}&quot;&gt;&lt;/property&gt;
    &lt;/bean&gt;


    &lt;!-- 配置数据源 - Write --&gt;
    &lt;bean id=&quot;writeDataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;  destroy-method=&quot;close&quot;  lazy-init=&quot;true&quot;&gt;
        &lt;property name=&quot;driverClass&quot; value=&quot;${jdbc.write.driver}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;jdbcUrl&quot; value=&quot;${jdbc.write.url}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;user&quot; value=&quot;${jdbc.write.username}&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;password&quot; value=&quot;${jdbc.write.password}&quot;&gt;&lt;/property&gt;
    &lt;/bean&gt;


    &lt;!-- 配置动态分配的读写 数据源 --&gt;
    &lt;bean id=&quot;dataSource&quot; class=&quot;cn.itcast.aop.datasource.ChooseDataSource&quot; lazy-init=&quot;true&quot;&gt;
        &lt;property name=&quot;targetDataSources&quot;&gt;
            &lt;map key-type=&quot;java.lang.String&quot; value-type=&quot;javax.sql.DataSource&quot;&gt;
                &lt;entry key=&quot;write&quot; value-ref=&quot;writeDataSource&quot;/&gt;
                &lt;entry key=&quot;read&quot; value-ref=&quot;readDataSource&quot;/&gt;
            &lt;/map&gt;
        &lt;/property&gt;

        &lt;property name=&quot;defaultTargetDataSource&quot; ref=&quot;writeDataSource&quot;/&gt;

        &lt;property name=&quot;methodType&quot;&gt;
            &lt;map key-type=&quot;java.lang.String&quot;&gt;
                &lt;entry key=&quot;read&quot; value=&quot;,get,select,count,list,query,find&quot;/&gt;
                &lt;entry key=&quot;write&quot; value=&quot;,add,create,update,delete,remove,insert&quot;/&gt;
            &lt;/map&gt;
        &lt;/property&gt;
    &lt;/bean&gt;

&lt;/beans&gt;
</code></pre>
<p>ChooseDataSource</p>
<pre><code class="language-java">public class ChooseDataSource extends AbstractRoutingDataSource {

    public static Map&lt;String, List&lt;String&gt;&gt; METHOD_TYPE_MAP = new HashMap&lt;String, List&lt;String&gt;&gt;();

    /**
     * 实现父类中的抽象方法，获取数据源名称
     * @return
     */
    protected Object determineCurrentLookupKey() {
        return DataSourceHandler.getDataSource();
    }

    // 设置方法名前缀对应的数据源
    public void setMethodType(Map&lt;String, String&gt; map) {
        for (String key : map.keySet()) {
            List&lt;String&gt; v = new ArrayList&lt;String&gt;();
            String[] types = map.get(key).split(&quot;,&quot;);
            for (String type : types) {
                if (!StringUtils.isEmpty(type)) {
                    v.add(type);
                }
            }
            METHOD_TYPE_MAP.put(key, v);
        }
        System.out.println(&quot;METHOD_TYPE_MAP : &quot;+METHOD_TYPE_MAP);
    }
}
</code></pre>
<p>DataSourceHandler</p>
<pre><code class="language-java">public class DataSourceHandler {

    // 数据源名称
    public static final ThreadLocal&lt;String&gt; holder = new ThreadLocal&lt;String&gt;();

    /**
     * 在项目启动的时候将配置的读、写数据源加到holder中
     */
    public static void putDataSource(String datasource) {
        holder.set(datasource);
    }

    /**
     * 从holer中获取数据源字符串
     */
    public static String getDataSource() {
        return holder.get();
    }
}
</code></pre>
<p>DataSourceAspect</p>
<pre><code class="language-java">@Aspect
@Component
@Order(-9999)
@EnableAspectJAutoProxy(proxyTargetClass = true)
public class DataSourceAspect {

    protected Logger logger = LoggerFactory.getLogger(this.getClass());

    /**
     * 配置前置通知,使用在方法aspect()上注册的切入点
     */
    @Before(&quot;execution(* cn.itcast.service.*.*(..))&quot;)
    @Order(-9999)
    public void before(JoinPoint point) {
        
        String className = point.getTarget().getClass().getName();
        String method = point.getSignature().getName();
        logger.info(className + &quot;.&quot; + method + &quot;(&quot; + Arrays.asList(point.getArgs())+ &quot;)&quot;);

        try {
            for (String key : ChooseDataSource.METHOD_TYPE_MAP.keySet()) {
                for (String type : ChooseDataSource.METHOD_TYPE_MAP.get(key)) {
                    if (method.startsWith(type)) {
                        System.out.println(&quot;key : &quot; + key);
                        DataSourceHandler.putDataSource(key);
                        break;
                    }
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
        }

    }
}
</code></pre>
<p>通过 @Order(-9999) 注解来控制事务管理器, 与该通知类的加载顺序 , 需要让通知类 , 先加载 , 来判定使用哪个数据源 .</p>
<h5 id="4113-验证">4.11.3 验证</h5>
<p>在主库和从库中，执行如下SQL语句，来查看是否读的时候， 从从库中读取 ； 写入操作的时候，是否写入到主库。</p>
<pre><code class="language-sql">show status like 'Innodb_rows_%' ;
</code></pre>
<figure data-type="image" tabindex="151"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/1555235982584.png" alt="" loading="lazy"></figure>
<h5 id="4114-原理">4.11.4 原理</h5>
<figure data-type="image" tabindex="152"><img src="https://memorykki.github.io/post-images/MySQL_hm/4/aop-datasource.png" alt="" loading="lazy"></figure>
<h4 id="412-性能优化-应用优化">4.12 性能优化 - 应用优化</h4>
<h5 id="4121-缓存">4.12.1 缓存</h5>
<p>可以在业务系统中使用redis来做缓存，缓存一些基础性的数据，来降低关系型数据库的压力，提高访问效率。</p>
<h5 id="4122-全文检索">4.12.2 全文检索</h5>
<p>如果业务系统中的数据量比较大（达到千万级别），这个时候，如果再对数据库进行查询，特别是进行分页查询，速度将变得很慢（因为在分页时首先需要count求合计数），为了提高访问效率，这个时候，可以考虑加入Solr 或者 ElasticSearch全文检索服务，来提高访问效率。</p>
<h5 id="4133-非关系数据库">4.13.3 非关系数据库</h5>
<p>也可以考虑将非核心（重要）数据，存在 MongoDB 中，这样可以提高插入以及查询的效率。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redis]]></title>
        <id>https://memorykki.github.io/Redis/</id>
        <link href="https://memorykki.github.io/Redis/">
        </link>
        <updated>2021-08-25T02:49:13.000Z</updated>
        <summary type="html"><![CDATA[<p>Redis（Remote Dictionary Server )，即远程字典服务，是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Redis（Remote Dictionary Server )，即远程字典服务，是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库。</p>
<!-- more -->
<h2 id="一-nosql概述">一、Nosql概述</h2>
<h3 id="为什么使用nosql">为什么使用Nosql</h3>
<blockquote>
<p>1、单机Mysql时代</p>
</blockquote>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/2020082010365930.png#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>90年代,一个网站的访问量一般不会太大，单个数据库完全够用。随着用户增多，网站出现以下问题</p>
<ol>
<li>数据量增加到一定程度，单机数据库就放不下了</li>
<li>数据的索引（B+ Tree）,一个机器内存也存放不下</li>
<li>访问量变大后（读写混合），一台服务器承受不住。</li>
</ol>
<blockquote>
<p>2、Memcached(缓存) + Mysql + 垂直拆分（读写分离）</p>
</blockquote>
<p>网站80%的情况都是在读，每次都要去查询数据库的话就十分的麻烦！所以说我们希望减轻数据库的压力，我们可以使用缓存来保证效率！</p>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20200820103713734.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<p>优化过程经历了以下几个过程：</p>
<ol>
<li>优化数据库的数据结构和索引(难度大)</li>
<li>文件缓存，通过IO流获取比每次都访问数据库效率略高，但是流量爆炸式增长时候，IO流也承受不了</li>
<li>MemCache,当时最热门的技术，通过在数据库和数据库访问层之间加上一层缓存，第一次访问时查询数据库，将结果保存到缓存，后续的查询先检查缓存，若有直接拿去使用，效率显著提升。</li>
</ol>
<blockquote>
<p>3、分库分表 + 水平拆分 + Mysql集群</p>
</blockquote>
<figure data-type="image" tabindex="3"><img src="https://img-blog.csdnimg.cn/20200820103739584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>4、如今最近的年代</p>
</blockquote>
<p>如今信息量井喷式增长，各种各样的数据出现（用户定位数据，图片数据等），大数据的背景下关系型数据库（RDBMS）无法满足大量数据要求。Nosql数据库就能轻松解决这些问题。</p>
<blockquote>
<p>目前一个基本的互联网项目</p>
</blockquote>
<figure data-type="image" tabindex="4"><img src="https://img-blog.csdnimg.cn/20200820103804572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>为什么要用NoSQL ？</p>
</blockquote>
<p>用户的个人信息，社交网络，地理位置。用户自己产生的数据，用户日志等等爆发式增长！<br>
这时候我们就需要使用NoSQL数据库的，Nosql可以很好的处理以上的情况！</p>
<h3 id="什么是nosql">什么是Nosql</h3>
<p><strong>NoSQL = Not Only SQL（不仅仅是SQL）</strong></p>
<p>Not Only Structured Query Language</p>
<p>关系型数据库：列+行，同一个表下数据的结构是一样的。</p>
<p>非关系型数据库：数据存储没有固定的格式，并且可以进行横向扩展。</p>
<p>NoSQL泛指非关系型数据库，随着web2.0互联网的诞生，传统的关系型数据库很难对付web2.0时代！尤其是超大规模的高并发的社区，暴露出来很多难以克服的问题，NoSQL在当今大数据环境下发展的十分迅速，Redis是发展最快的。</p>
<h3 id="nosql特点">Nosql特点</h3>
<ol>
<li>
<p>方便扩展（数据之间没有关系，很好扩展！）</p>
</li>
<li>
<p>大数据量高性能（Redis一秒可以写8万次，读11万次，NoSQL的缓存记录级，是一种细粒度的缓存，性能会比较高！）</p>
</li>
<li>
<p>数据类型是多样型的！（不需要事先设计数据库，随取随用）</p>
</li>
<li>
<p>传统的 RDBMS 和 NoSQL</p>
<pre><code>传统的 RDBMS(关系型数据库)
- 结构化组织
- SQL
- 数据和关系都存在单独的表中 row col
- 操作，数据定义语言
- 严格的一致性
- 基础的事务
- ...
</code></pre>
<pre><code>Nosql
- 不仅仅是数据
- 没有固定的查询语言
- 键值对存储，列存储，文档存储，图形数据库（社交关系）
- 最终一致性
- CAP定理和BASE
- 高性能，高可用，高扩展
- ...
</code></pre>
</li>
</ol>
<blockquote>
<p>了解：3V + 3高</p>
</blockquote>
<p>大数据时代的3V ：主要是<strong>描述问题</strong>的</p>
<ol>
<li>海量Velume</li>
<li>多样Variety</li>
<li>实时Velocity</li>
</ol>
<p>大数据时代的3高 ： 主要是<strong>对程序的要求</strong></p>
<ol>
<li>高并发</li>
<li>高可扩</li>
<li>高性能</li>
</ol>
<p>真正在公司中的实践：NoSQL + RDBMS 一起使用才是最强的。</p>
<h3 id="阿里巴巴演进分析">阿里巴巴演进分析</h3>
<p>推荐阅读：阿里云的这群疯子https://yq.aliyun.com/articles/653511</p>
<figure data-type="image" tabindex="5"><img src="https://img-blog.csdnimg.cn/20200820103829446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="1" loading="lazy"></figure>
<figure data-type="image" tabindex="6"><img src="https://img-blog.csdnimg.cn/20200820103851613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<pre><code class="language-bash"># 商品信息
- 一般存放在关系型数据库：Mysql,阿里巴巴使用的Mysql都是经过内部改动的。

# 商品描述、评论(文字居多)
- 文档型数据库：MongoDB

# 图片
- 分布式文件系统 FastDFS
- 淘宝：TFS
- Google: GFS
- Hadoop: HDFS
- 阿里云: oss

# 商品关键字 用于搜索
- 搜索引擎：solr,elasticsearch
- 阿里：Isearch 多隆

# 商品热门的波段信息
- 内存数据库：Redis，Memcache

# 商品交易，外部支付接口
- 第三方应用
</code></pre>
<h3 id="nosql的四大分类">Nosql的四大分类</h3>
<blockquote>
<p><strong>KV键值对</strong></p>
</blockquote>
<ul>
<li>新浪：<strong>Redis</strong></li>
<li>美团：Redis + Tair</li>
<li>阿里、百度：Redis + Memcache</li>
</ul>
<blockquote>
<p><strong>文档型数据库（bson数据格式）：</strong></p>
</blockquote>
<ul>
<li><strong>MongoDB</strong>(掌握)
<ul>
<li>基于分布式文件存储的数据库。C++编写，用于处理大量文档。</li>
<li>MongoDB是RDBMS和NoSQL的中间产品。MongoDB是非关系型数据库中功能最丰富的，NoSQL中最像关系型数据库的数据库。</li>
</ul>
</li>
<li>ConthDB</li>
</ul>
<blockquote>
<p><strong>列存储数据库</strong></p>
</blockquote>
<ul>
<li><strong>HBase</strong>(大数据必学)</li>
<li>分布式文件系统</li>
</ul>
<blockquote>
<p><strong>图关系数据库</strong></p>
</blockquote>
<p>用于广告推荐，社交网络</p>
<ul>
<li><strong>Neo4j</strong>、InfoGrid</li>
</ul>
<table>
<thead>
<tr>
<th><strong>分类</strong></th>
<th><strong>Examples举例</strong></th>
<th><strong>典型应用场景</strong></th>
<th><strong>数据模型</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>键值对（key-value）</strong></td>
<td>Tokyo Cabinet/Tyrant, Redis, Voldemort, Oracle BDB</td>
<td>内容缓存，主要用于处理大量数据的高访问负载，也用于一些日志系统等等。</td>
<td>Key 指向 Value 的键值对，通常用hash table来实现</td>
<td>查找速度快</td>
<td>数据无结构化，通常只被当作字符串或者二进制数据</td>
</tr>
<tr>
<td><strong>列存储数据库</strong></td>
<td>Cassandra, HBase, Riak</td>
<td>分布式的文件系统</td>
<td>以列簇式存储，将同一列数据存在一起</td>
<td>查找速度快，可扩展性强，更容易进行分布式扩展</td>
<td>功能相对局限</td>
</tr>
<tr>
<td><strong>文档型数据库</strong></td>
<td>CouchDB, MongoDb</td>
<td>Web应用（与Key-Value类似，Value是结构化的，不同的是数据库能够了解Value的内容）</td>
<td>Key-Value对应的键值对，Value为结构化数据</td>
<td>数据结构要求不严格，表结构可变，不需要像关系型数据库一样需要预先定义表结构</td>
<td>查询性能不高，而且缺乏统一的查询语法。</td>
</tr>
<tr>
<td><strong>图形(Graph)数据库</strong></td>
<td>Neo4J, InfoGrid, Infinite Graph</td>
<td>社交网络，推荐系统等。专注于构建关系图谱</td>
<td>图结构</td>
<td>利用图结构相关算法。比如最短路径寻址，N度关系查找等</td>
<td>很多时候需要对整个图做计算才能得出需要的信息，而且这种结构不太好做分布式的集群</td>
</tr>
</tbody>
</table>
<h2 id="二-redis入门">二、Redis入门</h2>
<h3 id="概述">概述</h3>
<blockquote>
<p>Redis是什么？</p>
</blockquote>
<p>Redis（Remote Dictionary Server )，即远程字典服务。</p>
<p>是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、<strong>Key-Value数据库</strong>，并提供多种语言的API。</p>
<p>与memcached一样，为了保证效率，<strong>数据都是缓存在内存中</strong>。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。</p>
<blockquote>
<p>Redis能该干什么？</p>
</blockquote>
<ol>
<li>内存存储、持久化，内存是断电即失的，所以需要持久化（RDB、AOF）</li>
<li>高效率、用于高速缓冲</li>
<li>发布订阅系统</li>
<li>地图信息分析</li>
<li>计时器、计数器(eg：浏览量)</li>
<li>。。。</li>
</ol>
<blockquote>
<p>特性</p>
</blockquote>
<ol>
<li>
<p>多样的数据类型</p>
</li>
<li>
<p>持久化</p>
</li>
<li>
<p>集群</p>
</li>
<li>
<p>事务</p>
<p>…</p>
</li>
</ol>
<h3 id="环境搭建">环境搭建</h3>
<p>官网：https://redis.io/</p>
<p>推荐使用Linux服务器学习。</p>
<p>windows版本的Redis已经停更很久了…</p>
<h3 id="windows安装">Windows安装</h3>
<p>https://github.com/dmajkic/redis</p>
<ol>
<li>解压安装包<br>
<img src="https://img-blog.csdnimg.cn/20200820103922318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></li>
<li>开启redis-server.exe</li>
<li>启动redis-cli.exe测试<img src="https://img-blog.csdnimg.cn/20200820103950934.png#pic_center" alt="在这里插入图片描述" loading="lazy"></li>
</ol>
<h3 id="linux安装">Linux安装</h3>
<ol>
<li>
<p>下载安装包！<code>redis-5.0.8.tar.gz</code></p>
</li>
<li>
<p>解压Redis的安装包！程序一般放在 <code>/opt</code> 目录下</p>
<figure data-type="image" tabindex="7"><img src="https://img-blog.csdnimg.cn/20200820104016426.png#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
</li>
<li>
<p>基本环境安装</p>
<pre><code class="language-bash">yum install gcc-c++
# 然后进入redis目录下执行
make
# 然后执行
make install
</code></pre>
</li>
</ol>
<figure data-type="image" tabindex="8"><img src="https://img-blog.csdnimg.cn/20200820104048327.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></figure>
<ol>
<li>
<p>redis默认安装路径 <code>/usr/local/bin</code><img src="https://img-blog.csdnimg.cn/20200820104140692.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" loading="lazy"></p>
</li>
<li>
<p>将redis的配置文件复制到 程序安装目录 <code>/usr/local/bin/kconfig</code>下</p>
<figure data-type="image" tabindex="9"><img src="https://img-blog.csdnimg.cn/20200820104157817.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-hxvGQ47d-1597890996509)(狂神说 Redis.assets/image-20200813114000868.png)]" loading="lazy"></figure>
</li>
<li>
<p>redis默认不是后台启动的，需要修改配置文件！</p>
<figure data-type="image" tabindex="10"><img src="https://img-blog.csdnimg.cn/20200820104213706.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-dDdKTUgd-1597890996510)(狂神说 Redis.assets/image-20200813114019063.png)]" loading="lazy"></figure>
</li>
<li>
<p>通过制定的配置文件启动redis服务</p>
<figure data-type="image" tabindex="11"><img src="https://img-blog.csdnimg.cn/20200820104228556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-jOypL57Z-1597890996511)(狂神说 Redis.assets/image-20200813114030597.png)]" loading="lazy"></figure>
</li>
<li>
<p>使用redis-cli连接指定的端口号测试，Redis的默认端口6379</p>
<figure data-type="image" tabindex="12"><img src="https://img-blog.csdnimg.cn/20200820104243223.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-LnDaISQ4-1597890996512)(狂神说 Redis.assets/image-20200813114045299.png)]" loading="lazy"></figure>
</li>
<li>
<p>查看redis进程是否开启</p>
<figure data-type="image" tabindex="13"><img src="https://img-blog.csdnimg.cn/20200820104300532.png#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9PhN1jC1-1597890996513)(狂神说 Redis.assets/image-20200813114103769.png)]" loading="lazy"></figure>
</li>
<li>
<p>关闭Redis服务 <code>shutdown</code></p>
<figure data-type="image" tabindex="14"><img src="https://img-blog.csdnimg.cn/20200820104314297.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Y54EuOYm-1597890996514)(狂神说 Redis.assets/image-20200813114116691.png)]" loading="lazy"></figure>
</li>
<li>
<p>再次查看进程是否存在</p>
</li>
<li>
<p>后面我们会使用单机多Redis启动集群测试</p>
</li>
</ol>
<h3 id="测试性能">测试性能</h3>
<p>**redis-benchmark：**Redis官方提供的性能测试工具，参数选项如下：</p>
<figure data-type="image" tabindex="15"><img src="https://img-blog.csdnimg.cn/20200513214125892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<p><strong>简单测试：</strong></p>
<pre><code class="language-bash"># 测试：100个并发连接 100000请求
redis-benchmark -h localhost -p 6379 -c 100 -n 100000
12
</code></pre>
<figure data-type="image" tabindex="16"><img src="https://img-blog.csdnimg.cn/20200820104343472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-plMshjFg-1597890996515)(狂神说 Redis.assets/image-20200813114143365.png)]" loading="lazy"></figure>
<h3 id="基础知识">基础知识</h3>
<blockquote>
<p>redis默认有16个数据库</p>
</blockquote>
<figure data-type="image" tabindex="17"><img src="https://img-blog.csdnimg.cn/20200820104357466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-v2S3n3Si-1597890996516)(狂神说 Redis.assets/image-20200813114158322.png)]" loading="lazy"></figure>
<p>默认使用的第0个;</p>
<p>16个数据库为：DB 0~DB 15<br>
默认使用DB 0 ，可以使用<code>select n</code>切换到DB n，<code>dbsize</code>可以查看当前数据库的大小，与key数量相关。</p>
<pre><code class="language-bash">127.0.0.1:6379&gt; config get databases # 命令行查看数据库数量databases
1) &quot;databases&quot;
2) &quot;16&quot;

127.0.0.1:6379&gt; select 8 # 切换数据库 DB 8
OK
127.0.0.1:6379[8]&gt; dbsize # 查看数据库大小
(integer) 0

# 不同数据库之间 数据是不能互通的，并且dbsize 是根据库中key的个数。
127.0.0.1:6379&gt; set name sakura 
OK
127.0.0.1:6379&gt; SELECT 8
OK
127.0.0.1:6379[8]&gt; get name # db8中并不能获取db0中的键值对。
(nil)
127.0.0.1:6379[8]&gt; DBSIZE
(integer) 0
127.0.0.1:6379[8]&gt; SELECT 0
OK
127.0.0.1:6379&gt; keys *
1) &quot;counter:__rand_int__&quot;
2) &quot;mylist&quot;
3) &quot;name&quot;
4) &quot;key:__rand_int__&quot;
5) &quot;myset:__rand_int__&quot;
127.0.0.1:6379&gt; DBSIZE # size和key个数相关
(integer) 5
</code></pre>
<p><code>keys *</code> ：查看当前数据库中所有的key。</p>
<p><code>flushdb</code>：清空当前数据库中的键值对。</p>
<p><code>flushall</code>：清空所有数据库的键值对。</p>
<blockquote>
<p><strong>Redis是单线程的，Redis是基于内存操作的。</strong></p>
</blockquote>
<p>所以Redis的性能瓶颈不是CPU,而是机器内存和网络带宽。</p>
<p>那么为什么Redis的速度如此快呢，性能这么高呢？QPS达到10W+。</p>
<p>Redis 选择使用单线程模型处理客户端的请求主要还是因为 CPU 不是 Redis 服务器的瓶颈，所以使用多线程模型带来的性能提升并不能抵消它带来的开发成本和维护成本，系统的性能瓶颈也主要在网络 I/O 操作上；而 Redis 6.0引入多线程操作也是出于性能上的考虑，对于一些大键值对的删除操作，通过多线程非阻塞地释放内存空间也能减少对 Redis 主线程阻塞的时间，提高执行的效率。</p>
<blockquote>
<p><strong>Redis为什么单线程还这么快？</strong></p>
</blockquote>
<ul>
<li>误区1：高性能的服务器一定是多线程的？</li>
<li>误区2：多线程（CPU上下文会切换！）一定比单线程效率高！</li>
</ul>
<p>核心：Redis是将所有的数据放在内存中的，所以说使用单线程去操作效率就是最高的，多线程（CPU上下文会切换：耗时的操作！），对于内存系统来说，如果没有上下文切换效率就是最高的，多次读写都是在一个CPU上的，在内存存储数据情况下，单线程就是最佳的方案。</p>
<p>Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为<strong>文件事件处理器</strong>（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。</p>
<p>文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。<br>
当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。<br>
虽然文件事件处理器以单线程方式运行， 但通过使用 <strong>I/O 多路复用</strong>程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。</p>
<h2 id="三-五大数据类型">三、五大数据类型</h2>
<p>Redis是一个开源（BSD许可），内存存储的数据结构服务器，可用作<strong>数据库</strong>，<strong>高速缓存</strong>和<strong>消息队列代理</strong>。它支持<a href="https://www.redis.net.cn/tutorial/3508.html">字符串</a>、<a href="https://www.redis.net.cn/tutorial/3509.html">哈希表</a>、<a href="https://www.redis.net.cn/tutorial/3510.html">列表</a>、<a href="https://www.redis.net.cn/tutorial/3511.html">集合</a>、<a href="https://www.redis.net.cn/tutorial/3512.html">有序集合</a>，<a href="https://www.redis.net.cn/tutorial/3508.html">位图</a>，<a href="https://www.redis.net.cn/tutorial/3513.html">hyperloglogs</a>等数据类型。内置复制、<a href="https://www.redis.net.cn/tutorial/3516.html">Lua脚本</a>、LRU收回、<a href="https://www.redis.net.cn/tutorial/3515.html">事务</a>以及不同级别磁盘持久化功能，同时通过Redis Sentinel提供高可用，通过Redis Cluster提供自动<a href="https://www.redis.net.cn/tutorial/3524.html">分区</a>。</p>
<h3 id="redis-key">Redis-key</h3>
<blockquote>
<p>在redis中无论什么数据类型，在数据库中都是以key-value形式保存，通过进行对Redis-key的操作，来完成对数据库中数据的操作。</p>
</blockquote>
<p>下面学习的命令：</p>
<ul>
<li><code>exists key</code>：判断键是否存在</li>
<li><code>del key</code>：删除键值对</li>
<li><code>move key db</code>：将键值对移动到指定数据库</li>
<li><code>expire key second</code>：设置键值对的过期时间</li>
<li><code>type key</code>：查看value的数据类型</li>
</ul>
<pre><code class="language-bash">127.0.0.1:6379&gt; keys * # 查看当前数据库所有key
(empty list or set)
127.0.0.1:6379&gt; set name qinjiang # set key
OK
127.0.0.1:6379&gt; set age 20
OK
127.0.0.1:6379&gt; keys *
1) &quot;age&quot;
2) &quot;name&quot;
127.0.0.1:6379&gt; move age 1 # 将键值对移动到指定数据库
(integer) 1
127.0.0.1:6379&gt; EXISTS age # 判断键是否存在
(integer) 0 # 不存在
127.0.0.1:6379&gt; EXISTS name
(integer) 1 # 存在
127.0.0.1:6379&gt; SELECT 1
OK
127.0.0.1:6379[1]&gt; keys *
1) &quot;age&quot;
127.0.0.1:6379[1]&gt; del age # 删除键值对
(integer) 1 # 删除个数


127.0.0.1:6379&gt; set age 20
OK
127.0.0.1:6379&gt; EXPIRE age 15 # 设置键值对的过期时间

(integer) 1 # 设置成功 开始计数
127.0.0.1:6379&gt; ttl age # 查看key的过期剩余时间
(integer) 13
127.0.0.1:6379&gt; ttl age
(integer) 11
127.0.0.1:6379&gt; ttl age
(integer) 9
127.0.0.1:6379&gt; ttl age
(integer) -2 # -2 表示key过期，-1表示key未设置过期时间

127.0.0.1:6379&gt; get age # 过期的key 会被自动delete
(nil)
127.0.0.1:6379&gt; keys *
1) &quot;name&quot;

127.0.0.1:6379&gt; type name # 查看value的数据类型
string
</code></pre>
<p>关于<code>TTL</code>命令</p>
<p>Redis的key，通过TTL命令返回key的过期时间，一般来说有3种：</p>
<ol>
<li>当前key没有设置过期时间，所以会返回-1.</li>
<li>当前key有设置过期时间，而且key已经过期，所以会返回-2.</li>
<li>当前key有设置过期时间，且key还没有过期，故会返回key的正常剩余时间.</li>
</ol>
<p>关于重命名<code>RENAME</code>和<code>RENAMENX</code></p>
<ul>
<li><code>RENAME key newkey</code>修改 key 的名称</li>
<li><code>RENAMENX key newkey</code>仅当 newkey 不存在时，将 key 改名为 newkey 。</li>
</ul>
<p>更多命令学习：https://www.redis.net.cn/order/</p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-wBVZtGVm-1597890996517)(狂神说 Redis.assets/image-20200813114228439.png)]</p>
<h3 id="string字符串">String(字符串)</h3>
<p>普通的set、get直接略过。</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>APPEND key value</code></td>
<td>向指定的key的value后追加字符串</td>
<td>127.0.0.1:6379&gt; set msg hello OK 127.0.0.1:6379&gt; append msg &quot; world&quot; (integer) 11 127.0.0.1:6379&gt; get msg “hello world”</td>
</tr>
<tr>
<td><code>DECR/INCR key</code></td>
<td>将指定key的value数值进行+1/-1(仅对于数字)</td>
<td>127.0.0.1:6379&gt; set age 20 OK 127.0.0.1:6379&gt; incr age (integer) 21 127.0.0.1:6379&gt; decr age (integer) 20</td>
</tr>
<tr>
<td><code>INCRBY/DECRBY key n</code></td>
<td>按指定的步长对数值进行加减</td>
<td>127.0.0.1:6379&gt; INCRBY age 5 (integer) 25 127.0.0.1:6379&gt; DECRBY age 10 (integer) 15</td>
</tr>
<tr>
<td><code>INCRBYFLOAT key n</code></td>
<td>为数值加上浮点型数值</td>
<td>127.0.0.1:6379&gt; INCRBYFLOAT age 5.2 “20.2”</td>
</tr>
<tr>
<td><code>STRLEN key</code></td>
<td>获取key保存值的字符串长度</td>
<td>127.0.0.1:6379&gt; get msg “hello world” 127.0.0.1:6379&gt; STRLEN msg (integer) 11</td>
</tr>
<tr>
<td><code>GETRANGE key start end</code></td>
<td>按起止位置获取字符串（闭区间，起止位置都取）</td>
<td>127.0.0.1:6379&gt; get msg “hello world” 127.0.0.1:6379&gt; GETRANGE msg 3 9 “lo worl”</td>
</tr>
<tr>
<td><code>SETRANGE key offset value</code></td>
<td>用指定的value 替换key中 offset开始的值</td>
<td>127.0.0.1:6379&gt; SETRANGE msg 2 hello (integer) 7 127.0.0.1:6379&gt; get msg “tehello”</td>
</tr>
<tr>
<td><code>GETSET key value</code></td>
<td>将给定 key 的值设为 value ，并返回 key 的旧值(old value)。</td>
<td>127.0.0.1:6379&gt; GETSET msg test “hello world”</td>
</tr>
<tr>
<td><code>SETNX key value</code></td>
<td>仅当key不存在时进行set</td>
<td>127.0.0.1:6379&gt; SETNX msg test (integer) 0 127.0.0.1:6379&gt; SETNX name sakura (integer) 1</td>
</tr>
<tr>
<td><code>SETEX key seconds value</code></td>
<td>set 键值对并设置过期时间</td>
<td>127.0.0.1:6379&gt; setex name 10 root OK 127.0.0.1:6379&gt; get name (nil)</td>
</tr>
<tr>
<td><code>MSET key1 value1 [key2 value2..]</code></td>
<td>批量set键值对</td>
<td>127.0.0.1:6379&gt; MSET k1 v1 k2 v2 k3 v3 OK</td>
</tr>
<tr>
<td><code>MSETNX key1 value1 [key2 value2..]</code></td>
<td><mark>原子性</mark>，批量设置键值对，仅当参数中所有的key都不存在时执行</td>
<td>127.0.0.1:6379&gt; MSETNX k1 v1 k4 v4 (integer) 0</td>
</tr>
<tr>
<td><code>MGET key1 [key2..]</code></td>
<td>批量获取多个key保存的值</td>
<td>127.0.0.1:6379&gt; MGET k1 k2 k3 1) “v1” 2) “v2” 3) “v3”</td>
</tr>
<tr>
<td><code>PSETEX key milliseconds value</code></td>
<td>和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间，</td>
<td></td>
</tr>
<tr>
<td><code>getset key value</code></td>
<td>如果不存在值，则返回nil，如果存在值，返回原来的值，并设置新的值,<mark>cas</mark></td>
<td></td>
</tr>
</tbody>
</table>
<p>String类似的使用场景：value除了是字符串还可以是数字，用途举例：</p>
<ul>
<li>计数器</li>
<li>统计多单位的数量：uid:123666：follow 0</li>
<li>粉丝数</li>
<li>对象存储缓存</li>
<li>setnx  分布式锁</li>
<li>key   -&gt;  user:{id}:{field}</li>
</ul>
<h3 id="list列表">List(列表)</h3>
<blockquote>
<p>Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）</p>
<p>一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)。</p>
</blockquote>
<p>首先我们列表，可以经过规则定义将其变为队列、栈、双端队列等</p>
<figure data-type="image" tabindex="18"><img src="https://img-blog.csdnimg.cn/20200820104440398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RERERlbmdf,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-VPvbIltc-1597890996518)(狂神说 Redis.assets/image-20200813114255459.png)]" loading="lazy"></figure>
<p>正如图Redis中List是可以进行双端操作的，所以命令也就分为了LXXX和RLLL两类，有时候L也表示List例如LLEN</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>LPUSH/RPUSH key value1[value2..]</code></td>
<td>从左边/右边向列表中PUSH值(一个或者多个)。</td>
</tr>
<tr>
<td><code>LRANGE key start end</code></td>
<td>获取list 起止元素==（索引从左往右 递增）==</td>
</tr>
<tr>
<td><code>LPUSHX/RPUSHX key value</code></td>
<td>向已存在的列名中push值（一个或者多个）</td>
</tr>
<tr>
<td>`LINSERT key BEFORE</td>
<td>AFTER pivot value`</td>
</tr>
<tr>
<td><code>LLEN key</code></td>
<td>查看列表长度</td>
</tr>
<tr>
<td><code>LINDEX key index</code></td>
<td>通过索引获取列表元素</td>
</tr>
<tr>
<td><code>LSET key index value</code></td>
<td>通过索引为元素设值</td>
</tr>
<tr>
<td><code>LPOP/RPOP key</code></td>
<td>从最左边/最右边移除值 并返回</td>
</tr>
<tr>
<td><code>RPOPLPUSH source destination</code></td>
<td>将列表的尾部(右)最后一个值弹出，并返回，然后加到另一个列表的头部</td>
</tr>
<tr>
<td><code>LTRIM key start end</code></td>
<td>通过下标截取指定范围内的列表</td>
</tr>
<tr>
<td><code>LREM key count value</code></td>
<td>List中是允许value重复的 <code>count &gt; 0</code>：从头部开始搜索 然后删除指定的value 至多删除count个 <code>count &lt; 0</code>：从尾部开始搜索… <code>count = 0</code>：删除列表中所有的指定value。</td>
</tr>
<tr>
<td><code>BLPOP/BRPOP key1[key2] timout</code></td>
<td>移出并获取列表的第一个/最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。</td>
</tr>
<tr>
<td><code>BRPOPLPUSH source destination timeout</code></td>
<td>和<code>RPOPLPUSH</code>功能相同，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。</td>
</tr>
</tbody>
</table>
<pre><code class="language-bash">---------------------------LPUSH---RPUSH---LRANGE--------------------------------

127.0.0.1:6379&gt; LPUSH mylist k1 # LPUSH mylist=&gt;{1}
(integer) 1
127.0.0.1:6379&gt; LPUSH mylist k2 # LPUSH mylist=&gt;{2,1}
(integer) 2
127.0.0.1:6379&gt; RPUSH mylist k3 # RPUSH mylist=&gt;{2,1,3}
(integer) 3
127.0.0.1:6379&gt; get mylist # 普通的get是无法获取list值的
(error) WRONGTYPE Operation against a key holding the wrong kind of value
127.0.0.1:6379&gt; LRANGE mylist 0 4 # LRANGE 获取起止位置范围内的元素
1) &quot;k2&quot;
2) &quot;k1&quot;
3) &quot;k3&quot;
127.0.0.1:6379&gt; LRANGE mylist 0 2
1) &quot;k2&quot;
2) &quot;k1&quot;
3) &quot;k3&quot;
127.0.0.1:6379&gt; LRANGE mylist 0 1
1) &quot;k2&quot;
2) &quot;k1&quot;
127.0.0.1:6379&gt; LRANGE mylist 0 -1 # 获取全部元素
1) &quot;k2&quot;
2) &quot;k1&quot;
3) &quot;k3&quot;

---------------------------LPUSHX---RPUSHX-----------------------------------

127.0.0.1:6379&gt; LPUSHX list v1 # list不存在 LPUSHX失败
(integer) 0
127.0.0.1:6379&gt; LPUSHX list v1 v2  
(integer) 0
127.0.0.1:6379&gt; LPUSHX mylist k4 k5 # 向mylist中 左边 PUSH k4 k5
(integer) 5
127.0.0.1:6379&gt; LRANGE mylist 0 -1
1) &quot;k5&quot;
2) &quot;k4&quot;
3) &quot;k2&quot;
4) &quot;k1&quot;
5) &quot;k3&quot;

---------------------------LINSERT--LLEN--LINDEX--LSET----------------------------

127.0.0.1:6379&gt; LINSERT mylist after k2 ins_key1 # 在k2元素后 插入ins_key1
(integer) 6
127.0.0.1:6379&gt; LRANGE mylist 0 -1
1) &quot;k5&quot;
2) &quot;k4&quot;
3) &quot;k2&quot;
4) &quot;ins_key1&quot;
5) &quot;k1&quot;
6) &quot;k3&quot;
127.0.0.1:6379&gt; LLEN mylist # 查看mylist的长度
(integer) 6
127.0.0.1:6379&gt; LINDEX mylist 3 # 获取下标为3的元素
&quot;ins_key1&quot;
127.0.0.1:6379&gt; LINDEX mylist 0
&quot;k5&quot;
127.0.0.1:6379&gt; LSET mylist 3 k6 # 将下标3的元素 set值为k6
OK
127.0.0.1:6379&gt; LRANGE mylist 0 -1
1) &quot;k5&quot;
2) &quot;k4&quot;
3) &quot;k2&quot;
4) &quot;k6&quot;
5) &quot;k1&quot;
6) &quot;k3&quot;

---------------------------LPOP--RPOP--------------------------

127.0.0.1:6379&gt; LPOP mylist # 左侧(头部)弹出
&quot;k5&quot;
127.0.0.1:6379&gt; RPOP mylist # 右侧(尾部)弹出
&quot;k3&quot;

---------------------------RPOPLPUSH--------------------------

127.0.0.1:6379&gt; LRANGE mylist 0 -1
1) &quot;k4&quot;
2) &quot;k2&quot;
3) &quot;k6&quot;
4) &quot;k1&quot;
127.0.0.1:6379&gt; RPOPLPUSH mylist newlist # 将mylist的最后一个值(k1)弹出，加入到newlist的头部
&quot;k1&quot;
127.0.0.1:6379&gt; LRANGE newlist 0 -1
1) &quot;k1&quot;
127.0.0.1:6379&gt; LRANGE mylist 0 -1
1) &quot;k4&quot;
2) &quot;k2&quot;
3) &quot;k6&quot;

---------------------------LTRIM--------------------------

127.0.0.1:6379&gt; LTRIM mylist 0 1 # 截取mylist中的 0~1部分
OK
127.0.0.1:6379&gt; LRANGE mylist 0 -1
1) &quot;k4&quot;
2) &quot;k2&quot;

# 初始 mylist: k2,k2,k2,k2,k2,k2,k4,k2,k2,k2,k2
---------------------------LREM--------------------------

127.0.0.1:6379&gt; LREM mylist 3 k2 # 从头部开始搜索 至多删除3个 k2
(integer) 3
# 删除后：mylist: k2,k2,k2,k4,k2,k2,k2,k2

127.0.0.1:6379&gt; LREM mylist -2 k2 #从尾部开始搜索 至多删除2个 k2
(integer) 2
# 删除后：mylist: k2,k2,k2,k4,k2,k2


---------------------------BLPOP--BRPOP--------------------------

mylist: k2,k2,k2,k4,k2,k2
newlist: k1

127.0.0.1:6379&gt; BLPOP newlist mylist 30 # 从newlist中弹出第一个值，mylist作为候选
1) &quot;newlist&quot; # 弹出
2) &quot;k1&quot;
127.0.0.1:6379&gt; BLPOP newlist mylist 30
1) &quot;mylist&quot; # 由于newlist空了 从mylist中弹出
2) &quot;k2&quot;
127.0.0.1:6379&gt; BLPOP newlist 30
(30.10s) # 超时了

127.0.0.1:6379&gt; BLPOP newlist 30 # 我们连接另一个客户端向newlist中push了test, 阻塞被解决。
1) &quot;newlist&quot;
2) &quot;test&quot;
(12.54s)
</code></pre>
<blockquote>
<p>小结</p>
</blockquote>
<ul>
<li>list实际上是一个链表，before Node after , left, right 都可以插入值</li>
<li><strong>如果key不存在，则创建新的链表</strong></li>
<li>如果key存在，新增内容</li>
<li>如果移除了所有值，空链表，也代表不存在</li>
<li>在两边插入或者改动值，效率最高！修改中间元素，效率相对较低</li>
</ul>
<p><strong>应用：</strong></p>
<p><strong>消息排队！消息队列（Lpush Rpop）,栈（Lpush Lpop）</strong></p>
<h3 id="set集合">Set(集合)</h3>
<blockquote>
<p>Redis的Set是<strong>string类型</strong>的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。</p>
<p>Redis 中 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。</p>
<p>集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员)。</p>
</blockquote>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SADD key member1[member2..]</code></td>
<td>向集合中无序增加一个/多个成员</td>
</tr>
<tr>
<td><code>SCARD key</code></td>
<td>获取集合的成员数</td>
</tr>
<tr>
<td><code>SMEMBERS key</code></td>
<td>返回集合中所有的成员</td>
</tr>
<tr>
<td><code>SISMEMBER key member</code></td>
<td>查询member元素是否是集合的成员,结果是无序的</td>
</tr>
<tr>
<td><code>SRANDMEMBER key [count]</code></td>
<td>随机返回集合中count个成员，count缺省值为1</td>
</tr>
<tr>
<td><code>SPOP key [count]</code></td>
<td>随机移除并返回集合中count个成员，count缺省值为1</td>
</tr>
<tr>
<td><code>SMOVE source destination member</code></td>
<td>将source集合的成员member移动到destination集合</td>
</tr>
<tr>
<td><code>SREM key member1[member2..]</code></td>
<td>移除集合中一个/多个成员</td>
</tr>
<tr>
<td><code>SDIFF key1[key2..]</code></td>
<td>返回所有集合的差集 key1- key2 - …</td>
</tr>
<tr>
<td><code>SDIFFSTORE destination key1[key2..]</code></td>
<td>在SDIFF的基础上，将结果保存到集合中==(覆盖)==。不能保存到其他类型key噢！</td>
</tr>
<tr>
<td><code>SINTER key1 [key2..]</code></td>
<td>返回所有集合的交集</td>
</tr>
<tr>
<td><code>SINTERSTORE destination key1[key2..]</code></td>
<td>在SINTER的基础上，存储结果到集合中。覆盖</td>
</tr>
<tr>
<td><code>SUNION key1 [key2..]</code></td>
<td>返回所有集合的并集</td>
</tr>
<tr>
<td><code>SUNIONSTORE destination key1 [key2..]</code></td>
<td>在SUNION的基础上，存储结果到及和张。覆盖</td>
</tr>
<tr>
<td><code>SSCAN KEY [MATCH pattern] [COUNT count]</code></td>
<td>在大量数据环境下，使用此命令遍历集合中元素，每次遍历部分</td>
</tr>
</tbody>
</table>
<pre><code class="language-bash">---------------SADD--SCARD--SMEMBERS--SISMEMBER--------------------

127.0.0.1:6379&gt; SADD myset m1 m2 m3 m4 # 向myset中增加成员 m1~m4
(integer) 4
127.0.0.1:6379&gt; SCARD myset # 获取集合的成员数目
(integer) 4
127.0.0.1:6379&gt; smembers myset # 获取集合中所有成员
1) &quot;m4&quot;
2) &quot;m3&quot;
3) &quot;m2&quot;
4) &quot;m1&quot;
127.0.0.1:6379&gt; SISMEMBER myset m5 # 查询m5是否是myset的成员
(integer) 0 # 不是，返回0
127.0.0.1:6379&gt; SISMEMBER myset m2
(integer) 1 # 是，返回1
127.0.0.1:6379&gt; SISMEMBER myset m3
(integer) 1

---------------------SRANDMEMBER--SPOP----------------------------------

127.0.0.1:6379&gt; SRANDMEMBER myset 3 # 随机返回3个成员
1) &quot;m2&quot;
2) &quot;m3&quot;
3) &quot;m4&quot;
127.0.0.1:6379&gt; SRANDMEMBER myset # 随机返回1个成员
&quot;m3&quot;
127.0.0.1:6379&gt; SPOP myset 2 # 随机移除并返回2个成员
1) &quot;m1&quot;
2) &quot;m4&quot;
# 将set还原到{m1,m2,m3,m4}

---------------------SMOVE--SREM----------------------------------------

127.0.0.1:6379&gt; SMOVE myset newset m3 # 将myset中m3成员移动到newset集合
(integer) 1
127.0.0.1:6379&gt; SMEMBERS myset
1) &quot;m4&quot;
2) &quot;m2&quot;
3) &quot;m1&quot;
127.0.0.1:6379&gt; SMEMBERS newset
1) &quot;m3&quot;
127.0.0.1:6379&gt; SREM newset m3 # 从newset中移除m3元素
(integer) 1
127.0.0.1:6379&gt; SMEMBERS newset
(empty list or set)

# 下面开始是多集合操作,多集合操作中若只有一个参数默认和自身进行运算
# setx=&gt;{m1,m2,m4,m6}, sety=&gt;{m2,m5,m6}, setz=&gt;{m1,m3,m6}

-----------------------------SDIFF------------------------------------

127.0.0.1:6379&gt; SDIFF setx sety setz # 等价于setx-sety-setz
1) &quot;m4&quot;
127.0.0.1:6379&gt; SDIFF setx sety # setx - sety
1) &quot;m4&quot;
2) &quot;m1&quot;
127.0.0.1:6379&gt; SDIFF sety setx # sety - setx
1) &quot;m5&quot;


-------------------------SINTER---------------------------------------
# 共同关注（交集）

127.0.0.1:6379&gt; SINTER setx sety setz # 求 setx、sety、setx的交集
1) &quot;m6&quot;
127.0.0.1:6379&gt; SINTER setx sety # 求setx sety的交集
1) &quot;m2&quot;
2) &quot;m6&quot;

-------------------------SUNION---------------------------------------

127.0.0.1:6379&gt; SUNION setx sety setz # setx sety setz的并集
1) &quot;m4&quot;
2) &quot;m6&quot;
3) &quot;m3&quot;
4) &quot;m2&quot;
5) &quot;m1&quot;
6) &quot;m5&quot;
127.0.0.1:6379&gt; SUNION setx sety # setx sety 并集
1) &quot;m4&quot;
2) &quot;m6&quot;
3) &quot;m2&quot;
4) &quot;m1&quot;
5) &quot;m5&quot;
</code></pre>
<h3 id="hash哈希">Hash（哈希）</h3>
<blockquote>
<p>Redis hash 是一个string类型的field和value的映射表，hash特别适合用于存储对象。</p>
<p>Set就是一种简化的Hash,只变动key,而value使用默认值填充。可以将一个Hash表作为一个对象进行存储，表中存放对象的信息。</p>
</blockquote>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>HSET key field value</code></td>
<td>将哈希表 key 中的字段 field 的值设为 value 。重复设置同一个field会覆盖,返回0</td>
</tr>
<tr>
<td><code>HMSET key field1 value1 [field2 value2..]</code></td>
<td>同时将多个 field-value (域-值)对设置到哈希表 key 中。</td>
</tr>
<tr>
<td><code>HSETNX key field value</code></td>
<td>只有在字段 field 不存在时，设置哈希表字段的值。</td>
</tr>
<tr>
<td><code>HEXISTS key field</code></td>
<td>查看哈希表 key 中，指定的字段是否存在。</td>
</tr>
<tr>
<td><code>HGET key field value</code></td>
<td>获取存储在哈希表中指定字段的值</td>
</tr>
<tr>
<td><code>HMGET key field1 [field2..]</code></td>
<td>获取所有给定字段的值</td>
</tr>
<tr>
<td><code>HGETALL key</code></td>
<td>获取在哈希表key 的所有字段和值</td>
</tr>
<tr>
<td><code>HKEYS key</code></td>
<td>获取哈希表key中所有的字段</td>
</tr>
<tr>
<td><code>HLEN key</code></td>
<td>获取哈希表中字段的数量</td>
</tr>
<tr>
<td><code>HVALS key</code></td>
<td>获取哈希表中所有值</td>
</tr>
<tr>
<td><code>HDEL key field1 [field2..]</code></td>
<td>删除哈希表key中一个/多个field字段</td>
</tr>
<tr>
<td><code>HINCRBY key field n</code></td>
<td>为哈希表 key 中的指定字段的整数值加上增量n，并返回增量后结果 一样只适用于整数型字段</td>
</tr>
<tr>
<td><code>HINCRBYFLOAT key field n</code></td>
<td>为哈希表 key 中的指定字段的浮点数值加上增量 n。</td>
</tr>
<tr>
<td><code>HSCAN key cursor [MATCH pattern] [COUNT count]</code></td>
<td>迭代哈希表中的键值对。</td>
</tr>
</tbody>
</table>
<pre><code class="language-bash">------------------------HSET--HMSET--HSETNX----------------
127.0.0.1:6379&gt; HSET studentx name sakura # 将studentx哈希表作为一个对象，设置name为sakura
(integer) 1
127.0.0.1:6379&gt; HSET studentx name gyc # 重复设置field进行覆盖，并返回0
(integer) 0
127.0.0.1:6379&gt; HSET studentx age 20 # 设置studentx的age为20
(integer) 1
127.0.0.1:6379&gt; HMSET studentx sex 1 tel 15623667886 # 设置sex为1，tel为15623667886
OK
127.0.0.1:6379&gt; HSETNX studentx name gyc # HSETNX 设置已存在的field
(integer) 0 # 失败
127.0.0.1:6379&gt; HSETNX studentx email 12345@qq.com
(integer) 1 # 成功

----------------------HEXISTS--------------------------------
127.0.0.1:6379&gt; HEXISTS studentx name # name字段在studentx中是否存在
(integer) 1 # 存在
127.0.0.1:6379&gt; HEXISTS studentx addr
(integer) 0 # 不存在

-------------------HGET--HMGET--HGETALL-----------
127.0.0.1:6379&gt; HGET studentx name # 获取studentx中name字段的value
&quot;gyc&quot;
127.0.0.1:6379&gt; HMGET studentx name age tel # 获取studentx中name、age、tel字段的value
1) &quot;gyc&quot;
2) &quot;20&quot;
3) &quot;15623667886&quot;
127.0.0.1:6379&gt; HGETALL studentx # 获取studentx中所有的field及其value
 1) &quot;name&quot;
 2) &quot;gyc&quot;
 3) &quot;age&quot;
 4) &quot;20&quot;
 5) &quot;sex&quot;
 6) &quot;1&quot;
 7) &quot;tel&quot;
 8) &quot;15623667886&quot;
 9) &quot;email&quot;
10) &quot;12345@qq.com&quot;


--------------------HKEYS--HLEN--HVALS--------------
127.0.0.1:6379&gt; HKEYS studentx # 查看studentx中所有的field
1) &quot;name&quot;
2) &quot;age&quot;
3) &quot;sex&quot;
4) &quot;tel&quot;
5) &quot;email&quot;
127.0.0.1:6379&gt; HLEN studentx # 查看studentx中的字段数量
(integer) 5
127.0.0.1:6379&gt; HVALS studentx # 查看studentx中所有的value
1) &quot;gyc&quot;
2) &quot;20&quot;
3) &quot;1&quot;
4) &quot;15623667886&quot;
5) &quot;12345@qq.com&quot;

-------------------------HDEL--------------------------
127.0.0.1:6379&gt; HDEL studentx sex tel # 删除studentx 中的sex、tel字段
(integer) 2
127.0.0.1:6379&gt; HKEYS studentx
1) &quot;name&quot;
2) &quot;age&quot;
3) &quot;email&quot;

-------------HINCRBY--HINCRBYFLOAT------------------------
127.0.0.1:6379&gt; HINCRBY studentx age 1 # studentx的age字段数值+1
(integer) 21
127.0.0.1:6379&gt; HINCRBY studentx name 1 # 非整数字型字段不可用
(error) ERR hash value is not an integer
127.0.0.1:6379&gt; HINCRBYFLOAT studentx weight 0.6 # weight字段增加0.6
&quot;90.8&quot;
</code></pre>
<p>Hash变更的数据user name age，尤其是用户信息之类的，经常变动的信息！<strong>Hash更适合于对象的存储，Sring更加适合字符串存储！</strong></p>
<h3 id="zset有序集合">Zset（有序集合）</h3>
<blockquote>
<p>不同的是每个元素都会关联一个double类型的分数（score）。redis正是通过分数来为集合中的成员进行从小到大的排序。</p>
<p>score相同：按字典顺序排序</p>
<p>有序集合的成员是唯一的,但分数(score)却可以重复。</p>
</blockquote>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ZADD key score member1 [score2 member2]</code></td>
<td>向有序集合添加一个或多个成员，或者更新已存在成员的分数</td>
</tr>
<tr>
<td><code>ZCARD key</code></td>
<td>获取有序集合的成员数</td>
</tr>
<tr>
<td><code>ZCOUNT key min max</code></td>
<td>计算在有序集合中指定区间score的成员数</td>
</tr>
<tr>
<td><code>ZINCRBY key n member</code></td>
<td>有序集合中对指定成员的分数加上增量 n</td>
</tr>
<tr>
<td><code>ZSCORE key member</code></td>
<td>返回有序集中，成员的分数值</td>
</tr>
<tr>
<td><code>ZRANK key member</code></td>
<td>返回有序集合中指定成员的索引</td>
</tr>
<tr>
<td><code>ZRANGE key start end</code></td>
<td>通过索引区间返回有序集合成指定区间内的成员</td>
</tr>
<tr>
<td><code>ZRANGEBYLEX key min max</code></td>
<td>通过字典区间返回有序集合的成员</td>
</tr>
<tr>
<td><code>ZRANGEBYSCORE key min max</code></td>
<td>通过分数返回有序集合指定区间内的成员==-inf 和 +inf分别表示最小最大值，只支持开区间()==</td>
</tr>
<tr>
<td><code>ZLEXCOUNT key min max</code></td>
<td>在有序集合中计算指定字典区间内成员数量</td>
</tr>
<tr>
<td><code>ZREM key member1 [member2..]</code></td>
<td>移除有序集合中一个/多个成员</td>
</tr>
<tr>
<td><code>ZREMRANGEBYLEX key min max</code></td>
<td>移除有序集合中给定的字典区间的所有成员</td>
</tr>
<tr>
<td><code>ZREMRANGEBYRANK key start stop</code></td>
<td>移除有序集合中给定的排名区间的所有成员</td>
</tr>
<tr>
<td><code>ZREMRANGEBYSCORE key min max</code></td>
<td>移除有序集合中给定的分数区间的所有成员</td>
</tr>
<tr>
<td><code>ZREVRANGE key start end</code></td>
<td>返回有序集中指定区间内的成员，通过索引，分数从高到底</td>
</tr>
<tr>
<td><code>ZREVRANGEBYSCORRE key max min</code></td>
<td>返回有序集中指定分数区间内的成员，分数从高到低排序</td>
</tr>
<tr>
<td><code>ZREVRANGEBYLEX key max min</code></td>
<td>返回有序集中指定字典区间内的成员，按字典顺序倒序</td>
</tr>
<tr>
<td><code>ZREVRANK key member</code></td>
<td>返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序</td>
</tr>
<tr>
<td><code>ZINTERSTORE destination numkeys key1 [key2 ..]</code></td>
<td>计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中，numkeys：表示参与运算的集合数，将score相加作为结果的score</td>
</tr>
<tr>
<td><code>ZUNIONSTORE destination numkeys key1 [key2..]</code></td>
<td>计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中</td>
</tr>
<tr>
<td><code>ZSCAN key cursor [MATCH pattern\] [COUNT count]</code></td>
<td>迭代有序集合中的元素（包括元素成员和元素分值）</td>
</tr>
</tbody>
</table>
<pre><code class="language-bash">-------------------ZADD--ZCARD--ZCOUNT--------------
127.0.0.1:6379&gt; ZADD myzset 1 m1 2 m2 3 m3 # 向有序集合myzset中添加成员m1 score=1 以及成员m2 score=2..
(integer) 2
127.0.0.1:6379&gt; ZCARD myzset # 获取有序集合的成员数
(integer) 2
127.0.0.1:6379&gt; ZCOUNT myzset 0 1 # 获取score在 [0,1]区间的成员数量
(integer) 1
127.0.0.1:6379&gt; ZCOUNT myzset 0 2
(integer) 2

----------------ZINCRBY--ZSCORE--------------------------
127.0.0.1:6379&gt; ZINCRBY myzset 5 m2 # 将成员m2的score +5
&quot;7&quot;
127.0.0.1:6379&gt; ZSCORE myzset m1 # 获取成员m1的score
&quot;1&quot;
127.0.0.1:6379&gt; ZSCORE myzset m2
&quot;7&quot;

--------------ZRANK--ZRANGE-----------------------------------
127.0.0.1:6379&gt; ZRANK myzset m1 # 获取成员m1的索引，索引按照score排序，score相同索引值按字典顺序顺序增加
(integer) 0
127.0.0.1:6379&gt; ZRANK myzset m2
(integer) 2
127.0.0.1:6379&gt; ZRANGE myzset 0 1 # 获取索引在 0~1的成员
1) &quot;m1&quot;
2) &quot;m3&quot;
127.0.0.1:6379&gt; ZRANGE myzset 0 -1 # 获取全部成员
1) &quot;m1&quot;
2) &quot;m3&quot;
3) &quot;m2&quot;

#testset=&gt;{abc,add,amaze,apple,back,java,redis} score均为0
------------------ZRANGEBYLEX---------------------------------
127.0.0.1:6379&gt; ZRANGEBYLEX testset - + # 返回所有成员
1) &quot;abc&quot;
2) &quot;add&quot;
3) &quot;amaze&quot;
4) &quot;apple&quot;
5) &quot;back&quot;
6) &quot;java&quot;
7) &quot;redis&quot;
127.0.0.1:6379&gt; ZRANGEBYLEX testset - + LIMIT 0 3 # 分页 按索引显示查询结果的 0,1,2条记录
1) &quot;abc&quot;
2) &quot;add&quot;
3) &quot;amaze&quot;
127.0.0.1:6379&gt; ZRANGEBYLEX testset - + LIMIT 3 3 # 显示 3,4,5条记录
1) &quot;apple&quot;
2) &quot;back&quot;
3) &quot;java&quot;
127.0.0.1:6379&gt; ZRANGEBYLEX testset (- [apple # 显示 (-,apple] 区间内的成员
1) &quot;abc&quot;
2) &quot;add&quot;
3) &quot;amaze&quot;
4) &quot;apple&quot;
127.0.0.1:6379&gt; ZRANGEBYLEX testset [apple [java # 显示 [apple,java]字典区间的成员
1) &quot;apple&quot;
2) &quot;back&quot;
3) &quot;java&quot;

-----------------------ZRANGEBYSCORE---------------------
127.0.0.1:6379&gt; ZRANGEBYSCORE myzset 1 10 # 返回score在 [1,10]之间的的成员
1) &quot;m1&quot;
2) &quot;m3&quot;
3) &quot;m2&quot;
127.0.0.1:6379&gt; ZRANGEBYSCORE myzset 1 5
1) &quot;m1&quot;
2) &quot;m3&quot;

--------------------ZLEXCOUNT-----------------------------
127.0.0.1:6379&gt; ZLEXCOUNT testset - +
(integer) 7
127.0.0.1:6379&gt; ZLEXCOUNT testset [apple [java
(integer) 3

------------------ZREM--ZREMRANGEBYLEX--ZREMRANGBYRANK--ZREMRANGEBYSCORE--------------------------------
127.0.0.1:6379&gt; ZREM testset abc # 移除成员abc
(integer) 1
127.0.0.1:6379&gt; ZREMRANGEBYLEX testset [apple [java # 移除字典区间[apple,java]中的所有成员
(integer) 3
127.0.0.1:6379&gt; ZREMRANGEBYRANK testset 0 1 # 移除排名0~1的所有成员
(integer) 2
127.0.0.1:6379&gt; ZREMRANGEBYSCORE myzset 0 3 # 移除score在 [0,3]的成员
(integer) 2


# testset=&gt; {abc,add,apple,amaze,back,java,redis} score均为0
# myzset=&gt; {(m1,1),(m2,2),(m3,3),(m4,4),(m7,7),(m9,9)}
----------------ZREVRANGE--ZREVRANGEBYSCORE--ZREVRANGEBYLEX-----------
127.0.0.1:6379&gt; ZREVRANGE myzset 0 3 # 按score递减排序，然后按索引，返回结果的 0~3
1) &quot;m9&quot;
2) &quot;m7&quot;
3) &quot;m4&quot;
4) &quot;m3&quot;
127.0.0.1:6379&gt; ZREVRANGE myzset 2 4 # 返回排序结果的 索引的2~4
1) &quot;m4&quot;
2) &quot;m3&quot;
3) &quot;m2&quot;
127.0.0.1:6379&gt; ZREVRANGEBYSCORE myzset 6 2 # 按score递减顺序 返回集合中分数在[2,6]之间的成员
1) &quot;m4&quot;
2) &quot;m3&quot;
3) &quot;m2&quot;
127.0.0.1:6379&gt; ZREVRANGEBYLEX testset [java (add # 按字典倒序 返回集合中(add,java]字典区间的成员
1) &quot;java&quot;
2) &quot;back&quot;
3) &quot;apple&quot;
4) &quot;amaze&quot;

-------------------------ZREVRANK------------------------------
127.0.0.1:6379&gt; ZREVRANK myzset m7 # 按score递减顺序，返回成员m7索引
(integer) 1
127.0.0.1:6379&gt; ZREVRANK myzset m2
(integer) 4


# mathscore=&gt;{(xm,90),(xh,95),(xg,87)} 小明、小红、小刚的数学成绩
# enscore=&gt;{(xm,70),(xh,93),(xg,90)} 小明、小红、小刚的英语成绩
-------------------ZINTERSTORE--ZUNIONSTORE-----------------------------------
127.0.0.1:6379&gt; ZINTERSTORE sumscore 2 mathscore enscore # 将mathscore enscore进行合并 结果存放到sumscore
(integer) 3
127.0.0.1:6379&gt; ZRANGE sumscore 0 -1 withscores # 合并后的score是之前集合中所有score的和
1) &quot;xm&quot;
2) &quot;160&quot;
3) &quot;xg&quot;
4) &quot;177&quot;
5) &quot;xh&quot;
6) &quot;188&quot;

127.0.0.1:6379&gt; ZUNIONSTORE lowestscore 2 mathscore enscore AGGREGATE MIN # 取两个集合的成员score最小值作为结果的
(integer) 3
127.0.0.1:6379&gt; ZRANGE lowestscore 0 -1 withscores
1) &quot;xm&quot;
2) &quot;70&quot;
3) &quot;xg&quot;
4) &quot;87&quot;
5) &quot;xh&quot;
6) &quot;93&quot;
</code></pre>
<p>应用案例：</p>
<ul>
<li>set排序 存储班级成绩表 工资表排序！</li>
<li>普通消息，1.重要消息 2.带权重进行判断</li>
<li>排行榜应用实现，取Top N测试</li>
</ul>
<h2 id="四-三种特殊数据类型">四、三种特殊数据类型</h2>
<h3 id="geospatial地理位置">Geospatial(地理位置)</h3>
<blockquote>
<p>使用经纬度定位地理坐标并用一个<strong>有序集合zset保存</strong>，所以zset命令也可以使用</p>
</blockquote>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>geoadd key longitud(经度) latitude(纬度) member [..]</code></td>
<td>将具体经纬度的坐标存入一个有序集合</td>
</tr>
<tr>
<td><code>geopos key member [member..]</code></td>
<td>获取集合中的一个/多个成员坐标</td>
</tr>
<tr>
<td><code>geodist key member1 member2 [unit]</code></td>
<td>返回两个给定位置之间的距离。默认以米作为单位。</td>
</tr>
<tr>
<td>`georadius key longitude latitude radius m</td>
<td>km</td>
</tr>
<tr>
<td><code>GEORADIUSBYMEMBER key member radius...</code></td>
<td>功能与GEORADIUS相同，只是中心位置不是具体的经纬度，而是使用结合中已有的成员作为中心点。</td>
</tr>
<tr>
<td><code>geohash key member1 [member2..]</code></td>
<td>返回一个或多个位置元素的Geohash表示。使用Geohash位置52点整数编码。</td>
</tr>
</tbody>
</table>
<p><strong>有效经纬度</strong></p>
<blockquote>
<ul>
<li>有效的经度从-180度到180度。</li>
<li>有效的纬度从-85.05112878度到85.05112878度。</li>
</ul>
</blockquote>
<p>指定单位的参数 <strong>unit</strong> 必须是以下单位的其中一个：</p>
<ul>
<li><strong>m</strong> 表示单位为米。</li>
<li><strong>km</strong> 表示单位为千米。</li>
<li><strong>mi</strong> 表示单位为英里。</li>
<li><strong>ft</strong> 表示单位为英尺。</li>
</ul>
<p><strong>关于GEORADIUS的参数</strong></p>
<blockquote>
<p>通过<code>georadius</code>就可以完成 <strong>附近的人</strong>功能</p>
<p>withcoord:带上坐标</p>
<p>withdist:带上距离，单位与半径单位相同</p>
<p>COUNT n : 只显示前n个(按距离递增排序)</p>
</blockquote>
<pre><code class="language-bash">----------------georadius---------------------
127.0.0.1:6379&gt; GEORADIUS china:city 120 30 500 km withcoord withdist # 查询经纬度(120,30)坐标500km半径内的成员
1) 1) &quot;hangzhou&quot;
   2) &quot;29.4151&quot;
   3) 1) &quot;120.20000249147415&quot;
      2) &quot;30.199999888333501&quot;
2) 1) &quot;shanghai&quot;
   2) &quot;205.3611&quot;
   3) 1) &quot;121.40000134706497&quot;
      2) &quot;31.400000253193539&quot;
     
------------geohash---------------------------
127.0.0.1:6379&gt; geohash china:city yichang shanghai # 获取成员经纬坐标的geohash表示
1) &quot;wmrjwbr5250&quot;
2) &quot;wtw6ds0y300&quot;
</code></pre>
<h3 id="hyperloglog基数统计">Hyperloglog(基数统计)</h3>
<blockquote>
<p>Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。</p>
<p>花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。</p>
<p>因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。</p>
<p>其底层使用string数据类型</p>
</blockquote>
<p><strong>什么是基数？</strong></p>
<blockquote>
<p>数据集中不重复的元素的个数。</p>
</blockquote>
<p><strong>应用场景：</strong></p>
<p>网页的访问量（UV）：一个用户多次访问，也只能算作一个人。</p>
<blockquote>
<p>传统实现，存储用户的id,然后每次进行比较。当用户变多之后这种方式及其浪费空间，而我们的目的只是<strong>计数</strong>，Hyperloglog就能帮助我们利用最小的空间完成。</p>
</blockquote>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>PFADD key element1 [elememt2..]</code></td>
<td>添加指定元素到 HyperLogLog 中</td>
</tr>
<tr>
<td><code>PFCOUNT key [key]</code></td>
<td>返回给定 HyperLogLog 的基数估算值。</td>
</tr>
<tr>
<td><code>PFMERGE destkey sourcekey [sourcekey..]</code></td>
<td>将多个 HyperLogLog 合并为一个 HyperLogLog</td>
</tr>
</tbody>
</table>
<pre><code class="language-bash">----------PFADD--PFCOUNT---------------------
127.0.0.1:6379&gt; PFADD myelemx a b c d e f g h i j k # 添加元素
(integer) 1
127.0.0.1:6379&gt; type myelemx # hyperloglog底层使用String
string
127.0.0.1:6379&gt; PFCOUNT myelemx # 估算myelemx的基数
(integer) 11
127.0.0.1:6379&gt; PFADD myelemy i j k z m c b v p q s
(integer) 1
127.0.0.1:6379&gt; PFCOUNT myelemy
(integer) 11

----------------PFMERGE-----------------------
127.0.0.1:6379&gt; PFMERGE myelemz myelemx myelemy # 合并myelemx和myelemy 成为myelemz
OK
127.0.0.1:6379&gt; PFCOUNT myelemz # 估算基数
(integer) 17
</code></pre>
<p>如果<mark>允许容错</mark>，那么一定可以使用Hyperloglog !</p>
<p>如果不允许容错，就使用set或者自己的数据类型即可 ！</p>
<h3 id="bitmaps位图">BitMaps(位图)</h3>
<blockquote>
<p>使用位存储，信息状态只有 0 和 1</p>
<p>Bitmap是一串连续的2进制数字（0或1），每一位所在的位置为偏移(offset)，在bitmap上可执行AND,OR,XOR,NOT以及其它位操作。</p>
</blockquote>
<p><strong>应用场景</strong></p>
<p>签到统计、状态统计</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>setbit key offset value</code></td>
<td>为指定key的offset位设置值</td>
</tr>
<tr>
<td><code>getbit key offset</code></td>
<td>获取offset位的值</td>
</tr>
<tr>
<td><code>bitcount key [start end]</code></td>
<td>统计字符串被设置为1的bit数，也可以指定统计范围按字节</td>
</tr>
<tr>
<td><code>bitop operration destkey key[key..]</code></td>
<td>对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。</td>
</tr>
<tr>
<td><code>BITPOS key bit [start] [end]</code></td>
<td>返回字符串里面第一个被设置为1或者0的bit位。start和end只能按字节,不能按位</td>
</tr>
</tbody>
</table>
<pre><code class="language-bash">------------setbit--getbit--------------
127.0.0.1:6379&gt; setbit sign 0 1 # 设置sign的第0位为 1 
(integer) 0
127.0.0.1:6379&gt; setbit sign 2 1 # 设置sign的第2位为 1  不设置默认 是0
(integer) 0
127.0.0.1:6379&gt; setbit sign 3 1
(integer) 0
127.0.0.1:6379&gt; setbit sign 5 1
(integer) 0
127.0.0.1:6379&gt; type sign
string

127.0.0.1:6379&gt; getbit sign 2 # 获取第2位的数值
(integer) 1
127.0.0.1:6379&gt; getbit sign 3
(integer) 1
127.0.0.1:6379&gt; getbit sign 4 # 未设置默认是0
(integer) 0

-----------bitcount----------------------------
127.0.0.1:6379&gt; BITCOUNT sign # 统计sign中为1的位数
(integer) 4
</code></pre>
<p><strong>bitmaps的底层</strong></p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9PlszjhS-1597890996519)(D:\我\MyBlog\狂神说 Redis.assets\image-20200803234336175.png)]</p>
<p>这样设置以后你能get到的值是：<strong>\xA2\x80</strong>，所以bitmaps是一串从左到右的二进制串</p>
<h2 id="五-事务">五、事务</h2>
<p><mark>Redis的单条命令是保证原子性的，但是redis事务不能保证原子性</mark></p>
<blockquote>
<p>Redis事务本质：一组命令的集合。</p>
<p>----------------- 队列 set set set 执行 -------------------</p>
<p>事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。</p>
<ul>
<li>一次性：一次性执行</li>
<li>顺序性：队列依次执行</li>
<li>排他性：不允许被干扰</li>
</ul>
<hr>
<ol>
<li>Redis事务没有隔离级别的概念</li>
<li>Redis单条命令是保证原子性的，但是事务不保证原子性，<strong>且没有回滚</strong>！</li>
<li><strong>Redis的事务总是具有ACID中的一致性和隔离性</strong>，其他特性是不支持的。当服务器运行在<em>AOF</em>持久化模式下，并且appendfsync选项的值为always时，事务也具有<strong>耐久性</strong>。</li>
</ol>
</blockquote>
<h3 id="redis事务其他实现">Redis事务其他实现</h3>
<ul>
<li>基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行，其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完</li>
<li>基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐</li>
</ul>
<h3 id="redis事务操作过程">Redis事务操作过程</h3>
<ul>
<li>开启事务（<code>multi</code>）</li>
<li>命令入队</li>
<li>执行事务（<code>exec</code>）</li>
</ul>
<p><mark>所以事务中的命令在加入时都没有被执行，直到提交时才会开始执行(Exec)一次性完成。</mark></p>
<pre><code class="language-bash">127.0.0.1:6379&gt; multi # 开启事务
OK
127.0.0.1:6379&gt; set k1 v1 # 命令入队
QUEUED
127.0.0.1:6379&gt; set k2 v2 # ..
QUEUED
127.0.0.1:6379&gt; get k1
QUEUED
127.0.0.1:6379&gt; set k3 v3
QUEUED
127.0.0.1:6379&gt; keys *
QUEUED
127.0.0.1:6379&gt; exec # 事务执行
1) OK
2) OK
3) &quot;v1&quot;
4) OK
5) 1) &quot;k3&quot;
   2) &quot;k2&quot;
   3) &quot;k1&quot;
</code></pre>
<p><strong>取消事务(<code>discurd</code>)</strong></p>
<pre><code class="language-bash">127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; set k1 v1
QUEUED
127.0.0.1:6379&gt; set k2 v2
QUEUED
127.0.0.1:6379&gt; DISCARD # 放弃事务
OK
127.0.0.1:6379&gt; EXEC 
(error) ERR EXEC without MULTI # 当前未开启事务
127.0.0.1:6379&gt; get k1 # 被放弃事务中命令并未执行
(nil)
</code></pre>
<h3 id="事务错误">事务错误</h3>
<blockquote>
<p>代码语法错误（编译时异常）所有的命令都不执行</p>
</blockquote>
<pre><code class="language-bash">127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; set k1 v1
QUEUED
127.0.0.1:6379&gt; set k2 v2
QUEUED
127.0.0.1:6379&gt; error k1 # 这是一条语法错误命令
(error) ERR unknown command `error`, with args beginning with: `k1`, # 会报错但是不影响后续命令入队 
127.0.0.1:6379&gt; get k2
QUEUED
127.0.0.1:6379&gt; EXEC
(error) EXECABORT Transaction discarded because of previous errors. # 执行报错
127.0.0.1:6379&gt; get k1 
(nil) # 其他命令并没有被执行
</code></pre>
<blockquote>
<p>代码逻辑错误 (运行时异常) **其他命令可以正常执行 ** &gt;&gt;&gt; 所以不保证事务原子性</p>
</blockquote>
<pre><code class="language-bash">127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; set k1 v1
QUEUED
127.0.0.1:6379&gt; set k2 v2
QUEUED
127.0.0.1:6379&gt; INCR k1 # 这条命令逻辑错误（对字符串进行增量）
QUEUED
127.0.0.1:6379&gt; get k2
QUEUED
127.0.0.1:6379&gt; exec
1) OK
2) OK
3) (error) ERR value is not an integer or out of range # 运行时报错
4) &quot;v2&quot; # 其他命令正常执行

# 虽然中间有一条命令报错了，但是后面的指令依旧正常执行成功了。
# 所以说Redis单条指令保证原子性，但是Redis事务不能保证原子性。
</code></pre>
<h3 id="监控">监控</h3>
<p><strong>悲观锁：</strong></p>
<ul>
<li>很悲观，认为什么时候都会出现问题，无论做什么都会加锁</li>
</ul>
<p><strong>乐观锁：</strong></p>
<ul>
<li>很乐观，认为什么时候都不会出现问题，所以不会上锁！更新数据的时候去判断一下，在此期间是否有人修改过这个数据</li>
<li>获取version</li>
<li>更新的时候比较version</li>
</ul>
<p>使用<code>watch key</code>监控指定数据，相当于乐观锁加锁。可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。</p>
<blockquote>
<p>正常执行</p>
</blockquote>
<pre><code class="language-bash">127.0.0.1:6379&gt; set money 100 # 设置余额:100
OK
127.0.0.1:6379&gt; set use 0 # 支出使用:0
OK
127.0.0.1:6379&gt; watch money # 监视money (上锁)
OK
127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; DECRBY money 20
QUEUED
127.0.0.1:6379&gt; INCRBY use 20
QUEUED
127.0.0.1:6379&gt; exec # 监视值没有被中途修改，事务正常执行
1) (integer) 80
2) (integer) 20
</code></pre>
<blockquote>
<p>测试多线程修改值，使用watch可以当做redis的乐观锁操作（<mark>相当于getversion</mark>，）</p>
<p>提交事务时比对之前watch时get到的值，发现和现在不同，就执行失败，相当于加锁。</p>
</blockquote>
<p>我们启动另外一个客户端模拟插队线程。</p>
<p>线程1：</p>
<pre><code class="language-bash">127.0.0.1:6379&gt; watch money # money上锁
OK
127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; DECRBY money 20
QUEUED
127.0.0.1:6379&gt; INCRBY use 20
QUEUED
127.0.0.1:6379&gt; 	# 此时事务并没有执行
</code></pre>
<p>模拟线程插队，线程2：</p>
<pre><code class="language-bash">127.0.0.1:6379&gt; INCRBY money 500 # 修改了线程一中监视的money
(integer) 600
12
</code></pre>
<p>回到线程1，执行事务：</p>
<pre><code class="language-bash">127.0.0.1:6379&gt; EXEC # 执行之前，另一个线程修改了我们的值，这个时候就会导致事务执行失败
(nil) # 没有结果，说明事务执行失败

127.0.0.1:6379&gt; get money # 线程2 修改生效
&quot;600&quot;
127.0.0.1:6379&gt; get use # 线程1事务执行失败，数值没有被修改
&quot;0&quot;
</code></pre>
<blockquote>
<p>失败了怎么办呢？解锁获取最新值，然后再加锁进行事务。</p>
<p><code>unwatch</code>进行解锁。</p>
</blockquote>
<figure data-type="image" tabindex="19"><img src="https://memorykki.github.io/post-images/redis/image-20210831113434480.png" alt="" loading="lazy"></figure>
<p>注意：每次提交执行exec后都会自动释放锁，不管是否成功</p>
<h2 id="六-jedis">六、Jedis</h2>
<p>使用Java来操作Redis，Jedis是Redis官方推荐使用的Java连接redis的客户端。</p>
<ol>
<li>
<p>导入依赖</p>
<pre><code class="language-xml">&lt;!--导入jredis的包--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;redis.clients&lt;/groupId&gt;
    &lt;artifactId&gt;jedis&lt;/artifactId&gt;
    &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;!--fastjson--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
    &lt;artifactId&gt;fastjson&lt;/artifactId&gt;
    &lt;version&gt;1.2.70&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
</li>
<li>
<p>编码测试</p>
<ul>
<li>
<p>连接数据库</p>
<ol>
<li>
<p>修改redis的配置文件</p>
<pre><code class="language-bash">vim /usr/local/bin/myconfig/redis.conf
1
</code></pre>
<ol>
<li>
<p>将只绑定本地注释</p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-4IRUFJ95-1597890996520)(狂神说 Redis.assets/image-20200813161921480.png)]</p>
</li>
<li>
<p>保护模式改为 no</p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-oKjIVapw-1597890996521)(狂神说 Redis.assets/image-20200813161939847.png)]</p>
</li>
<li>
<p>允许后台运行</p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-c2IMvpZL-1597890996522)(狂神说 Redis.assets/image-20200813161954567.png)]</p>
</li>
</ol>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p>开放端口6379</p>
<pre><code class="language-bash">firewall-cmd --zone=public --add-port=6379/tcp --permanet
1
</code></pre>
<p>重启防火墙服务</p>
<pre><code class="language-bash">systemctl restart firewalld.service
1
</code></pre>
<ol>
<li>
<p>阿里云服务器控制台配置安全组</p>
</li>
<li>
<p>重启redis-server</p>
<pre><code class="language-bash">[root@AlibabaECS bin]# redis-server myconfig/redis.conf 
1
</code></pre>
</li>
</ol>
</li>
</ol>
<ul>
<li>
<p>操作命令</p>
<p><strong>TestPing.java</strong></p>
<pre><code class="language-java">public class TestPing {
    public static void main(String[] args) {
        Jedis jedis = new Jedis(&quot;192.168.xx.xxx&quot;, 6379);
        String response = jedis.ping();
        System.out.println(response); // PONG
    }
}
</code></pre>
</li>
<li>
<p>断开连接</p>
</li>
</ul>
<ol>
<li>
<p><strong>事务</strong></p>
<pre><code class="language-java">public class TestTX {
    public static void main(String[] args) {
        Jedis jedis = new Jedis(&quot;39.99.xxx.xx&quot;, 6379);

        JSONObject jsonObject = new JSONObject();
        jsonObject.put(&quot;hello&quot;, &quot;world&quot;);
        jsonObject.put(&quot;name&quot;, &quot;kuangshen&quot;);
        // 开启事务
        Transaction multi = jedis.multi();
        String result = jsonObject.toJSONString();
        // jedis.watch(result)
        try {
            multi.set(&quot;user1&quot;, result);
            multi.set(&quot;user2&quot;, result);
            // 执行事务
            multi.exec();
        }catch (Exception e){
            // 放弃事务
            multi.discard();
        } finally {
            // 关闭连接
            System.out.println(jedis.get(&quot;user1&quot;));
            System.out.println(jedis.get(&quot;user2&quot;));
            jedis.close();
        }
    }
}
</code></pre>
</li>
</ol>
<h2 id="七-springboot整合">七、SpringBoot整合</h2>
<ol>
<li>导入依赖</li>
</ol>
<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>
<p>springboot 2.x后 ，原来使用的 Jedis 被 lettuce 替换。</p>
<blockquote>
<p>jedis：采用的直连，多个线程操作的话，是不安全的。如果要避免不安全，使用jedis pool连接池！更像BIO模式</p>
<p>lettuce：采用netty，实例可以在多个线程中共享，不存在线程不安全的情况！可以减少线程数据了，更像NIO模式</p>
</blockquote>
<p>我们在学习SpringBoot自动配置的原理时，整合一个组件并进行配置一定会有一个自动配置类xxxAutoConfiguration,并且在spring.factories中也一定能找到这个类的完全限定名。Redis也不例外。</p>
<figure data-type="image" tabindex="20"><img src="https://img-blog.csdnimg.cn/20200513214531573.png" alt="在这里插入图片描述" loading="lazy"></figure>
<p>那么就一定还存在一个RedisProperties类</p>
<figure data-type="image" tabindex="21"><img src="https://img-blog.csdnimg.cn/20200513214554661.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>之前我们说SpringBoot2.x后默认使用Lettuce来替换Jedis，现在我们就能来验证了。</p>
<p>先看Jedis:</p>
<figure data-type="image" tabindex="22"><img src="https://img-blog.csdnimg.cn/20200513214607475.png" alt="在这里插入图片描述" loading="lazy"></figure>
<p>@ConditionalOnClass注解中有两个类是默认不存在的，所以Jedis是无法生效的</p>
<p>然后再看Lettuce：</p>
<figure data-type="image" tabindex="23"><img src="https://img-blog.csdnimg.cn/20200513214618179.png" alt="在这里插入图片描述" loading="lazy"></figure>
<p>完美生效。</p>
<p>现在我们回到RedisAutoConfiguratio</p>
<figure data-type="image" tabindex="24"><img src="https://img-blog.csdnimg.cn/2020051321462777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<p>只有两个简单的Bean</p>
<ul>
<li><strong>RedisTemplate</strong></li>
<li><strong>StringRedisTemplate</strong></li>
</ul>
<p>当看到xxTemplate时可以对比RestTemplat、SqlSessionTemplate,通过使用这些Template来间接操作组件。那么这俩也不会例外。分别用于操作Redis和Redis中的String数据类型。</p>
<p>在RedisTemplate上也有一个条件注解，说明我们是可以对其进行定制化的</p>
<p>说完这些，我们需要知道如何编写配置文件然后连接Redis，就需要阅读RedisProperties</p>
<figure data-type="image" tabindex="25"><img src="https://img-blog.csdnimg.cn/20200513214638238.png" alt="在这里插入图片描述" loading="lazy"></figure>
<p>这是一些基本的配置属性。</p>
<figure data-type="image" tabindex="26"><img src="https://img-blog.csdnimg.cn/20200513214649380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>还有一些连接池相关的配置。注意使用时一定使用Lettuce的连接池。</p>
<figure data-type="image" tabindex="27"><img src="https://img-blog.csdnimg.cn/20200513214700372.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<ol>
<li>
<p>编写配置文件</p>
<pre><code class="language-properties"># 配置redis
spring.redis.host=39.99.xxx.xx
spring.redis.port=6379
</code></pre>
</li>
<li>
<p>使用RedisTemplate</p>
<pre><code class="language-java">@SpringBootTest
class Redis02SpringbootApplicationTests {

    @Autowired
    private RedisTemplate redisTemplate;

    @Test
    void contextLoads() {

        // redisTemplate 操作不同的数据类型，api和我们的指令是一样的
        // opsForValue 操作字符串 类似String
        // opsForList 操作List 类似List
        // opsForHah

        // 除了基本的操作，我们常用的方法都可以直接通过redisTemplate操作，比如事务和基本的CRUD

        // 获取连接对象
        //RedisConnection connection = redisTemplate.getConnectionFactory().getConnection();
        //connection.flushDb();
        //connection.flushAll();

        redisTemplate.opsForValue().set(&quot;mykey&quot;,&quot;kuangshen&quot;);
        System.out.println(redisTemplate.opsForValue().get(&quot;mykey&quot;));
    }
}
</code></pre>
</li>
<li>
<p>测试结果</p>
<p><strong>此时我们回到Redis查看数据时候，惊奇发现全是乱码，可是程序中可以正常输出：</strong></p>
<figure data-type="image" tabindex="28"><img src="https://img-blog.csdnimg.cn/20200513214734520.png" alt="在这里插入图片描述" loading="lazy"></figure>
<p>这时候就关系到存储对象的序列化问题，在网络中传输的对象也是一样需要序列化，否者就全是乱码。</p>
<p>我们转到看那个默认的RedisTemplate内部什么样子：</p>
<figure data-type="image" tabindex="29"><img src="https://img-blog.csdnimg.cn/20200513214746506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>在最开始就能看到几个关于序列化的参数。</p>
<p>默认的序列化器是采用JDK序列化器</p>
<figure data-type="image" tabindex="30"><img src="https://img-blog.csdnimg.cn/20200513214757247.png" alt="在这里插入图片描述" loading="lazy"></figure>
<p>而默认的RedisTemplate中的所有序列化器都是使用这个序列化器：</p>
<figure data-type="image" tabindex="31"><img src="https://img-blog.csdnimg.cn/20200513214809494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>后续我们定制RedisTemplate就可以对其进行修改。</p>
<p><code>RedisSerializer</code>提供了多种序列化方案：</p>
<ul>
<li>
<p>直接调用RedisSerializer的静态方法来返回序列化器，然后set</p>
<figure data-type="image" tabindex="32"><img src="https://img-blog.csdnimg.cn/20200513214818682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
</li>
<li>
<p>自己new 相应的实现类，然后set</p>
<figure data-type="image" tabindex="33"><img src="https://img-blog.csdnimg.cn/20200513214827233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
</li>
</ul>
</li>
<li>
<p><strong>定制RedisTemplate的模板：</strong></p>
<p>我们创建一个Bean加入容器，就会触发RedisTemplate上的条件注解使默认的RedisTemplate失效。</p>
<pre><code class="language-java">@Configuration
public class RedisConfig {

   @Bean
    public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException {
        // 将template 泛型设置为 &lt;String, Object&gt;
        RedisTemplate&lt;String, Object&gt; template = new RedisTemplate();
        // 连接工厂，不必修改
        template.setConnectionFactory(redisConnectionFactory);
        /*
         * 序列化设置
         */
        // key、hash的key 采用 String序列化方式
        template.setKeySerializer(RedisSerializer.string());
        template.setHashKeySerializer(RedisSerializer.string());
        // value、hash的value 采用 Jackson 序列化方式
        template.setValueSerializer(RedisSerializer.json());
        template.setHashValueSerializer(RedisSerializer.json());
        template.afterPropertiesSet();
        
        return template;
    }
}
</code></pre>
<p>这样一来，只要实体类进行了序列化，我们存什么都不会有乱码的担忧了。</p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-oc8kJP08-1597890996523)(狂神说 Redis.assets/image-20200817175638086.png)]</p>
</li>
</ol>
<h2 id="八-自定义redis工具类">八、自定义Redis工具类</h2>
<p>使用RedisTemplate需要频繁调用<code>.opForxxx</code>然后才能进行对应的操作，这样使用起来代码效率低下，工作中一般不会这样使用，而是将这些常用的公共API抽取出来封装成为一个工具类，然后直接使用工具类来间接操作Redis,不但效率高并且易用。</p>
<p>工具类参考博客：</p>
<p>https://www.cnblogs.com/zeng1994/p/03303c805731afc9aa9c60dbbd32a323.html</p>
<p>https://www.cnblogs.com/zhzhlong/p/11434284.html</p>
<h2 id="九-redisconf">九、Redis.conf</h2>
<blockquote>
<p>容量单位不区分大小写，G和GB有区别</p>
</blockquote>
<figure data-type="image" tabindex="34"><img src="https://img-blog.csdnimg.cn/2020051321485460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<blockquote>
<p>可以使用 include 组合多个配置问题</p>
</blockquote>
<figure data-type="image" tabindex="35"><img src="https://img-blog.csdnimg.cn/20200513214902552.png" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>网络配置</p>
</blockquote>
<figure data-type="image" tabindex="36"><img src="https://img-blog.csdnimg.cn/20200513214912813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>日志输出级别</p>
</blockquote>
<figure data-type="image" tabindex="37"><img src="https://img-blog.csdnimg.cn/20200513214923678.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<blockquote>
<p>日志输出文件</p>
</blockquote>
<figure data-type="image" tabindex="38"><img src="https://img-blog.csdnimg.cn/20200513214933713.png" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>持久化规则</p>
</blockquote>
<p>由于Redis是基于内存的数据库，需要将数据由内存持久化到文件中</p>
<p>持久化方式：</p>
<ul>
<li>RDB</li>
<li>AOF</li>
</ul>
<figure data-type="image" tabindex="39"><img src="https://img-blog.csdnimg.cn/20200513214944964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>RDB文件相关</p>
</blockquote>
<figure data-type="image" tabindex="40"><img src="https://img-blog.csdnimg.cn/20200513214955679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<figure data-type="image" tabindex="41"><img src="https://img-blog.csdnimg.cn/20200513215006207.png" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>主从复制</p>
</blockquote>
<figure data-type="image" tabindex="42"><img src="https://img-blog.csdnimg.cn/20200513215016371.png" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>Security模块中进行密码设置</p>
</blockquote>
<figure data-type="image" tabindex="43"><img src="https://img-blog.csdnimg.cn/20200513215026143.png" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>客户端连接相关</p>
</blockquote>
<pre><code class="language-bash">maxclients 10000  最大客户端数量
maxmemory &lt;bytes&gt; 最大内存限制
maxmemory-policy noeviction # 内存达到限制值的处理策略
</code></pre>
<p>redis 中的<strong>默认</strong>的过期策略是 <strong>volatile-lru</strong> 。</p>
<p><strong>设置方式</strong></p>
<pre><code class="language-bash">config set maxmemory-policy volatile-lru 
1
</code></pre>
<h4 id="过期键的删除策略3种">过期键的删除策略（3种）</h4>
<p>Redis的过期策略就是指当Redis中缓存的key过期了，Redis如何处理。</p>
<ul>
<li>
<p>定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而<strong>影响缓存的响应时间和吞吐量</strong>。</p>
</li>
<li>
<p>惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，<strong>占用大量内存</strong>。</p>
</li>
<li>
<p>定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个<strong>折中方案</strong>。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。<br>
(expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)</p>
<p>Redis中同时使用了惰性过期和定期过期两种过期策略。</p>
</li>
</ul>
<h4 id="maxmemory-policy-内存淘汰策略六种方式"><strong>maxmemory-policy 内存淘汰策略六种方式</strong></h4>
<p>MySQL里有2000w数据，redis中只存20w的数据，保证redis中的数据都是热点数据。</p>
<p>Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。</p>
<p>**volatile-lru：**只对设置了过期时间的key进行LRU（默认值）</p>
<p>**volatile-random：**随机删除即将过期key</p>
<p><strong>volatile-ttl ：</strong> 有更早过期时间的key优先移除</p>
<p><strong>allkeys-lru ：</strong> 删除lru算法的key</p>
<p>**allkeys-random：**随机删除</p>
<p><strong>noeviction ：</strong> 永不过期，返回错误</p>
<blockquote>
<p>AOF相关部分</p>
</blockquote>
<figure data-type="image" tabindex="44"><img src="https://img-blog.csdnimg.cn/20200513215037918.png" alt="在这里插入图片描述" loading="lazy"></figure>
<figure data-type="image" tabindex="45"><img src="https://img-blog.csdnimg.cn/20200513215047999.png" alt="在这里插入图片描述" loading="lazy"></figure>
<h2 id="十-持久化rdb">十、持久化—RDB</h2>
<p>RDB：Redis Databases</p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-C0mm1D4A-1597890996524)(狂神说 Redis.assets/image-20200818122236614.png)]</p>
<h3 id="什么是rdb">什么是RDB</h3>
<hr>
<p>在指定时间间隔后，将内存中的数据集快照写入数据库 ；在恢复时候，直接读取快照文件，进行数据的恢复 ；</p>
<figure data-type="image" tabindex="46"><img src="https://img-blog.csdnimg.cn/20200513215126515.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>默认情况下， Redis 将数据库快照保存在名字为 dump.rdb的二进制文件中。文件名可以在配置文件中进行自定义。</p>
<h3 id="工作原理">工作原理</h3>
<hr>
<p>在进行 <strong><code>RDB</code></strong> 的时候，<strong><code>redis</code></strong> 的主线程是不会做 <strong><code>io</code></strong> 操作的，主线程会 <strong><code>fork</code></strong> 一个子线程来完成该操作；</p>
<ol>
<li>Redis 调用forks。同时拥有父进程和子进程。</li>
<li>子进程将数据集写入到一个临时 RDB 文件中。</li>
<li>当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。</li>
</ol>
<p>这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益(因为是使用子进程进行写操作，而父进程依然可以接收来自客户端的请求。)</p>
<figure data-type="image" tabindex="47"><img src="https://img-blog.csdnimg.cn/20200513215141519.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="触发机制">触发机制</h3>
<hr>
<ol>
<li>save的规则满足的情况下，会自动触发rdb原则</li>
<li>执行flushall命令，也会触发我们的rdb原则</li>
<li>退出redis，也会自动产生rdb文件</li>
</ol>
<h4 id="save">save</h4>
<p>使用 <code>save</code> 命令，会立刻对当前内存中的数据进行持久化 ,但是会阻塞，也就是不接受其他操作了；</p>
<blockquote>
<p>由于 <code>save</code> 命令是同步命令，会占用Redis的主进程。若Redis数据非常多时，<code>save</code>命令执行速度会非常慢，阻塞所有客户端的请求。</p>
</blockquote>
<figure data-type="image" tabindex="48"><img src="https://img-blog.csdnimg.cn/20200513215150892.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h4 id="flushall命令">flushall命令</h4>
<p><code>flushall</code> 命令也   会触发持久化 ；</p>
<h4 id="触发持久化规则">触发持久化规则</h4>
<p>满足配置条件中的触发条件 ；</p>
<blockquote>
<p>可以通过配置文件对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动进行数据集保存操作。</p>
<figure data-type="image" tabindex="49"><img src="https://img-blog.csdnimg.cn/20200513215205970.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
</blockquote>
<figure data-type="image" tabindex="50"><img src="https://img-blog.csdnimg.cn/20200513215220858.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h4 id="bgsave">bgsave</h4>
<p><code>bgsave</code> 是异步进行，进行持久化的时候，<code>redis</code> 还可以将继续响应客户端请求 ；</p>
<figure data-type="image" tabindex="51"><img src="https://img-blog.csdnimg.cn/2020051321523151.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p><strong>bgsave和save对比</strong></p>
<table>
<thead>
<tr>
<th>命令</th>
<th>save</th>
<th>bgsave</th>
</tr>
</thead>
<tbody>
<tr>
<td>IO类型</td>
<td><mark>同步</mark></td>
<td><mark>异步</mark></td>
</tr>
<tr>
<td>阻塞？</td>
<td>是</td>
<td>是（阻塞发生在fock()，通常非常快）</td>
</tr>
<tr>
<td>复杂度</td>
<td>O(n)</td>
<td>O(n)</td>
</tr>
<tr>
<td>优点</td>
<td>不会消耗额外的内存</td>
<td>不阻塞客户端命令</td>
</tr>
<tr>
<td>缺点</td>
<td>阻塞客户端命令</td>
<td>需要fock子进程，消耗内存</td>
</tr>
</tbody>
</table>
<h3 id="优缺点">优缺点</h3>
<p><strong>优点：</strong></p>
<ol>
<li>适合大规模的数据恢复</li>
<li>对数据的完整性要求不高</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>需要一定的时间间隔进行操作，如果redis意外宕机了，这个最后一次修改的数据就没有了。</li>
<li>fork进程的时候，会占用一定的内容空间。</li>
</ol>
<h2 id="十一-持久化aof">十一、持久化AOF</h2>
<p><strong>Append Only File</strong></p>
<p>将我们所有的命令都记录下来，history，恢复的时候就把这个文件全部再执行一遍</p>
<figure data-type="image" tabindex="52"><img src="https://memorykki.github.io/post-images/redis/image-20210831213900233.png" alt="" loading="lazy"></figure>
<blockquote>
<p>以日志的形式来记录每个写的操作，将Redis执行过的所有指令记录下来（读操作不记录），只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。</p>
</blockquote>
<h3 id="什么是aof">什么是AOF</h3>
<figure data-type="image" tabindex="53"><img src="https://img2020.cnblogs.com/blog/2000608/202004/2000608-20200421153007546-585324670.png" alt="img" loading="lazy"></figure>
<p>快照功能（RDB）并不是非常耐久（durable)： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、以及未保存到快照中的那些数据。 从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化。</p>
<p>如果要使用AOF，需要修改配置文件：</p>
<figure data-type="image" tabindex="54"><img src="https://img-blog.csdnimg.cn/20200513215247113.png" alt="在这里插入图片描述" loading="lazy"></figure>
<p><code>appendonly no yes</code>则表示启用AOF</p>
<p>默认是不开启的，我们需要手动配置，然后重启redis，就可以生效了！</p>
<p>如果这个aof文件有错位，这时候redis是启动不起来的，我需要修改这个aof文件</p>
<p>redis给我们提供了一个工具<code>redis-check-aof --fix</code></p>
<blockquote>
<p>优点和缺点</p>
</blockquote>
<pre><code class="language-bash">appendonly yes  # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分的情况下，rdb完全够用
appendfilename &quot;appendonly.aof&quot;

# 判断是否需要将 AOF 缓存区中的内容写入和同步到 AOF 文件中
# appendfsync always # 每次修改都会sync 消耗性能
appendfsync everysec # 每秒执行一次 sync 可能会丢失这一秒的数据
# appendfsync no # 不执行 sync ,这时候操作系统自己同步数据，速度最快,Redis 在每一个事件循环都要将 AOF 缓冲区中的所有内容写入到 AOF 文件。而 AOF 文件的同步由操作系统控制。这种模式下速度最快，但是同步的时间间隔较长，出现故障时可能会丢失较多数据。 
</code></pre>
<p><strong><img src="https://img2020.cnblogs.com/blog/2000608/202004/2000608-20200421154006686-2034814519.png" alt="img" loading="lazy">优点</strong></p>
<ol>
<li>每一次修改都会同步，文件的完整性会更加好</li>
<li>没秒同步一次，可能会丢失一秒的数据</li>
<li>从不同步，效率最高</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>相对于数据文件来说，aof远远大于rdb，修复速度比rdb慢！</li>
<li>Aof运行效率也要比rdb慢，所以我们redis默认的配置就是rdb持久化</li>
</ol>
<h2 id="重写">重写</h2>
<h4 id="41重写机制介绍"><strong>4.1重写机制介绍</strong></h4>
<p>AOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了<strong>重写机制,</strong> 当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof</p>
<h4 id="42重写的原理"><strong>4.2重写的原理</strong></h4>
<p><strong>AOF文件持续增长而过大时，<strong>会fork出一条新进程来将文件重写</strong>(也是先写临时文件最后再rename)，<strong>遍历新进程的内存中数据，每条记录有一条的Set语句。重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写</strong>了一个新的aof文件，这点和快照有点类似</strong></p>
<h4 id="43重写的三种触发机制"><strong>4.3重写的三种触发机制</strong></h4>
<p>​     1.<strong>手动调用 bgrewriteaof 命令</strong>，如果当前有正在运行的 rewrite 子进程，则本次rewrite 会推迟执行，否则，直接触发一次 rewrite。</p>
<p>​     2.<strong>通过配置指令手动开启 AOF 功能</strong>，如果没有 RDB 子进程的情况下，会触发一次 rewrite，<strong>将当前数据库中的数据写入 rewrite 文件。</strong></p>
<p>​     3.在 Redis 定时器中，如果有需要退出执行的 rewrite 并且没有正在运行的 RDB 或者 rewrite 子进程时，触发一次或者 AOF 文件大小已经到达配置的 rewrite 条件也会自动触发一次。</p>
<p>​    4.<strong>Redis会记录上次重写时的AOF大小</strong>，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M是也会触发</p>
<h2 id="十二-rdb和aop选择">十二、RDB和AOP选择</h2>
<h3 id="rdb-和-aof-对比">RDB 和 AOF 对比</h3>
<table>
<thead>
<tr>
<th></th>
<th>RDB</th>
<th><strong>AOF</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>启动优先级</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>体积</td>
<td>小</td>
<td>大 -&gt; 重写</td>
</tr>
<tr>
<td>恢复速度</td>
<td>快</td>
<td>慢</td>
</tr>
<tr>
<td>数据安全性</td>
<td>丢数据</td>
<td>根据策略决定</td>
</tr>
<tr>
<td></td>
<td></td>
<td>更加完整，持续的IO</td>
</tr>
</tbody>
</table>
<h3 id="如何选择使用哪种持久化方式">如何选择使用哪种持久化方式？</h3>
<p>一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。</p>
<p>如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。</p>
<p>有很多用户都只使用 AOF 持久化， 但并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快。</p>
<h2 id="十三-redis发布与订阅">十三、Redis发布与订阅</h2>
<p>Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。</p>
<figure data-type="image" tabindex="55"><img src="https://memorykki.github.io/post-images/redis/image-20210831220653321.png" alt="" loading="lazy"></figure>
<p>下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系：</p>
<figure data-type="image" tabindex="56"><img src="https://img-blog.csdnimg.cn/20200513215523258.png" alt="在这里插入图片描述" loading="lazy"></figure>
<p>当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客 户端：</p>
<figure data-type="image" tabindex="57"><img src="https://img-blog.csdnimg.cn/2020051321553483.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="命令">命令</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>PSUBSCRIBE pattern [pattern..]</code></td>
<td>订阅一个或多个符合给定模式的频道。</td>
</tr>
<tr>
<td><code>PUNSUBSCRIBE pattern [pattern..]</code></td>
<td>退订一个或多个符合给定模式的频道。</td>
</tr>
<tr>
<td><code>PUBSUB subcommand [argument[argument]]</code></td>
<td>查看订阅与发布系统状态。</td>
</tr>
<tr>
<td><mark><code>PUBLISH channel message</code></mark></td>
<td>向指定频道发布消息</td>
</tr>
<tr>
<td><mark><code>SUBSCRIBE channel [channel..]</code></mark></td>
<td>订阅给定的一个或多个频道。</td>
</tr>
<tr>
<td><code>SUBSCRIBE channel [channel..]</code></td>
<td>退订一个或多个频道</td>
</tr>
</tbody>
</table>
<h3 id="示例">示例</h3>
<pre><code class="language-bash">------------订阅端----------------------
127.0.0.1:6379&gt; SUBSCRIBE sakura # 订阅sakura频道
Reading messages... (press Ctrl-C to quit) # 等待接收消息
1) &quot;subscribe&quot; # 订阅成功的消息
2) &quot;sakura&quot;
3) (integer) 1
1) &quot;message&quot; # 接收到来自sakura频道的消息 &quot;hello world&quot;
2) &quot;sakura&quot;
3) &quot;hello world&quot;
1) &quot;message&quot; # 接收到来自sakura频道的消息 &quot;hello i am sakura&quot;
2) &quot;sakura&quot;
3) &quot;hello i am sakura&quot;

--------------消息发布端-------------------
127.0.0.1:6379&gt; PUBLISH sakura &quot;hello world&quot; # 发布消息到sakura频道
(integer) 1
127.0.0.1:6379&gt; PUBLISH sakura &quot;hello i am sakura&quot; # 发布消息
(integer) 1

-----------------查看活跃的频道------------
127.0.0.1:6379&gt; PUBSUB channels
1) &quot;sakura&quot;
</code></pre>
<h3 id="原理">原理</h3>
<p>每个 Redis 服务器进程都维持着一个表示服务器状态的 redis.h/redisServer 结构， 结构的 pubsub_channels 属性是一个<mark>字典</mark>， 这个字典就用于保存订阅频道的信息，其中，字典的键为正在被订阅的频道， 而字典的值则是一个<mark>链表</mark>， 链表中保存了所有订阅这个频道的客户端。</p>
<figure data-type="image" tabindex="58"><img src="https://img-blog.csdnimg.cn/2020051321554964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>客户端订阅，就被链接到对应频道的链表的尾部，退订则就是将客户端节点从链表中移除。</p>
<h3 id="缺点">缺点</h3>
<ol>
<li>如果一个客户端订阅了频道，但自己读取消息的速度却不够快的话，那么<mark>不断积压的消息</mark>会使redis输出缓冲区的体积变得越来越大，这可能使得redis本身的速度变慢，甚至直接崩溃。</li>
<li>这和数据传输可靠性有关，如果在<mark>订阅方断线</mark>，那么他将会丢失所有在断线期间发布者发布的消息。</li>
</ol>
<h3 id="应用">应用</h3>
<ol>
<li>消息订阅：公众号订阅，微博关注等等（起始更多是使用消息队列来进行实现）</li>
<li>多人在线聊天室。</li>
</ol>
<p>稍微复杂的场景，我们就会使用消息中间件MQ处理。</p>
<h2 id="十四-redis主从复制">十四、Redis主从复制</h2>
<h3 id="概念">概念</h3>
<p>主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点（Master/Leader）,后者称为从节点（Slave/Follower）， 数据的复制是单向的！只能由主节点复制到从节点（主节点以写为主、从节点以读为主）。</p>
<p>默认情况下，每台Redis服务器都是主节点，一个主节点可以有0个或者多个从节点，但每个从节点只能由一个主节点。</p>
<h3 id="作用">作用</h3>
<ol>
<li>数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余的方式。</li>
<li>故障恢复：当主节点故障时，从节点可以暂时替代主节点提供服务，是一种服务冗余的方式</li>
<li>负载均衡：在主从复制的基础上，配合读写分离，由主节点进行写操作，从节点进行读操作，分担服务器的负载；尤其是在多读少写的场景下，通过多个从节点分担负载，提高并发量。</li>
<li>高可用基石：主从复制还是哨兵和集群能够实施的基础。</li>
</ol>
<h3 id="为什么使用集群">为什么使用集群</h3>
<ol>
<li>单台服务器难以负载大量的请求</li>
<li>单台服务器故障率高，系统崩坏概率大</li>
<li>单台服务器内存容量有限。</li>
</ol>
<h3 id="环境配置">环境配置</h3>
<p>我们在讲解配置文件的时候，注意到有一个<code>replication</code>模块 (见Redis.conf中第8条)</p>
<p>查看当前库的信息：<code>info replication</code></p>
<pre><code class="language-bash">127.0.0.1:6379&gt; info replication
# Replication
role:master # 角色
connected_slaves:0 # 从机数量
master_replid:3b54deef5b7b7b7f7dd8acefa23be48879b4fcff
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:0
second_repl_offset:-1
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
</code></pre>
<p>既然需要启动多个服务，就需要多个配置文件。每个配置文件对应修改以下信息：</p>
<ul>
<li>端口号</li>
<li>pid文件名</li>
<li>日志文件名</li>
<li>rdb文件名</li>
</ul>
<p>启动单机多服务集群：</p>
<figure data-type="image" tabindex="59"><img src="https://img-blog.csdnimg.cn/20200513215610163.png" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="一主二从配置">一主二从配置</h3>
<p>==默认情况下，每台Redis服务器都是主节点；==我们一般情况下只用配置从机就好了！</p>
<p>认老大！一主（79）二从（80，81）</p>
<p>使用<code>SLAVEOF host port</code>就可以为从机配置主机了。</p>
<figure data-type="image" tabindex="60"><img src="https://img-blog.csdnimg.cn/20200513215637483.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>然后主机上也能看到从机的状态：</p>
<figure data-type="image" tabindex="61"><img src="https://img-blog.csdnimg.cn/20200513215645778.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>我们这里是使用命令搭建，是暂时的，==真实开发中应该在从机的配置文件中进行配置，==这样的话是永久的。</p>
<figure data-type="image" tabindex="62"><img src="https://img-blog.csdnimg.cn/20200513215654634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="使用规则">使用规则</h3>
<ol>
<li>
<p>从机只能读，不能写，主机可读可写但是多用于写。</p>
<pre><code class="language-bash"> 127.0.0.1:6381&gt; set name sakura # 从机6381写入失败
(error) READONLY You can't write against a read only replica.

127.0.0.1:6380&gt; set name sakura # 从机6380写入失败
(error) READONLY You can't write against a read only replica.

127.0.0.1:6379&gt; set name sakura
OK
127.0.0.1:6379&gt; get name
&quot;sakura&quot;
</code></pre>
</li>
<li>
<p>当<mark>主机断电宕机</mark>后，默认情况下从机的角色不会发生变化 ，集群中只是失去了写操作，当主机恢复以后，又会连接上从机恢复原状。</p>
</li>
<li>
<p>当<mark>从机断电宕机</mark>后，若不是使用配置文件配置的从机，再次启动后作为主机是无法获取之前主机的数据的，若此时重新配置称为从机，又可以获取到主机的所有数据。这里就要提到一个同步原理。</p>
</li>
</ol>
<figure data-type="image" tabindex="63"><img src="https://memorykki.github.io/post-images/redis/image-20210901084036049.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>第二条中提到，默认情况下，主机故障后，不会出现新的主机，有两种方式可以产生新的主机：</p>
<ul>
<li>从机手动执行命令<code>slaveof no one</code>,这样执行以后从机会独立出来成为一个主机</li>
<li>使用哨兵模式（自动选举）</li>
</ul>
</li>
</ol>
<blockquote>
<p>如果没有老大了，这个时候能不能选择出来一个老大呢？手动！</p>
</blockquote>
<p>如果主机断开了连接，我们可以使用<code>SLAVEOF no one</code>让自己变成主机！其他的节点就可以手动连接到最新的主节点（手动）！如果这个时候老大修复了，那么久重新连接！</p>
<h2 id="十五-哨兵模式">十五、哨兵模式</h2>
<p>更多信息参考博客：https://www.jianshu.com/p/06ab9daf921d</p>
<p>集群监控：负责监控 redis master 和 slave 进程是否正常工作。<br>
消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。<br>
故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。<br>
配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。</p>
<ul>
<li>哨兵至少需要 3 个实例，来保证自己的健壮性。</li>
<li>哨兵 + redis 主从的部署架构，是<strong>不保证数据零丢失</strong>的，只能保证 redis 集群的高可用性。</li>
<li>对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。</li>
</ul>
<hr>
<p><strong>主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。<strong>这不是一种推荐的方式，更多时候，我们优先考虑</strong>哨兵模式</strong>。</p>
<p>单机单个哨兵</p>
<p>哨兵的作用：</p>
<ul>
<li>通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。</li>
<li>当哨兵监测到master宕机，会自动将slave切换成master，然后通过<strong>发布订阅模式</strong>通知其他的从服务器，修改配置文件，让它们切换主机。</li>
</ul>
<p>多哨兵模式</p>
<p>哨兵的核心配置</p>
<pre><code>sentinel monitor mymaster 127.0.0.1 6379 1
</code></pre>
<ul>
<li>数字1表示 ：当一个哨兵主观认为主机断开，就可以客观认为主机故障，然后开始选举新的主机。</li>
</ul>
<blockquote>
<p>测试</p>
</blockquote>
<pre><code>redis-sentinel xxx/sentinel.conf
</code></pre>
<p>成功启动哨兵模式</p>
<figure data-type="image" tabindex="64"><img src="https://img-blog.csdnimg.cn/20200513215752444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>此时哨兵监视着我们的主机6379，当我们断开主机后：</p>
<figure data-type="image" tabindex="65"><img src="https://img-blog.csdnimg.cn/20200513215806972.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>哨兵模式优缺点</p>
</blockquote>
<p><strong>优点：</strong></p>
<ol>
<li>哨兵集群，基于主从复制模式，所有主从复制的优点，它都有</li>
<li>主从可以切换，故障可以转移，系统的可用性更好</li>
<li>哨兵模式是主从模式的升级，手动到自动，更加健壮</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>Redis不好在线扩容，集群容量一旦达到上限，在线扩容就十分麻烦</li>
<li>实现哨兵模式的配置其实是很麻烦的，里面有很多配置项</li>
</ol>
<blockquote>
<p>哨兵模式的全部配置</p>
</blockquote>
<p>完整的哨兵模式配置文件 sentinel.conf</p>
<pre><code class="language-bash"># Example sentinel.conf
 
# 哨兵sentinel实例运行的端口 默认26379
port 26379
 
# 哨兵sentinel的工作目录
dir /tmp
 
# 哨兵sentinel监控的redis主节点的 ip port 
# master-name  可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成。
# quorum 当这些quorum个数sentinel哨兵认为master主节点失联 那么这时 客观上认为主节点失联了
# sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;
sentinel monitor mymaster 127.0.0.1 6379 1
 
# 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码
# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码
# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;
sentinel auth-pass mymaster MySUPER--secret-0123passw0rd
 
 
# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒
# sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;
sentinel down-after-milliseconds mymaster 30000
 
# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步，
这个数字越小，完成failover所需的时间就越长，
但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。
可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。
# sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt;
sentinel parallel-syncs mymaster 1
 
 
 
# 故障转移的超时时间 failover-timeout 可以用在以下这些方面： 
#1. 同一个sentinel对同一个master两次failover之间的间隔时间。
#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。
#3.当想要取消一个正在进行的failover所需要的时间。  
#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了
# 默认三分钟
# sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;
sentinel failover-timeout mymaster 180000
 
# SCRIPTS EXECUTION
 
#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。
#对于脚本的运行结果有以下规则：
#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10
#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。
#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。
#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。
 
#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本，
#这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，
#一个是事件的类型，
#一个是事件的描述。
#如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。
#通知脚本
# sentinel notification-script &lt;master-name&gt; &lt;script-path&gt;
  sentinel notification-script mymaster /var/redis/notify.sh
 
# 客户端重新配置主节点参数脚本
# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。
# 以下参数将会在调用脚本时传给脚本:
# &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt;
# 目前&lt;state&gt;总是“failover”,
# &lt;role&gt;是“leader”或者“observer”中的一个。 
# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的
# 这个脚本应该是通用的，能被多次调用，不是针对性的。
# sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt;
sentinel client-reconfig-script mymaster /var/redis/reconfig.sh
</code></pre>
<h2 id="十六-缓存穿透与雪崩">十六、缓存穿透与雪崩</h2>
<h3 id="缓存穿透查不到根本没有这个key">缓存穿透（查不到，根本没有这个key）</h3>
<blockquote>
<p>概念</p>
</blockquote>
<p>在默认情况下，用户请求数据时，会先在缓存(Redis)中查找，若没找到即缓存未命中，再在数据库中进行查找，数量少可能问题不大，可是一旦大量的请求数据（例如秒杀场景）缓存都没有命中的话，就会全部转移到数据库上，造成数据库极大的压力，就有可能导致数据库崩溃。网络安全中也有人恶意使用这种手段进行攻击被称为洪水攻击。</p>
<blockquote>
<p>解决方案</p>
</blockquote>
<p><strong>布隆过滤器</strong></p>
<p>对所有可能查询的参数以Hash的形式存储，以便快速确定是否存在这个值，在控制层先进行拦截校验，校验不通过直接打回，减轻了存储系统的压力。</p>
<img src="https://img-blog.csdnimg.cn/20200513215824722.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:67%;" />
<p>对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。<br>
Bitmap： 典型的就是<strong>哈希表</strong><br>
缺点是，Bitmap对于<strong>每个元素只能记录1bit信息</strong>，如果还想完成额外的功能，恐怕只能靠牺牲更多的空间、时间来完成了。</p>
<p>布隆过滤器（推荐）</p>
<p>就是引入了k(k&gt;1)k(k&gt;1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程。<br>
它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。<br>
Bloom-Filter算法的核心思想就是<strong>利用多个不同的Hash函数来解决“冲突”</strong>。<br>
Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突，我们可以多引入几个Hash，**如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。**这便是Bloom-Filter的基本思想。<br>
Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。</p>
<p><strong>缓存空对象</strong></p>
<p>一次请求若在缓存和数据库中都没找到，就在缓存中方一个空对象用于处理后续这个请求。</p>
<img src="https://img-blog.csdnimg.cn/20200513215836317.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />
<p>这样做有一个缺陷：存储空对象也需要空间，大量的空对象会耗费一定的空间，存储效率并不高。解决这个缺陷的方式就是设置较短过期时间</p>
<ul>
<li>接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截；</li>
</ul>
<p>即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。</p>
<h3 id="缓存击穿一条数据过期">缓存击穿（一条数据过期）</h3>
<blockquote>
<p>概念</p>
</blockquote>
<p>相较于缓存穿透，缓存击穿的目的性更强，一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。这就是缓存被击穿，只是针对其中某个key的缓存不可用而导致击穿，但是其他的key依然可以使用缓存响应。</p>
<p>比如热搜排行上，一个热点新闻被同时大量访问就可能导致缓存击穿。</p>
<blockquote>
<p>解决方案</p>
</blockquote>
<ol>
<li>
<p><strong>设置热点数据永不过期</strong></p>
<p>这样就不会出现热点数据过期的情况，但是当Redis内存空间满的时候也会清理部分数据，而且此种方案会占用空间，一旦热点数据多了起来，就会占用部分空间。</p>
</li>
<li>
<p><strong>加互斥锁(分布式锁)</strong></p>
<p>在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。保证同时刻只有一个线程访问。这样对锁的要求就十分高。</p>
</li>
</ol>
<h3 id="缓存雪崩大量数据过期">缓存雪崩（大量数据过期）</h3>
<blockquote>
<p>概念</p>
</blockquote>
<p>大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。</p>
<figure data-type="image" tabindex="66"><img src="https://img-blog.csdnimg.cn/20200513215850428.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg3MzIyNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>解决方案</p>
</blockquote>
<ul>
<li>
<p>redis高可用 - <strong>集群</strong></p>
<p>这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续工作，其实就是搭建的集群</p>
</li>
<li>
<p>限流降级 - <strong>加锁</strong></p>
<p>这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。</p>
</li>
<li>
<p>数据预热</p>
<p>数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同（随机）的过期时间，让缓存失效的时间点尽量均匀。</p>
</li>
<li>
<p>失效标记更新</p>
<p>给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。</p>
</li>
</ul>
<h2 id="集群方案">集群方案</h2>
<h3 id="1-哨兵模式">1、哨兵模式</h3>
<h3 id="2-redis-cluster服务端路由">2、Redis Cluster（服务端路由）</h3>
<figure data-type="image" tabindex="67"><img src="https://img-blog.csdnimg.cn/20200115173621637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua3dvbi5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<p>Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，<strong>接收到请求的节点会将查询请求发送到正确的节点上执行</strong>。</p>
<h4 id="方案说明">方案说明</h4>
<ul>
<li>通过哈希的方式，将数据分片，每个节点均分存储一定<strong>哈希槽</strong>(哈希值)区间的数据，默认分配了16384 个槽位</li>
<li>每份<strong>数据分片会存储在多个互为主从的多节点</strong>上，数据写入先写主节点，再同步到从节点(支持配置为阻塞同步)，同一分片多个节点间的数据不保持一致性</li>
<li>读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点</li>
<li>扩容时时需要需要把旧节点的数据迁移一部分到新节点</li>
<li>在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。</li>
</ul>
<p>16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 闲聊协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。</p>
<h4 id="节点间的内部通信机制">节点间的内部通信机制</h4>
<p>集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。</p>
<h3 id="3-redis-sharding客户端分配">3、Redis Sharding（客户端分配）</h3>
<figure data-type="image" tabindex="68"><img src="https://img-blog.csdnimg.cn/20200115173640248.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua3dvbi5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<p>Redis Sharding是Redis Cluster出来之前，业界普遍使用的多Redis实例集群方法。其主要思想是<strong>采用哈希算法将key进行散列</strong>，通过hash函数，<strong>特定的key会映射到特定的Redis节点</strong>上。</p>
<p><strong>优点</strong></p>
<p>优势在于非常简单，服务端的Redis实例彼此独立，相互无关联，每个Redis实例像单服务器一样运行，非常容易线性扩展，系统的灵活性很强</p>
<p><strong>缺点</strong></p>
<p>由于<strong>sharding处理放到客户端</strong>，规模进一步扩大时给运维带来挑战。<br>
客户端sharding不支持动态增删节点。服务端Redis实例群拓扑结构有变化时，每个客户端都需要更新调整。连接不能共享，当应用规模增大时，资源浪费制约优化</p>
<h3 id="4-代理服务器">4、代理服务器</h3>
<figure data-type="image" tabindex="69"><img src="https://img-blog.csdnimg.cn/20200115173630730.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua3dvbi5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="img" loading="lazy"></figure>
<p>客户端发送请求到一个代理组件，代理解析客户端的数据，并将请求转发至正确的节点，最后将结果回复给客户端</p>
<p>透明接入，业务程序不用关心后端Redis实例，切换成本低<br>
Proxy 的逻辑和存储的逻辑是隔离的<br>
代理层多了一次转发，性能有所损耗</p>
<h2 id="redis-主从架构">Redis 主从架构</h2>
<p>单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。</p>
<figure data-type="image" tabindex="70"><img src="https://img-blog.csdnimg.cn/20200115180329317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua3dvbi5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="redis-master-slave" loading="lazy"></figure>
<h4 id="redis-replication-的核心机制">redis replication 的核心机制</h4>
<p>redis 采用<strong>异步</strong>方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；<br>
一个 master node 是可以配置多个 slave node 的；<br>
slave node 也可以连接其他的 slave node；<br>
slave node 做复制的时候，不会 block master node 的正常工作；<br>
slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；<br>
slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的<strong>吞吐量</strong>。<br>
注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，<strong>不建议用 slave node 作为 master node 的数据热备</strong>，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。</p>
<h4 id="核心原理">核心原理</h4>
<p>当启动一个 slave node 的时候，它会发送一个 <strong>PSYNC</strong> 命令给 master node。</p>
<p>如果这是 slave node 初次连接到 master node，那么会触发一次 <strong>full resynchronization</strong> 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，</p>
<p>同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，</p>
<p>接着 master 会将内存中<strong>缓存的写命令</strong>发送到 slave，slave 也会同步这些数据。</p>
<p>slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave <strong>部分缺少的数据</strong>。</p>
<p><img src="https://img-blog.csdnimg.cn/20200115180337645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua3dvbi5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="redis-master-slave-replication" loading="lazy"><br>
缺点</p>
<p>所有的slave节点数据的复制和同步都由master节点来处理，会照成master节点压力太大，使用主从从结构来解决</p>
<h3 id="哈希槽-一致性">哈希槽、一致性</h3>
<ul>
<li>
<p>Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有<strong>16384</strong>个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。</p>
</li>
<li>
<p>集群最大节点个数是<strong>16384</strong>个</p>
</li>
<li>
<p>Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。</p>
</li>
<li>
<p>Redis集群目前无法做数据库选择，默认在0数据库。</p>
</li>
</ul>
<h2 id="分区">分区</h2>
<h3 id="redis是单线程的提高多核cpu的利用率">Redis是单线程的，提高多核CPU的利用率</h3>
<p>可以在同一个服务器部署多个Redis的实例，并把他们当作不同的服务器来使用，在某些时候，无论如何一个服务器是不够的， 所以，如果你想使用多个CPU，你可以考虑一下分片（shard）。就是集群！</p>
<ul>
<li><strong>为什么要做Redis分区？</strong></li>
</ul>
<p>分区可以让Redis管理更大的内存，Redis将可以使用所有机器的内存。<strong>如果没有分区，你最多只能使用一台机器的内存</strong>。分区使Redis的计算能力通过简单地增加计算机得到成倍提升，Redis的网络带宽也会随着计算机和网卡的增加而成倍增长。</p>
<ul>
<li><strong>Redis分区实现方案</strong></li>
</ul>
<p><strong>客户端分区</strong>就是在客户端就已经决定数据会被存储到哪个redis节点或者从哪个redis节点读取。大多数客户端已经实现了客户端分区。<br>
<strong>代理分区</strong>意味着客户端将请求发送给代理，然后代理决定去哪个节点写数据或者读数据。代理根据分区规则决定请求哪些Redis实例，然后根据Redis的响应结果返回给客户端。redis和memcached的一种代理实现就是Twemproxy<br>
<strong>服务端查询路由</strong>(Query routing) 的意思是客户端随机地请求任意一个redis实例，然后由Redis将请求转发给正确的Redis节点。Redis Cluster实现了一种混合形式的查询路由，但并不是直接将请求从一个redis节点转发到另一个redis节点，而是在客户端的帮助下直接<strong>redirected</strong>到正确的redis节点。</p>
<ul>
<li><strong>Redis分区缺点</strong></li>
</ul>
<p>涉及<strong>多个key的操作</strong>通常不会被支持。例如你不能对两个集合求交集，因为他们可能被存储到不同的Redis实例（实际上这种情况也有办法，但是不能直接使用交集指令）。<br>
同时操作多个key,则不能使用Redis<strong>事务</strong>.<br>
分区使用的粒度是key，不能使用一个非常长的排序key存储一个数据集<br>
当使用分区的时候，数据处理会非常复杂，例如为了备份你必须从不同的Redis实例和主机同时收集RDB / AOF文件。<br>
分区时<strong>动态扩容或缩容可能非常复杂</strong>。Redis集群在运行时增加或者删除Redis节点，能做到最大程度对用户透明地数据再平衡，但其他一些客户端分区或者代理分区方法则不支持这种特性。然而，有一种预分片的技术也可以较好的解决这个问题。</p>
<h2 id="分布式锁">分布式锁</h2>
<h3 id="redis">Redis</h3>
<p>Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系Redis中可以使用SETNX命令实现分布式锁。</p>
<p>当且仅当 key 不存在，将 key 的值设为 value。 若给定的 key 已经存在，则 SETNX 不做任何动作</p>
<p><strong>流程</strong></p>
<p>使用SETNX命令获取锁，若返回0（key已存在，锁已存在）则获取失败，反之获取成功</p>
<p>为了防止获取锁后程序出现异常，导致其他线程/进程调用SETNX命令总是返回0而进入死锁状态，需要为该key设置一个“合理”的过期时间</p>
<p>释放锁，使用DEL命令将锁数据删除</p>
<h3 id="zookeeper">Zookeeper</h3>
<p>每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。</p>
<p>判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。</p>
<p>以可靠性为主首推Zookeeper。</p>
<h3 id="redlock">RedLock</h3>
<p>Redis 官方站提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性：</p>
<p>安全特性：互斥访问，即永远只有一个 client 能拿到锁<br>
避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区<br>
容错性：只要大部分 Redis 节点存活就可以正常提供服务</p>
<h2 id="redis与memcached">Redis与Memcached</h2>
<figure data-type="image" tabindex="71"><img src="https://memorykki.github.io/post-images/redis/image-20211015140822481.png" alt="" loading="lazy"></figure>
<p>数据类型、持久化、原生集群模式、单线程/多线程</p>
<h2 id="双写一致性">双写一致性</h2>
<p><strong>先更新数据库，然后再删除缓存。</strong></p>
<h2 id="大量数据插入">大量数据插入</h2>
<p>Redis2.6开始redis-cli支持一种新的被称之为<strong>pipe</strong> mode的新模式用于执行大量数据插入工作。</p>
<h2 id="查找key">查找key</h2>
<p>假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？<br>
使用keys指令可以扫出指定模式的key列表。<br>
对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？<br>
redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。</p>
<h2 id="延时队列">延时队列</h2>
<p>使用sortedset，使用时间戳做score, 消息内容作为key,调用zadd来生产消息，消费者使用zrangbyscore获取n秒之前的数据做轮询处理。</p>
<h2 id="异步队列">异步队列</h2>
<p>使用list类型保存数据信息，rpush生产消息，lpop消费消息，<strong>当lpop没有消息时，可以sleep一段时间，然后再检查有没有信息</strong>，如果不想sleep的话，可以使用<strong>blpop</strong>, 在没有信息的时候，会一直阻塞，直到信息的到来。redis可以通过pub/sub主题订阅模式实现一个生产者，多个消费者，当然也存在一定的缺点，当消费者下线时，生产的消息会丢失。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mybatis]]></title>
        <id>https://memorykki.github.io/Mybatis/</id>
        <link href="https://memorykki.github.io/Mybatis/">
        </link>
        <updated>2021-08-19T06:23:35.000Z</updated>
        <summary type="html"><![CDATA[<p>MyBatis 是一个半自动化的<strong>ORM框架</strong></p>
]]></summary>
        <content type="html"><![CDATA[<p>MyBatis 是一个半自动化的<strong>ORM框架</strong></p>
<!-- more -->
<ul>
<li>
<p><a href="#01-mybatis%E4%B8%8Ehibernate%E6%9C%89%E5%93%AA%E4%BA%9B%E4%B8%8D%E5%90%8C">0.1. <strong>MyBatis与Hibernate有哪些不同？</strong></a></p>
</li>
<li>
<p><a href="#02-%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2like">0.2. <strong>模糊查询like</strong></a></p>
</li>
<li>
<p><a href="#03-%E5%BC%95%E5%85%A5%E8%B5%84%E6%BA%90%E6%96%B9%E5%BC%8F">0.3. <strong>引入资源方式</strong></a></p>
</li>
<li>
<p><a href="#04-namespace">0.4. namespace</a></p>
</li>
<li>
<p><a href="#05-dao%E6%8E%A5%E5%8F%A3%E5%92%8Cxml%E6%96%87%E4%BB%B6%E9%87%8C%E7%9A%84sql%E6%98%AF%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B%E5%85%B3%E7%B3%BB%E7%9A%84">0.5. Dao接口和XML文件里的SQL是如何建立关系的？</a></p>
<ul>
<li><a href="#051-%E4%B8%80%E8%A7%A3%E6%9E%90xml">0.5.1. 一、解析XML</a>
<ul>
<li><a href="#0511-%E5%88%9B%E5%BB%BAsqlsource">0.5.1.1. 、创建SqlSource</a></li>
<li><a href="#0512-%E5%88%9B%E5%BB%BAmappedstatement">0.5.1.2. 、创建MappedStatement</a></li>
</ul>
</li>
<li><a href="#052-%E4%BA%8Cdao%E6%8E%A5%E5%8F%A3%E4%BB%A3%E7%90%86">0.5.2. 二、Dao接口代理</a></li>
<li><a href="#053-%E4%B8%89%E6%89%A7%E8%A1%8C">0.5.3. 三、执行</a></li>
</ul>
</li>
<li>
<p><a href="#06-%E4%BD%9C%E7%94%A8%E5%9F%9F">0.6. <strong>作用域</strong></a></p>
</li>
<li>
<p><a href="#07-%E5%B1%9E%E6%80%A7%E5%90%8D%E5%92%8C%E5%AD%97%E6%AE%B5%E5%90%8D%E4%B8%8D%E4%B8%80%E8%87%B4">0.7. 属性名和字段名不一致</a></p>
</li>
<li>
<p><a href="#08-resultmap">0.8. ResultMap</a></p>
</li>
<li>
<p><a href="#09-%E5%88%86%E9%A1%B5">0.9. 分页</a></p>
</li>
<li>
<p><a href="#010-%E5%9C%A8mapper%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BC%A0%E9%80%92%E5%A4%9A%E4%B8%AA%E5%8F%82%E6%95%B0">0.10. <strong>在mapper中如何传递多个参数?</strong></a></p>
</li>
<li>
<p><a href="#011-mapper-%E7%BC%96%E5%86%99%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F">0.11. Mapper 编写有哪几种方式？</a></p>
</li>
<li>
<p><a href="#012-%E6%B3%A8%E8%A7%A3">0.12. 注解</a></p>
</li>
<li>
<p><a href="#013-mybatis%E8%AF%A6%E7%BB%86%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B">0.13. Mybatis详细的执行流程</a></p>
</li>
<li>
<p><a href="#014-mybatis%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B%E4%BB%8B%E7%BB%8D">0.14. MyBatis的工作原理以及核心流程介绍</a></p>
<ul>
<li><a href="#0141-mybatis%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3">0.14.1. 、MyBatis的工作原理以及核心流程详解</a></li>
</ul>
</li>
<li>
<p><a href="#015-param">0.15. @Param</a></p>
</li>
<li>
<p><a href="#016-%E4%B8%8E%E7%9A%84%E5%8C%BA%E5%88%AB">0.16. #与$的区别</a></p>
</li>
<li>
<p><a href="#017-%E6%95%B0%E6%8D%AE%E5%BA%93%E9%93%BE%E6%8E%A5%E4%B8%AD%E6%96%AD%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86">0.17. 数据库链接中断如何处理</a></p>
</li>
<li>
<p><a href="#018-%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8F%92%E5%85%A5%E9%87%8D%E5%A4%8D%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86">0.18. 数据库插入重复如何处理</a></p>
</li>
<li>
<p><a href="#019-%E4%B8%80%E4%B8%AAconnection%E5%9C%A8mysql%E4%B8%AD%E5%AF%B9%E5%BA%94%E4%B8%80%E4%B8%AA%E7%BA%BF%E7%A8%8B">0.19. 一个Connection在MySQL中对应一个线程？</a></p>
</li>
<li>
<p><a href="#020-%E9%A2%84%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B">0.20. 预编译的过程</a></p>
<ul>
<li><a href="#0201-jdbc%E7%9A%84%E9%A2%84%E7%BC%96%E8%AF%91%E7%94%A8%E6%B3%95">0.20.1. 、JDBC的预编译用法</a></li>
<li><a href="#0202-%E9%A2%84%E7%BC%96%E8%AF%91%E7%9A%84%E5%A5%BD%E5%A4%84">0.20.2. 、预编译的好处</a>
<ul>
<li><a href="#02021-%E9%A2%84%E7%BC%96%E8%AF%91%E8%83%BD%E9%81%BF%E5%85%8Dsql%E6%B3%A8%E5%85%A5">0.20.2.1. 、预编译能避免SQL注入</a></li>
<li><a href="#02022-%E9%A2%84%E7%BC%96%E8%AF%91%E8%83%BD%E6%8F%90%E9%AB%98sql%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87">0.20.2.2. 、预编译能提高SQL执行效率</a></li>
</ul>
</li>
<li><a href="#0203-%E9%A2%84%E7%BC%96%E8%AF%91%E7%9A%84%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B">0.20.3. 、预编译的实现过程</a></li>
</ul>
</li>
<li>
<p><a href="#021">0.21.</a></p>
</li>
<li>
<p><a href="#022-%E5%A4%9A%E5%AF%B9%E4%B8%80">0.22. 多对一</a></p>
<ul>
<li><a href="#0221-%E6%8C%89%E6%9F%A5%E8%AF%A2%E5%B5%8C%E5%A5%97%E5%A4%84%E7%90%86">0.22.1. 按查询嵌套处理</a></li>
<li><a href="#0222-%E6%8C%89%E7%BB%93%E6%9E%9C%E5%B5%8C%E5%A5%97%E5%A4%84%E7%90%86">0.22.2. 按结果嵌套处理</a></li>
</ul>
</li>
<li>
<p><a href="#023-%E4%B8%80%E5%AF%B9%E5%A4%9A">0.23. 一对多</a></p>
<ul>
<li><a href="#0231-%E6%8C%89%E7%BB%93%E6%9E%9C%E5%B5%8C%E5%A5%97%E5%A4%84%E7%90%86">0.23.1. 按结果嵌套处理</a></li>
<li><a href="#0232-%E6%8C%89%E6%9F%A5%E8%AF%A2%E5%B5%8C%E5%A5%97%E5%A4%84%E7%90%86">0.23.2. 按查询嵌套处理</a></li>
</ul>
</li>
<li>
<p><a href="#024-%E5%8A%A8%E6%80%81-sql">0.24. 动态 SQL</a></p>
</li>
<li>
<p><a href="#025-dao%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86">0.25. Dao接口的工作原理</a></p>
</li>
<li>
<p><a href="#026-%E7%BC%93%E5%AD%98">0.26. 缓存</a></p>
<ul>
<li><a href="#0261-%E4%B8%80%E7%BA%A7%E7%BC%93%E5%AD%98">0.26.1. 一级缓存</a></li>
<li><a href="#0262-%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98">0.26.2. 二级缓存</a></li>
</ul>
</li>
<li>
<p><a href="#027-%E5%92%8C%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88">0.27. 、#{}和${}的区别是什么？</a></p>
</li>
<li>
<p><a href="#028-xml-%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6%E4%B8%AD%E9%99%A4%E4%BA%86%E5%B8%B8%E8%A7%81%E7%9A%84-selectinsertupdatedelete-%E6%A0%87%E7%AD%BE%E4%B9%8B%E5%A4%96%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%87%E7%AD%BE">0.28. 、Xml 映射文件中，除了常见的 select|insert|update|delete 标签之外，还有哪些标签？</a></p>
</li>
<li>
<p><a href="#029-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B8%AD%E9%80%9A%E5%B8%B8%E4%B8%80%E4%B8%AA-xml-%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6%E9%83%BD%E4%BC%9A%E5%86%99%E4%B8%80%E4%B8%AA-dao-%E6%8E%A5%E5%8F%A3%E4%B8%8E%E4%B9%8B%E5%AF%B9%E5%BA%94%E8%AF%B7%E9%97%AE%E8%BF%99%E4%B8%AA-dao-%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88dao-%E6%8E%A5%E5%8F%A3%E9%87%8C%E7%9A%84%E6%96%B9%E6%B3%95%E5%8F%82%E6%95%B0%E4%B8%8D%E5%90%8C%E6%97%B6%E6%96%B9%E6%B3%95%E8%83%BD%E9%87%8D%E8%BD%BD%E5%90%97">0.29. 、最佳实践中，通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应，请问，这个 Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？</a></p>
<ul>
<li><a href="#0291-%E8%A1%A5%E5%85%85">0.29.1. ==补充：==</a></li>
</ul>
</li>
<li>
<p><a href="#030-mybatis-%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%88%86%E9%A1%B5%E7%9A%84%E5%88%86%E9%A1%B5%E6%8F%92%E4%BB%B6%E7%9A%84%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88">0.30. 、MyBatis 是如何进行分页的？分页插件的原理是什么？</a></p>
</li>
<li>
<p><a href="#031-%E7%AE%80%E8%BF%B0-mybatis-%E7%9A%84%E6%8F%92%E4%BB%B6%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA%E6%8F%92%E4%BB%B6">0.31. 、简述 MyBatis 的插件运行原理，以及如何编写一个插件。</a></p>
</li>
<li>
<p><a href="#032-mybatis-%E6%89%A7%E8%A1%8C%E6%89%B9%E9%87%8F%E6%8F%92%E5%85%A5%E8%83%BD%E8%BF%94%E5%9B%9E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%BB%E9%94%AE%E5%88%97%E8%A1%A8%E5%90%97">0.32. 、MyBatis 执行批量插入，能返回数据库主键列表吗？</a></p>
</li>
<li>
<p><a href="#033-mybatis-%E5%8A%A8%E6%80%81-sql-%E6%98%AF%E5%81%9A%E4%BB%80%E4%B9%88%E7%9A%84%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%E5%8A%A8%E6%80%81-sql%E8%83%BD%E7%AE%80%E8%BF%B0%E4%B8%80%E4%B8%8B%E5%8A%A8%E6%80%81-sql-%E7%9A%84%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86%E4%B8%8D">0.33. 、MyBatis 动态 sql 是做什么的？都有哪些动态 sql？能简述一下动态 sql 的执行原理不？</a></p>
</li>
<li>
<p><a href="#034-mybatis-%E6%98%AF%E5%A6%82%E4%BD%95%E5%B0%86-sql-%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%9C%E5%B0%81%E8%A3%85%E4%B8%BA%E7%9B%AE%E6%A0%87%E5%AF%B9%E8%B1%A1%E5%B9%B6%E8%BF%94%E5%9B%9E%E7%9A%84%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%98%A0%E5%B0%84%E5%BD%A2%E5%BC%8F">0.34. 、MyBatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？</a></p>
</li>
<li>
<p><a href="#035-mybatis-%E8%83%BD%E6%89%A7%E8%A1%8C%E4%B8%80%E5%AF%B9%E4%B8%80%E4%B8%80%E5%AF%B9%E5%A4%9A%E7%9A%84%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2%E5%90%97%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%E4%BB%A5%E5%8F%8A%E5%AE%83%E4%BB%AC%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB">0.35. 、MyBatis 能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别。</a></p>
</li>
<li>
<p><a href="#036-mybatis-%E6%98%AF%E5%90%A6%E6%94%AF%E6%8C%81%E5%BB%B6%E8%BF%9F%E5%8A%A0%E8%BD%BD%E5%A6%82%E6%9E%9C%E6%94%AF%E6%8C%81%E5%AE%83%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88">0.36. 、MyBatis 是否支持延迟加载？如果支持，它的实现原理是什么？</a></p>
</li>
<li>
<p><a href="#037-mybatis-%E7%9A%84-xml-%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6%E4%B8%AD%E4%B8%8D%E5%90%8C%E7%9A%84-xml-%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6id-%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E9%87%8D%E5%A4%8D">0.37. 、MyBatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复？</a></p>
</li>
<li>
<p><a href="#038-mybatis-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E6%89%B9%E5%A4%84%E7%90%86">0.38. 、MyBatis 中如何执行批处理？</a></p>
</li>
<li>
<p><a href="#039-mybatis-%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B-executor-%E6%89%A7%E8%A1%8C%E5%99%A8%E5%AE%83%E4%BB%AC%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88">0.39. 、MyBatis 都有哪些 Executor 执行器？它们之间的区别是什么？</a></p>
</li>
<li>
<p><a href="#040-mybatis-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%8C%87%E5%AE%9A%E4%BD%BF%E7%94%A8%E5%93%AA%E4%B8%80%E7%A7%8D-executor-%E6%89%A7%E8%A1%8C%E5%99%A8">0.40. 、MyBatis 中如何指定使用哪一种 Executor 执行器？</a></p>
</li>
<li>
<p><a href="#041-mybatis-%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E6%98%A0%E5%B0%84-enum-%E6%9E%9A%E4%B8%BE%E7%B1%BB">0.41. 、MyBatis 是否可以映射 Enum 枚举类？</a></p>
</li>
<li>
<p><a href="#042-mybatis-%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6%E4%B8%AD%E5%A6%82%E6%9E%9C-a-%E6%A0%87%E7%AD%BE%E9%80%9A%E8%BF%87-include-%E5%BC%95%E7%94%A8%E4%BA%86-b-%E6%A0%87%E7%AD%BE%E7%9A%84%E5%86%85%E5%AE%B9%E8%AF%B7%E9%97%AEb-%E6%A0%87%E7%AD%BE%E8%83%BD%E5%90%A6%E5%AE%9A%E4%B9%89%E5%9C%A8-a-%E6%A0%87%E7%AD%BE%E7%9A%84%E5%90%8E%E9%9D%A2%E8%BF%98%E6%98%AF%E8%AF%B4%E5%BF%85%E9%A1%BB%E5%AE%9A%E4%B9%89%E5%9C%A8-a-%E6%A0%87%E7%AD%BE%E7%9A%84%E5%89%8D%E9%9D%A2">0.42. 、MyBatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？</a></p>
</li>
<li>
<p><a href="#043-%E7%AE%80%E8%BF%B0-mybatis-%E7%9A%84-xml-%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6%E5%92%8C-mybatis-%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%97%B4%E7%9A%84%E6%98%A0%E5%B0%84%E5%85%B3%E7%B3%BB">0.43. 、简述 MyBatis 的 Xml 映射文件和 MyBatis 内部数据结构之间的映射关系？</a></p>
</li>
<li>
<p><a href="#044-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4-mybatis-%E6%98%AF%E5%8D%8A%E8%87%AA%E5%8A%A8-orm-%E6%98%A0%E5%B0%84%E5%B7%A5%E5%85%B7%E5%AE%83%E4%B8%8E%E5%85%A8%E8%87%AA%E5%8A%A8%E7%9A%84%E5%8C%BA%E5%88%AB%E5%9C%A8%E5%93%AA%E9%87%8C">0.44. 、为什么说 MyBatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？</a><br>
MyBatis 是一个半自动化的<strong>ORM框架</strong></p>
</li>
<li>
<p>所有的增删改操作都需要提交事务！</p>
</li>
<li>
<p>接口所有的普通参数，尽量都写上@Param参数，尤其是多个参数时，必须写上！</p>
</li>
<li>
<p>有时候根据业务的需求，可以考虑使用map传递参数！</p>
</li>
<li>
<p>为了规范操作，在SQL的配置文件中，我们尽量将Parameter参数和resultType都写上！</p>
</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://memorykki.github.io/post-images/Mybatis/20200606200123233.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://memorykki.github.io/post-images/Mybatis/image-20210818230659590.png" alt="" loading="lazy"></figure>
<p>Caching缓存</p>
<p>Simple</p>
<p>Batch批量</p>
<p>Reuse复用</p>
<p>http://www.mybatis.cn/category/interview/</p>
<h2 id="maven静态资源过滤问题">maven静态资源过滤问题</h2>
<pre><code class="language-xml">&lt;resources&gt;
   &lt;resource&gt;
       &lt;directory&gt;src/main/java&lt;/directory&gt;
       &lt;includes&gt;
           &lt;include&gt;**/*.properties&lt;/include&gt;
           &lt;include&gt;**/*.xml&lt;/include&gt;
       &lt;/includes&gt;
       &lt;filtering&gt;false&lt;/filtering&gt;
   &lt;/resource&gt;
   &lt;resource&gt;
       &lt;directory&gt;src/main/resources&lt;/directory&gt;
       &lt;includes&gt;
           &lt;include&gt;**/*.properties&lt;/include&gt;
           &lt;include&gt;**/*.xml&lt;/include&gt;
       &lt;/includes&gt;
       &lt;filtering&gt;false&lt;/filtering&gt;
   &lt;/resource&gt;
&lt;/resources&gt;
</code></pre>
<h2 id="01-mybatis与hibernate有哪些不同">0.1. <strong>MyBatis与Hibernate有哪些不同？</strong></h2>
<p>1、Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。</p>
<p>2、Mybatis直接编写原生态sql，可以严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需求变化要求迅速输出成果。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件，则需要自定义多套sql映射文件，工作量大。</p>
<p>3、Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件，如果用hibernate开发可以节省很多代码，提高效率。</p>
<h2 id="02-模糊查询like">0.2. <strong>模糊查询like</strong></h2>
<p>第1种：<strong>在Java代码中添加sql通配符。</strong></p>
<pre><code class="language-xml">string wildcardname = “%smi%”;
list&lt;name&gt; names = mapper.selectlike(wildcardname);

&lt;select id=”selectlike”&gt;
select * from foo where bar like #{value}
&lt;/select&gt;
</code></pre>
<p>第2种：在sql语句中拼接通配符，会引起sql注入</p>
<pre><code class="language-xml">string wildcardname = “smi”;
list&lt;name&gt; names = mapper.selectlike(wildcardname);

&lt;select id=”selectlike”&gt;
    select * from foo where bar like &quot;%&quot;#{value}&quot;%&quot;
&lt;/select&gt;
</code></pre>
<p><strong>或是利用sql的contact函数。</strong></p>
<pre><code class="language-xml">&lt;select id=&quot;selectLike&quot;&gt;
    select * from users where name like contact(&quot;%&quot;, #{value}, &quot;%&quot;)
&lt;/select&gt;
</code></pre>
<h2 id="03-引入资源方式">0.3. <strong>引入资源方式</strong></h2>
<pre><code class="language-xml">&lt;!-- 使用相对于类路径的资源引用 --&gt;
&lt;mappers&gt;
 &lt;mapper resource=&quot;org/mybatis/builder/PostMapper.xml&quot;/&gt;
&lt;/mappers&gt;
&lt;!-- 使用完全限定资源定位符（URL） --&gt;
&lt;mappers&gt;
 &lt;mapper url=&quot;file:///var/mappers/AuthorMapper.xml&quot;/&gt;
&lt;/mappers&gt;
&lt;!--
使用映射器接口实现类的完全限定类名
需要配置文件名称和接口名称一致，并且位于同一目录下
--&gt;
&lt;mappers&gt;
 &lt;mapper class=&quot;org.mybatis.builder.AuthorMapper&quot;/&gt;
&lt;/mappers&gt;
&lt;!--
将包内的映射器接口实现全部注册为映射器
但是需要配置文件名称和接口名称一致，并且位于同一目录下
--&gt;
&lt;mappers&gt;
 &lt;package name=&quot;org.mybatis.builder&quot;/&gt;
&lt;/mappers&gt;

&lt;!--优先级由上向下降低--&gt; 
</code></pre>
<h2 id="04-namespace">0.4. namespace</h2>
<p>namespace中文意思：命名空间，作用如下：</p>
<ul>
<li>
<ul>
<li>namespace的命名必须跟某个接口同名</li>
<li>接口中的方法与映射文件中sql语句id应该一一对应</li>
</ul>
</li>
<li>
<ol>
<li>namespace和子元素的id联合保证唯一  , 区别不同的mapper</li>
<li>绑定DAO接口</li>
<li>namespace命名规则 : 包名+类名</li>
</ol>
</li>
</ul>
<h2 id="05-dao接口和xml文件里的sql是如何建立关系的">0.5. <a href="http://www.mybatis.cn/archives/467.html">Dao接口和XML文件里的SQL是如何建立关系的？</a></h2>
<p><strong>通过Dao接口生成的动态代理调用查询，根据绑定的namespace确定唯一id，在注册中心里找到mappedStatement，通过sqlSource生成SQL语句，jdbc执行返回。</strong></p>
<h3 id="051-一-解析xml">0.5.1. 一、解析XML</h3>
<p>首先，Mybatis在初始化SqlSessionFactoryBean的时候，找到mapperLocations路径去解析里面所有的XML文件，这里我们重点关注两部分。</p>
<h4 id="0511-创建sqlsource">0.5.1.1. 、创建SqlSource</h4>
<p>Mybatis会把每个SQL标签封装成SqlSource对象，然后根据SQL语句的不同，又分为动态SQL和静态SQL。其中，静态SQL包含一段String类型的sql语句；而动态SQL则是由一个个SqlNode组成。</p>
<figure data-type="image" tabindex="3"><img src="https://memorykki.github.io/post-images/Mybatis/1118429449.jpg" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="4"><img src="https://memorykki.github.io/post-images/Mybatis/2906669416.jpg" alt="" loading="lazy"></figure>
<h4 id="0512-创建mappedstatement">0.5.1.2. 、创建MappedStatement</h4>
<p>XML文件中的每一个SQL标签就对应一个MappedStatement对象，这里面有两个属性很重要。</p>
<p><strong>id</strong>：全限定类名+方法名组成的ID。</p>
<p><strong>sqlSource</strong>：当前SQL标签对应的SqlSource对象。</p>
<p>创建完MappedStatement对象，将它缓存到Configuration#mappedStatements中。</p>
<p>Configuration对象就是Mybatis中的大管家，基本所有的配置信息都维护在这里。把所有的XML都解析完成之后，Configuration就包含了所有的SQL信息。</p>
<figure data-type="image" tabindex="5"><img src="https://memorykki.github.io/post-images/Mybatis/3090421271.jpg" alt="" loading="lazy"></figure>
<p>到目前为止，XML就解析完成了。当我们执行Mybatis方法的时候，就通过全限定类名+方法名找到MappedStatement对象，然后解析里面的SQL内容，执行即可。</p>
<h3 id="052-二-dao接口代理">0.5.2. 二、Dao接口代理</h3>
<p>我们的Dao接口并没有实现类，那么，我们在调用它的时候，它是怎样最终执行到我们的SQL语句的呢？</p>
<p>首先，我们在Spring配置文件中，一般会这样配置：</p>
<pre><code class="language-xml">&lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;
    &lt;property name=&quot;basePackage&quot; value=&quot;com.viewscenes.netsupervisor.dao&quot; /&gt;
    &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt;
&lt;/bean&gt;
</code></pre>
<p>或者你的项目是基于SpringBoot的，那么肯定也见过这种：</p>
<p>@MapperScan(&quot;com.xxx.dao&quot;)</p>
<p>它们的作用是一样的。将包路径下的所有类注册到Spring Bean中，并且将它们的beanClass设置为MapperFactoryBean。MapperFactoryBean实现了FactoryBean接口，俗称工厂Bean。那么，当我们通过@Autowired注入这个Dao接口的时候，返回的对象就是MapperFactoryBean这个工厂Bean中的getObject()方法对象。</p>
<p>简单来说，它就是<strong>通过JDK动态代理，返回了一个Dao接口的代理对象，这个代理对象的处理器是MapperProxy对象。所有，我们通过@Autowired注入Dao接口的时候，注入的就是这个代理对象，我们调用到Dao接口的方法时，则会调用到MapperProxy对象的invoke方法。</strong></p>
<p>曾经有个朋友问过这样一个问题：</p>
<p>对于有实现的dao接口，mapper还会用代理么？</p>
<p>答案是肯定，只要你配置了MapperScan，它就会去扫描，然后生成代理。但是，如果你的dao接口有实现类，并且这个实现类也是一个Spring Bean，那就要看你在Autowired的时候，去注入哪一个了。</p>
<p>会报错，因为在注入的时候，找到了两个UserDao的实例对象。其实我们通过名字注入就可以了。</p>
<h3 id="053-三-执行">0.5.3. 三、执行</h3>
<p>如上所述，当我们调用Dao接口方法的时候，实际调用到代理对象的invoke方法。 在这里，实际上调用的就是SqlSession里面的东西了。</p>
<pre><code class="language-java">public class DefaultSqlSession implements SqlSession {

    public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) {
        try {
            MappedStatement ms = configuration.getMappedStatement(statement);
            return executor.query(ms, 
                wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);
        }
    }
}
</code></pre>
<p>是通过statement全限定类型+方法名拿到MappedStatement 对象，然后通过执行器Executor去执行具体SQL并返回。</p>
<figure data-type="image" tabindex="6"><img src="https://memorykki.github.io/post-images/Mybatis/792538947.jpg" alt="" loading="lazy"></figure>
<h2 id="06-作用域">0.6. <strong>作用域</strong></h2>
<figure data-type="image" tabindex="7"><img src="https://mmbiz.qpic.cn/mmbiz_png/uJDAUKrGC7JdnS939HH5TayIhQo5s0aJbReBExSQO1U23XeLAXlhTWUeL87mJZL0lDzPstpY3CSIwvW0dN9ccA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片" loading="lazy"></figure>
<p><strong>作用域理解</strong></p>
<ul>
<li>SqlSessionFactoryBuilder 的作用在于创建 SqlSessionFactory，创建成功后，SqlSessionFactoryBuilder 就失去了作用，所以它只能存在于创建 SqlSessionFactory 的方法中，而不要让其长期存在。因此 <strong>SqlSessionFactoryBuilder 实例的最佳作用域是方法作用域</strong>（也就是局部方法变量）。</li>
<li>SqlSessionFactory 可以被认为是一个数据库连接池，它的作用是创建 SqlSession 接口对象。因为 MyBatis 的本质就是 Java 对数据库的操作，所以 SqlSessionFactory 的生命周期存在于整个 MyBatis 的应用之中，所以一旦创建了 SqlSessionFactory，就要长期保存它，直至不再使用 MyBatis 应用，所以可以认为 SqlSessionFactory 的生命周期就等同于 MyBatis 的应用周期。</li>
<li>由于 SqlSessionFactory 是一个对数据库的连接池，所以它占据着数据库的连接资源。如果创建多个 SqlSessionFactory，那么就存在多个数据库连接池，这样不利于对数据库资源的控制，也会导致数据库连接资源被消耗光，出现系统宕机等情况，所以尽量避免发生这样的情况。</li>
<li>因此在一般的应用中我们往往希望 SqlSessionFactory 作为一个单例，让它在应用中被共享。所以说 <strong>SqlSessionFactory 的最佳作用域是应用作用域。</strong></li>
<li>如果说 SqlSessionFactory 相当于数据库连接池，那么 SqlSession 就相当于一个数据库连接（Connection 对象），你可以在一个事务里面执行多条 SQL，然后通过它的 commit、rollback 等方法，提交或者回滚事务。所以它应该存活在一个业务请求中，处理完整个请求后，应该关闭这条连接，让它归还给 SqlSessionFactory，否则数据库资源就很快被耗费精光，系统就会瘫痪，所以用 try...catch...finally... 语句来保证其正确关闭。</li>
<li><strong>所以 SqlSession 的最佳的作用域是请求或方法作用域。</strong></li>
</ul>
<figure data-type="image" tabindex="8"><img src="https://mmbiz.qpic.cn/mmbiz_png/uJDAUKrGC7JdnS939HH5TayIhQo5s0aJJq1YuJCr3e9PsTBpBgc1tbicoshHB3qLkwgn3Jp2q8qI1dY9vGhIia3w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片" loading="lazy"></figure>
<h2 id="07-属性名和字段名不一致">0.7. 属性名和字段名不一致</h2>
<p>方案一：为列名指定别名 , 别名和java实体类的属性名一致 .</p>
<pre><code class="language-xml">&lt;select id=&quot;selectUserById&quot; resultType=&quot;User&quot;&gt;
  select id , name , pwd as password from user where id = #{id}
&lt;/select&gt;
</code></pre>
<p><strong>方案二：使用结果集映射-&gt;ResultMap</strong> 【推荐】</p>
<pre><code class="language-xml">&lt;resultMap id=&quot;UserMap&quot; type=&quot;User&quot;&gt;
   &lt;!-- id为主键 --&gt;
   &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt;
   &lt;!-- column是数据库表的列名 , property是对应实体类的属性名 --&gt;
   &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt;
   &lt;result column=&quot;pwd&quot; property=&quot;password&quot;/&gt;
&lt;/resultMap&gt;

&lt;select id=&quot;selectUserById&quot; resultMap=&quot;UserMap&quot;&gt;
  select id , name , pwd from user where id = #{id}
&lt;/select&gt;
</code></pre>
<h2 id="08-resultmap">0.8. ResultMap</h2>
<p><strong>自动映射</strong></p>
<p>简单地将所有的列映射到 <code>HashMap</code> 的键上，这由 <code>resultType</code> 属性指定。虽然在大部分情况下都够用，但是 HashMap 不是一个很好的模型。你的程序更可能会使用 JavaBean 或 POJO（Plain Old Java Objects，普通老式 Java 对象）作为模型。</p>
<p><strong>手动映射</strong></p>
<p>返回值类型为resultMap</p>
<h2 id="09-分页">0.9. 分页</h2>
<p><strong>limit物理分页</strong></p>
<pre><code class="language-sql">#语法
SELECT * FROM table LIMIT stratIndex，pageSize

SELECT * FROM table LIMIT 5,10; // 检索记录行 6-15  

#为了检索从某一个偏移量到记录集的结束所有的记录行，可以指定第二个参数为 -1：   
SELECT * FROM table LIMIT 95,-1; // 检索记录行 96-last.  

#如果只给定一个参数，它表示返回最大的记录行数目：   
SELECT * FROM table LIMIT 5; //检索前 5 个记录行  

#换句话说，LIMIT n 等价于 LIMIT 0,n。 
</code></pre>
<p><strong>RowBounds逻辑分页</strong></p>
<p>除了使用Limit在SQL层面实现分页，也可以使用RowBounds在Java代码层面实现分页</p>
<pre><code class="language-java">SqlSession session = MybatisUtils.getSession();

   int currentPage = 2;  //第几页
   int pageSize = 2;  //每页显示几个
   RowBounds rowBounds = new RowBounds((currentPage-1)*pageSize,pageSize);

   //通过session.**方法进行传递rowBounds，[此种方式现在已经不推荐使用了]
   List&lt;User&gt; users = session.selectList(&quot;com.kuang.mapper.UserMapper.getUserByRowBounds&quot;, null, rowBounds);
</code></pre>
<p><strong>PageHelper</strong></p>
<h2 id="010-在mapper中如何传递多个参数">0.10. <strong>在mapper中如何传递多个参数?</strong></h2>
<p>1、第一种：</p>
<p>DAO层的函数</p>
<p>2、第二种： 使用 @param 注解:</p>
<p>然后,就可以在xml像下面这样使用(推荐封装为一个map,作为单个参数传递给mapper):</p>
<p>3、第三种：多个参数封装成map</p>
<h2 id="011-mapper-编写有哪几种方式">0.11. Mapper 编写有哪几种方式？</h2>
<ul>
<li>
<p>接口实现类继承 SqlSessionDaoSupport：使用此种方法需要编写mapper 接口，mapper 接口实现类、mapper.xml 文件</p>
<p>1、在 sqlMapConfig.xml 中配置 mapper.xml 的位置</p>
</li>
</ul>
<pre><code class="language-xml">&lt;mappers&gt;
&lt;mapper resource=&quot;mapper.xml 文件的地址&quot; /&gt;
&lt;mapper resource=&quot;mapper.xml 文件的地址&quot; /&gt;
&lt;/mappers&gt;
</code></pre>
<pre><code>2、定义 mapper 接口

3、实现类集成 SqlSessionDaoSupport，mapper 方法中可以 this.getSqlSession()进行数据增删改查。

4、spring 配置
</code></pre>
<pre><code class="language-xml">&lt;bean id=&quot; &quot; class=&quot;mapper 接口的实现&quot;&gt;
    &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt;
&lt;/bean&gt;
</code></pre>
<ul>
<li>使用 org.mybatis.spring.mapper.MapperFactoryBean ：<br>
1、在 sqlMapConfig.xml 中配置 mapper.xml 的位置，如果 mapper.xml 和 mappre 接口的名称相同且在同一个目录，这里可以不用配置</li>
</ul>
<pre><code class="language-xml">&lt;mapper resource=&quot;mapper.xml 文件的地址&quot; /&gt;
&lt;mapper resource=&quot;mapper.xml 文件的地址&quot; /&gt;
&lt;/mappers&gt;
</code></pre>
<pre><code>	2、定义 mapper 接口：
	2.1、mapper.xml 中的 namespace 为 mapper 接口的地址
	2.2、mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一 致
	3、Spring 中定义
</code></pre>
<ul>
<li>
<p>使用 mapper 扫描器：</p>
<p>1、mapper.xml 文件编写：<br>
mapper.xml 中的 namespace 为 mapper 接口的地址；<br>
mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一致；<br>
如果将 mapper.xml 和 mapper 接口的名称保持一致则不用在 sqlMapConfig.xml 中进行配置</p>
<p>2、定义 mapper 接口：<br>
注意 mapper.xml 的文件名和 mapper 的接口名称保持一致，且放在同一个目录</p>
<p>3、配置 mapper 扫描器：</p>
<pre><code class="language-xml">&lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;
&lt;property name=&quot;basePackage&quot; value=&quot;mapper 接口包地址&quot;&gt;&lt;/property&gt;
&lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;/&gt;
&lt;/bean&gt;
</code></pre>
<p>4、使用扫描器后从 spring 容器中获取 mapper 的实现对象。</p>
</li>
</ul>
<h2 id="012-注解">0.12. 注解</h2>
<p>1、我们在我们的接口中添加注解</p>
<pre><code class="language-java">//查询全部用户
@Select(&quot;select id,name,pwd password from user&quot;)
public List&lt;User&gt; getAllUser();
</code></pre>
<p>2、在mybatis的核心配置文件中注入</p>
<pre><code class="language-xml">&lt;!--使用class绑定接口--&gt;
&lt;mappers&gt;
   &lt;mapper class=&quot;com.kuang.mapper.UserMapper&quot;/&gt;
&lt;/mappers&gt;
</code></pre>
<h2 id="013-mybatis详细的执行流程">0.13. Mybatis详细的执行流程</h2>
<figure data-type="image" tabindex="9"><img src="https://mmbiz.qpic.cn/mmbiz_png/uJDAUKrGC7LZwwtchlelS8kzAAyVia5uNvhic22X8ahJy5BdOfjy1LlDRfo8Nf3GOAzwALgvriau4SzmXZIhUUd2A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片" loading="lazy"></figure>
<h2 id="014-mybatis的工作原理以及核心流程介绍">0.14. MyBatis的工作原理以及核心流程介绍</h2>
<ul>
<li>
<p>JDBC有四个核心对象：<br>
（1）DriverManager，用于注册数据库连接<br>
（2）Connection，与数据库连接对象<br>
（3）Statement/PrepareStatement，操作数据库SQL语句的对象<br>
（4）ResultSet，结果集或一张虚拟表</p>
</li>
<li>
<p>MyBatis也有四大核心对象：<br>
（1）SqlSession对象，该对象中包含了执行SQL语句的所有方法。类似于JDBC里面的Connection<br>
（2）Executor接口，它将根据SqlSession传递的参数动态地生成需要执行的SQL语句，同时负责查询缓存的维护。类似于JDBC里面的Statement/PrepareStatement。<br>
（3）MappedStatement对象，该对象是对映射SQL的封装，用于存储要映射的SQL语句的id、参数等信息。<br>
（4）ResultHandler对象，用于对返回的结果进行处理，最终得到自己想要的数据格式或类型。可以自定义返回类型。</p>
</li>
</ul>
<h3 id="0141-mybatis的工作原理以及核心流程详解">0.14.1. 、MyBatis的工作原理以及核心流程详解</h3>
<p>MyBatis的工作原理如下图所示：</p>
<figure data-type="image" tabindex="10"><img src="https://memorykki.github.io/post-images/Mybatis/326517643.png" alt="" loading="lazy"></figure>
<p>（1）读取MyBatis的配置文件。mybatis-config.xml为MyBatis的全局配置文件，用于配置数据库连接信息。</p>
<p>（2）加载映射文件。映射文件即SQL映射文件，该文件中配置了操作数据库的SQL语句，需要在MyBatis配置文件mybatis-config.xml中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表。</p>
<p>（3）构造会话工厂。通过MyBatis的环境配置信息构建会话工厂SqlSessionFactory。</p>
<p>（4）创建会话对象。由会话工厂创建SqlSession对象，该对象中包含了执行SQL语句的所有方法。</p>
<p>（5）Executor执行器。MyBatis底层定义了一个Executor接口来操作数据库，它将根据SqlSession传递的参数动态地生成需要执行的SQL语句，同时负责查询缓存的维护。</p>
<p>（6）MappedStatement对象。在Executor接口的执行方法中有一个MappedStatement类型的参数，该参数是对映射信息的封装，用于存储要映射的SQL语句的id、参数等信息。</p>
<p>（7）输入参数映射。输入参数类型可以是Map、List等集合类型，也可以是基本数据类型和POJO类型。输入参数映射过程类似于JDBC对preparedStatement对象设置参数的过程。</p>
<p>（8）输出结果映射。输出结果类型可以是Map、List等集合类型，也可以是基本数据类型和POJO类型。输出结果映射过程类似于JDBC对结果集的解析过程。</p>
<h2 id="015-param">0.15. @Param</h2>
<p>@Param注解用于给方法参数起一个名字。以下是总结的使用原则：</p>
<ul>
<li>
<p>在方法只接受一个参数的情况下，可以不使用@Param。</p>
</li>
<li>
<p>在方法接受多个参数的情况下，建议一定要使用@Param注解给参数命名。</p>
</li>
<li>
<p>如果参数是 JavaBean ， 则不能使用@Param。</p>
</li>
<li>
<p>不使用@Param注解时，参数只能有一个，并且是Javabean。</p>
</li>
</ul>
<h2 id="016-与的区别">0.16. #与$的区别</h2>
<p>/#{} 的作用主要是替换预编译语句(PrepareStatement)中的占位符? 【推荐使用】</p>
<pre><code class="language-sql">INSERT INTO user (name) VALUES (#{name});
INSERT INTO user (name) VALUES (?);
</code></pre>
<p>${} 的作用是直接进行字符串替换</p>
<pre><code class="language-sql">INSERT INTO user (name) VALUES ('${name}');
INSERT INTO user (name) VALUES ('kuangshen');
</code></pre>
<p>（3）使用#{}可以有效的防止SQL注入，提高系统安全性。原因在于：预编译机制。<strong>预编译完成之后，SQL的结构已经固定，即便用户输入非法参数，也不会对SQL的结构产生影响，从而避免了潜在的安全风险。</strong></p>
<p>（4）预编译是提前对SQL语句进行预编译，而其后注入的参数将不会再进行SQL编译。我们知道，SQL注入是发生在编译的过程中，因为恶意注入了某些特殊字符，最后被编译成了恶意的执行操作。而预编译机制则可以很好的防止SQL注入。</p>
<h2 id="017-数据库链接中断如何处理">0.17. 数据库链接中断如何处理</h2>
<p>数据库的访问底层是通过tcp实现的，如果数据库链接中断，那么应用程序是不知道的，跟时间有关的设置有：max_idle_time，connect_timeout。max_idle_time表明最大的空闲时间，超过这个时间socket就会关闭，timeout。</p>
<h2 id="018-数据库插入重复如何处理">0.18. 数据库插入重复如何处理</h2>
<p>插入的过程一般都是分两步的：先判断是否存在记录，没有存在则插入否则不插入。如果存在并发操作，那么同时进行了第一步，然后大家都发现没有记录，最后在第二步的时候都插入了数据从而造成数据的重复。解决插入重复的思路可以是这样的：</p>
<p>下面场景，假设同时有三个线程：线程a、线程b、线程c，进行插入操作。</p>
<p>（1）判断数据库是否有数据，有的话则无所作为。没有数据的话，则进行下面第2步。<br>
（2）大家都要去竞争锁，用redis当锁，即：redis set key，其中只有一个操作a会成功，其他并发的线程b和c会失败的。<br>
（3）上面set key 成功的线程a，开始执行插入数据操作，无论是否插入数据成功，都在最后del key。【注】插入不成功可以多尝试几次，增加成功的概率。<br>
（4）如果拿到锁的线程a没有插入成功，即便是尝试了数次也没有插入成功，此时定是系统出现了bug，应该搞一个短信报警机制，让研发人员及时发现问题。</p>
<h2 id="019-一个connection在mysql中对应一个线程">0.19. 一个Connection在MySQL中对应一个线程？</h2>
<p>在高性能服务器端端开发底层往往靠io复用来处理，这种模式就是：单线程+事件处理机制。在MySQL里面往往有一个主线程，这是单线程（与Java中处处强调多线程的思想有点不同哦），它不断的循环查看是否有socket是否有读写事件，如果有读写事件，再从线程池里面找个工作线程处理这个socket的读写事件，完事之后工作线程会回到线程池。所以：Java客户端中的一个Connection不是在MySQL中就对应一个线程来处理这个链接，而是由监听socket的主线程+线程池里面固定数目的工作线程来处理的。</p>
<h2 id="020-预编译的过程">0.20. 预编译的过程</h2>
<h3 id="0201-jdbc的预编译用法">0.20.1. 、JDBC的预编译用法</h3>
<p>相信每个人都应该了解JDBC中的PreparedStatement接口，它是用来实现SQL预编译的功能。其用法是这样的：</p>
<pre><code class="language-java">Class.forName(&quot;com.mysql.jdbc.Driver&quot;);
String url = &quot;jdbc:mysql://127.0.0.1:3306/mybatis&quot;;
String user = &quot;root&quot;;
String password = &quot;123456&quot;;
//建立数据库连接
Connection conn = DriverManager.getConnection(url, user, password);

String sql = &quot;insert into user(username, sex, address) values(?,?,?)&quot;;
PreparedStatement ps = conn.preparedStatement(sql);
ps.setString(1, &quot;张三&quot;);  //为第一个问号赋值  
ps.setInt(2, 2);    //为第二个问号赋值
ps.setString(3, &quot;北京&quot;);    //为第三个问号赋值
ps.executeUpdate();
conn.close();
</code></pre>
<h3 id="0202-预编译的好处">0.20.2. 、预编译的好处</h3>
<h4 id="02021-预编译能避免sql注入">0.20.2.1. 、预编译能避免SQL注入</h4>
<p>预编译功能可以避免SQL注入，因为SQL已经编译完成，其结构已经固定，用户的输入只能当做参数传入进去，不能再破坏SQL的结果，无法造成曲解SQL原本意思的破坏。</p>
<h4 id="02022-预编译能提高sql执行效率">0.20.2.2. 、预编译能提高SQL执行效率</h4>
<p>预编译功能除了避免SQL注入，还能提高SQL执行效率。当客户发送一条SQL语句给服务器后，服务器首先需要校验SQL语句的语法格式是否正确，然后把SQL语句编译成可执行的函数，最后才是执行SQL语句。其中校验语法，和编译所花的时间可能比执行SQL语句花的时间还要多。</p>
<p>如果我们需要执行多次insert语句，但只是每次插入的值不同，MySQL服务器也是需要每次都去校验SQL语句的语法格式以及编译，这就浪费了太多的时间。如果使用预编译功能，那么只对SQL语句进行一次语法校验和编译，所以效率要高。</p>
<h3 id="0203-预编译的实现过程">0.20.3. 、预编译的实现过程</h3>
<p>预编译功能如此重要，那么数据库是如何实现预编译的呢？这个问题其实可以当做一个面试题，能很好的考察面试者对预编译的理解。下面以MySQL为例说明一下预编译的过程：</p>
<p>MySQL执行预编译分为如三步：</p>
<p>第一步：执行预编译语句，例如：prepare myperson from 'select * from t_person where name=?'<br>
第二步：设置变量，例如：set @name='Jim'<br>
第三步：执行语句，例如：execute myperson using @name</p>
<p>如果需要再次执行myperson，那么就不再需要第一步，即不需要再编译语句了：</p>
<p>设置变量，例如：set @name='Tom'<br>
执行语句，例如：execute myperson using @name</p>
<h2 id="021">0.21.</h2>
<h2 id="022-多对一">0.22. 多对一</h2>
<p>多对一的理解：</p>
<ul>
<li>多个学生对应一个老师</li>
<li>如果对于学生这边，就是一个多对一的现象，即从学生这边关联一个老师！</li>
</ul>
<h3 id="0221-按查询嵌套处理">0.22.1. 按查询嵌套处理</h3>
<p>1、给StudentMapper接口增加方法</p>
<pre><code class="language-java">//获取所有学生及对应老师的信息
public List&lt;Student&gt; getStudents();
</code></pre>
<p>2、编写对应的Mapper文件</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE mapper
       PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
       &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
&lt;mapper namespace=&quot;com.kuang.mapper.StudentMapper&quot;&gt;

   &lt;!--
   需求：获取所有学生及对应老师的信息
   思路：
       1. 获取所有学生的信息
       2. 根据获取的学生信息的老师ID-&gt;获取该老师的信息
       3. 思考问题，这样学生的结果集中应该包含老师，该如何处理呢，数据库中我们一般使用关联查询？
           1. 做一个结果集映射：StudentTeacher
           2. StudentTeacher结果集的类型为 Student
           3. 学生中老师的属性为teacher，对应数据库中为tid。
              多个 [1,...）学生关联一个老师=&gt; 一对一，一对多
           4. 查看官网找到：association – 一个复杂类型的关联；使用它来处理关联查询
   --&gt;
   &lt;select id=&quot;getStudents&quot; resultMap=&quot;StudentTeacher&quot;&gt;
    select * from student
   &lt;/select&gt;
   &lt;resultMap id=&quot;StudentTeacher&quot; type=&quot;Student&quot;&gt;
       &lt;!--association关联属性 property属性名 javaType属性类型 column在多的一方的表中的列名--&gt;
       &lt;association property=&quot;teacher&quot;  column=&quot;tid&quot; javaType=&quot;Teacher&quot; select=&quot;getTeacher&quot;/&gt;
   &lt;/resultMap&gt;
   &lt;!--
   这里传递过来的id，只有一个属性的时候，下面可以写任何值
   association中column多参数配置：
       column=&quot;{key=value,key=value}&quot;
       其实就是键值对的形式，key是传给下个sql的取值名称，value是片段一中sql查询的字段名。
   --&gt;
   &lt;select id=&quot;getTeacher&quot; resultType=&quot;teacher&quot;&gt;
      select * from teacher where id = #{id}
   &lt;/select&gt;

&lt;/mapper&gt;
</code></pre>
<p>3、编写完毕去Mybatis配置文件中，注册Mapper！</p>
<p>4、注意点说明：</p>
<pre><code class="language-xml">&lt;resultMap id=&quot;StudentTeacher&quot; type=&quot;Student&quot;&gt;
   &lt;!--association关联属性 property属性名 javaType属性类型 column在多的一方的表中的列名--&gt;
   &lt;association property=&quot;teacher&quot;  column=&quot;{id=tid,name=tid}&quot; javaType=&quot;Teacher&quot; select=&quot;getTeacher&quot;/&gt;
&lt;/resultMap&gt;
&lt;!--
这里传递过来的id，只有一个属性的时候，下面可以写任何值
association中column多参数配置：
   column=&quot;{key=value,key=value}&quot;
   其实就是键值对的形式，key是传给下个sql的取值名称，value是片段一中sql查询的字段名。
--&gt;
&lt;select id=&quot;getTeacher&quot; resultType=&quot;teacher&quot;&gt;
  select * from teacher where id = #{id} and name = #{name}
&lt;/select&gt;
</code></pre>
<h3 id="0222-按结果嵌套处理">0.22.2. 按结果嵌套处理</h3>
<p>1、接口方法编写</p>
<pre><code class="language-java">public List&lt;Student&gt; getStudents2();
</code></pre>
<p>2、编写对应的mapper文件</p>
<pre><code class="language-xml">&lt;!--
按查询结果嵌套处理
思路：
   1. 直接查询出结果，进行结果集的映射
--&gt;
&lt;select id=&quot;getStudents2&quot; resultMap=&quot;StudentTeacher2&quot; &gt;
  select s.id sid, s.name sname , t.name tname
  from student s,teacher t
  where s.tid = t.id
&lt;/select&gt;

&lt;resultMap id=&quot;StudentTeacher2&quot; type=&quot;Student&quot;&gt;
   &lt;id property=&quot;id&quot; column=&quot;sid&quot;/&gt;
   &lt;result property=&quot;name&quot; column=&quot;sname&quot;/&gt;
   &lt;!--关联对象property 关联对象在Student实体类中的属性--&gt;
   &lt;association property=&quot;teacher&quot; javaType=&quot;Teacher&quot;&gt;
       &lt;result property=&quot;name&quot; column=&quot;tname&quot;/&gt;
   &lt;/association&gt;
&lt;/resultMap&gt;
</code></pre>
<h2 id="023-一对多">0.23. 一对多</h2>
<p>对多的理解：</p>
<ul>
<li>一个老师拥有多个学生</li>
<li>如果对于老师这边，就是一个一对多的现象，即从一个老师下面拥有一群学生（集合）！</li>
</ul>
<h3 id="0231-按结果嵌套处理">0.23.1. 按结果嵌套处理</h3>
<p>1、TeacherMapper接口编写方法</p>
<pre><code class="language-java">//获取指定老师，及老师下的所有学生
public Teacher getTeacher(int id);
</code></pre>
<p>2、编写接口对应的Mapper配置文件</p>
<pre><code class="language-xml">&lt;mapper namespace=&quot;com.kuang.mapper.TeacherMapper&quot;&gt;

   &lt;!--
   思路:
       1. 从学生表和老师表中查出学生id，学生姓名，老师姓名
       2. 对查询出来的操作做结果集映射
           1. 集合的话，使用collection！
               JavaType和ofType都是用来指定对象类型的
               JavaType是用来指定pojo中属性的类型
               ofType指定的是映射到list集合属性中pojo的类型。
   --&gt;
   &lt;select id=&quot;getTeacher&quot; resultMap=&quot;TeacherStudent&quot;&gt;
      select s.id sid, s.name sname , t.name tname, t.id tid
      from student s,teacher t
      where s.tid = t.id and t.id=#{id}
   &lt;/select&gt;

   &lt;resultMap id=&quot;TeacherStudent&quot; type=&quot;Teacher&quot;&gt;
       &lt;result  property=&quot;name&quot; column=&quot;tname&quot;/&gt;
       &lt;collection property=&quot;students&quot; ofType=&quot;Student&quot;&gt;
           &lt;result property=&quot;id&quot; column=&quot;sid&quot; /&gt;
           &lt;result property=&quot;name&quot; column=&quot;sname&quot; /&gt;
           &lt;result property=&quot;tid&quot; column=&quot;tid&quot; /&gt;
       &lt;/collection&gt;
   &lt;/resultMap&gt;
&lt;/mapper&gt;
</code></pre>
<h3 id="0232-按查询嵌套处理">0.23.2. 按查询嵌套处理</h3>
<p>1、TeacherMapper接口编写方法</p>
<pre><code class="language-java">public Teacher getTeacher2(int id);
</code></pre>
<p>2、编写接口对应的Mapper配置文件</p>
<pre><code class="language-xml">&lt;select id=&quot;getTeacher2&quot; resultMap=&quot;TeacherStudent2&quot;&gt;
select * from teacher where id = #{id}
&lt;/select&gt;
&lt;resultMap id=&quot;TeacherStudent2&quot; type=&quot;Teacher&quot;&gt;
   &lt;!--column是一对多的外键 , 写的是一的主键的列名--&gt;
   &lt;collection property=&quot;students&quot; javaType=&quot;ArrayList&quot; ofType=&quot;Student&quot; column=&quot;id&quot; select=&quot;getStudentByTeacherId&quot;/&gt;
&lt;/resultMap&gt;
&lt;select id=&quot;getStudentByTeacherId&quot; resultType=&quot;Student&quot;&gt;
  select * from student where tid = #{id}
&lt;/select&gt;
</code></pre>
<p><strong>小结</strong></p>
<p>1、关联-association</p>
<p>2、集合-collection</p>
<p>3、所以association是用于一对一和多对一，而collection是用于一对多的关系</p>
<p>4、JavaType和ofType都是用来指定对象类型的</p>
<ul>
<li>JavaType是用来指定pojo中属性的类型</li>
<li>ofType指定的是映射到list集合属性中pojo的类型。</li>
</ul>
<h2 id="024-动态-sql">0.24. 动态 SQL</h2>
<p>动态SQL指的是根据不同的查询条件 , 生成不同的Sql语句.</p>
<pre><code> -------------------------------
  - if
  - choose (when, otherwise)
  - trim (where, set)
  - foreach
  -------------------------------
</code></pre>
<h2 id="025-dao接口的工作原理">0.25. Dao接口的工作原理</h2>
<p>Dao接口即Mapper接口。接口的全限名，就是映射文件中的namespace的值；接口的方法名，就是映射文件中Mapper的Statement的id值；接口方法内的参数，就是传递给sql的参数。</p>
<p>Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MapperStatement。在Mybatis中，每一个 <select>、<insert>、<update>、<delete>标签，都会被解析为一个MapperStatement对象。</p>
<p>举例来说：cn.mybatis.mappers.StudentDao.findStudentById，可以唯一找到namespace为 com.mybatis.mappers.StudentDao下面 id 为 findStudentById 的 MapperStatement。</p>
<p>Mapper接口里的方法，是不能重载的，因为是使用 全限名+方法名 的保存和寻找策略。Mapper 接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Mapper接口生成代理对象proxy，代理对象会拦截接口方法，转而执行MapperStatement所代表的sql，然后将sql执行结果返回。</p>
<h2 id="026-缓存">0.26. 缓存</h2>
<p>1、什么是缓存 [ Cache ]？</p>
<ul>
<li>存在内存中的临时数据。</li>
<li>将用户经常查询的数据放在缓存（内存）中，用户去查询数据就不用从磁盘上(关系型数据库数据文件)查询，从缓存中查询，从而提高查询效率，解决了高并发系统的性能问题。</li>
</ul>
<p>2、为什么使用缓存？</p>
<ul>
<li>减少和数据库的交互次数，减少系统开销，提高系统效率。</li>
</ul>
<p>3、什么样的数据能使用缓存？</p>
<ul>
<li>经常查询并且不经常改变的数据。</li>
</ul>
<blockquote>
<p>Mybatis的缓存实际上就是一个HashMap，key是真正执行的sql语句，value是缓存的结果。</p>
</blockquote>
<h3 id="0261-一级缓存">0.26.1. 一级缓存</h3>
<p>一级缓存也叫本地缓存：</p>
<ul>
<li>与数据库同一次会话期间查询到的数据会放在本地缓存中。</li>
<li>以后如果需要获取相同的数据，直接从缓存中拿，没必须再去查询数据库；</li>
</ul>
<figure data-type="image" tabindex="11"><img src="https://mmbiz.qpic.cn/mmbiz_png/uJDAUKrGC7KickRVspms8t4ZU0jXovPT2qe5QluO0MoibU09bTKiaGG923AzFwOSxICrM7BZFWNJqiaCUOGxDA54Tg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片" loading="lazy"></figure>
<p>一级缓存失效的四种情况：</p>
<p>一级缓存是SqlSession级别的缓存，是一直开启的，我们关闭不了它；</p>
<p>一级缓存失效情况：没有使用到当前的一级缓存，效果就是，还需要再向数据库中发起一次查询请求！</p>
<p>1、sqlSession不同。<strong>每个sqlSession中的缓存相互独立</strong></p>
<p>2、sqlSession相同，查询条件不同。<strong>当前缓存中，不存在这个数据</strong></p>
<p>3、sqlSession相同，两次查询之间执行了增删改操作！<strong>因为增删改操作可能会对当前数据产生影响</strong></p>
<p>4、sqlSession相同，手动清除一级缓存。session.clearCache();</p>
<h3 id="0262-二级缓存">0.26.2. 二级缓存</h3>
<ul>
<li>
<p>二级缓存也叫全局缓存，一级缓存作用域太低了，所以诞生了二级缓存</p>
</li>
<li>
<p>基于namespace级别的缓存，一个名称空间，对应一个二级缓存；</p>
</li>
<li>
<p>工作机制</p>
</li>
<li>
<ul>
<li>一个会话查询一条数据，这个数据就会被放在当前会话的一级缓存中；</li>
<li>如果当前会话关闭了，这个会话对应的一级缓存就没了；但是我们想要的是，会话关闭了，一级缓存中的数据被保存到二级缓存中；</li>
<li>新的会话查询信息，就可以从二级缓存中获取内容；</li>
<li>不同的mapper查出的数据会放在自己对应的缓存（map）中；</li>
</ul>
</li>
</ul>
<p><strong>总结</strong></p>
<ul>
<li>只要开启了二级缓存，我们在同一个Mapper中的查询，可以在二级缓存中拿到数据</li>
<li>查出的数据都会被默认先放在一级缓存中</li>
<li>只有会话提交或者关闭以后，一级缓存中的数据才会转到二级缓存中</li>
<li>查询时顺序：二级 -&gt; 一级 -&gt; 数据库</li>
<li>对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。</li>
</ul>
<figure data-type="image" tabindex="12"><img src="https://mmbiz.qpic.cn/mmbiz_png/uJDAUKrGC7KickRVspms8t4ZU0jXovPT2egdNicaJuVnzMYxibyYFvB0COWW4sgDhHPqvFbG9F9KS1vX7ibIMNqefg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片" loading="lazy"></figure>
<h2 id="027-和的区别是什么">0.27. 、#{}和${}的区别是什么？</h2>
<p>注：这道题是面试官面试我同事的。</p>
<p>答：</p>
<ul>
<li><code>${}</code>是 Properties 文件中的变量占位符，它可以用于标签属性值和 sql 内部，属于静态文本替换，比如${driver}会被静态替换为<code>com.mysql.jdbc.Driver</code>。</li>
<li><code>#{}</code>是 sql 的参数占位符，MyBatis 会将 sql 中的<code>#{}</code>替换为?号，在 sql 执行前会使用 PreparedStatement 的参数设置方法，按序给 sql 的?号占位符设置参数值，比如 ps.setInt(0, parameterValue)，<code>#{item.name}</code> 的取值方式为使用反射从参数对象中获取 item 对象的 name 属性值，相当于 <code>param.getItem().getName()</code>。</li>
</ul>
<h2 id="028-xml-映射文件中除了常见的-selectinsertupdatedelete-标签之外还有哪些标签">0.28. 、Xml 映射文件中，除了常见的 select|insert|update|delete 标签之外，还有哪些标签？</h2>
<p>注：这道题是京东面试官面试我时问的。</p>
<p>答：还有很多其他的标签，<code>&lt;resultMap&gt;</code>、<code>&lt;parameterMap&gt;</code>、<code>&lt;sql&gt;</code>、<code>&lt;include&gt;</code>、<code>&lt;selectKey&gt;</code>，加上动态 sql 的 9 个标签，<code>trim|where|set|foreach|if|choose|when|otherwise|bind</code>等，其中<sql>为 sql 片段标签，通过<code>&lt;include&gt;</code>标签引入 sql 片段，<code>&lt;selectKey&gt;</code>为不支持自增的主键生成策略标签。</p>
<h2 id="029-最佳实践中通常一个-xml-映射文件都会写一个-dao-接口与之对应请问这个-dao-接口的工作原理是什么dao-接口里的方法参数不同时方法能重载吗">0.29. 、最佳实践中，通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应，请问，这个 Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？</h2>
<p>注：这道题也是京东面试官面试我被问的。</p>
<p>答：Dao 接口，就是人们常说的 <code>Mapper</code>接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中<code>MappedStatement</code>的 id 值，接口方法内的参数，就是传递给 sql 的参数。<code>Mapper</code>接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为 key 值，可唯一定位一个<code>MappedStatement</code>，举例：<code>com.mybatis3.mappers.StudentDao.findStudentById</code>，可以唯一找到 namespace 为<code>com.mybatis3.mappers.StudentDao</code>下面<code>id = findStudentById</code>的<code>MappedStatement</code>。在 MyBatis 中，每一个<code>&lt;select&gt;</code>、<code>&lt;insert&gt;</code>、<code>&lt;update&gt;</code>、<code>&lt;delete&gt;</code>标签，都会被解析为一个<code>MappedStatement</code>对象。</p>
<p><s>Dao 接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。</s></p>
<p>Dao 接口里的方法可以重载，但是Mybatis的XML里面的ID不允许重复。</p>
<p>Mybatis版本3.3.0，亲测如下：</p>
<pre><code class="language-java">/** * Mapper接口里面方法重载 */public interface StuMapper {	List&lt;Student&gt; getAllStu();    	List&lt;Student&gt; getAllStu(@Param(&quot;id&quot;) Integer id);}
</code></pre>
<p>然后在 <code>StuMapper.xml</code> 中利用Mybatis的动态sql就可以实现。</p>
<pre><code class="language-java">	&lt;select id=&quot;getAllStu&quot; resultType=&quot;com.pojo.Student&quot;&gt; 		select * from student		&lt;where&gt;			&lt;if test=&quot;id != null&quot;&gt;				id = #{id}			&lt;/if&gt;		&lt;/where&gt; 	&lt;/select&gt;
</code></pre>
<p>能正常运行，并能得到相应的结果，这样就实现了在Dao接口中写重载方法。</p>
<p><strong>Mybatis 的 Dao 接口可以有多个重载方法，但是多个接口对应的映射必须只有一个，否则启动会报错。</strong></p>
<p>相关 issue ：<a href="https://github.com/Snailclimb/JavaGuide/issues/1122">更正：Dao 接口里的方法可以重载，但是Mybatis的XML里面的ID不允许重复！</a>。</p>
<p>Dao 接口的工作原理是 JDK 动态代理，MyBatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行<code>MappedStatement</code>所代表的 sql，然后将 sql 执行结果返回。</p>
<h3 id="0291-补充">0.29.1. <mark>补充：</mark></h3>
<p>Dao接口方法可以重载，但是需要满足以下条件：</p>
<ol>
<li>仅有一个无参方法和一个有参方法</li>
<li>多个有参方法时，参数数量必须一致。且使用相同的 <code>@Param</code> ，或者使用 <code>param1</code> 这种</li>
</ol>
<p>测试如下：</p>
<p><code>PersonDao.java</code></p>
<pre><code class="language-java">Person queryById();Person queryById(@Param(&quot;id&quot;) Long id);Person queryById(@Param(&quot;id&quot;) Long id, @Param(&quot;name&quot;) String name);
</code></pre>
<p><code>PersonMapper.xml</code></p>
<pre><code class="language-xml">&lt;select id=&quot;queryById&quot; resultMap=&quot;PersonMap&quot;&gt;    select      id, name, age, address    from person    &lt;where&gt;        &lt;if test=&quot;id != null&quot;&gt;            id = #{id}        &lt;/if&gt;        &lt;if test=&quot;name != null and name != ''&quot;&gt;            name = #{name}        &lt;/if&gt;    &lt;/where&gt;    limit 1&lt;/select&gt;
</code></pre>
<p><code>org.apache.ibatis.scripting.xmltags.DynamicContext.ContextAccessor#getProperty</code>方法用于获取<code>&lt;if&gt;</code>标签中的条件值</p>
<pre><code class="language-java">public Object getProperty(Map context, Object target, Object name) {  Map map = (Map) target;  Object result = map.get(name);  if (map.containsKey(name) || result != null) {    return result;  }  Object parameterObject = map.get(PARAMETER_OBJECT_KEY);  if (parameterObject instanceof Map) {    return ((Map)parameterObject).get(name);  }  return null;}
</code></pre>
<p><code>parameterObject</code>为map，存放的是Dao接口中参数相关信息。</p>
<p><code>((Map)parameterObject).get(name)</code>方法如下</p>
<pre><code class="language-java">public V get(Object key) {  if (!super.containsKey(key)) {    throw new BindingException(&quot;Parameter '&quot; + key + &quot;' not found. Available parameters are &quot; + keySet());  }  return super.get(key);}
</code></pre>
<ol>
<li><code>queryById()</code>方法执行时，<code>parameterObject</code>为null，<code>getProperty</code>方法返回null值，<code>&lt;if&gt;</code>标签获取的所有条件值都为null，所有条件不成立，动态sql可以正常执行。</li>
<li><code>queryById(1L)</code>方法执行时，<code>parameterObject</code>为map，包含了<code>id</code>和<code>param1</code>两个key值。当获取<code>&lt;if&gt;</code>标签中<code>name</code>的属性值时，进入<code>((Map)parameterObject).get(name)</code>方法中，map中key不包含<code>name</code>，所以抛出异常。</li>
<li><code>queryById(1L,&quot;1&quot;)</code>方法执行时，<code>parameterObject</code>中包含<code>id</code>,<code>param1</code>,<code>name</code>,<code>param2</code>四个key值，<code>id</code>和<code>name</code>属性都可以获取到，动态sql正常执行。</li>
</ol>
<h2 id="030-mybatis-是如何进行分页的分页插件的原理是什么">0.30. 、MyBatis 是如何进行分页的？分页插件的原理是什么？</h2>
<p>注：我出的。</p>
<p>答：<strong>(1)</strong> MyBatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页；<strong>(2)</strong> 可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，<strong>(3)</strong> 也可以使用分页插件来完成物理分页。</p>
<p>分页插件的基本原理是使用 MyBatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。</p>
<p>举例：<code>select _ from student</code>，拦截 sql 后重写为：<code>select t._ from （select \* from student）t limit 0，10</code></p>
<h2 id="031-简述-mybatis-的插件运行原理以及如何编写一个插件">0.31. 、简述 MyBatis 的插件运行原理，以及如何编写一个插件。</h2>
<p>注：我出的。</p>
<p>答：MyBatis 仅可以编写针对 <code>ParameterHandler</code>、<code>ResultSetHandler</code>、<code>StatementHandler</code>、<code>Executor</code> 这 4 种接口的插件，MyBatis 使用 JDK 的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 <code>InvocationHandler</code> 的 <code>invoke()</code>方法，当然，只会拦截那些你指定需要拦截的方法。</p>
<p>实现 MyBatis 的 Interceptor 接口并复写<code> intercept()</code>方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。</p>
<h2 id="032-mybatis-执行批量插入能返回数据库主键列表吗">0.32. 、MyBatis 执行批量插入，能返回数据库主键列表吗？</h2>
<p>注：我出的。</p>
<p>答：能，JDBC 都能，MyBatis 当然也能。</p>
<h2 id="033-mybatis-动态-sql-是做什么的都有哪些动态-sql能简述一下动态-sql-的执行原理不">0.33. 、MyBatis 动态 sql 是做什么的？都有哪些动态 sql？能简述一下动态 sql 的执行原理不？</h2>
<p>注：我出的。</p>
<p>答：MyBatis 动态 sql 可以让我们在 Xml 映射文件内，以标签的形式编写动态 sql，完成逻辑判断和动态拼接 sql 的功能，MyBatis 提供了 9 种动态 sql 标签 <code>trim|where|set|foreach|if|choose|when|otherwise|bind</code>。</p>
<p>其执行原理为，使用 OGNL 从 sql 参数对象中计算表达式的值，根据表达式的值动态拼接 sql，以此来完成动态 sql 的功能。</p>
<h2 id="034-mybatis-是如何将-sql-执行结果封装为目标对象并返回的都有哪些映射形式">0.34. 、MyBatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？</h2>
<p>注：我出的。</p>
<p>答：第一种是使用<code>&lt;resultMap&gt;</code>标签，逐一定义列名和对象属性名之间的映射关系。第二种是使用 sql 列的别名功能，将列别名书写为对象属性名，比如 T_NAME AS NAME，对象属性名一般是 name，小写，但是列名不区分大小写，MyBatis 会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成 T_NAME AS NaMe，MyBatis 一样可以正常工作。</p>
<p>有了列名与属性名的映射关系后，MyBatis 通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。</p>
<h2 id="035-mybatis-能执行一对一-一对多的关联查询吗都有哪些实现方式以及它们之间的区别">0.35. 、MyBatis 能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别。</h2>
<p>注：我出的。</p>
<p>答：能，MyBatis 不仅可以执行一对一、一对多的关联查询，还可以执行多对一，多对多的关联查询，多对一查询，其实就是一对一查询，只需要把 <code>selectOne()</code>修改为 <code>selectList()</code>即可；多对多查询，其实就是一对多查询，只需要把 <code>selectOne()</code>修改为 <code>selectList()</code>即可。</p>
<p>关联对象查询，有两种实现方式，一种是单独发送一个 sql 去查询关联对象，赋给主对象，然后返回主对象。另一种是使用嵌套查询，嵌套查询的含义为使用 join 查询，一部分列是 A 对象的属性值，另外一部分列是关联对象 B 的属性值，好处是只发一个 sql 查询，就可以把主对象和其关联对象查出来。</p>
<p>那么问题来了，join 查询出来 100 条记录，如何确定主对象是 5 个，而不是 100 个？其去重复的原理是<code>&lt;resultMap&gt;</code>标签内的<code>&lt;id&gt;</code>子标签，指定了唯一确定一条记录的 id 列，MyBatis 根据<id>列值来完成 100 条记录的去重复功能，<code>&lt;id&gt;</code>可以有多个，代表了联合主键的语意。</p>
<p>同样主对象的关联对象，也是根据这个原理去重复的，尽管一般情况下，只有主对象会有重复记录，关联对象一般不会重复。</p>
<p>举例：下面 join 查询出来 6 条记录，一、二列是 Teacher 对象列，第三列为 Student 对象列，MyBatis 去重复处理后，结果为 1 个老师 6 个学生，而不是 6 个老师 6 个学生。</p>
<table>
<thead>
<tr>
<th>t_id</th>
<th>t_name</th>
<th>s_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>teacher</td>
<td>38</td>
</tr>
<tr>
<td>1</td>
<td>teacher</td>
<td>39</td>
</tr>
<tr>
<td>1</td>
<td>teacher</td>
<td>40</td>
</tr>
<tr>
<td>1</td>
<td>teacher</td>
<td>41</td>
</tr>
<tr>
<td>1</td>
<td>teacher</td>
<td>42</td>
</tr>
<tr>
<td>1</td>
<td>teacher</td>
<td>43</td>
</tr>
</tbody>
</table>
<h2 id="036-mybatis-是否支持延迟加载如果支持它的实现原理是什么">0.36. 、MyBatis 是否支持延迟加载？如果支持，它的实现原理是什么？</h2>
<p>注：我出的。</p>
<p>答：MyBatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 MyBatis 配置文件中，可以配置是否启用延迟加载 <code>lazyLoadingEnabled=true|false。</code></p>
<p>它的原理是，使用<code> CGLIB</code> 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 <code>a.getB().getName()</code>，拦截器 <code>invoke()</code>方法发现 <code>a.getB()</code>是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 <code>a.getB().getName()</code>方法的调用。这就是延迟加载的基本原理。</p>
<p>当然了，不光是 MyBatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。</p>
<h2 id="037-mybatis-的-xml-映射文件中不同的-xml-映射文件id-是否可以重复">0.37. 、MyBatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复？</h2>
<p>注：我出的。</p>
<p>答：不同的 Xml 映射文件，如果配置了 namespace，那么 id 可以重复；如果没有配置 namespace，那么 id 不能重复；毕竟 namespace 不是必须的，只是最佳实践而已。</p>
<p>原因就是 namespace+id 是作为 <code>Map&lt;String, MappedStatement&gt;</code>的 key 使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然也就不同。</p>
<h2 id="038-mybatis-中如何执行批处理">0.38. 、MyBatis 中如何执行批处理？</h2>
<p>注：我出的。</p>
<p>答：使用 BatchExecutor 完成批处理。</p>
<h2 id="039-mybatis-都有哪些-executor-执行器它们之间的区别是什么">0.39. 、MyBatis 都有哪些 Executor 执行器？它们之间的区别是什么？</h2>
<p>注：我出的</p>
<p>答：MyBatis 有三种基本的 Executor 执行器，<strong><code>SimpleExecutor</code>、<code>ReuseExecutor</code>、<code>BatchExecutor</code>。</strong></p>
<p>**<code>SimpleExecutor</code>：**每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。</p>
<p>**<code>ReuseExecutor</code>：**执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map&lt;String, Statement&gt;内，供下一次使用。简言之，就是重复使用 Statement 对象。</p>
<p>**<code>BatchExecutor</code>：**执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。</p>
<p>作用范围：Executor 的这些特点，都严格限制在 SqlSession 生命周期范围内。</p>
<h2 id="040-mybatis-中如何指定使用哪一种-executor-执行器">0.40. 、MyBatis 中如何指定使用哪一种 Executor 执行器？</h2>
<p>注：我出的</p>
<p>答：在 MyBatis 配置文件中，可以指定默认的 ExecutorType 执行器类型，也可以手动给 <code>DefaultSqlSessionFactory</code> 的创建 SqlSession 的方法传递 ExecutorType 类型参数。</p>
<h2 id="041-mybatis-是否可以映射-enum-枚举类">0.41. 、MyBatis 是否可以映射 Enum 枚举类？</h2>
<p>注：我出的</p>
<p>答：MyBatis 可以映射枚举类，不单可以映射枚举类，MyBatis 可以映射任何对象到表的一列上。映射方式为自定义一个 <code>TypeHandler</code>，实现 <code>TypeHandler</code> 的 <code>setParameter()</code>和 <code>getResult()</code>接口方法。<code>TypeHandler</code> 有两个作用，一是完成从 javaType 至 jdbcType 的转换，二是完成 jdbcType 至 javaType 的转换，体现为 <code>setParameter()</code>和 <code>getResult()</code>两个方法，分别代表设置 sql 问号占位符参数和获取列查询结果。</p>
<h2 id="042-mybatis-映射文件中如果-a-标签通过-include-引用了-b-标签的内容请问b-标签能否定义在-a-标签的后面还是说必须定义在-a-标签的前面">0.42. 、MyBatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？</h2>
<p>注：我出的</p>
<p>答：虽然 MyBatis 解析 Xml 映射文件是按照顺序解析的，但是，被引用的 B 标签依然可以定义在任何地方，MyBatis 都可以正确识别。</p>
<p>原理是，MyBatis 解析 A 标签，发现 A 标签引用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时，MyBatis 会将 A 标签标记为未解析状态，然后继续解析余下的标签，包含 B 标签，待所有标签解析完毕，MyBatis 会重新解析那些被标记为未解析的标签，此时再解析 A 标签时，B 标签已经存在，A 标签也就可以正常解析完成了。</p>
<h2 id="043-简述-mybatis-的-xml-映射文件和-mybatis-内部数据结构之间的映射关系">0.43. 、简述 MyBatis 的 Xml 映射文件和 MyBatis 内部数据结构之间的映射关系？</h2>
<p>注：我出的</p>
<p>答：MyBatis 将所有 Xml 配置信息都封装到 All-In-One 重量级对象 Configuration 内部。在 Xml 映射文件中，<code>&lt;parameterMap&gt;</code>标签会被解析为 <code>ParameterMap</code> 对象，其每个子元素会被解析为 ParameterMapping 对象。<code>&lt;resultMap&gt;</code>标签会被解析为 <code>ResultMap</code> 对象，其每个子元素会被解析为 <code>ResultMapping</code> 对象。每一个<code>&lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt;</code>标签均会被解析为 <code>MappedStatement</code> 对象，标签内的 sql 会被解析为 BoundSql 对象。</p>
<h2 id="044-为什么说-mybatis-是半自动-orm-映射工具它与全自动的区别在哪里">0.44. 、为什么说 MyBatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？</h2>
<p>注：我出的</p>
<p>答：Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而 MyBatis 在查询关联对象或关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自动 ORM 映射工具。</p>
<p>面试题看似都很简单，但是想要能正确回答上来，必定是研究过源码且深入的人，而不是仅会使用的人或者用的很熟的人，以上所有面试题及其答案所涉及的内容，在我的 MyBatis 系列博客中都有详细讲解和原理分析。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpringBoot]]></title>
        <id>https://memorykki.github.io/SpringBoot/</id>
        <link href="https://memorykki.github.io/SpringBoot/">
        </link>
        <updated>2021-08-12T02:49:13.000Z</updated>
        <summary type="html"><![CDATA[<p>Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。</p>
<!-- more -->
<h2 id="springboot">SpringBoot</h2>
<p>创建独立Spring应用</p>
<p>内嵌web服务器</p>
<p>自动starter依赖，简化构建配置</p>
<p>自动配置Spring以及第三方功能</p>
<p>提供生产级别的监控、健康检查及外部化配置</p>
<p>无代码生成、无需编写XML</p>
<p>避免大量的 Maven 导入和各种版本冲突。</p>
<h2 id="springbootapplication">@SpringBootApplication</h2>
<p>@SpringBootApplication == @SpringBootConfiguration + @EnableAutoConfiguration + @ComponentScan</p>
<p>主程序所在包及其下面的所有子包里面的组件都会被默认扫描进来</p>
<h2 id="configuration">@Configuration</h2>
<p>配置类本身也是组件</p>
<ul>
<li>Full模式：配置类组件之间<strong>有依赖</strong>关系，方法会被调用得到之前单实例组件，用Full模式；proxyBeanMethods = true</li>
<li>Lite模式：配置类组件之间<strong>无依赖</strong>关系用Lite模式加速容器启动过程，减少判断；proxyBeanMethods = false</li>
</ul>
<p>@Bean、@Component、@Controller、@Service、@Repository、@ComponentScan、@Import</p>
<p>@Import：自动以类的NoArgsConstrutor创建实例放入IoC</p>
<p>@Conditional</p>
<p>@ConditionalOnMissingBean(name = &quot;tom&quot;)：满足指定条件则进行组件注入。</p>
<p>@ImportResource：引入原生的配置文件，比如beans.xml。</p>
<h2 id="配置绑定">配置绑定</h2>
<p>两种方式：</p>
<ul>
<li>@Component @ConfigurationProperties(prefix = &quot;mycar&quot;)声明在要绑定的类的上方</li>
<li>@EnableConfigurationProperties(Car.class)，开启对应类的配置绑定功能，把Car这个组件自动注入到容器中；</li>
</ul>
<p>如果@ConfigurationProperties是在第三方包中，那么@component是不能注入到容器的。只有@EnableConfigurationProperties才可以注入到容器。</p>
<p>@ConfigurationProperties(prefix = &quot;mycar&quot;)：配置绑定。读取到properties文件中的内容，并且把它封装到JavaBean中。</p>
<h2 id="自动配置原理">自动配置原理</h2>
<p>@SpringBootApplication == @SpringBootConfiguration + @EnableAutoConfiguration + @ComponentScan</p>
<h3 id="springbootconfiguration">@SpringBootConfiguration</h3>
<p>相当于@Configuration， 两者之间的唯一区别是@SpringBootConfiguration允许自动找到配置。</p>
<h3 id="enableautoconfiguration">@EnableAutoConfiguration</h3>
<p>@EnableAutoConfiguration == @AutoConfigurationPackage + @Import(AutoConfigurationImportSelector.class)</p>
<p><strong>@AutoConfigurationPackage</strong></p>
<p>使用@import将AutoConfigurationPackages包下的Registrar类作为组件导入到容器中，然后使用Registrar中的方法批量完成组件的注册。</p>
<p><strong>@Import(AutoConfigurationImportSelector.class)</strong></p>
<pre><code class="language-java">1、利用getAutoConfigurationEntry(annotationMetadata);给容器中批量导入一些组件 
2、调用List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes)获取到所有需要导入到容器中的配置类 
3、利用工厂加载 Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader)；得到所有的组件 
4、**从META-INF/spring.factories位置来加载一个文件。** 默认扫描我们当前系统里面所有META-INF/spring.factories位置的文件 
文件里面写死了spring-boot一启动就要给容器中加载的127个配置类
</code></pre>
<p>虽然我们127个场景的所有自动配置启动的时候默认全部加载。xxxxAutoConfiguration 按照条件装配规则（@Conditional），最终会<strong>按需配置</strong>。</p>
<p>SpringBoot默认会在底层配好所有的组件。但是如果用户自己配置了<strong>以用户的优先</strong>，<strong>约定大于配置</strong>。</p>
<pre><code class="language-java">@ConditionalOnMissingBean，如果没有存在这个bean，那么springboot就会自动帮你配置
</code></pre>
<h3 id="总结">总结</h3>
<p>SpringBoot先加载所有的自动配置类  xxxxxAutoConfiguration</p>
<ul>
<li>
<p>每个自动配置类按照条件进行生效，默认都会绑定配置文件指定的值。xxxxProperties里面拿。xxxProperties和配置文件进行了绑定</p>
</li>
<li>
<p>生效的配置类就会给容器中装配很多组件</p>
</li>
<li>
<p>只要容器中有这些组件，相当于这些功能就有了</p>
</li>
<li>
<p>定制化配置</p>
<ul>
<li>
<p>用户直接自己@Bean替换底层的组件</p>
</li>
<li>
<p>用户去看这个组件是获取的配置文件什么值就去修改。</p>
<p>xxxxxAutoConfiguration ---&gt; 组件  ---&gt; xxxxProperties里面拿值  ----&gt; application.properties</p>
</li>
</ul>
</li>
</ul>
<h2 id="静态资源配置原理"><strong>静态资源配置原理</strong></h2>
<p>SpringBoot启动默认加载  xxxAutoConfiguration 类（自动配置类）<br>
SpringMVC功能的自动配置类 WebMvcAutoConfiguration，生效。</p>
<p>请求 -&gt; Controller -&gt; 静态资源处理器 -&gt; 404</p>
<p>配置文件的相关属性和xxx进行了绑定。WebMvcProperties - spring.mvc、ResourceProperties - spring.resources</p>
<h2 id="rest">REST</h2>
<ul>
<li>
<p>表单提交会带上_method=PUT</p>
</li>
<li>
<p>请求过来被HiddenHttpMethodFilter拦截</p>
<ul>
<li>
<p>请求是否正常，并且是POST</p>
<ul>
<li>
<p>获取到_method的值。</p>
</li>
<li>
<p>兼容以下请求；PUT.DELETE.PATCH</p>
</li>
<li>
<p>原生request（post），包装模式requesWrapper重写了getMethod方法，返回的是传入的值。</p>
</li>
<li>
<p>过滤器链放行的时候用wrapper。以后的方法调用getMethod是调用requesWrapper的。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="请求映射原理">请求映射原理</h2>
<p>1、处理http，重写servlet、doGet、doPost，实际调用doService</p>
<p>2、所有的请求映射都在HandlerMapping中。RequestMappingHandlerMapping：保存了所有@RequestMapping 和handler的映射规则。</p>
<p>3、请求进来，挨个尝试所有的HandlerMapping看是否有请求信息。</p>
<ul>
<li>如果有就找到这个请求对应的handler</li>
<li>如果没有就是下一个 HandlerMapping</li>
</ul>
<h2 id="请求参数注解">请求参数注解</h2>
<p>@PathVariable、@RequestHeader、@ModelAttribute、@RequestParam、@MatrixVariable、@CookieValue、@RequestBody</p>
<h2 id="参数处理原理">参数处理原理</h2>
<p>HandlerMapping中找到能处理请求的Handler<br>
为当前Handler 找一个适配器 HandlerAdapter<br>
适配器执行目标方法并确定方法参数的每一个值</p>
<p><strong>参数解析器-HandlerMethodArgumentResolver</strong></p>
<p>SpringMVC目标方法能写多少种参数类型。取决于26个参数解析器。</p>
<p>●当前解析器是否支持解析这种参数<br>
●支持就调用 resolveArgument</p>
<p><strong>返回值处理器</strong></p>
<p>15个遍历</p>
<p><strong>handlerMapping中找到适合的handler(1/4) -&gt; 为Handler找一个HandlerAdapter -&gt; 参数处理（参数解析器）-&gt; 执行目标方法 -&gt; 返回值处理器 -&gt; 处理派发结果（将所有的数据都放在 ModelAndViewContainer；包含要去的页面地址View。还包含Model数据）</strong></p>
<h2 id="pojo封装参数">POJO封装参数</h2>
<p>1、判断参数是不是简单参数</p>
<p>2、创建空的实体类</p>
<p>3、绑定数据</p>
<p>4、类型转换器 -&gt; javaBean</p>
<h2 id="响应json">响应JSON</h2>
<ul>
<li>返回值处理器判断是否支持这种类型返回值 supportsReturnType</li>
<li>返回值处理器调用 handleReturnValue 进行处理</li>
<li>RequestResponseBodyMethodProcessor 可以处理返回值标了@ResponseBody 注解的。
<ul>
<li>利用 MessageConverters 进行处理 将数据写为json
<ul>
<li>1、内容协商（浏览器默认会以请求头的方式告诉服务器他能<strong>接受什么样的内容类型</strong>）（Accept字段）</li>
<li>2、服务器最终根据自己自身的能力，决定服务器能<strong>生产出什么样内容类型</strong>的数据，</li>
<li>3、SpringMVC会挨个遍历所有容器底层的 HttpMessageConverter ，看谁能处理？
<ul>
<li>1、得到Mapping<strong>Jackson</strong>2HttpMessageConverter可以将对象写为json</li>
<li>2、利用MappingJackson2HttpMessageConverter将对象转为json再写出去。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="内容协商">内容协商</h2>
<p>根据客户端接收能力不同，返回不同媒体类型的数据。</p>
<ul>
<li>
<p>1、判断当前响应头中是否已经有确定的媒体类型。MediaType</p>
</li>
<li>
<p>2、获取客户端（PostMan、浏览器）支持接收的内容类型。（获取客户端Accept请求头字段）【application/xml】</p>
</li>
</ul>
<p><strong>○</strong>contentNegotiationManager 内容协商管理器 默认使用基于请求头的策略</p>
<ul>
<li>
<p>3、遍历循环所有当前系统的 MessageConverter，看谁支持操作这个对象（Person）</p>
</li>
<li>
<p>4、找到支持操作Person的converter，把converter支持的媒体类型统计出来。</p>
</li>
<li>
<p>5、客户端需要【application/xml】。服务端能力【10种、json、xml】</p>
</li>
<li>
<p>6、进行内容协商的最佳匹配媒体类型</p>
</li>
<li>
<p>7、用 支持 将对象转为 最佳匹配媒体类型 的converter。调用它进行转化 。</p>
</li>
</ul>
<h2 id="视图解析">视图解析</h2>
<p>1、目标方法处理的过程中，所有数据都会被放在 ModelAndViewContainer 里面。包括数据和视图地址<br>
2、方法的参数是一个自定义类型对象（从请求参数中确定的），把他重新放在 ModelAndViewContainer<br>
3、任何目标方法执行完成以后都会返回 ModelAndView（数据和视图地址）。<br>
4、processDispatchResult  处理派发结果（页面改如何响应）</p>
<ul>
<li>1、render(mv, request, response); 进行页面渲染逻辑<br>
○1、根据方法的String返回值得到 View 对象【定义了页面的渲染逻辑】
<ul>
<li>1、所有的视图解析器尝试是否能根据当前返回值得到View对象</li>
<li>2、得到了  redirect:/main.html --&gt; Thymeleaf new RedirectView()</li>
<li>3、ContentNegotiationViewResolver 里面包含了下面所有的视图解析器，内部还是利用下面所有视图解析器得到视图对象。</li>
<li>4、view.render(mv.getModelInternal(), request, response);   视图对象调用自定义的render进行页面渲染工作</li>
<li>RedirectView 如何渲染【重定向到一个页面】</li>
<li>1、获取目标url地址</li>
<li>2、response.sendRedirect(encodedURL);</li>
</ul>
</li>
</ul>
<h2 id="拦截器">拦截器</h2>
<p>implements HandlerInterceptor，重写preHandle、postHandle、afterCompletion。</p>
<p>1、根据当前请求，找到HandlerExecutionChain【可以处理请求的handler以及handler的所有 拦截器】<br>
2、先来顺序执行 所有拦截器的 preHandle方法</p>
<ul>
<li>1、如果当前拦截器prehandler返回为true。则执行下一个拦截器的preHandle</li>
<li>2、如果当前拦截器返回为false。直接倒序执行所有已经执行了的拦截器的  afterCompletion；</li>
</ul>
<p>3、如果任何一个拦截器返回false。直接跳出不执行目标方法<br>
4、所有拦截器都返回True。执行目标方法<br>
5、倒序执行所有拦截器的postHandle方法<br>
6、前面的步骤有任何异常都会直接倒序触发 afterCompletion<br>
7、页面成功渲染完成以后，也会倒序触发 afterCompletion</p>
<img src="https://cdn.nlark.com/yuque/0/2020/png/1354552/1605765121071-64cfc649-4892-49a3-ac08-88b52fb4286f.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_35%2Ctext_YXRndWlndS5jb20g5bCa56GF6LC3%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10" alt="image.png" style="zoom:67%;" />
<h2 id="spring-security-和-shiro">Spring Security 和 Shiro</h2>
<p>由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，包括 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。当然这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。Shiro 和 Spring Security 相比，主要有如下一些特点：</p>
<p>Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架<br>
Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单<br>
Spring Security 功能强大；Shiro 功能简单</p>
<h2 id="跨域问题">跨域问题</h2>
<p>跨域可以在前端通过 JSONP 来解决，但是 JSONP 只可以发送 GET 请求，无法发送其他类型的请求，在 RESTful 风格的应用中，就显得非常鸡肋，因此我们推荐在后端通过 （CORS，Cross-origin resource sharing） 来解决跨域问题。</p>
<p>在传统的 SSM 框架中，就可以通过 CORS 来解决跨域问题，只不过之前我们是在 XML 文件中配置 CORS ，现在可以通过实现WebMvcConfigurer接口然后重写addCorsMappings方法解决跨域问题。</p>
<h2 id="starter">starter</h2>
<p>首先它提供了一个自动化配置类，一般命名为 XXXAutoConfiguration ，在这个配置类中通过条件注解来决定一个配置是否生效（条件注解就是 Spring 中原本就有的），然后它还会提供一系列的默认配置，也允许开发者根据实际情况自定义相关配置，然后通过类型安全的属性注入将这些配置属性注入进来，新注入的属性会代替掉默认属性。正因为如此，很多第三方框架，我们只需要引入依赖就可以直接使用了。当然，开发者也可以自定义 Starter</p>
<h2 id="spring-boot-starter-parent">spring-boot-starter-parent</h2>
<p>我们都知道，新创建一个 Spring Boot 项目，默认都是有 parent 的，这个 parent 就是 spring-boot-starter-parent ，spring-boot-starter-parent 主要有如下作用：</p>
<p>定义了 Java 编译版本为 1.8 。<br>
使用 UTF-8 格式编码。<br>
继承自 spring-boot-dependencies，这个里边定义了依赖的版本，也正是因为继承了这个依赖，所以我们在写依赖时才不需要写版本号。<br>
执行打包操作的配置。<br>
自动化的资源过滤。<br>
自动化的插件配置。<br>
针对 application.properties 和 application.yml 的资源过滤，包括通过 profile 定义的不同环境的配置文件，例如 application-dev.properties 和 application-dev.yml。</p>
<h2 id="spring-boot-打成的-jar-和普通的-ja">Spring Boot 打成的 jar 和普通的 ja</h2>
<p>Spring Boot 项目最终打包成的 jar 是可执行 jar ，这种 jar 可以直接通过 java -jar xxx.jar 命令来运行，这种 jar 不可以作为普通的 jar 被其他项目依赖，即使依赖了也无法使用其中的类。</p>
<p>Spring Boot 的 jar 无法被其他项目依赖，主要还是他和普通 jar 的结构不同。普通的 jar 包，解压后直接就是包名，包里就是我们的代码，而 Spring Boot 打包成的可执行 jar 解压后，在 \BOOT-INF\classes 目录下才是我们的代码，因此无法被直接引用。如果非要引用，可以在 pom.xml 文件中增加配置，将 Spring Boot 项目打包成两个 jar ，一个可执行，一个可引用。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpringMVC]]></title>
        <id>https://memorykki.github.io/SpringMVC/</id>
        <link href="https://memorykki.github.io/SpringMVC/">
        </link>
        <updated>2021-08-11T02:49:13.000Z</updated>
        <summary type="html"><![CDATA[<p>Spring MVC 是 Spring 提供的一个基于 MVC 设计模式的轻量级 Web 开发框架,本质上相当于 Servlet。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Spring MVC 是 Spring 提供的一个基于 MVC 设计模式的轻量级 Web 开发框架,本质上相当于 Servlet。</p>
<!-- more -->
<h2 id="springmvc">SpringMVC</h2>
<p>Spring MVC是一个基于Java的实现了MVC设计模式的请求驱动类型的轻量级Web框架，通过把模型-视图-控制器分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。</p>
<h2 id="核心组件">核心组件</h2>
<ul>
<li>
<p>前端控制器 DispatcherServlet（不需要程序员开发）：处理所有的HTTP请求和响应，减少了其它组件之间的耦合度。</p>
</li>
<li>
<p>处理器映射器HandlerMapping（不需要程序员开发）：根据请求的URL来查找Handler</p>
</li>
<li>
<p>处理器适配器HandlerAdapter</p>
</li>
<li>
<p>处理器Handler（需要程序员开发）</p>
</li>
<li>
<p>视图解析器 ViewResolver（不需要程序员开发）：进行视图的解析，根据视图逻辑名解析成真正的视图（view）</p>
</li>
<li>
<p>视图View（需要程序员开发jsp）：View是一个接口， 它的实现类支持不同的视图类型（jsp，freemarker，pdf等等）</p>
</li>
</ul>
<h2 id="控制器">控制器</h2>
<p>解析用户输入并将其转换为一个由视图呈现给用户的模型。Spring用一个非常抽象的方式实现了一个控制层，允许用户创建多种用途的控制器。</p>
<p><strong>单例模式</strong>，所以在多线程访问的时候有线程安全问题，同步会影响性能，解决方案是在控制器里面不能写字段。</p>
<h2 id="工作原理">工作原理</h2>
<p><strong>request -&gt; DispatcherServlet -&gt; HandlerMapping -&gt; HandlerAdapter -&gt; Handler -&gt; ModelAndView -&gt; ViewResolver -&gt; View -&gt; reponse</strong></p>
<p>（1）用户发送请求至前端控制器DispatcherServlet；<br>
（2） DispatcherServlet收到请求后，调用HandlerMapping处理器映射器，请求获取Handle；<br>
（3）处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet；<br>
（4）DispatcherServlet 调用 HandlerAdapter处理器适配器；<br>
（5）HandlerAdapter 经过适配调用 具体处理器(<strong>Handler</strong>，也叫后端控制器)；<br>
（6）Handler执行完成返回<strong>ModelAndView</strong>；<br>
（7）HandlerAdapter将Handler执行结果ModelAndView返回给DispatcherServlet；<br>
（8）DispatcherServlet将ModelAndView传给ViewResolver视图解析器进行解析；<br>
（9）ViewResolver解析后返回具体<strong>View</strong>；<br>
（10）DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中）<br>
（11）DispatcherServlet响应用户。</p>
<figure data-type="image" tabindex="1"><img src="https://memorykki.github.io/post-images/SpringMVC/image-20211014222339319.png" alt="" loading="lazy"></figure>
<h2 id="mvc设计模式">MVC设计模式</h2>
<p>模型（model）- 视图（view）- 控制器（controller），三层架构的设计模式。用于实现前端页面的展现与后端业务数据处理的分离。</p>
<ul>
<li>
<p>分层设计，实现了业务系统各个组件之间的解耦，有利于业务系统的可扩展性，可维护性。</p>
</li>
<li>
<p>有利于系统的并行开发，提升开发效率。</p>
</li>
</ul>
<h2 id="注解">注解</h2>
<p>@RequestMapping：用于处理请求 url 映射的注解，可用于类或方法上，用于类上表示类父路径。</p>
<p>@RequestBody：接收http请求的json数据，将json转换为java对象。</p>
<p>@ResponseBody：将controller方法返回对象转化为json对象响应给客户。</p>
<p>@RestController == @ResponseBody ＋ @Controller</p>
<h2 id="controller">@Controller</h2>
<p>控制器Controller 负责处理由DispatcherServlet 分发的请求，它把用户请求的数据经过业务处理层处理之后封装成一个Model ，然后再把该Model 返回给对应的View 进行展示。</p>
<p>@Controller 只是定义了一个控制器类，而使用@RequestMapping 注解的方法才是真正处理请求的处理器。</p>
<h2 id="pathvariable-requestparam">@PathVariable  @RequestParam</h2>
<p>@PathVariable来获取 @RequestMapping(value = “/page/{id}”, method = RequestMethod.GET)</p>
<p>@RequestParam用来获得静态的URL请求入参 spring注解时action里用到。</p>
<h2 id="spring-mvc与struts2">Spring MVC与Struts2</h2>
<p><strong>相同点</strong></p>
<p>都是基于mvc的表现层框架，都用于web项目的开发。</p>
<p><strong>不同点</strong></p>
<ul>
<li>
<p><strong>前端控制器</strong>不一样。Spring MVC的前端控制器是servlet：DispatcherServlet。struts2的前端控制器是filter：StrutsPreparedAndExcutorFilter。</p>
</li>
<li>
<p><strong>请求参数的接收方式</strong>不一样。Spring MVC是使用方法的<strong>形参</strong>接收请求的参数，基于方法的开发，线程安全，可以设计为单例或者多例的开发，推荐使用单例模式的开发（执行效率更高），默认就是<strong>单例</strong>开发模式。struts2是通过<strong>类的成员变量</strong>接收请求的参数，是基于类的开发，线程不安全，只能设计为<strong>多例</strong>的开发。</p>
</li>
<li>
<p>Struts采用值栈存储请求和响应的数据，通过<strong>OGNL</strong>存取数据，Spring MVC通过<strong>参数解析器</strong>是将request请求内容解析，并给方法形参赋值，将数据和视图封装成ModelAndView对象，最后又将ModelAndView中的模型数据通过reques域传输到页面。Jsp视图解析器默认使用jstl。</p>
</li>
</ul>
<h2 id="重定向和转发">重定向和转发</h2>
<p>转发：在返回值前面加&quot;forward:&quot;，譬如&quot;forward:user.do?name=method4&quot;</p>
<p>重定向：在返回值前面加&quot;redirect:&quot;，譬如&quot;redirect:http://www.baidu.com&quot;</p>
<h2 id="中文乱码">中文乱码</h2>
<h3 id="post">POST</h3>
<p>在web.xml中配置一个CharacterEncodingFilter过滤器，设置成utf-8；</p>
<pre><code class="language-xml">&lt;filter&gt;
    &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;
    &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;
    &lt;init-param&gt;
        &lt;param-name&gt;encoding&lt;/param-name&gt;
        &lt;param-value&gt;utf-8&lt;/param-value&gt;
    &lt;/init-param&gt;
&lt;/filter&gt;

&lt;filter-mapping&gt;
    &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;
    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
&lt;/filter-mapping&gt;

</code></pre>
<h3 id="get">GET</h3>
<ul>
<li>修改tomcat配置文件添加编码与工程编码一致</li>
</ul>
<pre><code class="language-xml">&lt;ConnectorURIEncoding=&quot;utf-8&quot; connectionTimeout=&quot;20000&quot; port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; redirectPort=&quot;8443&quot;/&gt;
</code></pre>
<ul>
<li>对参数进行重新编码：</li>
</ul>
<pre><code class="language-java">String userName = new String(request.getParamter(“userName”).getBytes(“ISO8859-1”),“utf-8”)
</code></pre>
<h2 id="异常处理">异常处理</h2>
<p>可以将异常抛给Spring框架，由Spring框架来处理；我们只需要配置简单的异常处理器，在异常处理器中添视图页面即可。</p>
<h2 id="拦截get方式提交的方法">拦截GET方式提交的方法</h2>
<p>可以在@RequestMapping注解里面加上method=RequestMethod.GET。</p>
<h2 id="获取-request-session">获取 Request、Session</h2>
<p>直接在方法的形参中声明<strong>request</strong>，Spring MVC就自动把request对象传入。</p>
<h2 id="拦截的方法里面得到入参">拦截的方法里面得到入参</h2>
<p>直接在形参里面声明这个参数就可以,但必须<strong>名字</strong>和传过来的参数一样。</p>
<h2 id="接收对象属性的参数">接收对象属性的参数</h2>
<p>直接在方法中声明这个对象，Spring MVC就自动会把属性赋值到这个对象里面。</p>
<h2 id="后台向前台传递数据">后台向前台传递数据</h2>
<p>通过<strong>ModelMap</strong>对象,可以在这个对象里面调用put方法,把对象加到里面,前台就可以通过el表达式拿到。</p>
<h2 id="modelmap数据放入session">ModelMap数据放入Session</h2>
<p>可以在类上面加上@SessionAttributes注解，里面包含的字符串就是要放入session里面的key。</p>
<h2 id="拦截器">拦截器</h2>
<p>Spring MVC中的拦截器（Interceptor）类似于Servlet中的过滤器（Filter），它主要用于拦截用户请求并作相应的处理。例如通过拦截器可以进行权限验证、记录请求信息的日志、判断用户是否登录等。</p>
<ul>
<li>
<p>通过实现HandlerInterceptor接口；</p>
<p>实现 preHandle（之前）、postHandle（之后）、afterCompletion（该方法会在整个请求完成，即视图渲染结束之后执行。可以通过此方法实现一些资源清理、记录日志信息等工作。）</p>
</li>
<li>
<p>通过实现WebRequestInterceptor接口。</p>
</li>
</ul>
<h3 id="单个拦截器">单个拦截器</h3>
<figure data-type="image" tabindex="2"><img src="https://images2017.cnblogs.com/blog/1240732/201711/1240732-20171114200159843-1367757713.png" alt="img" loading="lazy"></figure>
<h2 id="多个拦截器">多个拦截器</h2>
<p>preHandle()方法会按照配置文件中拦截器的配置<strong>顺序</strong>执行，postHandle()方法和afterCompletion()方法则会按照配置顺序的<strong>反序</strong>执行。</p>
<figure data-type="image" tabindex="3"><img src="https://images2017.cnblogs.com/blog/1240732/201711/1240732-20171114200511874-738520900.png" alt="img" loading="lazy"></figure>
<h2 id="webapplicationcontext">WebApplicationContext</h2>
<p>WebApplicationContext 继承了ApplicationContext 并增加了一些WEB应用必备的特有功能，它不同于一般的ApplicationContext ，因为它能处理主题，并找到被关联的servlet。</p>
]]></content>
    </entry>
</feed>