{"posts":[{"title":"Nginx","content":"Nginx 是一个高性能的 HTTP 和反向代理服务器 概述 Nginx (&quot;engine x&quot;) 是一个高性能的 HTTP 和反向代理服务器,特点是占有内存少，并发能力强，事实上 nginx 的并发能力确实在同类型的网页服务器中表现较好， Nginx 可以作为静态页面的 web 服务器，同时还支持 CGI 协议的动态语言，比如 perl、 php等。但是不支持 java。 Java 程序只能通过与 tomcat 配合完成。 Nginx 专为性能优化而开发，性能是其最重要的考量,实现上非常注重效率 ，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。 正向代理 Nginx 不仅可以做反向代理，实现负载均衡。还能用作正向代理来进行上网等功能。 正向代理：如果把局域网外的 Internet 想象成一个巨大的资源库，则局域网中的客户端要访问 Internet，则需要通过代理服务器来访问，这种代理服务就称为正向代理。 反向代理 反向代理，其实客户端对代理是无感知的，因为客户端不需要任何配置就可以访问，我们只需要将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器获取数据后，在返回给客户端，此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器 地址，隐藏了真实服务器 IP 地址。 负载均衡 客户端发送多个请求到服务器，服务器处理请求，有一些可能要与数据库进行交互，服务器处理完毕后，再将结果返回给客户端。 这种架构模式对于早期的系统相对单一，并发请求相对较少的情况下是比较适合的，成本也低。但是随着信息数量的不断增长，访问量和数据量的飞速增长，以及系统业务的复杂度增加，这种架构会造成服务器相应客户端的请求日益缓慢，并发量特别大的时候，还容易造成服务器直接崩溃。很明显这是由于服务器性能的瓶颈造成的问题，那么如何解决这种情况呢？ 我们首先想到的可能是升级服务器的配置，比如提高 CPU 执行频率，加大内存等提高机器的物理性能来解决此问题，但是我们知道摩尔定律的日益失效，硬件的性能提升已经不能满足日益提升的需求了。最明显的一个例子，天猫双十一当天，某个热销商品的瞬时访问量 是极其庞大的，那么类似上面的系统架构，将机器都增加到现有的顶级物理配置，都是不能够满足需求的。那么怎么办呢？ 上面的分析我们去掉了增加服务器物理配置来解决问题的办法，也就是说纵向解决问题的办法行不通了，那么横向增加服务器的数量呢？这时候集群的概念产生了，单个服务器解决不了，我们增加服务器的数量，然后将请求分发到各个服务器上，将原先请求集中到单个 服务器上的情况改为将请求分发到多个服务器上，将负载分发到不同的服务器，也就是我们所说的负载均衡 动静分离 为了加快网站的解析速度，可以把动态页面和静态页面由不同的服务器来解析，加快解析速 度。降低原来单个服务器的压力。 负载均衡 轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。 weight weight 代表权,重默认为 1,权重越高被分配的客户端越多指定轮询几率， weight 和访问比率成正比，用于后端服务器性能不均的情况。 least_conn 此策略是指每次将请求分发到当前连接数最少的服务器上，试图转发给相对空闲的服务器以实现负载平衡； ip_hash 每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 的问题。从本质上说，ip hash算法是一种变相的轮询算法，如果两个ip的初始hash值恰好相同，那么来自这两个ip的请求将永远落在同一台服务器上，这为均衡性埋下了较深隐患。 url hash 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 fair（第三方） 根据后端服务器的响应时间判断负载情况，从中选出负载最轻的机器进行分流。 动静分离 &lt; Nginx 动静分离简单来说就是把动态跟静态请求分开，不能理解成只是单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用 Nginx处理静态页面， Tomcat 处理动态页面。 实现方案： 一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案； 另外一种方法就是动态跟静态文件混合在一起发布，通过 nginx 来分开。通过 location 指定不同的后缀名实现不同的请求转发。通过 expires 参数设置，可以使浏览器缓存过期时间，减少与服务器之前的请求和流量。 工作原理 Nginx本身做的工作实际很少，当它接到一个HTTP请求时，它仅仅是通过查找配置文件将此次请求映射到一个location block，而此location中所配置的各个指令则会启动不同的模块去完成工作，因此模块可以看做Nginx真正的劳动工作者。通常一个location中的指令会涉及一个handler模块和多个filter模块（当然，多个location可以复用同一个模块）。handler模块负责处理请求，完成响应内容的生成，而filter模块对响应内容进行处理。 模块： Handlers（处理器模块）。此类模块直接处理请求，并进行输出内容和修改headers信息等操作。Handlers处理器模块一般只能有一个。 Filters （过滤器模块）。此类模块主要对其他处理器模块输出的内容进行修改操作，最后由Nginx输出。 Proxies （代理类模块）。此类模块是Nginx的HTTP Upstream之类的模块，这些模块主要与后端一些服务比如FastCGI等进行交互，实现服务代理和负载均衡等功能。 进程模型 Nginx默认采用多进程工作方式。 Nginx启动后，会运行一个master进程和多个worker进程。其中master充当整个进程组与用户的交互接口，同时对进程进行监护，管理worker进程来实现重启服务、平滑升级、更换日志文件、配置文件实时生效等功能。worker用来处理基本的网络事件，worker之间是平等的，他们共同竞争来处理来自客户端的请求。 master：管理worker master进程主要用来管理worker进程，具体包括如下4个主要功能： （1）接收来自外界的信号。 （2）向各worker进程发送信号。 （3）监控woker进程的运行状态。 （4）当woker进程退出后（异常情况下），会自动重新启动新的woker进程。 worker：处理请求 多个worker进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个worker进程中处理，一个worker进程，不可能处理其它进程的请求。worker进程的个数是可以设置的，一般我们会设置与机器cpu核数一致，这里面的原因与nginx的进程模型以及事件处理模型是分不开的。 Nginx采用异步非阻塞的方式来处理网络事件： 接收请求： 在创建master进程时，先建立需要监听的socket（listenfd），然后从master进程中fork()出多个worker进程，如此一来每个worker进程都可以监听用户请求的socket。一般来说，当一个连接进来后，所有在Worker都会收到通知，但是只有一个进程可以接受这个连接请求，其它的都失败，这是所谓的惊群现象。nginx提供了一个accept_mutex（互斥锁），有了这把锁之后，同一时刻，就只会有一个进程在accpet连接，这样就不会有惊群问题了。 处理请求 当一个worker进程在accept这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。 我们可以看到，一个请求，完全由worker进程来处理，而且只在一个worker进程中处理。worker进程之间是平等的，每个进程，处理请求的机会也是一样的。 性能高的原因 nginx是以多进程的方式来工作的，当然nginx也是支持多线程的方式的，只是我们主流的方式还是多进程的方式，也是nginx的默认方式。 多进程模型 异步非阻塞 多进程模型 VS 多线程模型 首先，对于每个worker进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销； 其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master进程则很快启动新的worker进程。 apache的常用工作方式（apache也有异步非阻塞版本，但因其与自带某些模块冲突，所以不常用），每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。这对操作系统来说，是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的cpu开销很大，自然性能就上不去了，而这些开销完全是没有意义的。 同步阻塞 VS 异步非阻塞 同步阻塞的：处理请求时遇到读写事件，而当读写事件没有准备好时，那就只能等了，等事件准备好了，你再继续吧。cpu空闲下来没人用，cpu利用率自然上不去了，更别谈高并发了。 非阻塞就是，事件虽没有准备好，但不会让你一直在等待，马上返回ErrorAgain，先去处理别的请求，但是需要不时地过来检查一下事件的状态，这种开销也是不小的。 所以有了异步非阻塞的事件处理机制，具体到系统调用就是像select/poll/epoll/kqueue这样的系统调用。**同时监控多个事件，调用他们是阻塞的，但可以设置超时时间，在超时时间之内准备好就返回。**epoll为例，当事件没准备好时，放到epoll里面，事件准备好了，我们就去读写，当读写返回ErrorAgain时，我们将它再次加入到epoll里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在epoll里面等着。 这样，高并发下，线程虽然只有一个，但是一直在循环切换、处理请求而不停止，切换是没有任何代价，理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式不需要创建线程，每个请求占用的内存也很少，没有上下文切换。并发数再多也不会导致无谓的资源浪费（上下文切换）。更多的并发数，只是会占用更多的内存而已。 参考文章 worker 数量 推荐设置worker的个数 == cpu的核数，在这里就很容易理解了，更多的worker数，只会导致进程来竞争cpu资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了cpu亲缘性的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来cache的失效。 对于一个基本的web服务器来说，事件通常有三种类型，网络事件、信号、定时器。从上面的讲解中知道，网络事件通过异步非阻塞可以很好的解决掉。如何处理信号与定时器？ 信号处理 首先，信号的处理。对nginx来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重入。关于信号的处理，大家可以学习一些专业书籍，这里不多说。对于nginx来说，如果nginx正在等待事件（epoll_wait时），如果程序收到信号，在信号处理函数处理完后，epoll_wait会返回错误，然后程序可再次进入epoll_wait调用。 定时器 由于epoll_wait等函数在调用的时候是可以设置一个超时时间的，所以nginx借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入epoll_wait前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出epoll_wait的超时时间后进入epoll_wait。所以当没有事件产生，也没有中断信号时，epoll_wait会超时，也就是说定时器事件到了。这时nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。 ","link":"https://memorykki.github.io/nginx/"},{"title":"ElasticSearch","content":"Elasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。 核心概念 集群，节点，索引，类型，文档，分片，映射是什么？ elasticsearch是面向文档，关系型数据库和elasticsearch客观的对比！一切都是json Relational DB Elasticsearch 数据库（database） 索引（indices） 表（tables） types 行（rows） documents 字段（columns） fields 物理设计： elasticsearch在后台把每个索引划分成多个分片。每个分片可以在集群中的不同服务器间迁移 逻辑设计： 一个索引类型中，抱哈an多个文档，当我们索引一篇文档时，可以通过这样的一个顺序找到它：索引-》类型-》文档id，通过这个组合我们就能索引到某个具体的文档。注意：ID不必是整数，实际上它是一个字符串。 文档 文档 就是我们的一条条的记录 之前说elasticsearch是面向文档的,那么就意味着索弓和搜索数据的最小单位是文档, elasticsearch中,文档有几个重要属性: 自我包含, - -篇文档同时包含字段和对应的值,也就是同时包含key:value ! 可以是层次型的，-一个文档中包含自文档,复杂的逻辑实体就是这么来的! {就是一 个json对象! fastjson进行自动转换!} 灵活的结构,文档不依赖预先定义的模式,我们知道关系型数据库中,要提前定义字段才能使用,在elasticsearch中,对于字段是非常灵活的,有时候,我们可以忽略该字段,或者动态的添加一个新的字段。 尽管我们可以随意的新增或者忽略某个字段,但是,每个字段的类型非常重要,比如一一个年龄字段类型,可以是字符串也可以是整形。因为elasticsearch会保存字段和类型之间的映射及其他的设置。这种映射具体到每个映射的每种类型,这也是为什么在elasticsearch中,类型有时候也称为映射类型。 类型 类型 类型是文档的逻辑容器,就像关系型数据库一样,表格是行的容器。类型中对于字段的定 义称为映射,比如name映射为字符串类型。我们说文档是无模式的 ,它们不需要拥有映射中所定义的所有字段,比如新增一个字段,那么elasticsearch是怎么做的呢?elasticsearch会自动的将新字段加入映射,但是这个字段的不确定它是什么类型, elasticsearch就开始猜,如果这个值是18 ,那么elasticsearch会认为它是整形。但是elasticsearch也可能猜不对 ，所以最安全的方式就是提前定义好所需要的映射,这点跟关系型数据库殊途同归了,先定义好字段,然后再使用,别整什么幺蛾子。 索引 索引 就是数据库! 索引是映射类型的容器, elasticsearch中的索引是一个非常大的文档集合。索|存储了映射类型的字段和其他设置。然后它们被存储到了各个分片上了。我们来研究下分片是如何工作的。 物理设计:节点和分片如何工作 一个集群至少有一 个节点,而一个节点就是一-个elasricsearch进程 ,节点可以有多个索引默认的,如果你创建索引,那么索引将会有个5个分片( primary shard ,又称主分片)构成的,每一个主分片会有-一个副本( replica shard ,又称复制分片） 上图是一个有3个节点的集群,可以看到主分片和对应的复制分片都不会在同-个节点内,这样有利于某个节点挂掉了,数据也不至于丢失。实际上, 一个分片是- -个Lucene索引, -一个包含倒排索引的文件目录,倒排索引的结构使得elasticsearch在不扫描全部文档的情况下,就能告诉你哪些文档包含特定的关键字。不过,等等,倒排索引是什么鬼? 倒排索引 倒排索引 elasticsearch使用的是一种称为倒排索引 |的结构,采用Lucene倒排索作为底层。这种结构适用于快速的全文搜索，一个索引由文 档中所有不重复的列表构成,对于每一个词,都有一个包含它的文档列表。 例如,现在有两个文档，每个文档包含如下内容: Study every day， good good up to forever # 文 档1包含的内容 To forever, study every day，good good up # 文档2包含的内容 12 为为创建倒排索引,我们首先要将每个文档拆分成独立的词(或称为词条或者tokens) ,然后创建一一个包含所有不重 复的词条的排序列表,然后列出每个词条出现在哪个文档: term doc_1 doc_2 Study √ x To x x every √ √ forever √ √ day √ √ study x √ good √ √ every √ √ to √ x up √ √ 现在，我们试图搜索 to forever，只需要查看包含每个词条的文档 term doc_1 doc_2 to √ x forever √ √ total 2 1 两个文档都匹配,但是第一个文档比第二个匹配程度更高。如果没有别的条件,现在,这两个包含关键字的文档都将返回。 再来看一个示例,比如我们通过博客标签来搜索博客文章。那么倒排索引列表就是这样的一个结构: 博客文章(原始数据) 博客文章(原始数据) 索引列表(倒排索引) 索引列表(倒排索引) 博客文章ID 标签 标签 博客文章ID 1 python python 1，2，3 2 python linux 3，4 3 linux，python 4 linux 如果要搜索含有python标签的文章,那相对于查找所有原始数据而言，查找倒排索引后的数据将会快的多。只需要查看标签这一栏,然后获取相关的文章ID即可。完全过滤掉无关的所有数据,提高效率! elasticsearch的索引和Lucene的索引对比 在elasticsearch中，索引(库)这个词被频繁使用,这就是术语的使用。在elasticsearch中 ,索引被分为多个分片,每份分片是-个Lucene的索引。所以一个elasticsearch索引是由多 个Lucene索引组成的。别问为什么,谁让elasticsearch使用Lucene作为底层呢!如无特指，说起索引都是指elasticsearch的索引。 接下来的一切操作都在kibana中Dev Tools下的Console里完成。基础操作! ik分词器 什么是IK分词器 ? 分词:即把一-段中文或者别的划分成一个个的关键字,我们在搜索时候会把自己的信息进行分词,会把数据库中或者索引库中的数据进行分词,然后进行一个匹配操作,默认的中文分词是将每个字看成一个词,比如“我爱狂神”会被分为&quot;我&quot;,“爱”,“狂”,“神” ,这显然是不符合要求的,所以我们需要安装中文分词器ik来解决这个问题。 如果要使用中文,建议使用ik分词器! IK提供了两个分词算法: ik_ smart和ik_ max_ word ,其中ik_ smart为最少切分, ik_ max_ _word为最细粒度划分!一会我们测试! 什么是IK分词器： 把一句话分词 如果使用中文：推荐IK分词器 两个分词算法：ik_smart（最少切分），ik_max_word（最细粒度划分） 【ik_smart】测试： GET _analyze { &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;: &quot;我是社会主义接班人&quot; } //输出 { &quot;tokens&quot; : [ { &quot;token&quot; : &quot;我&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 0 }, { &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 }, { &quot;token&quot; : &quot;社会主义&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 }, { &quot;token&quot; : &quot;接班人&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 } ] } 123456789101112131415161718192021222324252627282930313233343536373839 【ik_max_word】测试： GET _analyze { &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;我是社会主义接班人&quot; } //输出 { &quot;tokens&quot; : [ { &quot;token&quot; : &quot;我&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 0 }, { &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 }, { &quot;token&quot; : &quot;社会主义&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 }, { &quot;token&quot; : &quot;社会&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 }, { &quot;token&quot; : &quot;主义&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 4 }, { &quot;token&quot; : &quot;接班人&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 }, { &quot;token&quot; : &quot;接班&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 }, { &quot;token&quot; : &quot;人&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 7 } ] } 命令使用 3.1 Rest风格说明 一种软件架构风格，而不是标准。更易于实现缓存等机制 method url地址 描述 PUT localhost:9200/索引名称/类型名称/文档id 创建文档(指定文档id) POST localhost:9200/索引名称/类型名称 创建文档（随机文档id） POST localhost:9200/索引名称/类型名称/文档id/_update 修改文档 DELETE localhost:9200/索引名称/类型名称/文档id 删除文档 GET localhost:9200/索引名称/类型名称/文档id 通过文档id查询文档 POST localhost:9200/索引名称/类型名称/_search 查询所有的数据 基础测试 1.创建一个索引 PUT /索引名/类型名(高版本都不写了，都是_doc)/文档id {请求体} 完成了自动添加了索引！数据也成功的添加了。 那么name这个字段用不用指定类型呢 指定字段的类型properties 就比如sql创表 获得这个规则！可以通过GET请求获得具体的信息 如果自己不设置文档字段类型，那么es会自动给默认类型 3.2 cat命令 获取健康值 获取所有的信息 GET _cat/indices?v 1 还有很多 可以自动展示 都试试 修改索引 1.修改我们可以还是用原来的PUT的命令，根据id来修改 但是如果没有填写的字段 会重置为空了 ，相当于java接口传对象修改，如果只是传id的某些字段，那其他没传的值都为空了。 2.还有一种update方法 这种不设置某些值 数据不会丢失 POST /test3/_doc/1/_update { &quot;doc&quot;:{ &quot;name&quot;:&quot;212121&quot; } } //下面两种都是会将不修改的值清空的 POST /test3/_doc/1 { &quot;name&quot;:&quot;212121&quot; } POST /test3/_doc/1 { &quot;doc&quot;:{ &quot;name&quot;:&quot;212121&quot; } } 带doc修改 查询也是带doc的（document） 删除索引 关于删除索引或者文档的操作 通过DELETE命令实现删除，根据你的请求来判断是删除索引还是删除文档记录 使用RESTFUL的风格是我们ES推荐大家使用的！ 3.3 关于文档的基本操作 查询 最简单的搜索是GET 搜索功能search 这边name是text 所以做了分词的查询 如果是keyword就不会分词搜索了 复杂操作搜索select（排序，分页，高亮，模糊查询，精准查询） //测试只能一个字段查询 GET lisen/user/_search { &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;李森&quot; } } } 结果过滤，就是只展示列表中某些字段 包含 不包含 排序 分页 代码 GET lisen/user/_search { &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;李森&quot; } }, &quot;sort&quot;:{ &quot;age&quot;:{ &quot;order&quot;:&quot;asc&quot; } }, &quot;from&quot;: 0, &quot;size&quot;: 1 } 多条件查询 布尔值查询 must（and），所有的条件都要符合 should（or）或者的 跟数据库一样 must_not（not） 条件区间 gt大于 gte大于等于 lte小于 lte小于等于 匹配多个条件（数组） match没用倒排索引 这边改正一下 精确查找 term查询是直接通过倒排索引指定的词条进程精确查找的 关于分词 term，直接查询精确的 match，会使用分词器解析！（先分析文档，然后通过分析的文档进行查询） 默认的是被分词了 keyword没有被分词 精确查询多个值 高亮 还能自定义高亮的样式 springboot集成 4.1 引入依赖包 创建一个springboot的项目 同时勾选上springboot-web的包以及Nosql的elasticsearch的包 如果没有就手动引入 &lt;!--es客户端--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;!--springboot的elasticsearch服务--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; 注意下spring-boot的parent包内的依赖的es的版本是不是你对应的版本 不是的话就在pom文件下写个properties的版本 &lt;!--这边配置下自己对应的版本--&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;elasticsearch.version&gt;7.6.2&lt;/elasticsearch.version&gt; &lt;/properties&gt; 4.2 注入RestHighLevelClient 客户端 @Configuration public class ElasticSearchClientConfig { @Bean public RestHighLevelClient restHighLevelClient(){ RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(&quot;127.0.0.1&quot;,9200,&quot;http&quot;)) ); return client; } } 4.3 索引的增、删、是否存在 //测试索引的创建 @Test void testCreateIndex() throws IOException { //1.创建索引的请求 CreateIndexRequest request = new CreateIndexRequest(&quot;lisen_index&quot;); //2客户端执行请求，请求后获得响应 CreateIndexResponse response = client.indices().create(request, RequestOptions.DEFAULT); System.out.println(response); } //测试索引是否存在 @Test void testExistIndex() throws IOException { //1.创建索引的请求 GetIndexRequest request = new GetIndexRequest(&quot;lisen_index&quot;); //2客户端执行请求，请求后获得响应 boolean exist = client.indices().exists(request, RequestOptions.DEFAULT); System.out.println(&quot;测试索引是否存在-----&quot;+exist); } //删除索引 @Test void testDeleteIndex() throws IOException { DeleteIndexRequest request = new DeleteIndexRequest(&quot;lisen_index&quot;); AcknowledgedResponse delete = client.indices().delete(request,RequestOptions.DEFAULT); System.out.println(&quot;删除索引--------&quot;+delete.isAcknowledged()); } 4.4 文档的操作 //测试添加文档 @Test void testAddDocument() throws IOException { User user = new User(&quot;lisen&quot;,27); IndexRequest request = new IndexRequest(&quot;lisen_index&quot;); request.id(&quot;1&quot;); //设置超时时间 request.timeout(&quot;1s&quot;); //将数据放到json字符串 request.source(JSON.toJSONString(user), XContentType.JSON); //发送请求 IndexResponse response = client.index(request,RequestOptions.DEFAULT); System.out.println(&quot;添加文档-------&quot;+response.toString()); System.out.println(&quot;添加文档-------&quot;+response.status()); // 结果 // 添加文档-------IndexResponse[index=lisen_index,type=_doc,id=1,version=1,result=created,seqNo=0,primaryTerm=1,shards={&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0}] // 添加文档-------CREATED } //测试文档是否存在 @Test void testExistDocument() throws IOException { //测试文档的 没有index GetRequest request= new GetRequest(&quot;lisen_index&quot;,&quot;1&quot;); //没有indices()了 boolean exist = client.exists(request, RequestOptions.DEFAULT); System.out.println(&quot;测试文档是否存在-----&quot;+exist); } //测试获取文档 @Test void testGetDocument() throws IOException { GetRequest request= new GetRequest(&quot;lisen_index&quot;,&quot;1&quot;); GetResponse response = client.get(request, RequestOptions.DEFAULT); System.out.println(&quot;测试获取文档-----&quot;+response.getSourceAsString()); System.out.println(&quot;测试获取文档-----&quot;+response); // 结果 // 测试获取文档-----{&quot;age&quot;:27,&quot;name&quot;:&quot;lisen&quot;} // 测试获取文档-----{&quot;_index&quot;:&quot;lisen_index&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:1,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;found&quot;:true,&quot;_source&quot;:{&quot;age&quot;:27,&quot;name&quot;:&quot;lisen&quot;}} } //测试修改文档 @Test void testUpdateDocument() throws IOException { User user = new User(&quot;李逍遥&quot;, 55); //修改是id为1的 UpdateRequest request= new UpdateRequest(&quot;lisen_index&quot;,&quot;1&quot;); request.timeout(&quot;1s&quot;); request.doc(JSON.toJSONString(user),XContentType.JSON); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(&quot;测试修改文档-----&quot;+response); System.out.println(&quot;测试修改文档-----&quot;+response.status()); // 结果 // 测试修改文档-----UpdateResponse[index=lisen_index,type=_doc,id=1,version=2,seqNo=1,primaryTerm=1,result=updated,shards=ShardInfo{total=2, successful=1, failures=[]}] // 测试修改文档-----OK // 被删除的 // 测试获取文档-----null // 测试获取文档-----{&quot;_index&quot;:&quot;lisen_index&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;found&quot;:false} } //测试删除文档 @Test void testDeleteDocument() throws IOException { DeleteRequest request= new DeleteRequest(&quot;lisen_index&quot;,&quot;1&quot;); request.timeout(&quot;1s&quot;); DeleteResponse response = client.delete(request, RequestOptions.DEFAULT); System.out.println(&quot;测试删除文档------&quot;+response.status()); } //测试批量添加文档 @Test void testBulkAddDocument() throws IOException { ArrayList&lt;User&gt; userlist=new ArrayList&lt;User&gt;(); userlist.add(new User(&quot;cyx1&quot;,5)); userlist.add(new User(&quot;cyx2&quot;,6)); userlist.add(new User(&quot;cyx3&quot;,40)); userlist.add(new User(&quot;cyx4&quot;,25)); userlist.add(new User(&quot;cyx5&quot;,15)); userlist.add(new User(&quot;cyx6&quot;,35)); //批量操作的Request BulkRequest request = new BulkRequest(); request.timeout(&quot;1s&quot;); //批量处理请求 for (int i = 0; i &lt; userlist.size(); i++) { request.add( new IndexRequest(&quot;lisen_index&quot;) .id(&quot;&quot;+(i+1)) .source(JSON.toJSONString(userlist.get(i)),XContentType.JSON) ); } BulkResponse response = client.bulk(request, RequestOptions.DEFAULT); //response.hasFailures()是否是失败的 System.out.println(&quot;测试批量添加文档-----&quot;+response.hasFailures()); // 结果:false为成功 true为失败 // 测试批量添加文档-----false } //测试查询文档 @Test void testSearchDocument() throws IOException { SearchRequest request = new SearchRequest(&quot;lisen_index&quot;); //构建搜索条件 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //设置了高亮 sourceBuilder.highlighter(); //term name为cyx1的 TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(&quot;name&quot;, &quot;cyx1&quot;); sourceBuilder.query(termQueryBuilder); sourceBuilder.timeout(new TimeValue(60, TimeUnit.SECONDS)); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); System.out.println(&quot;测试查询文档-----&quot;+JSON.toJSONString(response.getHits())); System.out.println(&quot;=====================&quot;); for (SearchHit documentFields : response.getHits().getHits()) { System.out.println(&quot;测试查询文档--遍历参数--&quot;+documentFields.getSourceAsMap()); } // 测试查询文档-----{&quot;fragment&quot;:true,&quot;hits&quot;:[{&quot;fields&quot;:{},&quot;fragment&quot;:false,&quot;highlightFields&quot;:{},&quot;id&quot;:&quot;1&quot;,&quot;matchedQueries&quot;:[],&quot;primaryTerm&quot;:0,&quot;rawSortValues&quot;:[],&quot;score&quot;:1.8413742,&quot;seqNo&quot;:-2,&quot;sortValues&quot;:[],&quot;sourceAsMap&quot;:{&quot;name&quot;:&quot;cyx1&quot;,&quot;age&quot;:5},&quot;sourceAsString&quot;:&quot;{\\&quot;age\\&quot;:5,\\&quot;name\\&quot;:\\&quot;cyx1\\&quot;}&quot;,&quot;sourceRef&quot;:{&quot;fragment&quot;:true},&quot;type&quot;:&quot;_doc&quot;,&quot;version&quot;:-1}],&quot;maxScore&quot;:1.8413742,&quot;totalHits&quot;:{&quot;relation&quot;:&quot;EQUAL_TO&quot;,&quot;value&quot;:1}} // ===================== // 测试查询文档--遍历参数--{name=cyx1, age=5} } 原理 ","link":"https://memorykki.github.io/ElasticSearch/"},{"title":"Java NIO","content":"BIO、NIO 聊聊IO多路复用之select、poll、epoll详解 聊聊 IO 多路复用 同步异步 BIO 同步阻塞IO 每个连接对应一个线程，改用多线程/线程池执行还是容易达到最大并发量的瓶颈，并且如果请求并没有发送数据，还是占用资源，优点是简单。 NIO non-block io 同步非阻塞IO 三大核心： AIO 相对于BIO，NIO在接受请求连接、处理空连接（无数据的请求）设置为非阻塞，这就是高并发的前提。线程模式为一个线程处理多个请求。 以上代码的问题： 1、死循环使得没有请求时CPU占满； 2、十万个连接放到 List ，但是其中仅少量的连接有数据，大量无效循环； 3、如果有上万个事件，每个事件执行时间很长，就会影响后面连接的建立； 解决： 1、先阻塞线程，等到有accept时放行 2、增加有数据的List，循环遍历 3、交给线程池来处理事件，主线程只接收请求连接。（redis没有用多线程，还是单线程，但它会限制每个事件的执行时间，不能太长，虽然它io收发可能是多线程的，但事件处理还是单线程） selector解决了这两个问题！！ 引入selector多路复用器 过程： 把serversocket 注册到 selector ，并监听 accpet 事件； 循环中，无请求时阻塞，让出CPU，有请求时selector.select()放行（解决1） 循环selector.selectKeys（无数据的连接不会被获取到，也就不会被遍历，解决2） 判断如果是accept事件，注册这个serversocket的read事件到selector； 如果是read事件，就执行自定义方法； 接着循环阻塞。 底层： open()、select()、register() Selector.open()底层实现基于不同平台，linux下，实际调用返回的是EpollSelector，其中放着一个集合EpollArrayWrapper，保存着channel，最终是native本地方法实现的，操作系统的内核函数epoll_create（创建epoll实例）、epoll_ctl、epoll_wait 所以selector底层就是一个epoll结构体，包含channels集合，监听其中的事件，有事件发生时就放到就绪列表rdlist中。 redis底层也是通过epoll函数实现的。 IO多路复用底层主要用的linux内核函数select、poll、epoll： netty就是一个处理数据的，底层就是对NIO api的封装，达到百万并发级别，开发者不需要写建立连接等重复代码，而是交给netty框架执行，只需要自定义一些接口实现就行。 netty线程模型 利用线程池处理事件，但是线程池也是有限的 AIO NIO 2.0版本 异步非阻塞 一个有效请求对应一个线程，客户端的IO请求都是由OS完成后再通知服务器应开启线程处理。适用于连接数多且时间较长的应用 NIO BIO 比较 适用场景 1. NIO与IO的区别 NIO：New IO 1.4就有NIO了，1.7对NIO进行了改进。1.7对NIO的改动，称之为NIO2.NIO在现在企业中使用的比较多。 NIO的几个概念： 缓冲区 通道 选择器 IO NIO 面向流 面向缓冲区 原来的IO是面向流，是单向传输。 NIO是双向的传输。 2. 缓冲区 缓冲区（Bufffer）：在JavaNIO中负责数据的存储。缓冲区就是数组。用于存储不同类型的数据。 根据数据的不同，提供了相应类型的缓冲区。（Boolean类型除外，其他的7个基本类型都有） 有： ByteBuffer ; CharBuffer ; ShortBuffer ; IntBuffer ; LongBuffer ; FloatBuffer ; DoubleBuffer 上述缓冲区的管理方式都几乎一致。通过allocate();获取缓冲区 最常用的就是ByteBuffer 2.1. 缓冲区的基本属性 分配一个指定大小的缓冲区： ByteBuffer byteBuffer = ByteBuffer.allocate(10);//获取一个10字节大小的缓冲区 从缓冲区存取数据的两个核心方法： get();和put()； byteBuffer.put(&quot;abcde&quot;.getBytes());//存5个Byte的数据 byterBuffer.get(); 缓冲区的几个核心属性： capacity：容量，表示缓冲区中最大的容量，一旦生命，不得改变！ limit：界限，第一个不应该读取或写入的数据的索引，即位于 limit 后的数据 不可读写。缓冲区的限制不能为负，并且不能大于其容量。 position：位置，表示缓冲区中正在操作数据的位置。（即将要操作的位置，position下的位置是空的） !- （position &lt;= limit &lt;= capacity) 2.2. flip方法（切换读数据模式） flip方法：可以切换到读数据模式。 切换到读取模式的时候，即切换到读模式，则position变为0，limit变为数据最大的位置。 !- !- 2.3. 读取Buffer数据 byteBuffer.flip();//切换到读模式 byte[] data = new byte[byteBuffer.limit()]; byteBuffer.get(data);//获取数据 get完成之后，各个属性的位置变化情况为？ position：变为读之前的limit limit：不变 capacity：不变 2.4. Buffer常用方法 !- rewind：倒带，倒片。可重复读取数据，将position改为0。可以再次读取。 clear：清空，将buffer中的数据清空。将limit变为capacity，但是缓冲区的数据仍然在，数据处在被遗忘状态，只不过是将limit置为capacity，可以再次重新写入。 mark：标记。记录此时的position。 reset：把position恢复到上次mark的位置。 remaining：获取还可以操作的缓冲区的数量（即 limit - position） hasRemaining：返回一个boolean值，是否还有剩余的位置可以读取 （即 return (limit - position) &gt; 0 ? true : false;） 总结，缓冲区的四个核心属性： 0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity 3. 直接缓冲区和非直接缓冲区 非直接缓存区：通过allocate() 方法分配缓冲区，将缓冲区建立在JVM的内存中。 直接缓冲区：通过allocateDirect() 方法分配缓冲区，将缓冲区直接建立在物理内存中。 可以提高效率 直接缓冲区，只有ByteBuffer支持，其他Buffer不支持！ 非直接缓存区： !- 直接缓存区： !- 3.1. 创建两种缓冲区 ByteBuffer.allocate(1024);//创建非直接缓冲区 ByteBuffer.allocateDirect(1024);//创建直接缓冲区 如何判断缓冲区是否为直接缓冲区？ byteBuffer.isDirect(); //返回一个boolean，true是直接缓冲区，false是非直接缓冲区 4. 通道 通道（Channel）：由 java.nio.channels 包定义 的。Channel 表示 IO 源与目标打开的连接。 Channel 类似于传统的“流”。只不过 Channel 本身不能直接访问数据，Channel 只能与 Buffer 进行交互。 Java 为 Channel 接口提供的最主要实现类如下 FileChannel：用于读取、写入、映射和操作文件的通道。 DatagramChannel：通过 UDP 读写网络中的数据通道。 SocketChannel：通过 TCP 读写网络中的数据。 •ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来 的连接都会创建一个 SocketChannel 4.1 获取通道的三种方式： Java针对支持通道的类提供了getChannel()方法来获取通道 支持通道的类如下： FileInputStream FileOutputStream RandomAccessFile DatagramSocket Socket ServerSocket 获取通道的其他方式是使用 Files 工具类的静态方法 newByteChannel() 获取字节通道。 或者通过通道的静态方法 open() 打开并返回指定通道。 （2和3都是JDK1.7以后的NIO2才支持这种方法） 第二种获取DirectBuffer的方式：使用FileChannel的map()方法将文件区域直接映射到内存中来创建。该方法返回 MappedByteBuffer 。 MappedByteBuffer mappedByteBuffer = inChannel.map(MapMode.READ_ONLY,0,inChannel.size()); //只读模式 从0 到size 4.2. 通过getChannel()获取通道 //获取文件流 FileInputStream fileInputStream = new FileInputStream(&quot;1.jpg&quot;); FileOutputStream fileOutputStream = new FileOutputStream(&quot;2.jpg&quot;); //获取文件通道 FileChannel fileInputChannel = fileInputStream.openChannel(); FileChannel fileOutputChannel = fileOutputStream.openChannel(); //获取一个Buffer ByteBuffer byteBuffer = ByteBuffer.allocate(1024); while((fileInputChannel.read(byteBuffer)) != -1){ byteBuffer.flip();//切换到读模式 fileOutputChannel.wirte(byteBuffer); byteBuffer.clear();//清空buf } //关闭通道 fileOutChannel.close(); fileInputChannel.close(); fileInputStream.close(); fileOutputStream.close(); 4.3. 使用Channel的open()方法类获取通道 使用直接缓冲区完成文件的复制（内存映射文件） MappedByteBuffer是内存映射文件，道理和ByteBuffer.allocateDirect();一摸一样。 MappedByteBuffer是在物理内存中。 内存映射文件，只有ByteBuffer支持。 映射的字节缓冲区及其表示的文件映射在缓冲区本身被垃圾收集之前保持有效。 映射字节缓冲区的内容可以随时改变，例如，如果该程序或其他程序改变了映射文件的相应区域的内容。 这些变化是否发生以及何时发生，取决于操作系统，因此未指定。 映射字节缓冲区的行为与普通直接字节缓冲区没有区别。 FileChannel inChannel = FileChannel.open(Paths.get(&quot;d:/1.mkv&quot;), StandardOpenOption.READ); FileChannel outChannel = FileChannel.open(Paths.get(&quot;d:/2.mkv&quot;), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE); //StandardOpenOption是一个枚举类，其中有多个选项，用于设置该通道的作用 //如读取：StandardOpenOption.READ //如读和写，则可利用Java的不定参数： //其中：StandardOpenOption.CREATE_NEW的类型意为 // |— 如果存在就报错，如果不存在就新建 //FileChannel.open(&quot;d:/1.mkv&quot;,StandardOpenOption.READ,StandardOpenOption.WRITE); //可以利用channel.map()获取内存映射文件Buffer //MappedByteBuffer是内存映射文件，道理和ByteBuffer.allocateDirect();一摸一样 //MappedByteBuffer是在物理内存中。 //内存映射文件，只有ByteBuffer支持！ MappedByteBuffer inMappedBuf = inChannel.map(MapMode.READ_ONLY, 0, inChannel.size()); MappedByteBuffer outMappedBuf = outChannel.map(MapMode.READ_WRITE, 0, inChannel.size()); //MapMode 也是一个选项枚举类 //直接对缓冲区进行数据的读写操作 byte[] dst = new byte[inMappedBuf.limit()]; inMappedBuf.get(dst); outMappedBuf.put(dst); //关闭通道 inChannel.close(); outChannel.close(); 4.4. transferFrom和transferTo方法 transferFrom -&gt; transferFrom(ReadableByteChannel src, long position, long count) 从给定的可读字节通道将字节传输到此通道的文件中。 transferTo -&gt; transferTo(long position, long count, WritableByteChannel target) 将字节从此通道的文件传输到给定的可写字节通道。 FileChannel inChannel = FileChannel.open(Paths.get(&quot;d:/1.mkv&quot;), StandardOpenOption.READ); FileChannel outChannel = FileChannel.open(Paths.get(&quot;d:/2.mkv&quot;), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE); //transferTo与transferFrom是一个效果 //inChannel.transferTo(0, inChannel.size(), outChannel); //从0开始读取，读取到inChannel.size()位置，输出给outChannel outChannel.transferFrom(inChannel, 0, inChannel.size()); //从inChannel获取，从第0个开始获取，获取到inChannel.size()大小的位置 //channel.size(); -&gt; 返回此通道文件的当前大小，以字节为单位。 //FileChannel实例的size()方法将返回该实例所关联文件的大小。如: //long fileSize = channel.size(); inChannel.close(); outChannel.close(); 5. 分散与聚集 分散（Scatter） 分散读取（Scatter Reads）：将**通道中的**数据分散到多个缓冲区中 !- 聚集（Gather） 聚集写入（Gather Writes）：将多个缓冲区中的数据**聚集到一个通道**中 !- 分散读取/聚集写入，都是按照顺序进行操作的 5.1. 分散读取 RandomAccessFile raf = new RandomAccessFile(&quot;1.txt&quot;,&quot;rw&quot;); //1.获取通道 FileChannel fileChannel = raf.getChannel(); //创建一个缓冲区 ByteBuffer byteBuffer1 = ByteBuffer.allocate(100); ByteBuffer byteBuffer2 = ByteBuffer,allocate(1024); //通过分散读取进行读取 ByteBuffer[] bufs = {byteBuffer1,byteBuffer2}; fileChannel.read(bufs); for(ByteBuffer bb : bufs){ bb.flip();//切换到读模式 } //输出前100个字节 System.out.println(new String(bufs[0].array(),0,bufs[0].limit())); //输出后1024个字节 System.out.println(new String(bufs[1].array(),0,bufs[1].limit())); /* 通过结果我们可以看到，分散读取的确是按照顺序写入的 */ 5.2. 聚集写入 RandomAccessFile raf = new RandomAccessFile(&quot;1.txt&quot;,&quot;rw&quot;); RandomAccessFile raf2 = new RandomAccessFile(&quot;2.txt&quot;,&quot;rw&quot;); //1.获取通道 FileChannel fileChannel = raf.getChannel(); FileChannel fileChannel2 = raf2.getChannel(); //创建一个缓冲区 ByteBuffer byteBuffer1 = ByteBuffer.allocate(100); ByteBuffer byteBuffer2 = ByteBuffer,allocate(1024); ByteBuffer[] bufs = {byteBuffer1,byteBuffer2}; fileChannel.read(bufs);//从文件中读取数据到bufs for(ByteBuffer bb : bufs){ bb.flip();//切换到读模式 } fileChannel2.write(bufs);//写出文件 fileChannel.close(); fileChannel2.close(); raf.close(); raf2.close(); //最终还是会按照顺序进行写入 6. 字符集（Charset）编码与解码 编码：字符串转换成字节数组的过程 解码：字节数组转换成字符串的过程 Java中提供了Charset类(java.nio.charset.Charset) Map&lt;String,Charset&gt; charsets = Charset.availableCharsets();//获取所有支持的编码。（构造从规范字符集名称到字符集对象的有序映射。 ） 获取编码器和解码器： Charset charset = Charset.fromName(&quot;GBK&quot;); CharsetEncoder ce = charset.newEncoder(); CharsetDecoder cd = charset.newDecoder(); CharBuffer charBuffer = CharBuffer.allocate(1024); charBuffer.put(&quot;尚硅谷威武！&quot;); charBuffer.flip();//切换读模式 //编码 ByteBuffer byteBuffer = ce.encode(charBuffer); byteBuffer.flip();//切换成读模式 for(int i=0;i&lt;byteBuffer.limit;i++{ System.out.println(byteBuffer.get); } byteBuffer.reset();//重置position指针 //解码 charBuffer = cd.decode(byteBuffer); 7. NIO的非阻塞式网络通信 传统的 IO 流都是阻塞式的。也就是说，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取或写入，该线程在此期间不 能执行其他任务。因此，在完成网络通信进行 IO 操作时，由于线程会 阻塞，所以服务器端必须为每个客户端都提供一个独立的线程进行处理， 当服务器端需要处理大量客户端时，性能急剧下降。 Java NIO 是非阻塞模式的。当线程从某通道进行读写数据时，若没有数 据可用时，该线程可以进行其他任务。线程通常将非阻塞 IO 的空闲时 间用于在其他通道上执行 IO 操作，所以单独的线程可以管理多个输入 和输出通道。因此，NIO 可以让服务器端使用一个或有限几个线程来同 时处理连接到服务器端的所有客户端。 7.1. 选择器 选择器（Selector） 是 SelectableChannle 对象的多路复用器，Selector 可 以同时监控多个 SelectableChannel 的 IO 状况，也就是说，利用 Selector 可使一个单独的线程管理多个 Channel。Selector 是非阻塞 IO 的核心。 SelectableChannle 的结构如下图： !- **选择器的作用：**当客户端发送的通道的数据完全准备就绪之后，选择器才会将该任务分配到服务端的一个或多个线程上。 !- 也就意味着，当客户端的数据未准备就绪，服务端不会处理该任务，就不会占用线程。 更能利用CPU的资源 使用NIO完成网络通信的三个核心： 通道：负责连接 java.nio.channels.Channel |—SelectableChannel |—SocketChannel |—ServerSocketChannel |—DatagramChannel ​ |—Pipe.SinkChannel ​ |—Pipe.SourceChannel 缓冲区：数据的存取 选择器：是SelectableChannel的多路复用器，用于监控SelectableChannel的IO状况 SelectionKey：选择件 !- TCP通信： //客户端 @Test public void client() throws IOException{ //1. 获取通道 SocketChannel sChannel = SocketChannel.open(new InetSocketAddress(&quot;127.0.0.1&quot;, 9898)); //2. 切换非阻塞模式 sChannel.configureBlocking(false); //3. 分配指定大小的缓冲区 ByteBuffer buf = ByteBuffer.allocate(1024); //4. 发送数据给服务端 Scanner scan = new Scanner(System.in); while(scan.hasNext()){ String str = scan.next(); buf.put((new Date().toString() + &quot;\\n&quot; + str).getBytes()); buf.flip(); sChannel.write(buf); buf.clear(); } //5. 关闭通道 sChannel.close(); } //服务端 @Test public void server() throws IOException{ //1. 获取通道 ServerSocketChannel ssChannel = ServerSocketChannel.open(); //2. 切换非阻塞模式 ssChannel.configureBlocking(false); //3. 绑定连接 ssChannel.bind(new InetSocketAddress(9898)); //4. 获取选择器 Selector selector = Selector.open(); //5. 将通道注册到选择器上, 并且指定“监听接收事件” ssChannel.register(selector, SelectionKey.OP_ACCEPT); //6. 轮询式的获取选择器上已经“准备就绪”的事件 while(selector.select() &gt; 0){ //7. 获取当前选择器中所有注册的“选择键(已就绪的监听事件)” Iterator&lt;SelectionKey&gt; it = selector.selectedKeys().iterator(); while(it.hasNext()){ //8. 获取准备“就绪”的是事件 SelectionKey sk = it.next(); //9. 判断具体是什么事件准备就绪 if(sk.isAcceptable()){ //10. 若“接收就绪”，获取客户端连接 SocketChannel sChannel = ssChannel.accept(); //11. 切换非阻塞模式 sChannel.configureBlocking(false); //12. 将该通道注册到选择器上 sChannel.register(selector, SelectionKey.OP_READ); }else if(sk.isReadable()){ //13. 获取当前选择器上“读就绪”状态的通道 SocketChannel sChannel = (SocketChannel) sk.channel(); //14. 读取数据 ByteBuffer buf = ByteBuffer.allocate(1024); int len = 0; while((len = sChannel.read(buf)) &gt; 0 ){ buf.flip(); System.out.println(new String(buf.array(), 0, len)); buf.clear(); } } //15. 取消选择键 SelectionKey it.remove(); } } } UDP通信： @Test public void send() throws IOException{ DatagramChannel dc = DatagramChannel.open(); dc.configureBlocking(false); ByteBuffer buf = ByteBuffer.allocate(1024); Scanner scan = new Scanner(System.in); while(scan.hasNext()){ String str = scan.next(); buf.put((new Date().toString() + &quot;:\\n&quot; + str).getBytes()); buf.flip(); dc.send(buf, new InetSocketAddress(&quot;127.0.0.1&quot;, 9898)); buf.clear(); } dc.close(); } @Test public void receive() throws IOException{ DatagramChannel dc = DatagramChannel.open(); dc.configureBlocking(false); dc.bind(new InetSocketAddress(9898)); Selector selector = Selector.open(); dc.register(selector, SelectionKey.OP_READ); while(selector.select() &gt; 0){ Iterator&lt;SelectionKey&gt; it = selector.selectedKeys().iterator(); while(it.hasNext()){ SelectionKey sk = it.next(); if(sk.isReadable()){ ByteBuffer buf = ByteBuffer.allocate(1024); dc.receive(buf); buf.flip(); System.out.println(new String(buf.array(), 0, buf.limit())); buf.clear(); } } it.remove(); } } 8. Pipe管道 管道（Pipe）：Java NIO 管道是==2个线程==之间的**单向数据连接**。 Pipe有一个source通道和一个sink通道。数据会 被写到sink通道，从source通道读取。 !- @Test public void test1() throws IOException{ //1. 获取管道 Pipe pipe = Pipe.open(); //2. 将缓冲区中的数据写入管道 ByteBuffer buf = ByteBuffer.allocate(1024); Pipe.SinkChannel sinkChannel = pipe.sink(); buf.put(&quot;通过单向管道发送数据&quot;.getBytes()); buf.flip(); sinkChannel.write(buf); //3. 读取缓冲区中的数据 Pipe.SourceChannel sourceChannel = pipe.source(); buf.flip(); int len = sourceChannel.read(buf); System.out.println(new String(buf.array(), 0, len)); sourceChannel.close(); sinkChannel.close(); } ","link":"https://memorykki.github.io/nio/"},{"title":"JUC","content":"java.util.concurrent 1.前言 1.1 什么是JUC JUC —— （java.util.concurrent）是一个包名的缩写，该包下存放的均为多线程相关类 1.2 Java中默认有几个线程？ 一共有 2 个 Java程序线程 GC回收器线程 2. 线程 2.1 线程的六大状态 NEW 新生 RUNNABLE 运行 BLOCKED 阻塞 WAITING 等待 TIMED_WAITING 超时等待 TERMINATED 死亡 2.2 wait与sleep的区别 来自不同的类 wait -&gt; Object sleep -&gt; Thread 关于锁的释放 wait：释放锁 sleep：不释放锁、 使用的范围不同 wait只能在synchronized中使用 ​ 为什么？看我原创整理笔记👇 ​ Java线程之为何wait()和notify()必须要用同步块中 - VioletTec's Blog (mcplugin.cn) sleep可以在任意地方使用 3. Lock（锁） Lock是一个接口，位于java.util.concurrent.locks包下 其有多个实现类。 3.1 可重入锁ReentrantLock 默认情况下，ReentrantLock的构造方法默认是new一个不公平（unfair）锁 但是在构造方法中传入一个boolean，即可控制new的锁是否为公平锁 true：公平锁（fair lock） false：不公平锁（unfair lock） 为什么默认要用非公平锁？ 因为公平。因为如果使用公平锁，会有可能导致执行耗时长的线程优先执行，会导致CPU使用效率下降。 3.1.1 公平锁和非公平锁 公平锁：先来后到（必须是先来的先执行） 非公平锁：非前来后到，可插队（根据CPU进行调度） 3.2 Lock与synchronized的区别 Synchronized是内置关键字，Lock是一个类 Synchronized无法判断是否获取到了锁，Lock可判断是否获得到了锁 Synchronized会自动获取和释放锁 Synchronized 线程 1（获得锁，阻塞）、线程2（等待，傻傻的等）；Lock锁就不一定会等待下去 Synchronized 可重入锁，不可以中断的，非公平 Lock ：可重入锁，可以判断锁，非公平（可以自己设置） Synchronized 适合锁少量的代码同步问题，Lock 适合锁大量的同步代码 3.3 虚假唤醒 当一个条件满足时，很多线程都被唤醒了，但是只有其中部分是有用的唤醒，其它的唤醒都是无用功 1.比如说买货，如果商品本来没有货物，突然进了一件商品，这是所有的线程都被唤醒了，但是只能一个人买，所以其他人都是假唤醒，获取不到对象的锁 比如一下代码： package duoxiancheng.bao; /* * 虚假唤醒的解决： * wait要始终保证在while循环当中。 */ public class LockTest { public static void main(String[] args) { Clerk clerk = new Clerk(); Producter producter = new Producter(clerk); Customer customer = new Customer(clerk); new Thread(producter,&quot;生产者A&quot;).start(); new Thread(customer,&quot;消费者A&quot;).start(); new Thread(producter,&quot;生产者B&quot;).start(); new Thread(customer,&quot;消费者B&quot;).start(); } } // 售货员 class Clerk { private int product = 0; // 进货 public synchronized void add() { // 产品已满 while (product &gt;=1) { System.out.println(Thread.currentThread().getName() + &quot;: &quot; + &quot;已满！&quot;); try { this.wait(); } catch (InterruptedException e) { } } ++product; // 该线程从while中出来的时候，是满足条件的 System.out.println(Thread.currentThread().getName() + &quot;: &quot; +&quot;....................进货成功，剩下&quot;+product); this.notifyAll(); } // 卖货 public synchronized void sale() { while (product &lt;=0) { System.out.println(Thread.currentThread().getName() + &quot;: &quot; + &quot;没有买到货&quot;); try { this.wait(); } catch (InterruptedException e) { } } --product; System.out.println(Thread.currentThread().getName() + &quot;:买到了货物，剩下 &quot; + product); this.notifyAll(); } } // 生产者 class Producter implements Runnable { private Clerk clerk; public Producter(Clerk clerk) { this.clerk = clerk; } // 进货 @Override public void run() { for(int i = 0; i &lt; 20; ++i) { try { Thread.sleep(200); } catch (InterruptedException e) { } clerk.add(); } } } // 消费者 class Customer implements Runnable { private Clerk clerk; public Customer(Clerk clerk) { this.clerk = clerk; } // 买货 @Override public void run() { for(int i = 0; i &lt; 20; ++i) { clerk.sale(); } } } 当add了一个产品时，会notifyAll，唤醒所有线程。但是并非所有线程都需要sale一个产品。 如果使用if，若当一个线程执行完if(product &gt;= 1)后跳过wait语句，然后将CPU时间让出。 如果重复以上步骤，有许多线程都出现该问题时，当他们返回CPU现场，获得CPU运行时间的时候，则会继续执行sale方法，导致多个线程同时sale。如果加上while，当一个县城跳过wait时，让出CPU后，再获得CPU时，会跳到while(product &gt;= 0)重新判断一次，防止虚假唤醒 （add同理） 4. 8锁的现象 意义：加深我们对被锁的物体的理解。 具体代码：见视频，懒得写了（dog 5. Callable（简单） **callable特点：**可以返回内容，可以抛出异常 package com.kuang.callable; import java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.FutureTask; import java.util.concurrent.locks.ReentrantLock; /** * 1、探究原理 * 2、觉自己会用 */ public class CallableTest { public static void main(String[] args) throws ExecutionException,InterruptedException { // new Thread(new Runnable()).start(); // new Thread(new FutureTask&lt;V&gt;()).start(); // new Thread(new FutureTask&lt;V&gt;( Callable )).start(); new Thread().start(); // 怎么启动Callable MyThread thread = new MyThread(); FutureTask futureTask = new FutureTask(thread); // 适配类 new Thread(futureTask,&quot;A&quot;).start(); new Thread(futureTask,&quot;B&quot;).start(); // 结果会被缓存，效率高 Integer o = (Integer) futureTask.get(); //这个get 方法可能会产生阻塞！把他放到 最后 // 或者使用异步通信来处理！ System.out.println(o); } } class MyThread implements Callable&lt;Integer&gt; { @Override public Integer call() { System.out.println(&quot;call()&quot;); // 会打印几个call // 耗时的操作 return 1024; } } 6. 常用的辅助类(必会) 6.1 CountDownLatch 允许一个或多个线程等待直到在其他线程中执行的一组操作完成的同步辅助 CountDownLatch coutnDownLatch = new CoutDownLatch(6);//减法计数器 for(int i=1;i&lt;=6;i++){ new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName + &quot;go out&quot;); coutDownLatch.countDown();//计数器-1 },String.valueof(i)).start(); } coutDownLatch.await();//等待计数器归零 System.out.println(&quot;Close the Door!&quot;); CountDownLath常用方法： new CoutDownLatch(int count);//构造方法，用于初始化计数器的值 countDown();//计数器-1 await();//等待线程直到计数器为0为止 6.2 CyclicBarrier 允许一组线程全部等待达到彼此共同的屏障点的同步辅助。 可以理解为：加法计数器 //集齐七颗龙珠召唤神龙。 CyclicBarrier cyclicbarrier = new CyclicBarrier(7,()-&gt;{ System.out.println(&quot;召唤神龙成功！&quot;); }); for(int i=1;i&lt;=7;i++){ new Thread(()-&gt;{ System.out.println(&quot;收集了&quot;+i+&quot;颗龙珠&quot;); },String.valueof(i)).start(); } cyclicBarrier.await();//等待计数器变成7时 CyclicBarrier和CountDownLatch的区别就是： CountDownLatch不可以重置计数。 如果想要重置计数，可以使用CyclicBarrier。 6.3 Semaphore Semaphore：信号量 一个计数信号量，在概念上，信号量维持一组许可证。如果有必要，每个acquire()都会阻塞，直到许可证可用，然后才能使用它 //acquire() 得到 //release() 释放 package com.kuang.add; import java.util.concurrent.Semaphore; import java.util.concurrent.TimeUnit; public class SemaphoreDemo { public static void main(String[] args) { // 线程数量：停车位! 限流！ Semaphore semaphore = new Semaphore(3); for (int i = 1; i &lt;=6 ; i++) { new Thread(()-&gt;{ // acquire() 得到 try { semaphore.acquire(); System.out.println(Thread.currentThread().getName()+&quot;抢到车 位&quot;); TimeUnit.SECONDS.sleep(2); System.out.println(Thread.currentThread().getName()+&quot;离开车 位&quot;); } catch (InterruptedException e) { e.printStackTrace(); } finally { semaphore.release(); // release() 释放 } },String.valueOf(i)).start(); } } } 原理： semaphore.acquire() 获得，假设如果已经满了，等待，等待被释放为止！ semaphore.release(); 释放，会将当前的信号量释放 + 1，然后唤醒等待的线程！ 6.4 Exchanger（笔者补充） 允许两个线程在集合点交换对象，并且在多个管道设计中很有用。 简单来说，Exchanger的.exchange(Object obj)方法，是一个阻塞的交换点。 当两个线程都在.exchange()阻塞时，进行交换。 class FillAndEmpty { Exchanger&lt;DataBuffer&gt; exchanger = new Exchanger&lt;&gt;(); //下面是两个装数据的实体类 DataBuffer initialEmptyBuffer = ... a made-up type DataBuffer initialFullBuffer = ... //FillingLoop ： 不断向DataBuffer中添加数据，并交换给EmptyingLoop class FillingLoop implements Runnable { public void run() { DataBuffer currentBuffer = initialEmptyBuffer; try { while (currentBuffer != null) { addToBuffer(currentBuffer);//从currentBuffer里取出一个数据 if (currentBuffer.isFull()){ //如果是装满了，就交还给EmptyingLoop类进行消费 currentBuffer = exchanger.exchange(currentBuffer); } } } catch (InterruptedException ex) { ... handle ... } } } //EmptyingLoop ： 不断从DataBuffer中取出数据，并交换给FillingLoop class EmptyingLoop implements Runnable { public void run() { DataBuffer currentBuffer = initialFullBuffer; try { while (currentBuffer != null) { takeFromBuffer(currentBuffer); if (currentBuffer.isEmpty()){ //如果消费完了，则交换buffer给FillingLoop进行生产 currentBuffer = exchanger.exchange(currentBuffer); } } } catch (InterruptedException ex) { ... handle ...} } } void takeFromBuffer(DataBuffer dataBuffer){ //..... dataBuffer.take(); //...... } void addToBuffer(DataBuffer dataBuffer){ //..... dataBuffer.add(xxxxx); //...... } void start() { new Thread(new FillingLoop()).start(); new Thread(new EmptyingLoop()).start(); } } 7. 读写锁 独占锁（写锁） 一次只能被一个线程占有 共享锁（读锁） 多个线程可以同时占有 ReadWriteLock 读-读 可以共存！ 读-写 不能共存！ 写-写 不能共存！ 8. 阻塞队列 BlockingQueue：阻塞队列 SynchronousQueue 同步队列： 和其他的BlockingQueue不一样。 没有容量， 进去一个元素，必须等待取出来之后，才能再往里面放一个元素！ 只有 put()、take()两个方法 9. 线程池（重点） 池化技术 程序的运行，本质：占用系统的资源！优化资源的使用=》池化技术 线程池、连接池、内存池、对象池 创建和销毁线程需要从用户态内陷到内核态进行操作，是一个耗时的操作。 线程池：三大方法、7大参数、4种拒绝策略 池化技术：池化技术简单点来说，就是提前保存大量的资源，以备不时之需。 ↓↓↓为了方便理解线程池，我插入一些题外话，方便深入理解线程池↓↓↓： 什么是线程？ 线程是调度CPU的最小单元，也叫做轻量级进程LWP（Light Weight Process） Java中有两种线程模型： 用户级线程（ULT）（Uer Level Thread） 内核级线程（KLT）（Kernel Level Thread） ULT：用户程序实现，不依赖操作系统核心，应用提供创建，同步，调度和管理线程的函数来控制用户线程，不需要用户态/内核态的切换，速度快，内核对ULT无感知，线程阻塞则进程（包括它的所有线程）阻塞。 KLT：系统内核管理线程（KLT），内核保存线程的状态和上下文的信息，线程阻塞不会引起进程阻塞。在处理器系统上多线程在多处理器上并行运行。现成的创建，调度和管理由内核完成，效率比UTL慢，比进程操作快。 市面上绝大多数的Java虚拟机都是使用的KLT，及系统内核管理线程。 文字描述比较抽象，我们来画一个图描述一下ULT和KLT的区别 JVM由于在用户空间，无权使用内核空间，只能调用系统开放的API（如：Linux开放的p_thread函数）去操作线程，在映射到底层的CPU上，由于调度API需要提高权限，所以会把自身状态陷入到内核态来取得权限。 用户所有的线程都会存放在线程表中，由内核统一的调度和维护。 这就是为什么会进行用户态/内核态状态切换的原因。 根据上图，JVM创建和执行线程可以列为一下这么几个步骤 线程会使用库调度器 之后陷入到内核空间 创建内核线程 内核中的线程会被维护到线程表中 由操作系统调度程序去调度 CPU会根据调度算法分配时间 把没有执行完的线程写入给你高速内存区（SSP） 内核空间中有一个高速内存区SSP（程序任务运行状态段）是用于存储还没有执行完成，但是被分配的时间已经用完的线程中的数据，，等待下一次被分配到了时间后，就把保存在SSP里的上下文信息加载到CPU的缓存。 综上所述，线程是一个稀缺资源，他的创建和销毁是一个相对偏重且消耗资源的操作，而Java线程依赖于内核进程，创建线程需要进行操作系统状态切换，为避免过度消耗，我们要设法重用线程执行多个任务。 线程池就是一个线程缓存，负责对线程进行统一分配，调度与监控 他的优点有很多，最突出的优点就是： 重用存在的线程，减少线程的创建，消亡所用开销，提升性能 提高响应速度。当任务到达时，可以不需要等待线程的创建就可以立即执行 提高线程的可管理性，可统一分配，调度和监控。 那么线程池是如何把线程统一分配调度与监控的呢？ 【画图太累了，我就用的网图，图片来源视频：https://www.bilibili.com/video/av88030891 】 【画图太累了，我就用的网图，图片来源：https://blog.csdn.net/lchq1995/article/details/85230399 】 我们在NEW一个最基本的线程池的时候，会传入这么一下几个参数： corePoolSize：线程池核心线程数量 maximumPoolSize：线程池最大线程数两 keepAliveTime：空闲线程存活时间 corePoolSize顾名思义就是最大核心线程数量，是线程池可以同时执行的线程数量。 maximumPoolSize，既然有corePoolSize，那么如果corePoolSize满了怎么办呢？这时候就会用到一个队列，叫阻塞队列（Block Queue） 什么是BlockQueue？它有什么特点？ 既然是Queue，那么久满足队列模型的（FIFO）原则，一端放入，另一端取出。（First In First Out） 阻塞队列有一个特点就是：在任意时刻，不管并发量有多高，永远只有一个线程能进行队列的如对或出队，所以BlockQueue是一个线程安全的队列。 并且如果队列满了，只能进行出队操作，所有入队操作必须等待，也就是阻塞。 如果队列为空，那么就只能进行入队操作，所有出队操作必须等待，也就是阻塞。 一旦线程池的线程量满了，那么新被execute进来的线程，就会被存储进BlockQueue，BlockQueue的大小就是maximumPoolSize - corePoolSize的大小。 线程池和五种状态： Running：能接受新的execute以及处理已添加的任务 Shutdown：不接收任何新的execute，可以处理已添加的任务 Stop：不接受任何新的execute，不处理已添加的任务 Tidying：所有任务已经终止，ctl记录的任务数量为0. Termiated：线程池彻底终止，则线程池转换为Terminated状态。 那么这么多线程池状态和这么多线程的信息，是如何保存的呢？ 这里线程池内部用到了一个32字节的Integer类型来记录线程池的状态和线程数量信息。 这个Integer类型的高3未二进制用来表示线程池的状态，后29为用来表示线程的数量。 线程池定义了这么几个数字作为线程的状态 RUNNING = -1 SHUTDOWN = 0 STOP = 1 TIDYING = 2 TERMINATED = 3 并且所有数字都想做移位29位。 &lt;&lt; COUNT_BITS（COUNT_BITS=29) 最终会得到高三位为： RUNNING = 111 SHUTDOWN = 000 STOP = 001 TIDYING = 010 TERMINATED = 011 他是怎么得到的呢？ 我们来回顾一下基础 拿-1来举个例子 众所周知， 1在32位Integer的类型中二进制为： 0000 0000 0000 0000 0000 0000 0000 0001 那么-1就应该去1的反位后+1，并且再加上一个符号位1000 则，-1就应该为： 1000 1111 1111 1111 1111 1111 1111 1111 那么-1向左移位29位，低位补0，那么则 -1&lt;&lt;29 等于 1110 0000 0000 0000 0000 0000 0000 0000 所以高三位为111 所以这就是RUNNING的高三为为111的由来 后面的29位用于存储线程的数量。 这种应用基本数据高效存储的思想可以用于存储一些记录，有点就是不用去多个变量的读取，提升速度。 具体线程池的实现可以百度搜索JAVA线程池的实现，在这里只是浅谈一下线程池的好处以及浅层原理。 ↑↑↑以上为插入的题外内容，方便对线程池的理解。转载自本人原创博客。↑↑↑ 原文地址：https://blog.mcplugin.cn/p/225 根据阿里巴巴的《Java开发手册》中 不允许使用Executors去创建线程/线程池，而是使用ThreadPoolExecutor 为什么会发生OOM？因额外FixedThreadPool、SingleThreadPool的允许请求的最大队列长度，以及CachedThreadPool允许创建的最大线程数量为Integer.MAXVALUE，Integer最大值为21亿多（2147483647），若发生意外，则容易在线程池的队列中发生OOM 禁止以下列形式创建线程/线程池❌↓ ExecutorService threadPool1 = Executors.newSingleThreadExecutor();// 单个线程 ExecutorService threadPool2 = Executors.newFixedThreadPool(5); // 创建一个固定的线程池的大小 ExecutorService threadPool3 = Executors.newCachedThreadPool(); // 可伸缩的，遇强则强，遇弱则弱 通过查看Executors.newXxxThreadPool();的方法，我们可以看到它底层都使用了ThreadPoolExecutor类。 我们可以看到CachedThreadPool的最大线程数量的确为Interger的最大值，即2147483647 即，Executors的本质还是使用了ThreadPoolExecutor(); 我们接着来看ThreadPoolExecutor()的构造方法： 最大线程应如何定义？ CPU 密集型 -&gt; 有几个核心就是可以同时并行几个 将最大线程可以定义为CPU的核心数量 获取CPU核心数量: Runtime.getRuntime().availableProcess(); IO 密集型 -&gt; 判断你的程序中十分耗IO的线程有多少个。至要大于这个数字就可以了！一般是耗IO线程数量的2倍 10. 四大函数式接口（重点、必须掌握） 新时代程序员特色社会主义编程技能： lambda表达式 链式编程 函数式接口 流式计算 函数式接口：只有一个方法的接口。这个接口有一个注解：@FunctionalInterface 学习目的：简化编程模型，在新版本的底层大量应用。 如：list.foreatch(消费者类型的函数式接口); 四大原生的函数式接口： Consumer Function Predicate Supplier 就记到这里，下面都没在手动记笔记了...懒了。 ","link":"https://memorykki.github.io/juc/"},{"title":"MySQL","content":"Mysql高级 索引是帮助MySQL高效获取数据的排好序的数据结构，将无序的数据编程有序的查询。 二叉树：单链 红黑树：二叉平衡树，提高一倍。旋转成本，树的高度可能会很高，效率很差 Hash表： B-Tree：改进红黑树，使高度尽可能小，节点横向扩展，一个存储更多的节点 。 索引数据分布在叶子、非叶子中，索引不重复，索引从左至右递增。 B+Tree：叶子节点指针连接，非叶子节点不存储数据，叶子节点包含所有索引，非叶子节点包含一部分索引，冗余索引（只需要冗余主键），数据放在叶子节点。 双向链表便于大于、小于的索引查找 树的高度越小越好，一个节点上存储更多索引（16KB），所以从磁盘一次加载的数据多，二分查找速度快 B+树非叶子节点不存储数据，也就能放更多的索引，存储量大，B树节点有数据就不行。所以决定树的高度：非叶子节点中能放的索引数量。 三层即可存储2000w+索引，在mysql启动时将上层索引加载到内存中，索引取数据只需要大约一次IO。 .frm 表结构 .MYD 表数据 .MYI 表索引 myisam查询过程：条件是否是索引，是则遍历myiB+树，根据地址找到myd磁盘地址。 .frm 表结构 .ibd 数据+索引，表数据也是按照B+组织 innodb推荐建立主键：自增主键： 聚集索引：叶节点包含了完整的数据记录，数据和索引聚集在一起，innodb主键索引。 innodb非主键索引（回表、二次）：叶子节点存储主键值：一致性和节省存储空间。 非聚集索引：索引和数据分开存储。（myisam）效率不如聚集，回表：二次索引（拿到索引再去磁盘遍历索引） 必须要建主键？Mysql会使用主键构建B+树，没有主键Mysql会找一个不重复的列来构建，找不到就生成一个隐藏列rowid来构建，所以节省空间。 为什么不能用UUID？UUID非整型非自增，比较大小效率慢，所以推荐整型。B+范围查找只需两次，非自增只向后插，不用分页，分裂次数少，维护B+结构成本小，推荐自增。 索引就是目录，用空间换时间，B+的节点就是16KB的页，查找就是遍历页上的有序数据，一个页上有多个节点，但是页越来越长，形成链表效率会降低，所以构建目录的目录，即树。 mysql运行时间长之后，缓存加载到内存越来越大， Hash索引： 一次计算。不支持范围查询，hash冲突问题 联合索引： 多个字段，按先后顺序，谁先排好序就以谁的排序为准，后面的忽略。 建立联合索引需要重新构建B+树，所以会和主键索引构建的B+树大量冗余数据，资源浪费，所以可以使联合索引树叶子节点不存储数据，只存储主键，然后回表，是个折中的办法。 主键索引： select * from t where b&gt;1;(bcd联合索引) 选择索引 利用联合索引可以找到数据，但是查找的是*全部数据，叶子节点只有bcd三列数据，所以需要7次回表查出全部数据，全表扫描走主键只需要四次，更快，所以实际执行的是全表扫描，非联合索引。 如果是select b,c,d,a；那么也能走联合索引，因为想要的四个字段都在一棵树上，叶子是主键a select b;联合索引的叶子能够找出满足要求的数据，就不用走全表扫描。 同样地，如果联合索引回表次数&lt;全表扫描加载页次数，也不用走全扫描。 select b from t;没有where条件也能走联合索引，主键索引和联合索引都能找到所需数据，并且如果二者的页数相等，但是因为联合索引叶子节点存储主键，主键索引叶子存储数据，所以联合索引的一页比主键索引的一页存储的数据更多，虽然不完整，只有bcd，但是查询我们只要b就够了，所以不走主键。 select * from t order by b,c,d;走bcd不用排序但需要回表八次，全表扫描需要四次，需要排序。因为只有8条数据，排序很快，所以走的是全表扫描。如果select b不需要回表，走的是联合索引。 最左前缀法则：查询从最左前列开始并且二不能跳过中间。也就是联合索引触发必须从包含第一个列的条件开始。 原理：比如age=30，如果不考虑name是否有序而使用age索引的前提是前面的name相同，那么就应该理解为同一列的节点的age是有序的，但是不是有序的，需要全表扫描（从左向右逐个检查）。判断索引是否会用到的原则，当前列是否有序。 Mysql中类型不匹配时，字符转数字，数字型字符会自动类型转成数字，非数字型字符会转成0，即： select 'a'=0; true select 'b12'=0; true select '1'=0; false 隐式的类型转换可能会导致索引失效。 Mysql5.8之前仅支持升序索引，5.7支持语法层面的降序，但是索引组织仍然是升序的。 B树 B+树 B树每个节点都存储key和data，所有节点组成这棵树，并且叶子节点指针为null。 B+树只有叶子节点存储data，叶子节点包含了这棵树的所有键值，叶子节点不存储指针。 MySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指 针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。 原因有很多，最主要的是这棵树矮胖，呵呵。一般来说，索引很大，往往以索引文件的形式存储的磁盘上，索引查找时产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的时间复杂度。树高度越小，I/O次数越少。 那为什么是B+树而不是B树呢，因为它内节点不存储data，这样一个节点就可以存储更多的key。 索引类型 普通可重复，唯一不可重复，唯一可以有null，可以有多个，主键不能有null，且只能有一个。全文索引like关键字。 聚簇 非聚簇 innodb仅叶子节点将索引数据放在一起。 表中除了主键索引的其他索引都是辅助索引，其叶子节点存储地十主键，类非聚簇索引，但存储的不是行地址，而是主键，所以非聚簇索引都是辅助索引。 索引数据结构 索引的设计原则 适合索引的是where 使用前缀段短索引 不能在更新频繁、数据区分度小、重复度高、数据量小的列建索引 锁 &lt; &lt; &lt; &lt; 1 4 5 7，间隙锁锁住234，临建锁锁住1234. 行锁页锁会出现死锁。 临建锁next key = 记录锁record + 间隙锁gap，三者都属于行锁。 意向锁提高了加锁效率。 innodb默认采用行锁，myisam默认采用表锁。 执行计划 id：select的序号，顺序增长，越大优先级越高； select_type： simple：简单查询，不包含子查询和union； primary：复杂查询中最外层的select； subquery：select中的子查询； derived：from中的子查询。 table：正在访问哪个表 from有子查询时，drivenN，N代表id=N的查询 有union时，union1,2，表示参与union的id type：关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。 依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL 一般来说，得保证查询达到range级别，最好达到ref NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可以单独查找索引来完成，不需要在执行时访问表 const：通过索引一次命中； system：表中只有一行记录； eq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。唯一性索引扫描 explain select * from film_actor left join film on film_actor.film_id = film.id; ref：相比 eq_ref，非唯一性索引扫描，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行。 和eq_ref都需要回表 range：范围扫描，通常出现在 in(), between ,&gt; ,&lt;, &gt;= 等操作中。使用一个索引来检索给定范围的行。 index：扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这种通常比ALL快一些 ALL：即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了 key：显示mysql实际采用哪个索引来优化对该表的访问。 如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index ； possible_keys：显示查询可能使用哪些索引来查找。 explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引对此查询帮助不大，选择了全表查询。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提高查询性能，然后用 explain 查看效果 。 key_len：mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列，特别是联合索引。 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引 ref：在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（等值查询），字段名（例：film.id） rows：mysql估计要读取并检测的行数，越少越好，注意这个不是结果集里的行数 filtered：读取rows行，返回x行，x/rows 返回百分比 Extra：额外信息 Using index：使用覆盖索引 ，性能高 覆盖索引定义：mysql执行计划explain结果里的key有使用索引，如果select后面查询的字段都可以从这个索引的树中获取，这种情况一般可以说是用到了覆盖索引，extra里一般都有using index；覆盖索引一般针对的是辅助索引，整个查询结果只通过辅助索引就能拿到结果，不需要通过辅助索引树找到主键，再通过主键去主键索引树里获取其它字段值。 explain select film_id from film_actor where film_id = 1; Using where：使用 where 语句来处理结果，和是否读取索引无关 explain select * from actor where name = 'a'; Using index condition：查询的列不完全被索引覆盖，where条件中是一个前导列的范围； explain select * from film_actor where film_id &gt; 1; Using temporary：排序、分组时等，mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化 explain select distinct name from film; Using filesort：将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一般也是要考虑使用索引来优化的 Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段是 explain select min(id) from film; 事务 原子性、一致性、隔离性、持久性。 一致性：事务之前id唯一，不能操作之后不唯一了。 readview只针对查询操作，如果在此期间其他事务插入了新数据，就会导致之幻读。配合间隙锁解决。 主从同步 Buffer Pool 客户端 - server（连接器 - 查询缓存 - 分析器 - 优化器 - 执行器） - 存储引擎（查询结果，返回结果集） Buffer Pool 128MB：查找的结果从磁盘复制到BufferPool，先从pool查，修改先改pool，离散的页数组： free list：记录空闲的页，便于插入 flush list：记录脏页，便于后台线程寻找 lru list：pool占满时，最近最久未使用的页被淘汰，头部是新的，尾部是旧的 对于全表扫描，大量数据会将pool换掉。Innodb将lru分为热点区5/8，冷数据区3/8，优先淘汰冷数据区，两次访问到数据的间隔&gt;1s，表示一个正常的频率，转移冷数据-&gt;热数据，全表扫描时间隔小于1s，就不会发生冷替换热的情况。 redo log 脏页刷新丢失： 1、修改pool产生脏页 2、生成逻辑redo log（mysql内存中脏页持久化到磁盘需要，挂掉之后重新执行redo恢复） -&gt; log buffer 3、redo log 持久化（当事务提交时） 4、bin log 持久化 5、undo log 6、修改成功 redo(log file0,log file1)file满检查点：将log file持久化到磁盘。logfile太小持久化频繁，太大恢复启动慢。 **持久化机制：**不持久化、立即持久、立即刷新到OS缓存而不立即持久化 执行事务生成bin log redo bin log innodb mysql级别 物理的、记录一页的某个位置的数据修改，速度快 逻辑的sql语句，慢。用于主从复制 undo log：反向日志，记录修改之前的数据，回滚使用，实现事物的隔离级别。 double write （OS中） innodb数据页16KB，OS页4KB分四次写入。解决问题：如果中途挂了，就不清楚是写入了还是没写入，即没有原子性，所以出现双写缓存。 innodb写入双写缓存即认为成功，并产生redo log，成功之后log失效 刷新一次先写入双写缓存，再写入表空间两部完成。如果第一步挂了 ，这时候可以使用redo log恢复，如果第二部挂了，可以重新从双写缓存拿到完整的数据重新写。 Change Buffer 插入缓冲区：写操作更新数据页、索引页，数据页由日志优化，change优化索引页。 储存在buffer pool（索引页+数据页）中，占25%。存储修改的信息（update语句），修改时数据页被更新，但索引页不更新，暂时存在change buffer中，等到下次查询走索引调用的时候将磁盘中的索引调到pool中，再和change中对应的update整合，拿到正确的索引页，即延迟更新机制，使update效率变高了。 可重复读 开启两个事务，a先读取，b修改此值，a再次读取，结果仍为之前的旧值。 undo实现：每个事务通过链表readview记录和它同时存在的活跃的事务，以及它们的undo log，在第二次查询时，按照记录的undolog，判断生成的事务是否在readview里，若在则执行undolog，这样就可以还原之前的值，然后输出。 可重复读在第一次读时生成readview，后面使用同一个readview；读已提交在别的事务提交之后发生更新，判断undo链发现不在readview里，就直接取最新值。也就是说，在别的事务提交之后，查找的数据还不是最新的，而是事务启动时候的，读取已提交却可以拿到最新数据，所以可重复读相对更严。 隔离级别越来越严，越难读到最新值，效率也就越低。 视图 视图（View）是一种虚拟存在的表。视图并不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的。通俗的讲，视图就是一条SELECT语句执行后返回的结果集。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。 能够对数据进行修改，但是只能修改一张表中的数据。 视图相对于普通的表的优势主要包括以下几项。 简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。 数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。 存储过程 存储过程和函数是 事先经过编译并存储在数据库中的一段 SQL 语句的集合，调用存储过程和函数可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。 函数 ： 是一个有返回值的过程 ； 过程 ： 是一个没有返回值的函数 ； 触发器 触发器是与表有关的数据库对象，指在 insert/update/delete 之前或之后，触发并执行触发器中定义的SQL语句集合。触发器的这种特性可以协助应用在数据库端确保数据的完整性 , 日志记录 , 数据校验等操作 。 体系结构 整个MySQL Server由以下组成 Connection Pool : 连接池组件 Management Services &amp; Utilities : 管理服务和工具组件 SQL Interface : SQL接口组件 Parser : 查询分析器组件 Optimizer : 优化器组件 Caches &amp; Buffers : 缓冲池组件 Pluggable Storage Engines : 存储引擎 File System : 文件系统 连接层、服务层、引擎层、存储层。 MySQL提供了插件式的存储引擎架构，存储引擎是基于表的，而不是基于库的。 MyISAM InnoDB MYISAM适合查询，InnoDB适合写。 特点 InnoDB MyISAM MEMORY MERGE NDB 存储限制 64TB 有 有 没有 有 事务安全 支持 锁机制 行锁(适合高并发) 表锁 表锁 表锁 行锁 B树索引 支持 支持 支持 支持 支持 哈希索引 支持 全文索引 支持(5.6版本之后) 支持 集群索引 支持 数据索引 支持 支持 支持 索引缓存 支持 支持 支持 支持 支持 数据可压缩 支持 空间使用 高 低 N/A 低 低 内存使用 高 低 中等 低 高 批量插入速度 低 高 高 高 高 支持外键 支持 外键 MySQL支持外键的存储引擎只有InnoDB ， 在创建外键的时候， 要求父表必须有对应的索引 ， 子表在创建外键的时候， 也会自动的创建对应的索引。 MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。 存储方式 InnoDB 存储表和索引有以下两种方式 ： ①. 使用共享表空间存储， 这种方式创建的表的表结构保存在.frm文件中， 数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。 MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。 ②. 使用多表空间存储， 这种方式创建的表的表结构仍然存在 .frm 文件中，但是每个表的数据和索引单独保存在 .ibd 中。 MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。 每个MyISAM在磁盘上存储成3个文件，其文件名都和表名相同，但拓展名分别是 ： .frm (存储表定义)； .MYD(MYData , 存储数据)； .MYI(MYIndex , 存储索引)； 选择 InnoDB : 是Mysql的默认存储引擎，用于事务处理应用程序，支持外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询意外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。InnoDB存储引擎除了有效的降低由于删除和更新导致的锁定， 还可以确保事务的完整提交和回滚，对于类似于计费系统或者财务系统等对数据准确性要求比较高的系统，InnoDB是最合适的选择。 MyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。 避免索引失效 1). 全值匹配 ，对索引中所有列都指定具体值。 改情况下，索引生效，执行效率高。 explain select * from tb_seller where name='小米科技' and status='1' and address='北京市'\\G; 2). 最左前缀法则 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始，并且不跳过索引中的列。 匹配最左前缀法则，走索引： 违法最左前缀法则 ， 索引失效： 如果符合最左法则，但是出现跳跃某一列，只有最左列索引生效： 3). 范围查询右边的列，不能使用索引 。 根据前面的两个字段name ， status 查询是走索引的， 但是最后一个条件address 没有用到索引。 4). 不要在索引列上进行运算操作， 索引将失效。 5). 字符串不加单引号，造成索引失效。 由于，在查询是，没有对字符串加单引号，MySQL的查询优化器，会自动的进行类型转换，造成索引失效。 6). 尽量使用覆盖索引，避免select * 尽量使用覆盖索引（只访问索引的查询（索引列完全包含查询列）），减少select * 。 如果查询列，超出索引列，也会降低性能。 7). 用or分割开的条件， 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。 示例，name字段是索引列 ， 而createtime不是索引列，中间是or进行连接是不走索引的 ： explain select * from tb_seller where name='黑马程序员' or createtime = '2088-01-01 12:00:00'\\G; 8). 以%开头的Like模糊查询，索引失效。 解决：不使用select *，使用覆盖索引select 9). 如果MySQL评估使用索引比全表更慢，则不使用索引。 10). is NULL ， is NOT NULL 有时索引失效。 11). in 走索引， not in 索引失效。 12). 单列索引和复合索引。 尽量使用复合索引，而少使用单列索引 。 SQL优化 大量插入 按照主键插入 关闭唯一校验 导入数据前执行 SET UNIQUE_CHECKS=0，关闭唯一性校验，在导入结束后执行SET UNIQUE_CHECKS=1， 手动提交事务 导入前执行 SET AUTOCOMMIT=0，关闭自动提交，导入结束后再执行 SET AUTOCOMMIT=1，打开自动提交 insert语句 尽量使用一条语句包含多个值插入 开一个事务插入 有序插入 分页 一般分页查询时，通过创建覆盖索引能够比较好地提高性能。一个常见又非常头疼的问题就是 limit 2000000,10 ，此时需要MySQL排序前2000010 记录，仅仅返回2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。 先查主键，在根据主键查全部记录 适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。 使用SQL提示 USE INDEX：提供希望MySQL去参考的索引列表，就可以让MySQL不再考虑其他可用的索引。 IGNORE INDEX：忽略一个或者多个索引 FORCE INDEX：强制MySQL使用一个特定的索引 MVCC https://www.cnblogs.com/xuwc/p/13873611.html 什么是MVCC MVCC MVCC，全称Multi-Version Concurrency Control，即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。 多版本控制: 指的是一种提高并发的技术。最早的数据库系统，只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，只有写写之间相互阻塞，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。在内部实现中，与Postgres在数据行上实现多版本不同，InnoDB是在undolog中实现的，通过undolog可以找回数据的历史版本。找回的数据历史版本可以提供给用户读(按照隔离级别的定义，有些读请求只能看到比较老的数据版本)，也可以在回滚的时候覆盖数据页上的数据。在InnoDB内部中，会记录一个全局的活跃读写事务数组，其主要用来判断事务的可见性。 MVCC是一种多版本并发控制机制。 MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读 当前读 快照读 在学习MVCC多版本并发控制之前，我们必须先了解一下，什么是MySQL InnoDB下的当前读和快照读? 当前读 像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁 快照读 像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本 说白了MVCC就是为了实现读-写冲突不加锁，而这个读指的就是快照读, 而非当前读，当前读实际上是一种加锁的操作，是悲观锁的实现 当前读 快照读 MVCC的关系 准确的说，MVCC多版本并发控制指的是 “维持一个数据的多个版本，使得读写操作没有冲突” 这么一个概念。仅仅是一个理想概念 而在MySQL中，实现这么一个MVCC理想概念，我们就需要MySQL提供具体的功能去实现它，而快照读就是MySQL为我们实现MVCC理想模型的其中一个具体非阻塞读功能。而相对而言，当前读就是悲观锁的具体功能实现 要说的再细致一些，快照读本身也是一个抽象概念，再深入研究。MVCC模型在MySQL中的具体实现则是由 3个隐式字段，undo日志 ，Read View 等去完成的，具体可以看下面的MVCC实现原理 MVCC能解决什么问题 数据库并发场景有三种，分别为： 读-读：不存在任何问题，也不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失 备注：第1类丢失更新：事务A撤销时，把已经提交的事务B的更新数据覆盖了；第2类丢失更新：事务A覆盖事务B已经提交的数据，造成事务B所做的操作丢失 MVCC带来的好处是？ 多版本并发控制（MVCC）是一种用来解决读-写冲突的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题 小结一下咯 总之，MVCC就是因为大牛们，不满意只让数据库采用悲观锁这样性能不佳的形式去解决读-写冲突问题，而提出的解决方案，所以在数据库中，因为有了MVCC，所以我们可以形成两个组合： MVCC + 悲观锁 MVCC解决读写冲突，悲观锁解决写写冲突 MVCC + 乐观锁 MVCC解决读写冲突，乐观锁解决写写冲突 这种组合的方式就可以最大程度的提高数据库并发性能，并解决读写冲突，和写写冲突导致的问题 MVCC的实现原理 MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。所以我们先来看看这个三个point的概念 隐式字段 每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段 DB_TRX_ID 6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID DB_ROLL_PTR 7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里） DB_ROW_ID 6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引 实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了 如上图，DB_ROW_ID是数据库默认为该行记录生成的唯一隐式主键，DB_TRX_ID是当前操作该记录的事务ID,而DB_ROLL_PTR是一个回滚指针，用于配合undo日志，指向上一个旧版本 undo log undo log主要分为两种： insert undo log 代表事务在insert新记录时产生的undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log 事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除 purge 从前面的分析可以看出，为了实现InnoDB的MVCC机制，更新或者删除操作都只是设置一下老记录的deleted_bit，并不真正将过时的记录删除。 为了节省磁盘空间，InnoDB有专门的purge线程来清理deleted_bit为true的记录。为了不影响MVCC的正常工作，purge线程自己也维护了一个read view（这个read view相当于系统中最老活跃事务的read view）;如果某个记录的deleted_bit为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。 对MVCC有帮助的实质是update undo log ，undo log实际上就是存在rollback segment中旧记录链，它的执行流程如下： 一、 比如一个有个事务插入persion表插入了一条新记录，记录如下，name为Jerry, age为24岁，隐式主键是1，事务ID和回滚指针，我们假设为NULL 二、 现在来了一个事务1对该记录的name做出了修改，改为Tom 在事务1修改该行(记录)数据时，数据库会先对该行加排他锁 然后把该行数据拷贝到undo log中，作为旧记录，既在undo log中有当前行的拷贝副本 拷贝完毕后，修改该行name为Tom，并且修改隐藏字段的事务ID为当前事务1的ID, 我们默认从1开始，之后递增，回滚指针指向拷贝到undo log的副本记录，既表示我的上一个版本就是它 事务提交后，释放锁 三、 又来了个事务2修改person表的同一个记录，将age修改为30岁 在事务2修改该行数据时，数据库也先为该行加锁 然后把该行数据拷贝到undo log中，作为旧记录，发现该行记录已经有undo log了，那么最新的旧数据作为链表的表头，插在该行记录的undo log最前面 修改该行age为30岁，并且修改隐藏字段的事务ID为当前事务2的ID, 那就是2，回滚指针指向刚刚拷贝到undo log的副本记录 事务提交，释放锁 从上面，我们就可以看出，不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表，undo log的链首就是最新的旧记录，链尾就是最早的旧记录（当然就像之前说的该undo log的节点可能是会purge线程清除掉，向图中的第一条insert undo log，其实在事务提交之后可能就被删除丢失了，不过这里为了演示，所以还放在这里） Read View 什么是Read View? 什么是Read View，说白了Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大) 所以我们知道 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。 Read View`遵循一个可见性算法，主要是将`要被修改的数据`的最新记录中的`DB_TRX_ID`（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果`DB_TRX_ID`跟Read View的属性做了某些比较，不符合可见性，那就通过`DB_ROLL_PTR`回滚指针去取出`Undo Log`中的`DB_TRX_ID`再比较，即遍历链表的`DB_TRX_ID`（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的`DB_TRX_ID`, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新`老版本 那么这个判断条件是什么呢？即changes_visible方法（不完全哈，但能看出大致逻辑），该方法展示了我们拿DB_TRX_ID去跟Read View某些属性进行怎么样的比较 在展示之前，我先简化一下Read View，我们可以把Read View简单的理解成有三个全局属性 trx_list（名字我随便取的） 一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID up_limit_id 记录trx_list列表中事务ID最小的ID low_limit_id ReadView生成时刻系统尚未分配的下一个事务ID，也就是目前已出现过的事务ID的最大值+1 首先比较DB_TRX_ID &lt; up_limit_id, 如果小于，则当前事务能看到DB_TRX_ID 所在的记录，如果大于等于进入下一个判断 接下来判断 DB_TRX_ID 大于等于 low_limit_id , 如果大于等于则代表DB_TRX_ID 所在的记录在Read View生成后才出现的，那对当前事务肯定不可见，如果小于则进入下一个判断 判断DB_TRX_ID 是否在活跃事务之中，trx_list.contains(DB_TRX_ID)，如果在，则代表我Read View生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的；如果不在，则说明，你这个事务在Read View生成之前就已经Commit了，你修改的结果，我当前事务是能看见的 整体流程 我们在了解了隐式字段，undo log， 以及Read View的概念之后，就可以来看看MVCC实现的整体流程是怎么样了 整体的流程是怎么样的呢？我们可以模拟一下 当事务2对某行数据执行了快照读，数据库为该行数据生成一个Read View读视图，假设当前事务ID为2，此时还有事务1和事务3在活跃中，事务4在事务2快照读前一刻提交更新了，所以Read View记录了系统当前活跃事务1，3的ID，维护在一个列表上，假设我们称为trx_list 事务1 事务2 事务3 事务4 事务开始 事务开始 事务开始 事务开始 … … … 修改且已提交 进行中 快照读 进行中 … … … Read View不仅仅会通过一个列表trx_list来维护事务2执行快照读那刻系统正活跃的事务ID，还会有两个属性up_limit_id（记录trx_list列表中事务ID最小的ID），low_limit_id(记录trx_list列表中事务ID最大的ID，也有人说快照读那刻系统尚未分配的下一个事务ID也就是目前已出现过的事务ID的最大值+1，我更倾向于后者 &gt;&gt;&gt;资料传送门 | 呵呵一笑百媚生的回答) ；所以在这里例子中up_limit_id就是1，low_limit_id就是4 + 1 = 5，trx_list集合的值是1,3，Read View如下图 我们的例子中，只有事务4修改过该行记录，并在事务2执行快照读前，就提交了事务，所以当前该行当前数据的undo log如下图所示；我们的事务2在快照读该行记录的时候，就会拿该行记录的DB_TRX_ID去跟up_limit_id,low_limit_id和活跃事务ID列表(trx_list)进行比较，判断当前事务2能看到该记录的版本是哪个。 所以先拿该记录DB_TRX_ID字段记录的事务ID 4去跟Read View的的up_limit_id比较，看4是否小于up_limit_id(1)，所以不符合条件，继续判断 4 是否大于等于 low_limit_id(5)，也不符合条件，最后判断4是否处于trx_list中的活跃事务, 最后发现事务ID为4的事务不在当前活跃事务列表中, 符合可见性条件，所以事务4修改后提交的最新结果对事务2快照读时是可见的，所以事务2能读到的最新数据记录是事务4所提交的版本，而事务4提交的版本也是全局角度上最新的版本 也正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同 RC RR 正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同 在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View, 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见； 即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见 而在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因 总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。 ","link":"https://memorykki.github.io/Mysql/"},{"title":"MySQL_黑马","content":"Mysql黑马程序员 Mysql高级-day01 MySQL高级课程简介 序号 Day01 Day02 Day03 Day04 1 Linux系统安装MySQL 体系结构 应用优化 MySQL 常用工具 2 索引 存储引擎 查询缓存优化 MySQL 日志 3 视图 优化SQL步骤 内存管理及优化 MySQL 主从复制 4 存储过程和函数 索引使用 MySQL锁问题 综合案例 5 触发器 SQL优化 常用SQL技巧 1. Linux 系统安装MySQL 1.1 下载Linux 安装包 https://dev.mysql.com/downloads/mysql/5.7.html#downloads 1.2 安装MySQL 1). 卸载 centos 中预安装的 mysql rpm -qa | grep -i mysql rpm -e mysql-libs-5.1.71-1.el6.x86_64 --nodeps 2). 上传 mysql 的安装包 alt + p -------&gt; put E:/test/MySQL-5.6.22-1.el6.i686.rpm-bundle.tar 3). 解压 mysql 的安装包 mkdir mysql tar -xvf MySQL-5.6.22-1.el6.i686.rpm-bundle.tar -C /root/mysql 4). 安装依赖包 yum -y install libaio.so.1 libgcc_s.so.1 libstdc++.so.6 libncurses.so.5 --setopt=protected_multilib=false yum update libstdc++-4.4.7-4.el6.x86_64 5). 安装 mysql-client rpm -ivh MySQL-client-5.6.22-1.el6.i686.rpm 6). 安装 mysql-server rpm -ivh MySQL-server-5.6.22-1.el6.i686.rpm 1.3 启动 MySQL 服务 service mysql start service mysql stop service mysql status service mysql restart 1.4 登录MySQL mysql 安装完成之后, 会自动生成一个随机的密码, 并且保存在一个密码文件中 : /root/.mysql_secret mysql -u root -p 登录之后, 修改密码 : set password = password('itcast'); 授权远程访问 : grant all privileges on *.* to 'root' @'%' identified by 'itcast'; flush privileges; 2. 索引 2.1 索引概述 MySQL官方对索引的定义为：索引（index）是帮助MySQL高效获取数据的数据结构（有序）。在数据之外，数据库系统还维护者满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据， 这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。如下面的示意图所示 : 左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找快速获取到相应数据。 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。索引是数据库中用来提高性能的最常用的工具。 2.2 索引优势劣势 优势 1） 类似于书籍的目录索引，提高数据检索的效率，降低数据库的IO成本。 2） 通过索引列对数据进行排序，降低数据排序的成本，降低CPU的消耗。 劣势 1） 实际上索引也是一张表，该表中保存了主键与索引字段，并指向实体类的记录，所以索引列也是要占用空间的。 2） 虽然索引大大提高了查询效率，同时却也降低更新表的速度，如对表进行INSERT、UPDATE、DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。 2.3 索引结构 索引是在MySQL的存储引擎层中实现的，而不是在服务器层实现的。所以每种存储引擎的索引都不一定完全相同，也不是所有的存储引擎都支持所有的索引类型的。MySQL目前提供了以下4种索引： BTREE 索引 ： 最常见的索引类型，大部分索引都支持 B 树索引。 HASH 索引：只有Memory引擎支持 ， 使用场景简单 。 R-tree 索引（空间索引）：空间索引是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少，不做特别介绍。 Full-text （全文索引） ：全文索引也是MyISAM的一个特殊索引类型，主要用于全文索引，InnoDB从Mysql5.6版本开始支持全文索引。 MyISAM、InnoDB、Memory三种存储引擎对各种索引类型的支持 索引 InnoDB引擎 MyISAM引擎 Memory引擎 BTREE索引 支持 支持 支持 HASH 索引 不支持 不支持 支持 R-tree 索引 不支持 支持 不支持 Full-text 5.6版本之后支持 支持 不支持 我们平常所说的索引，如果没有特别指明，都是指B+树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引、复合索引、前缀索引、唯一索引默认都是使用 B+tree 索引，统称为 索引。 2.3.1 BTREE 结构 BTree又叫多路平衡搜索树，一颗m叉的BTree特性如下： 树中每个节点最多包含m个孩子。 除根节点与叶子节点外，每个节点至少有[ceil(m/2)]个孩子。 若根节点不是叶子节点，则至少有两个孩子。 所有的叶子节点都在同一层。 每个非叶子节点由n个key与n+1个指针组成，其中[ceil(m/2)-1] &lt;= n &lt;= m-1 以5叉BTree为例，key的数量：公式推导[ceil(m/2)-1] &lt;= n &lt;= m-1。所以 2 &lt;= n &lt;=4 。当n&gt;4时，中间节点分裂到父节点，两边节点分裂。 插入 C N G A H E K Q M F W L T Z D P R X Y S 数据为例。 演变过程如下： 1). 插入前4个字母 C N G A 2). 插入H，n&gt;4，中间元素G字母向上分裂到新的节点 3). 插入E，K，Q不需要分裂 4). 插入M，中间元素M字母向上分裂到父节点G 5). 插入F，W，L，T不需要分裂 6). 插入Z，中间元素T向上分裂到父节点中 7). 插入D，中间元素D向上分裂到父节点中。然后插入P，R，X，Y不需要分裂 8). 最后插入S，NPQR节点n&gt;5，中间节点Q向上分裂，但分裂后父节点DGMT的n&gt;5，中间节点M向上分裂 到此，该BTREE树就已经构建完成了， BTREE树 和 二叉树 相比， 查询数据的效率更高， 因为对于相同的数据量来说，BTREE的层级结构比二叉树小，因此搜索速度快。 2.3.3 B+TREE 结构 B+Tree为BTree的变种，B+Tree与BTree的区别为： 1). n叉B+Tree最多含有n个key，而BTree最多含有n-1个key。 2). B+Tree的叶子节点保存所有的key信息，依key大小顺序排列。 3). 所有的非叶子节点都可以看作是key的索引部分。 由于B+Tree只有叶子节点保存key信息，查询任何key都要从root走到叶子。所以B+Tree的查询效率更加稳定。 2.3.3 MySQL中的B+Tree MySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。 MySQL中的 B+Tree 索引结构示意图: 2.4 索引分类 1） 单值索引 ：即一个索引只包含单个列，一个表可以有多个单列索引 2） 唯一索引 ：索引列的值必须唯一，但允许有空值 3） 复合索引 ：即一个索引包含多个列 2.5 索引语法 索引在创建表的时候，可以同时创建， 也可以随时增加新的索引。 准备环境: create database demo_01 default charset=utf8mb4; use demo_01; CREATE TABLE `city` ( `city_id` int(11) NOT NULL AUTO_INCREMENT, `city_name` varchar(50) NOT NULL, `country_id` int(11) NOT NULL, PRIMARY KEY (`city_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; CREATE TABLE `country` ( `country_id` int(11) NOT NULL AUTO_INCREMENT, `country_name` varchar(100) NOT NULL, PRIMARY KEY (`country_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into `city` (`city_id`, `city_name`, `country_id`) values(1,'西安',1); insert into `city` (`city_id`, `city_name`, `country_id`) values(2,'NewYork',2); insert into `city` (`city_id`, `city_name`, `country_id`) values(3,'北京',1); insert into `city` (`city_id`, `city_name`, `country_id`) values(4,'上海',1); insert into `country` (`country_id`, `country_name`) values(1,'China'); insert into `country` (`country_id`, `country_name`) values(2,'America'); insert into `country` (`country_id`, `country_name`) values(3,'Japan'); insert into `country` (`country_id`, `country_name`) values(4,'UK'); 2.5.1 创建索引 语法 ： CREATE [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name [USING index_type] ON tbl_name(index_col_name,...) index_col_name : column_name[(length)][ASC | DESC] 示例 ： 为city表中的city_name字段创建索引 ； ​ ​ 2.5.2 查看索引 语法： show index from table_name; 示例：查看city表中的索引信息； 2.5.3 删除索引 语法 ： DROP INDEX index_name ON tbl_name; 示例 ： 想要删除city表上的索引idx_city_name，可以操作如下： 2.5.4 ALTER命令 1). alter table tb_name add primary key(column_list); 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL 2). alter table tb_name add unique index_name(column_list); 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次） 3). alter table tb_name add index index_name(column_list); 添加普通索引， 索引值可以出现多次。 4). alter table tb_name add fulltext index_name(column_list); 该语句指定了索引为FULLTEXT， 用于全文索引 2.6 索引设计原则 ​ 索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率，更高效的使用索引。 对查询频次较高，且数据量比较大的表建立索引。 索引字段的选择，最佳候选列应当从where子句的条件中提取，如果where子句中的组合比较多，那么应当挑选最常用、过滤效果最好的列的组合。 使用唯一索引，区分度越高，使用索引的效率越高。 索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价自然也就水涨船高。对于插入、更新、删除等DML操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低DML操作的效率，增加相应操作的时间消耗。另外索引过多的话，MySQL也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但无疑提高了选择的代价。 使用短索引，索引创建之后也是使用硬盘来存储的，因此提升索引访问的I/O效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升MySQL访问索引的I/O效率。 利用最左前缀，N个列组合而成的组合索引，那么相当于是创建了N个索引，如果查询时where子句中使用了组成该索引的前几个字段，那么这条查询SQL可以利用组合索引来提升查询效率。 创建复合索引: CREATE INDEX idx_name_email_status ON tb_seller(NAME,email,STATUS);就相当于 对name 创建索引 ; 对name , email 创建了索引 ; 对name , email, status 创建了索引 ; 3. 视图 3.1 视图概述 ​ 视图（View）是一种虚拟存在的表。视图并不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的。通俗的讲，视图就是一条SELECT语句执行后返回的结果集。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。 视图相对于普通的表的优势主要包括以下几项。 简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。 数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。 3.2 创建或者修改视图 创建视图的语法为： CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION] 修改视图的语法为： ALTER [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION] 选项 : WITH [CASCADED | LOCAL] CHECK OPTION 决定了是否允许更新数据使记录不再满足视图的条件。 LOCAL ： 只要满足本视图的条件就可以更新。 CASCADED ： 必须满足所有针对该视图的所有视图的条件才可以更新。 默认值. 示例 , 创建city_country_view视图 , 执行如下SQL : create or replace view city_country_view as select t.*,c.country_name from country c , city t where c.country_id = t.country_id; 查询视图 : 3.3 查看视图 ​ 从 MySQL 5.1 版本开始，使用 SHOW TABLES 命令的时候不仅显示表的名字，同时也会显示视图的名字，而不存在单独显示视图的 SHOW VIEWS 命令。 同样，在使用 SHOW TABLE STATUS 命令的时候，不但可以显示表的信息，同时也可以显示视图的信息。 如果需要查询某个视图的定义，可以使用 SHOW CREATE VIEW 命令进行查看 ： 3.4 删除视图 语法 : DROP VIEW [IF EXISTS] view_name [, view_name] ...[RESTRICT | CASCADE] 示例 , 删除视图city_country_view : DROP VIEW city_country_view ; 4. 存储过程和函数 4.1 存储过程和函数概述 ​ 存储过程和函数是 事先经过编译并存储在数据库中的一段 SQL 语句的集合，调用存储过程和函数可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。 ​ 存储过程和函数的区别在于函数必须有返回值，而存储过程没有。 ​ 函数 ： 是一个有返回值的过程 ； ​ 过程 ： 是一个没有返回值的函数 ； 4.2 创建存储过程 CREATE PROCEDURE procedure_name ([proc_parameter[,...]])begin -- SQL语句end ; 示例 ： delimiter $create procedure pro_test1()begin select 'Hello Mysql' ;end$delimiter ; 知识小贴士 DELIMITER ​ 该关键字用来声明SQL语句的分隔符 , 告诉 MySQL 解释器，该段命令是否已经结束了，mysql是否可以执行了。默认情况下，delimiter是分号;。在命令行客户端中，如果有一行命令以分号结束，那么回车后，mysql将会执行该命令。 4.3 调用存储过程 call procedure_name() ; 4.4 查看存储过程 -- 查询db_name数据库中的所有的存储过程select name from mysql.proc where db='db_name';-- 查询存储过程的状态信息show procedure status;-- 查询某个存储过程的定义show create procedure test.pro_test1 \\G; 4.5 删除存储过程 DROP PROCEDURE [IF EXISTS] sp_name ； 4.6 语法 存储过程是可以编程的，意味着可以使用变量，表达式，控制结构 ， 来完成比较复杂的功能。 4.6.1 变量 DECLARE 通过 DECLARE 可以定义一个局部变量，该变量的作用范围只能在 BEGIN…END 块中。 DECLARE var_name[,...] type [DEFAULT value] 示例 : delimiter $ create procedure pro_test2() begin declare num int default 5; select num+ 10; end$ delimiter ; SET 直接赋值使用 SET，可以赋常量或者赋表达式，具体语法如下： SET var_name = expr [, var_name = expr] ... 示例 : DELIMITER $ CREATE PROCEDURE pro_test3() BEGIN DECLARE NAME VARCHAR(20); SET NAME = 'MYSQL'; SELECT NAME ; END$ DELIMITER ; 也可以通过select ... into 方式进行赋值操作 : DELIMITER $CREATE PROCEDURE pro_test5()BEGIN declare countnum int; select count(*) into countnum from city; select countnum;END$DELIMITER ; 4.6.2 if条件判断 语法结构 : if search_condition then statement_list [elseif search_condition then statement_list] ... [else statement_list] end if; 需求： 根据定义的身高变量，判定当前身高的所属的身材类型 180 及以上 ----------&gt; 身材高挑 170 - 180 ---------&gt; 标准身材 170 以下 ----------&gt; 一般身材 示例 : delimiter $create procedure pro_test6()begin declare height int default 175; declare description varchar(50); if height &gt;= 180 then set description = '身材高挑'; elseif height &gt;= 170 and height &lt; 180 then set description = '标准身材'; else set description = '一般身材'; end if; select description ;end$delimiter ; 调用结果为 : 4.6.3 传递参数 语法格式 : create procedure procedure_name([in/out/inout] 参数名 参数类型)...IN : 该参数可以作为输入，也就是需要调用方传入值 , 默认OUT: 该参数作为输出，也就是该参数可以作为返回值INOUT: 既可以作为输入参数，也可以作为输出参数 IN - 输入 需求 : 根据定义的身高变量，判定当前身高的所属的身材类型 示例 : delimiter $create procedure pro_test5(in height int)begin declare description varchar(50) default ''; if height &gt;= 180 then set description='身材高挑'; elseif height &gt;= 170 and height &lt; 180 then set description='标准身材'; else set description='一般身材'; end if; select concat('身高 ', height , '对应的身材类型为:',description);end$delimiter ; OUT-输出 需求 : 根据传入的身高变量，获取当前身高的所属的身材类型 示例: create procedure pro_test5(in height int , out description varchar(100))begin if height &gt;= 180 then set description='身材高挑'; elseif height &gt;= 170 and height &lt; 180 then set description='标准身材'; else set description='一般身材'; end if;end$ 调用: call pro_test5(168, @description)$select @description$ 小知识 @description : 这种变量要在变量名称前面加上“@”符号，叫做用户会话变量，代表整个会话过程他都是有作用的，这个类似于全局变量一样。 @@global.sort_buffer_size : 这种在变量前加上 &quot;@@&quot; 符号, 叫做 系统变量 4.6.4 case结构 语法结构 : 方式一 : CASE case_value WHEN when_value THEN statement_list [WHEN when_value THEN statement_list] ... [ELSE statement_list] END CASE;方式二 : CASE WHEN search_condition THEN statement_list [WHEN search_condition THEN statement_list] ... [ELSE statement_list] END CASE; 需求: 给定一个月份, 然后计算出所在的季度 示例 : delimiter $create procedure pro_test9(month int)begin declare result varchar(20); case when month &gt;= 1 and month &lt;=3 then set result = '第一季度'; when month &gt;= 4 and month &lt;=6 then set result = '第二季度'; when month &gt;= 7 and month &lt;=9 then set result = '第三季度'; when month &gt;= 10 and month &lt;=12 then set result = '第四季度'; end case; select concat('您输入的月份为 :', month , ' , 该月份为 : ' , result) as content ; end$delimiter ; 4.6.5 while循环 语法结构: while search_condition do statement_list end while; 需求: 计算从1加到n的值 示例 : delimiter $create procedure pro_test8(n int)begin declare total int default 0; declare num int default 1; while num&lt;=n do set total = total + num; set num = num + 1; end while; select total;end$delimiter ; 4.6.6 repeat结构 有条件的循环控制语句, 当满足条件的时候退出循环 。while 是满足条件才执行，repeat 是满足条件就退出循环。 语法结构 : REPEAT statement_list UNTIL search_conditionEND REPEAT; 需求: 计算从1加到n的值 示例 : delimiter $create procedure pro_test10(n int)begin declare total int default 0; repeat set total = total + n; set n = n - 1; until n=0 end repeat; select total ; end$delimiter ; 4.6.7 loop语句 LOOP 实现简单的循环，退出循环的条件需要使用其他的语句定义，通常可以使用 LEAVE 语句实现，具体语法如下： [begin_label:] LOOP statement_listEND LOOP [end_label] 如果不在 statement_list 中增加退出循环的语句，那么 LOOP 语句可以用来实现简单的死循环。 4.6.8 leave语句 用来从标注的流程构造中退出，通常和 BEGIN ... END 或者循环一起使用。下面是一个使用 LOOP 和 LEAVE 的简单例子 , 退出循环： delimiter $CREATE PROCEDURE pro_test11(n int)BEGIN declare total int default 0; ins: LOOP IF n &lt;= 0 then leave ins; END IF; set total = total + n; set n = n - 1; END LOOP ins; select total;END$delimiter ; 4.6.9 游标/光标 游标是用来存储查询结果集的数据类型 , 在存储过程和函数中可以使用光标对结果集进行循环的处理。光标的使用包括光标的声明、OPEN、FETCH 和 CLOSE，其语法分别如下。 声明光标： DECLARE cursor_name CURSOR FOR select_statement ; OPEN 光标： OPEN cursor_name ; FETCH 光标： FETCH cursor_name INTO var_name [, var_name] ... CLOSE 光标： CLOSE cursor_name ; 示例 : 初始化脚本: create table emp( id int(11) not null auto_increment , name varchar(50) not null comment '姓名', age int(11) comment '年龄', salary int(11) comment '薪水', primary key(`id`))engine=innodb default charset=utf8 ;insert into emp(id,name,age,salary) values(null,'金毛狮王',55,3800),(null,'白眉鹰王',60,4000),(null,'青翼蝠王',38,2800),(null,'紫衫龙王',42,1800); -- 查询emp表中数据, 并逐行获取进行展示create procedure pro_test11()begin declare e_id int(11); declare e_name varchar(50); declare e_age int(11); declare e_salary int(11); declare emp_result cursor for select * from emp; open emp_result; fetch emp_result into e_id,e_name,e_age,e_salary; select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary); fetch emp_result into e_id,e_name,e_age,e_salary; select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary); fetch emp_result into e_id,e_name,e_age,e_salary; select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary); fetch emp_result into e_id,e_name,e_age,e_salary; select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary); fetch emp_result into e_id,e_name,e_age,e_salary; select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary); close emp_result;end$ 通过循环结构 , 获取游标中的数据 : DELIMITER $create procedure pro_test12()begin DECLARE id int(11); DECLARE name varchar(50); DECLARE age int(11); DECLARE salary int(11); DECLARE has_data int default 1; DECLARE emp_result CURSOR FOR select * from emp; DECLARE EXIT HANDLER FOR NOT FOUND set has_data = 0; open emp_result; repeat fetch emp_result into id , name , age , salary; select concat('id为',id, ', name 为' ,name , ', age为 ' ,age , ', 薪水为: ', salary); until has_data = 0 end repeat; close emp_result;end$DELIMITER ; 4.7 存储函数 语法结构: CREATE FUNCTION function_name([param type ... ]) RETURNS type BEGIN ...END; 案例 : 定义一个存储过程, 请求满足条件的总记录数 ; delimiter $create function count_city(countryId int)returns intbegin declare cnum int ; select count(*) into cnum from city where country_id = countryId; return cnum;end$delimiter ; 调用: select count_city(1);select count_city(2); 5. 触发器 5.1 介绍 触发器是与表有关的数据库对象，指在 insert/update/delete 之前或之后，触发并执行触发器中定义的SQL语句集合。触发器的这种特性可以协助应用在数据库端确保数据的完整性 , 日志记录 , 数据校验等操作 。 使用别名 OLD 和 NEW 来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。 触发器类型 NEW 和 OLD的使用 INSERT 型触发器 NEW 表示将要或者已经新增的数据 UPDATE 型触发器 OLD 表示修改之前的数据 , NEW 表示将要或已经修改后的数据 DELETE 型触发器 OLD 表示将要或者已经删除的数据 5.2 创建触发器 语法结构 : create trigger trigger_name before/after insert/update/deleteon tbl_name [ for each row ] -- 行级触发器begin trigger_stmt ;end; 示例 需求 通过触发器记录 emp 表的数据变更日志 , 包含增加, 修改 , 删除 ; 首先创建一张日志表 : create table emp_logs( id int(11) not null auto_increment, operation varchar(20) not null comment '操作类型, insert/update/delete', operate_time datetime not null comment '操作时间', operate_id int(11) not null comment '操作表的ID', operate_params varchar(500) comment '操作参数', primary key(`id`))engine=innodb default charset=utf8; 创建 insert 型触发器，完成插入数据时的日志记录 : DELIMITER $create trigger emp_logs_insert_triggerafter insert on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'insert',now(),new.id,concat('插入后(id:',new.id,', name:',new.name,', age:',new.age,', salary:',new.salary,')')); end $DELIMITER ; 创建 update 型触发器，完成更新数据时的日志记录 : DELIMITER $create trigger emp_logs_update_triggerafter update on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'update',now(),new.id,concat('修改前(id:',old.id,', name:',old.name,', age:',old.age,', salary:',old.salary,') , 修改后(id',new.id, 'name:',new.name,', age:',new.age,', salary:',new.salary,')')); end $DELIMITER ; 创建delete 行的触发器 , 完成删除数据时的日志记录 : DELIMITER $create trigger emp_logs_delete_triggerafter delete on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'delete',now(),old.id,concat('删除前(id:',old.id,', name:',old.name,', age:',old.age,', salary:',old.salary,')')); end $DELIMITER ; 测试： insert into emp(id,name,age,salary) values(null, '光明左使',30,3500);insert into emp(id,name,age,salary) values(null, '光明右使',33,3200);update emp set age = 39 where id = 3;delete from emp where id = 5; 5.3 删除触发器 语法结构 : drop trigger [schema_name.]trigger_name 如果没有指定 schema_name，默认为当前数据库 。 5.4 查看触发器 可以通过执行 SHOW TRIGGERS 命令查看触发器的状态、语法等信息。 语法结构 ： show triggers ； Mysql高级-day02 1. Mysql的体系结构概览 整个MySQL Server由以下组成 Connection Pool : 连接池组件 Management Services &amp; Utilities : 管理服务和工具组件 SQL Interface : SQL接口组件 Parser : 查询分析器组件 Optimizer : 优化器组件 Caches &amp; Buffers : 缓冲池组件 Pluggable Storage Engines : 存储引擎 File System : 文件系统 1） 连接层 最上层是一些客户端和链接服务，包含本地sock 通信和大多数基于客户端/服务端工具实现的类似于 TCP/IP的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。 2） 服务层 第二层架构主要完成大多数的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化，部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如 过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询的顺序，是否利用索引等， 最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存，如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。 3） 引擎层 存储引擎层， 存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API和存储引擎进行通信。不同的存储引擎具有不同的功能，这样我们可以根据自己的需要，来选取合适的存储引擎。 4）存储层 数据存储层， 主要是将数据存储在文件系统之上，并完成与存储引擎的交互。 和其他数据库相比，MySQL有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎上，插件式的存储引擎架构，将查询处理和其他的系统任务以及数据的存储提取分离。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。 2. 存储引擎 2.1 存储引擎概述 ​ 和大多数的数据库不同, MySQL中有一个存储引擎的概念, 针对不同的存储需求可以选择最优的存储引擎。 ​ 存储引擎就是存储数据，建立索引，更新查询数据等等技术的实现方式 。存储引擎是基于表的，而不是基于库的。所以存储引擎也可被称为表类型。 ​ Oracle，SqlServer等数据库只有一种存储引擎。MySQL提供了插件式的存储引擎架构。所以MySQL存在多种存储引擎，可以根据需要使用相应引擎，或者编写存储引擎。 ​ MySQL5.0支持的存储引擎包含 ： InnoDB 、MyISAM 、BDB、MEMORY、MERGE、EXAMPLE、NDB Cluster、ARCHIVE、CSV、BLACKHOLE、FEDERATED等，其中InnoDB和BDB提供事务安全表，其他存储引擎是非事务安全表。 可以通过指定 show engines ， 来查询当前数据库支持的存储引擎 ： 创建新表时如果不指定存储引擎，那么系统就会使用默认的存储引擎，MySQL5.5之前的默认存储引擎是MyISAM，5.5之后就改为了InnoDB。 查看Mysql数据库默认的存储引擎 ， 指令 ： show variables like '%storage_engine%' ； 2.2 各种存储引擎特性 下面重点介绍几种常用的存储引擎， 并对比各个存储引擎之间的区别， 如下表所示 ： 特点 InnoDB MyISAM MEMORY MERGE NDB 存储限制 64TB 有 有 没有 有 事务安全 支持 锁机制 行锁(适合高并发) 表锁 表锁 表锁 行锁 B树索引 支持 支持 支持 支持 支持 哈希索引 支持 全文索引 支持(5.6版本之后) 支持 集群索引 支持 数据索引 支持 支持 支持 索引缓存 支持 支持 支持 支持 支持 数据可压缩 支持 空间使用 高 低 N/A 低 低 内存使用 高 低 中等 低 高 批量插入速度 低 高 高 高 高 支持外键 支持 下面我们将重点介绍最长使用的两种存储引擎： InnoDB、MyISAM ， 另外两种 MEMORY、MERGE ， 了解即可。 2.2.1 InnoDB ​ InnoDB存储引擎是Mysql的默认存储引擎。InnoDB存储引擎提供了具有提交、回滚、崩溃恢复能力的事务安全。但是对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引。 InnoDB存储引擎不同于其他存储引擎的特点 ： 事务控制 create table goods_innodb( id int NOT NULL AUTO_INCREMENT, name varchar(20) NOT NULL, primary key(id) )ENGINE=innodb DEFAULT CHARSET=utf8; start transaction; insert into goods_innodb(id,name)values(null,'Meta20'); commit; 测试，发现在InnoDB中是存在事务的 ； 外键约束 ​ MySQL支持外键的存储引擎只有InnoDB ， 在创建外键的时候， 要求父表必须有对应的索引 ， 子表在创建外键的时候， 也会自动的创建对应的索引。 ​ 下面两张表中 ， country_innodb是父表 ， country_id为主键索引，city_innodb表是子表，country_id字段为外键，对应于country_innodb表的主键country_id 。 create table country_innodb( country_id int NOT NULL AUTO_INCREMENT, country_name varchar(100) NOT NULL, primary key(country_id) )ENGINE=InnoDB DEFAULT CHARSET=utf8; create table city_innodb( city_id int NOT NULL AUTO_INCREMENT, city_name varchar(50) NOT NULL, country_id int NOT NULL, primary key(city_id), key idx_fk_country_id(country_id), CONSTRAINT `fk_city_country` FOREIGN KEY(country_id) REFERENCES country_innodb(country_id) ON DELETE RESTRICT ON UPDATE CASCADE )ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into country_innodb values(null,'China'),(null,'America'),(null,'Japan'); insert into city_innodb values(null,'Xian',1),(null,'NewYork',2),(null,'BeiJing',1); 在创建索引时， 可以指定在删除、更新父表时，对子表进行的相应操作，包括 RESTRICT、CASCADE、SET NULL 和 NO ACTION。 RESTRICT和NO ACTION相同， 是指限制在子表有关联记录的情况下， 父表不能更新； CASCADE表示父表在更新或者删除时，更新或者删除子表对应的记录； SET NULL 则表示父表在更新或者删除的时候，子表的对应字段被SET NULL 。 针对上面创建的两个表， 子表的外键指定是ON DELETE RESTRICT ON UPDATE CASCADE 方式的， 那么在主表删除记录的时候， 如果子表有对应记录， 则不允许删除， 主表在更新记录的时候， 如果子表有对应记录， 则子表对应更新 。 表中数据如下图所示 ： 外键信息可以使用如下两种方式查看 ： show create table city_innodb ; 删除country_id为1 的country数据： delete from country_innodb where country_id = 1; 更新主表country表的字段 country_id : update country_innodb set country_id = 100 where country_id = 1; 更新后， 子表的数据信息为 ： 存储方式 InnoDB 存储表和索引有以下两种方式 ： ①. 使用共享表空间存储， 这种方式创建的表的表结构保存在.frm文件中， 数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。 ②. 使用多表空间存储， 这种方式创建的表的表结构仍然存在 .frm 文件中，但是每个表的数据和索引单独保存在 .ibd 中。 2.2.2 MyISAM ​ MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。有以下两个比较重要的特点： 不支持事务 create table goods_myisam( id int NOT NULL AUTO_INCREMENT, name varchar(20) NOT NULL, primary key(id) )ENGINE=myisam DEFAULT CHARSET=utf8; 通过测试，我们发现，在MyISAM存储引擎中，是没有事务控制的 ； 文件存储方式 每个MyISAM在磁盘上存储成3个文件，其文件名都和表名相同，但拓展名分别是 ： .frm (存储表定义)； .MYD(MYData , 存储数据)； .MYI(MYIndex , 存储索引)； 2.2.3 MEMORY ​ Memory存储引擎将表的数据存放在内存中。每个MEMORY表实际对应一个磁盘文件，格式是.frm ，该文件中只存储表的结构，而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。MEMORY 类型的表访问非常地快，因为他的数据是存放在内存中的，并且默认使用HASH索引 ， 但是服务一旦关闭，表中的数据就会丢失。 2.2.4 MERGE ​ MERGE存储引擎是一组MyISAM表的组合，这些MyISAM表必须结构完全相同，MERGE表本身并没有存储数据，对MERGE类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部的MyISAM表进行的。 ​ 对于MERGE类型表的插入操作，是通过INSERT_METHOD子句定义插入的表，可以有3个不同的值，使用FIRST 或 LAST 值使得插入操作被相应地作用在第一或者最后一个表上，不定义这个子句或者定义为NO，表示不能对这个MERGE表执行插入操作。 ​ 可以对MERGE表进行DROP操作，但是这个操作只是删除MERGE表的定义，对内部的表是没有任何影响的。 下面是一个创建和使用MERGE表的示例 ： 1）. 创建3个测试表 order_1990, order_1991, order_all , 其中order_all是前两个表的MERGE表 ： create table order_1990( order_id int , order_money double(10,2), order_address varchar(50), primary key (order_id))engine = myisam default charset=utf8;create table order_1991( order_id int , order_money double(10,2), order_address varchar(50), primary key (order_id))engine = myisam default charset=utf8;create table order_all( order_id int , order_money double(10,2), order_address varchar(50), primary key (order_id))engine = merge union = (order_1990,order_1991) INSERT_METHOD=LAST default charset=utf8; 2）. 分别向两张表中插入记录 insert into order_1990 values(1,100.0,'北京');insert into order_1990 values(2,100.0,'上海');insert into order_1991 values(10,200.0,'北京');insert into order_1991 values(11,200.0,'上海'); 3）. 查询3张表中的数据。 order_1990中的数据 ： order_1991中的数据 ： order_all中的数据 ： ​ 4）. 往order_all中插入一条记录 ，由于在MERGE表定义时，INSERT_METHOD 选择的是LAST，那么插入的数据会想最后一张表中插入。 insert into order_all values(100,10000.0,'西安')； 2.3 存储引擎的选择 ​ 在选择存储引擎时，应该根据应用系统的特点选择合适的存储引擎。对于复杂的应用系统，还可以根据实际情况选择多种存储引擎进行组合。以下是几种常用的存储引擎的使用环境。 InnoDB : 是Mysql的默认存储引擎，用于事务处理应用程序，支持外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询意外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。InnoDB存储引擎除了有效的降低由于删除和更新导致的锁定， 还可以确保事务的完整提交和回滚，对于类似于计费系统或者财务系统等对数据准确性要求比较高的系统，InnoDB是最合适的选择。 MyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。 MEMORY：将所有数据保存在RAM中，在需要快速定位记录和其他类似数据环境下，可以提供几块的访问。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，其次是要确保表的数据可以恢复，数据库异常终止后表中的数据是可以恢复的。MEMORY表通常用于更新不太频繁的小表，用以快速得到访问结果。 MERGE：用于将一系列等同的MyISAM表以逻辑方式组合在一起，并作为一个对象引用他们。MERGE表的优点在于可以突破对单个MyISAM表的大小限制，并且通过将不同的表分布在多个磁盘上，可以有效的改善MERGE表的访问效率。这对于存储诸如数据仓储等VLDB环境十分合适。 3. 优化SQL步骤 在应用的的开发过程中，由于初期数据量小，开发人员写 SQL 语句时更重视功能上的实现，但是当应用系统正式上线后，随着生产数据量的急剧增长，很多 SQL 语句开始逐渐显露出性能问题，对生产的影响也越来越大，此时这些有问题的 SQL 语句就成为整个系统性能的瓶颈，因此我们必须要对它们进行优化，本章将详细介绍在 MySQL 中优化 SQL 语句的方法。 当面对一个有 SQL 性能问题的数据库时，我们应该从何处入手来进行系统的分析，使得能够尽快定位问题 SQL 并尽快解决问题。 3.1 查看SQL执行频率 MySQL 客户端连接成功后，通过 show [session|global] status 命令可以提供服务器状态信息。show [session|global] status 可以根据需要加上参数“session”或者“global”来显示 session 级（当前连接）的计结果和 global 级（自数据库上次启动至今）的统计结果。如果不写，默认使用参数是“session”。 下面的命令显示了当前 session 中所有统计参数的值： show status like 'Com_______'; show status like 'Innodb_rows_%'; Com_xxx 表示每个 xxx 语句执行的次数，我们通常比较关心的是以下几个统计参数。 参数 含义 Com_select 执行 select 操作的次数，一次查询只累加 1。 Com_insert 执行 INSERT 操作的次数，对于批量插入的 INSERT 操作，只累加一次。 Com_update 执行 UPDATE 操作的次数。 Com_delete 执行 DELETE 操作的次数。 Innodb_rows_read select 查询返回的行数。 Innodb_rows_inserted 执行 INSERT 操作插入的行数。 Innodb_rows_updated 执行 UPDATE 操作更新的行数。 Innodb_rows_deleted 执行 DELETE 操作删除的行数。 Connections 试图连接 MySQL 服务器的次数。 Uptime 服务器工作时间。 Slow_queries 慢查询的次数。 Com_*** : 这些参数对于所有存储引擎的表操作都会进行累计。 Innodb_*** : 这几个参数只是针对InnoDB 存储引擎的，累加的算法也略有不同。 3.2 定位低效率执行SQL 可以通过以下两种方式定位执行效率较低的 SQL 语句。 慢查询日志 : 通过慢查询日志定位那些执行效率较低的 SQL 语句，用--log-slow-queries[=file_name]选项启动时，mysqld 写一个包含所有执行时间超过 long_query_time 秒的 SQL 语句的日志文件。具体可以查看本书第 26 章中日志管理的相关部分。 show processlist : 慢查询日志在查询结束以后才纪录，所以在应用反映执行效率出现问题的时候查询慢查询日志并不能定位问题，可以使用show processlist命令查看当前MySQL在进行的线程，包括线程的状态、是否锁表等，可以实时地查看 SQL 的执行情况，同时对一些锁表操作进行优化。 1） id列，用户登录mysql时，系统分配的&quot;connection_id&quot;，可以使用函数connection_id()查看2） user列，显示当前用户。如果不是root，这个命令就只显示用户权限范围的sql语句3） host列，显示这个语句是从哪个ip的哪个端口上发的，可以用来跟踪出现问题语句的用户4） db列，显示这个进程目前连接的是哪个数据库5） command列，显示当前连接的执行的命令，一般取值为休眠（sleep），查询（query），连接（connect）等6） time列，显示这个状态持续的时间，单位是秒7） state列，显示使用当前连接的sql语句的状态，很重要的列。state描述的是语句执行中的某一个状态。一个sql语句，以查询为例，可能需要经过copying to tmp table、sorting result、sending data等状态才可以完成8） info列，显示这个sql语句，是判断问题语句的一个重要依据 3.3 explain分析执行计划 通过以上步骤查询到效率低的 SQL 语句后，可以通过 EXPLAIN或者 DESC命令获取 MySQL如何执行 SELECT 语句的信息，包括在 SELECT 语句执行过程中表如何连接和连接的顺序。 查询SQL语句的执行计划 ： explain select * from tb_item where id = 1; explain select * from tb_item where title = '阿尔卡特 (OT-979) 冰川白 联通3G手机3'; 字段 含义 id select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。 select_type 表示 SELECT 的类型，常见的取值有 SIMPLE（简单表，即不使用表连接或者子查询）、PRIMARY（主查询，即外层的查询）、UNION（UNION 中的第二个或者后面的查询语句）、SUBQUERY（子查询中的第一个 SELECT）等 table 输出结果集的表 type 表示表的连接类型，性能由好到差的连接类型为( system ---&gt; const -----&gt; eq_ref ------&gt; ref -------&gt; ref_or_null----&gt; index_merge ---&gt; index_subquery -----&gt; range -----&gt; index ------&gt; all ) possible_keys 表示查询时，可能使用的索引 key 表示实际使用的索引 key_len 索引字段的长度 rows 扫描行的数量 extra 执行情况的说明和描述 3.3.1 环境准备 CREATE TABLE `t_role` ( `id` varchar(32) NOT NULL, `role_name` varchar(255) DEFAULT NULL, `role_code` varchar(255) DEFAULT NULL, `description` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `unique_role_name` (`role_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `t_user` ( `id` varchar(32) NOT NULL, `username` varchar(45) NOT NULL, `password` varchar(96) NOT NULL, `name` varchar(45) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `unique_user_username` (`username`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `user_role` ( `id` int(11) NOT NULL auto_increment , `user_id` varchar(32) DEFAULT NULL, `role_id` varchar(32) DEFAULT NULL, PRIMARY KEY (`id`), KEY `fk_ur_user_id` (`user_id`), KEY `fk_ur_role_id` (`role_id`), CONSTRAINT `fk_ur_role_id` FOREIGN KEY (`role_id`) REFERENCES `t_role` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_ur_user_id` FOREIGN KEY (`user_id`) REFERENCES `t_user` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into `t_user` (`id`, `username`, `password`, `name`) values('1','super','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','超级管理员');insert into `t_user` (`id`, `username`, `password`, `name`) values('2','admin','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','系统管理员');insert into `t_user` (`id`, `username`, `password`, `name`) values('3','itcast','$2a$10$8qmaHgUFUAmPR5pOuWhYWOr291WJYjHelUlYn07k5ELF8ZCrW0Cui','test02');insert into `t_user` (`id`, `username`, `password`, `name`) values('4','stu1','$2a$10$pLtt2KDAFpwTWLjNsmTEi.oU1yOZyIn9XkziK/y/spH5rftCpUMZa','学生1');insert into `t_user` (`id`, `username`, `password`, `name`) values('5','stu2','$2a$10$nxPKkYSez7uz2YQYUnwhR.z57km3yqKn3Hr/p1FR6ZKgc18u.Tvqm','学生2');insert into `t_user` (`id`, `username`, `password`, `name`) values('6','t1','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','老师1');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('5','学生','student','学生');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('7','老师','teacher','老师');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('8','教学管理员','teachmanager','教学管理员');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('9','管理员','admin','管理员');INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('10','超级管理员','super','超级管理员');INSERT INTO user_role(id,user_id,role_id) VALUES(NULL, '1', '5'),(NULL, '1', '7'),(NULL, '2', '8'),(NULL, '3', '9'),(NULL, '4', '8'),(NULL, '5', '10') ; 3.3.2 explain 之 id id 字段是 select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。id 情况有三种 ： 1） id 相同表示加载表的顺序是从上到下。 explain select * from t_role r, t_user u, user_role ur where r.id = ur.role_id and u.id = ur.user_id ; 2） id 不同id值越大，优先级越高，越先被执行。 EXPLAIN SELECT * FROM t_role WHERE id = (SELECT role_id FROM user_role WHERE user_id = (SELECT id FROM t_user WHERE username = 'stu1')) 3） id 有相同，也有不同，同时存在。id相同的可以认为是一组，从上往下顺序执行；在所有的组中，id的值越大，优先级越高，越先执行。 EXPLAIN SELECT * FROM t_role r , (SELECT * FROM user_role ur WHERE ur.`user_id` = '2') a WHERE r.id = a.role_id ; 3.3.3 explain 之 select_type 表示 SELECT 的类型，常见的取值，如下表所示： select_type 含义 SIMPLE 简单的select查询，查询中不包含子查询或者UNION PRIMARY 查询中若包含任何复杂的子查询，最外层查询标记为该标识 SUBQUERY 在SELECT 或 WHERE 列表中包含了子查询 DERIVED 在FROM 列表中包含的子查询，被标记为 DERIVED（衍生） MYSQL会递归执行这些子查询，把结果放在临时表中 UNION 若第二个SELECT出现在UNION之后，则标记为UNION ； 若UNION包含在FROM子句的子查询中，外层SELECT将被标记为 ： DERIVED UNION RESULT 从UNION表获取结果的SELECT 3.3.4 explain 之 table 展示这一行的数据是关于哪一张表的 3.3.5 explain 之 type type 显示的是访问类型，是较为重要的一个指标，可取值为： type 含义 NULL MySQL不访问任何表，索引，直接返回结果 system 表只有一行记录(等于系统表)，这是const类型的特例，一般不会出现 const 表示通过索引一次就找到了，const 用于比较primary key 或者 unique 索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL 就能将该查询转换为一个常亮。const于将 &quot;主键&quot; 或 &quot;唯一&quot; 索引的所有部分与常量值进行比较 eq_ref 类似ref，区别在于使用的是唯一索引，使用主键的关联查询，关联查询出的记录只有一条。常见于主键或唯一索引扫描 ref 非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，返回所有匹配某个单独值的所有行（多个） range 只检索给定返回的行，使用一个索引来选择行。 where 之后出现 between ， &lt; , &gt; , in 等操作。 index index 与 ALL的区别为 index 类型只是遍历了索引树， 通常比ALL 快， ALL 是遍历数据文件。 all 将遍历全表以找到匹配的行 结果值从最好到最坏以此是： NULL &gt; system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALLsystem &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL 一般来说， 我们需要保证查询至少达到 range 级别， 最好达到ref 。 3.3.6 explain 之 key possible_keys : 显示可能应用在这张表的索引， 一个或多个。 key ： 实际使用的索引， 如果为NULL， 则没有使用索引。key_len : 表示索引中使用的字节数， 该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下， 长度越短越好 。 3.3.7 explain 之 rows 扫描行的数量。 3.3.8 explain 之 extra 其他的额外的执行计划信息，在该列展示 。 extra 含义 using filesort 说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取， 称为 “文件排序”, 效率低。 using temporary 使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于 order by 和 group by； 效率低 using index 表示相应的select操作使用了覆盖索引， 避免访问表的数据行， 效率不错。 3.4 show profile分析SQL Mysql从5.0.37版本开始增加了对 show profiles 和 show profile 语句的支持。show profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪里去了。 通过 have_profiling 参数，能够看到当前MySQL是否支持profile： 默认profiling是关闭的，可以通过set语句在Session级别开启profiling： set profiling=1; //开启profiling 开关； 通过profile，我们能够更清楚地了解SQL执行的过程。 首先，我们可以执行一系列的操作，如下图所示： show databases;use db01;show tables;select * from tb_item where id &lt; 5;select count(*) from tb_item; 执行完上述命令之后，再执行show profiles 指令， 来查看SQL语句执行的耗时： 通过show profile for query query_id 语句可以查看到该SQL执行过程中每个线程的状态和消耗的时间： TIP ： Sending data 状态表示MySQL线程开始访问数据行并把结果返回给客户端，而不仅仅是返回个客户端。由于在Sending data状态下，MySQL线程往往需要做大量的磁盘读取操作，所以经常是整各查询中耗时最长的状态。 在获取到最消耗时间的线程状态后，MySQL支持进一步选择all、cpu、block io 、context switch、page faults等明细类型类查看MySQL在使用什么资源上耗费了过高的时间。例如，选择查看CPU的耗费时间 ： 字段 含义 Status sql 语句执行的状态 Duration sql 执行过程中每一个步骤的耗时 CPU_user 当前用户占有的cpu CPU_system 系统占有的cpu 3.5 trace分析优化器执行计划 MySQL5.6提供了对SQL的跟踪trace, 通过trace文件能够进一步了解为什么优化器选择A计划, 而不是选择B计划。 打开trace ， 设置格式为 JSON，并设置trace最大能够使用的内存大小，避免解析过程中因为默认内存过小而不能够完整展示。 SET optimizer_trace=&quot;enabled=on&quot;,end_markers_in_json=on;set optimizer_trace_max_mem_size=1000000; 执行SQL语句 ： select * from tb_item where id &lt; 4; 最后， 检查information_schema.optimizer_trace就可以知道MySQL是如何执行SQL的 ： select * from information_schema.optimizer_trace\\G; *************************** 1. row ***************************QUERY: select * from tb_item where id &lt; 4TRACE: { &quot;steps&quot;: [ { &quot;join_preparation&quot;: { &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;expanded_query&quot;: &quot;/* select#1 */ select `tb_item`.`id` AS `id`,`tb_item`.`title` AS `title`,`tb_item`.`price` AS `price`,`tb_item`.`num` AS `num`,`tb_item`.`categoryid` AS `categoryid`,`tb_item`.`status` AS `status`,`tb_item`.`sellerid` AS `sellerid`,`tb_item`.`createtime` AS `createtime`,`tb_item`.`updatetime` AS `updatetime` from `tb_item` where (`tb_item`.`id` &lt; 4)&quot; } ] /* steps */ } /* join_preparation */ }, { &quot;join_optimization&quot;: { &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;condition_processing&quot;: { &quot;condition&quot;: &quot;WHERE&quot;, &quot;original_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot;, &quot;steps&quot;: [ { &quot;transformation&quot;: &quot;equality_propagation&quot;, &quot;resulting_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot; }, { &quot;transformation&quot;: &quot;constant_propagation&quot;, &quot;resulting_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot; }, { &quot;transformation&quot;: &quot;trivial_condition_removal&quot;, &quot;resulting_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot; } ] /* steps */ } /* condition_processing */ }, { &quot;table_dependencies&quot;: [ { &quot;table&quot;: &quot;`tb_item`&quot;, &quot;row_may_be_null&quot;: false, &quot;map_bit&quot;: 0, &quot;depends_on_map_bits&quot;: [ ] /* depends_on_map_bits */ } ] /* table_dependencies */ }, { &quot;ref_optimizer_key_uses&quot;: [ ] /* ref_optimizer_key_uses */ }, { &quot;rows_estimation&quot;: [ { &quot;table&quot;: &quot;`tb_item`&quot;, &quot;range_analysis&quot;: { &quot;table_scan&quot;: { &quot;rows&quot;: 9816098, &quot;cost&quot;: 2.04e6 } /* table_scan */, &quot;potential_range_indices&quot;: [ { &quot;index&quot;: &quot;PRIMARY&quot;, &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;id&quot; ] /* key_parts */ } ] /* potential_range_indices */, &quot;setup_range_conditions&quot;: [ ] /* setup_range_conditions */, &quot;group_index_range&quot;: { &quot;chosen&quot;: false, &quot;cause&quot;: &quot;not_group_by_or_distinct&quot; } /* group_index_range */, &quot;analyzing_range_alternatives&quot;: { &quot;range_scan_alternatives&quot;: [ { &quot;index&quot;: &quot;PRIMARY&quot;, &quot;ranges&quot;: [ &quot;id &lt; 4&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, &quot;rowid_ordered&quot;: true, &quot;using_mrr&quot;: false, &quot;index_only&quot;: false, &quot;rows&quot;: 3, &quot;cost&quot;: 1.6154, &quot;chosen&quot;: true } ] /* range_scan_alternatives */, &quot;analyzing_roworder_intersect&quot;: { &quot;usable&quot;: false, &quot;cause&quot;: &quot;too_few_roworder_scans&quot; } /* analyzing_roworder_intersect */ } /* analyzing_range_alternatives */, &quot;chosen_range_access_summary&quot;: { &quot;range_access_plan&quot;: { &quot;type&quot;: &quot;range_scan&quot;, &quot;index&quot;: &quot;PRIMARY&quot;, &quot;rows&quot;: 3, &quot;ranges&quot;: [ &quot;id &lt; 4&quot; ] /* ranges */ } /* range_access_plan */, &quot;rows_for_plan&quot;: 3, &quot;cost_for_plan&quot;: 1.6154, &quot;chosen&quot;: true } /* chosen_range_access_summary */ } /* range_analysis */ } ] /* rows_estimation */ }, { &quot;considered_execution_plans&quot;: [ { &quot;plan_prefix&quot;: [ ] /* plan_prefix */, &quot;table&quot;: &quot;`tb_item`&quot;, &quot;best_access_path&quot;: { &quot;considered_access_paths&quot;: [ { &quot;access_type&quot;: &quot;range&quot;, &quot;rows&quot;: 3, &quot;cost&quot;: 2.2154, &quot;chosen&quot;: true } ] /* considered_access_paths */ } /* best_access_path */, &quot;cost_for_plan&quot;: 2.2154, &quot;rows_for_plan&quot;: 3, &quot;chosen&quot;: true } ] /* considered_execution_plans */ }, { &quot;attaching_conditions_to_tables&quot;: { &quot;original_condition&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot;, &quot;attached_conditions_computation&quot;: [ ] /* attached_conditions_computation */, &quot;attached_conditions_summary&quot;: [ { &quot;table&quot;: &quot;`tb_item`&quot;, &quot;attached&quot;: &quot;(`tb_item`.`id` &lt; 4)&quot; } ] /* attached_conditions_summary */ } /* attaching_conditions_to_tables */ }, { &quot;refine_plan&quot;: [ { &quot;table&quot;: &quot;`tb_item`&quot;, &quot;access_type&quot;: &quot;range&quot; } ] /* refine_plan */ } ] /* steps */ } /* join_optimization */ }, { &quot;join_execution&quot;: { &quot;select#&quot;: 1, &quot;steps&quot;: [ ] /* steps */ } /* join_execution */ } ] /* steps */} 4. 索引的使用 索引是数据库优化最常用也是最重要的手段之一, 通过索引通常可以帮助用户解决大多数的MySQL的性能优化问题。 4.1 验证索引提升查询效率 在我们准备的表结构tb_item 中， 一共存储了 300 万记录； A. 根据ID查询 select * from tb_item where id = 1999\\G; 查询速度很快， 接近0s ， 主要的原因是因为id为主键， 有索引； 2). 根据 title 进行精确查询 select * from tb_item where title = 'iphoneX 移动3G 32G941'\\G; 查看SQL语句的执行计划 ： 处理方案 ， 针对title字段， 创建索引 ： create index idx_item_title on tb_item(title); 索引创建完成之后，再次进行查询 ： 通过explain ， 查看执行计划，执行SQL时使用了刚才创建的索引 4.2 索引的使用 4.2.1 准备环境 create table `tb_seller` ( `sellerid` varchar (100), `name` varchar (100), `nickname` varchar (50), `password` varchar (60), `status` varchar (1), `address` varchar (100), `createtime` datetime, primary key(`sellerid`))engine=innodb default charset=utf8mb4; insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('alibaba','阿里巴巴','阿里小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('baidu','百度科技有限公司','百度小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('huawei','华为科技有限公司','华为小店','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('itcast','传智播客教育科技有限公司','传智播客','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('itheima','黑马程序员','黑马程序员','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('luoji','罗技科技有限公司','罗技小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('oppo','OPPO科技有限公司','OPPO官方旗舰店','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('ourpalm','掌趣科技股份有限公司','掌趣小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('qiandu','千度科技','千度小店','e10adc3949ba59abbe56e057f20f883e','2','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('sina','新浪科技有限公司','新浪官方旗舰店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('xiaomi','小米科技','小米官方旗舰店','e10adc3949ba59abbe56e057f20f883e','1','西安市','2088-01-01 12:00:00');insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('yijia','宜家家居','宜家家居旗舰店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');create index idx_seller_name_sta_addr on tb_seller(name,status,address); 4.2.2 避免索引失效 1). 全值匹配 ，对索引中所有列都指定具体值。 改情况下，索引生效，执行效率高。 explain select * from tb_seller where name='小米科技' and status='1' and address='北京市'\\G; 2). 最左前缀法则 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始，并且不跳过索引中的列。 匹配最左前缀法则，走索引： 违法最左前缀法则 ， 索引失效： 如果符合最左法则，但是出现跳跃某一列，只有最左列索引生效： 3). 范围查询右边的列，不能使用索引 。 根据前面的两个字段name ， status 查询是走索引的， 但是最后一个条件address 没有用到索引。 4). 不要在索引列上进行运算操作， 索引将失效。 5). 字符串不加单引号，造成索引失效。 由于，在查询是，没有对字符串加单引号，MySQL的查询优化器，会自动的进行类型转换，造成索引失效。 6). 尽量使用覆盖索引，避免select * 尽量使用覆盖索引（只访问索引的查询（索引列完全包含查询列）），减少select * 。 如果查询列，超出索引列，也会降低性能。 TIP : using index ：使用覆盖索引的时候就会出现 using where：在查找使用索引的情况下，需要回表去查询所需的数据 using index condition：查找使用了索引，但是需要回表查询数据 using index ; using where：查找使用了索引，但是需要的数据都在索引列中能找到，所以不需要回表查询数据 7). 用or分割开的条件， 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。 示例，name字段是索引列 ， 而createtime不是索引列，中间是or进行连接是不走索引的 ： explain select * from tb_seller where name='黑马程序员' or createtime = '2088-01-01 12:00:00'\\G; 8). 以%开头的Like模糊查询，索引失效。 如果仅仅是尾部模糊匹配，索引不会失效。如果是头部模糊匹配，索引失效。 解决方案 ： 通过覆盖索引来解决 9). 如果MySQL评估使用索引比全表更慢，则不使用索引。 10). is NULL ， is NOT NULL 有时索引失效。 11). in 走索引， not in 索引失效。 12). 单列索引和复合索引。 尽量使用复合索引，而少使用单列索引 。 创建复合索引 create index idx_name_sta_address on tb_seller(name, status, address);就相当于创建了三个索引 ： name name + status name + status + address 创建单列索引 create index idx_seller_name on tb_seller(name);create index idx_seller_status on tb_seller(status);create index idx_seller_address on tb_seller(address); 数据库会选择一个最优的索引（辨识度最高索引）来使用，并不会使用全部索引 。 4.3 查看索引使用情况 show status like 'Handler_read%'; show global status like 'Handler_read%'; Handler_read_first：索引中第一条被读的次数。如果较高，表示服务器正执行大量全索引扫描（这个值越低越好）。Handler_read_key：如果索引正在工作，这个值代表一个行被索引值读的次数，如果值越低，表示索引得到的性能改善不高，因为索引不经常使用（这个值越高越好）。Handler_read_next ：按照键顺序读下一行的请求数。如果你用范围约束或如果执行索引扫描来查询索引列，该值增加。Handler_read_prev：按照键顺序读前一行的请求数。该读方法主要用于优化ORDER BY ... DESC。Handler_read_rnd ：根据固定位置读一行的请求数。如果你正执行大量查询并需要对结果进行排序该值较高。你可能使用了大量需要MySQL扫描整个表的查询或你的连接没有正确使用键。这个值较高，意味着运行效率低，应该建立索引来补救。Handler_read_rnd_next：在数据文件中读下一行的请求数。如果你正进行大量的表扫描，该值较高。通常说明你的表索引不正确或写入的查询没有利用索引。 5. SQL优化 5.1 大批量插入数据 环境准备 ： CREATE TABLE `tb_user_2` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(45) NOT NULL, `password` varchar(96) NOT NULL, `name` varchar(45) NOT NULL, `birthday` datetime DEFAULT NULL, `sex` char(1) DEFAULT NULL, `email` varchar(45) DEFAULT NULL, `phone` varchar(45) DEFAULT NULL, `qq` varchar(32) DEFAULT NULL, `status` varchar(32) NOT NULL COMMENT '用户状态', `create_time` datetime NOT NULL, `update_time` datetime DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `unique_user_username` (`username`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 ; 当使用load 命令导入数据的时候，适当的设置可以提高导入的效率。 对于 InnoDB 类型的表，有以下几种方式可以提高导入的效率： 1） 主键顺序插入 因为InnoDB类型的表是按照主键的顺序保存的，所以将导入的数据按照主键的顺序排列，可以有效的提高导入数据的效率。如果InnoDB表没有主键，那么系统会自动默认创建一个内部列作为主键，所以如果可以给表创建一个主键，将可以利用这点，来提高导入数据的效率。 脚本文件介绍 : sql1.log ----&gt; 主键有序 sql2.log ----&gt; 主键无序 插入ID顺序排列数据： 插入ID无序排列数据： 2） 关闭唯一性校验 在导入数据前执行 SET UNIQUE_CHECKS=0，关闭唯一性校验，在导入结束后执行SET UNIQUE_CHECKS=1，恢复唯一性校验，可以提高导入的效率。 3） 手动提交事务 如果应用使用自动提交的方式，建议在导入前执行 SET AUTOCOMMIT=0，关闭自动提交，导入结束后再执行 SET AUTOCOMMIT=1，打开自动提交，也可以提高导入的效率。 5.2 优化insert语句 当进行数据的insert操作的时候，可以考虑采用以下几种优化方案。 如果需要同时对一张表插入很多行数据时，应该尽量使用多个值表的insert语句，这种方式将大大的缩减客户端与数据库之间的连接、关闭等消耗。使得效率比分开执行的单个insert语句快。 示例， 原始方式为： insert into tb_test values(1,'Tom');insert into tb_test values(2,'Cat');insert into tb_test values(3,'Jerry'); 优化后的方案为 ： insert into tb_test values(1,'Tom'),(2,'Cat')，(3,'Jerry'); 在事务中进行数据插入。 start transaction;insert into tb_test values(1,'Tom');insert into tb_test values(2,'Cat');insert into tb_test values(3,'Jerry');commit; 数据有序插入 insert into tb_test values(4,'Tim');insert into tb_test values(1,'Tom');insert into tb_test values(3,'Jerry');insert into tb_test values(5,'Rose');insert into tb_test values(2,'Cat'); 优化后 insert into tb_test values(1,'Tom');insert into tb_test values(2,'Cat');insert into tb_test values(3,'Jerry');insert into tb_test values(4,'Tim');insert into tb_test values(5,'Rose'); 5.3 优化order by语句 5.3.1 环境准备 CREATE TABLE `emp` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(100) NOT NULL, `age` int(3) NOT NULL, `salary` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;insert into `emp` (`id`, `name`, `age`, `salary`) values('1','Tom','25','2300');insert into `emp` (`id`, `name`, `age`, `salary`) values('2','Jerry','30','3500');insert into `emp` (`id`, `name`, `age`, `salary`) values('3','Luci','25','2800');insert into `emp` (`id`, `name`, `age`, `salary`) values('4','Jay','36','3500');insert into `emp` (`id`, `name`, `age`, `salary`) values('5','Tom2','21','2200');insert into `emp` (`id`, `name`, `age`, `salary`) values('6','Jerry2','31','3300');insert into `emp` (`id`, `name`, `age`, `salary`) values('7','Luci2','26','2700');insert into `emp` (`id`, `name`, `age`, `salary`) values('8','Jay2','33','3500');insert into `emp` (`id`, `name`, `age`, `salary`) values('9','Tom3','23','2400');insert into `emp` (`id`, `name`, `age`, `salary`) values('10','Jerry3','32','3100');insert into `emp` (`id`, `name`, `age`, `salary`) values('11','Luci3','26','2900');insert into `emp` (`id`, `name`, `age`, `salary`) values('12','Jay3','37','4500');create index idx_emp_age_salary on emp(age,salary); 5.3.2 两种排序方式 1). 第一种是通过对返回数据进行排序，也就是通常说的 filesort 排序，所有不是通过索引直接返回排序结果的排序都叫 FileSort 排序。 2). 第二种通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。 多字段排序 了解了MySQL的排序方式，优化目标就清晰了：尽量减少额外的排序，通过索引直接返回有序数据。where 条件和Order by 使用相同的索引，并且Order By 的顺序和索引顺序相同， 并且Order by 的字段都是升序，或者都是降序。否则肯定需要额外的操作，这样就会出现FileSort。 5.3.3 Filesort 的优化 通过创建合适的索引，能够减少 Filesort 的出现，但是在某些情况下，条件限制不能让Filesort消失，那就需要加快 Filesort的排序操作。对于Filesort ， MySQL 有两种排序算法： 1） 两次扫描算法 ：MySQL4.1 之前，使用该方式排序。首先根据条件取出排序字段和行指针信息，然后在排序区 sort buffer 中排序，如果sort buffer不够，则在临时表 temporary table 中存储排序结果。完成排序之后，再根据行指针回表读取记录，该操作可能会导致大量随机I/O操作。 2）一次扫描算法：一次性取出满足条件的所有字段，然后在排序区 sort buffer 中排序后直接输出结果集。排序时内存开销较大，但是排序效率比两次扫描算法要高。 MySQL 通过比较系统变量 max_length_for_sort_data 的大小和Query语句取出的字段总大小， 来判定是否那种排序算法，如果max_length_for_sort_data 更大，那么使用第二种优化之后的算法；否则使用第一种。 可以适当提高 sort_buffer_size 和 max_length_for_sort_data 系统变量，来增大排序区的大小，提高排序的效率。 5.4 优化group by 语句 由于GROUP BY 实际上也同样会进行排序操作，而且与ORDER BY 相比，GROUP BY 主要只是多了排序之后的分组操作。当然，如果在分组的时候还使用了其他的一些聚合函数，那么还需要一些聚合函数的计算。所以，在GROUP BY 的实现过程中，与 ORDER BY 一样也可以利用到索引。 如果查询包含 group by 但是用户想要避免排序结果的消耗， 则可以执行order by null 禁止排序。如下 ： drop index idx_emp_age_salary on emp;explain select age,count(*) from emp group by age; 优化后 explain select age,count(*) from emp group by age order by null; 从上面的例子可以看出，第一个SQL语句需要进行&quot;filesort&quot;，而第二个SQL由于order by null 不需要进行 &quot;filesort&quot;， 而上文提过Filesort往往非常耗费时间。 创建索引 ： create index idx_emp_age_salary on emp(age,salary)； 5.5 优化嵌套查询 Mysql4.1版本之后，开始支持SQL的子查询。这个技术可以使用SELECT语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的SQL操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询是可以被更高效的连接（JOIN）替代。 示例 ，查找有角色的所有的用户信息 : explain select * from t_user where id in (select user_id from user_role ); 执行计划为 : 优化后 : explain select * from t_user u , user_role ur where u.id = ur.user_id; 连接(Join)查询之所以更有效率一些 ，是因为MySQL不需要在内存中创建临时表来完成这个逻辑上需要两个步骤的查询工作。 5.6 优化OR条件 对于包含OR的查询子句，如果要利用索引，则OR之间的每个条件列都必须用到索引 ， 而且不能使用到复合索引； 如果没有索引，则应该考虑增加索引。 获取 emp 表中的所有的索引 ： 示例 ： explain select * from emp where id = 1 or age = 30; 建议使用 union 替换 or ： 我们来比较下重要指标，发现主要差别是 type 和 ref 这两项 type 显示的是访问类型，是较为重要的一个指标，结果值从好到坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL UNION 语句的 type 值为 ref，OR 语句的 type 值为 range，可以看到这是一个很明显的差距 UNION 语句的 ref 值为 const，OR 语句的 type 值为 null，const 表示是常量值引用，非常快 这两项的差距就说明了 UNION 要优于 OR 。 5.7 优化分页查询 一般分页查询时，通过创建覆盖索引能够比较好地提高性能。一个常见又非常头疼的问题就是 limit 2000000,10 ，此时需要MySQL排序前2000010 记录，仅仅返回2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。 5.7.1 优化思路一 在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。 5.7.2 优化思路二 该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。 5.8 使用SQL提示 SQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。 5.8.1 USE INDEX 在查询语句中表名的后面，添加 use index 来提供希望MySQL去参考的索引列表，就可以让MySQL不再考虑其他可用的索引。 create index idx_seller_name on tb_seller(name); 5.8.2 IGNORE INDEX 如果用户只是单纯的想让MySQL忽略一个或者多个索引，则可以使用 ignore index 作为 hint 。 explain select * from tb_seller ignore index(idx_seller_name) where name = '小米科技'; 5.8.3 FORCE INDEX 为强制MySQL使用一个特定的索引，可在查询中使用 force index 作为hint 。 create index idx_seller_address on tb_seller(address); Mysql高级-day03 1. 应用优化 前面章节，我们介绍了很多数据库的优化措施。但是在实际生产环境中，由于数据库本身的性能局限，就必须要对前台的应用进行一些优化，来降低数据库的访问压力。 1.1 使用连接池 对于访问数据库来说，建立连接的代价是比较昂贵的，因为我们频繁的创建关闭连接，是比较耗费资源的，我们有必要建立 数据库连接池，以提高访问的性能。 1.2 减少对MySQL的访问 1.2.1 避免对数据进行重复检索 在编写应用代码时，需要能够理清对数据库的访问逻辑。能够一次连接就获取到结果的，就不用两次连接，这样可以大大减少对数据库无用的重复请求。 比如 ，需要获取书籍的id 和name字段 ， 则查询如下： select id , name from tb_book; 之后，在业务逻辑中有需要获取到书籍状态信息， 则查询如下： select id , status from tb_book; 这样，就需要向数据库提交两次请求，数据库就要做两次查询操作。其实完全可以用一条SQL语句得到想要的结果。 select id, name , status from tb_book; 1.2.2 增加cache层 在应用中，我们可以在应用中增加 缓存 层来达到减轻数据库负担的目的。缓存层有很多种，也有很多实现方式，只要能达到降低数据库的负担又能满足应用需求就可以。 因此可以部分数据从数据库中抽取出来放到应用端以文本方式存储， 或者使用框架(Mybatis, Hibernate)提供的一级缓存/二级缓存，或者使用redis数据库来缓存数据 。 1.3 负载均衡 负载均衡是应用中使用非常普遍的一种优化方法，它的机制就是利用某种均衡算法，将固定的负载量分布到不同的服务器上， 以此来降低单台服务器的负载，达到优化的效果。 1.3.1 利用MySQL复制分流查询 通过MySQL的主从复制，实现读写分离，使增删改操作走主节点，查询操作走从节点，从而可以降低单台服务器的读写压力。 1.3.2 采用分布式数据库架构 分布式数据库架构适合大数据量、负载高的情况，它有良好的拓展性和高可用性。通过在多台服务器之间分布数据，可以实现在多台服务器之间的负载均衡，提高访问效率。 2. Mysql中查询缓存优化 2.1 概述 开启Mysql的查询缓存，当执行完全相同的SQL语句的时候，服务器就会直接从缓存中读取结果，当数据被修改，之前的缓存会失效，修改比较频繁的表不适合做查询缓存。 2.2 操作流程 客户端发送一条查询给服务器； 服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段； 服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划； MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询； 将结果返回给客户端。 2.3 查询缓存配置 查看当前的MySQL数据库是否支持查询缓存： SHOW VARIABLES LIKE 'have_query_cache'; 查看当前MySQL是否开启了查询缓存 ： SHOW VARIABLES LIKE 'query_cache_type'; 查看查询缓存的占用大小 ： SHOW VARIABLES LIKE 'query_cache_size'; 查看查询缓存的状态变量： SHOW STATUS LIKE 'Qcache%'; 各个变量的含义如下： 参数 含义 Qcache_free_blocks 查询缓存中的可用内存块数 Qcache_free_memory 查询缓存的可用内存量 Qcache_hits 查询缓存命中数 Qcache_inserts 添加到查询缓存的查询数 Qcache_lowmen_prunes 由于内存不足而从查询缓存中删除的查询数 Qcache_not_cached 非缓存查询的数量（由于 query_cache_type 设置而无法缓存或未缓存） Qcache_queries_in_cache 查询缓存中注册的查询数 Qcache_total_blocks 查询缓存中的块总数 2.4 开启查询缓存 MySQL的查询缓存默认是关闭的，需要手动配置参数 query_cache_type ， 来开启查询缓存。query_cache_type 该参数的可取值有三个 ： 值 含义 OFF 或 0 查询缓存功能关闭 ON 或 1 查询缓存功能打开，SELECT的结果符合缓存条件即会缓存，否则，不予缓存，显式指定 SQL_NO_CACHE，不予缓存 DEMAND 或 2 查询缓存功能按需进行，显式指定 SQL_CACHE 的SELECT语句才会缓存；其它均不予缓存 在 /usr/my.cnf 配置中，增加以下配置 ： 配置完毕之后，重启服务既可生效 ； 然后就可以在命令行执行SQL语句进行验证 ，执行一条比较耗时的SQL语句，然后再多执行几次，查看后面几次的执行时间；获取通过查看查询缓存的缓存命中数，来判定是否走查询缓存。 2.5 查询缓存SELECT选项 可以在SELECT语句中指定两个与查询缓存相关的选项 ： SQL_CACHE : 如果查询结果是可缓存的，并且 query_cache_type 系统变量的值为ON或 DEMAND ，则缓存查询结果 。 SQL_NO_CACHE : 服务器不使用查询缓存。它既不检查查询缓存，也不检查结果是否已缓存，也不缓存查询结果。 例子： SELECT SQL_CACHE id, name FROM customer; SELECT SQL_NO_CACHE id, name FROM customer; ​ 2.6 查询缓存失效的情况 1） SQL 语句不一致的情况， 要想命中查询缓存，查询的SQL语句必须一致。 SQL1 : select count(*) from tb_item; SQL2 : Select count(*) from tb_item; 2） 当查询语句中有一些不确定的时，则不会缓存。如 ： now() , current_date() , curdate() , curtime() , rand() , uuid() , user() , database() 。 SQL1 : select * from tb_item where updatetime &lt; now() limit 1; SQL2 : select user(); SQL3 : select database(); 3） 不使用任何表查询语句。 select 'A'; 4） 查询 mysql， information_schema或 performance_schema 数据库中的表时，不会走查询缓存。 select * from information_schema.engines; 5） 在存储的函数，触发器或事件的主体内执行的查询。 6） 如果表更改，则使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除。这包括使用MERGE映射到已更改表的表的查询。一个表可以被许多类型的语句，如被改变 INSERT， UPDATE， DELETE， TRUNCATE TABLE， ALTER TABLE， DROP TABLE，或 DROP DATABASE 。 3. Mysql内存管理及优化 3.1 内存优化原则 1） 将尽量多的内存分配给MySQL做缓存，但要给操作系统和其他程序预留足够内存。 2） MyISAM 存储引擎的数据文件读取依赖于操作系统自身的IO缓存，因此，如果有MyISAM表，就要预留更多的内存给操作系统做IO缓存。 3） 排序区、连接区等缓存是分配给每个数据库会话（session）专用的，其默认值的设置要根据最大连接数合理分配，如果设置太大，不但浪费资源，而且在并发连接较高时会导致物理内存耗尽。 3.2 MyISAM 内存优化 myisam存储引擎使用 key_buffer 缓存索引块，加速myisam索引的读写速度。对于myisam表的数据块，mysql没有特别的缓存机制，完全依赖于操作系统的IO缓存。 key_buffer_size key_buffer_size决定MyISAM索引块缓存区的大小，直接影响到MyISAM表的存取效率。可以在MySQL参数文件中设置key_buffer_size的值，对于一般MyISAM数据库，建议至少将1/4可用内存分配给key_buffer_size。 在/usr/my.cnf 中做如下配置： key_buffer_size=512M read_buffer_size 如果需要经常顺序扫描myisam表，可以通过增大read_buffer_size的值来改善性能。但需要注意的是read_buffer_size是每个session独占的，如果默认值设置太大，就会造成内存浪费。 read_rnd_buffer_size 对于需要做排序的myisam表的查询，如带有order by子句的sql，适当增加 read_rnd_buffer_size 的值，可以改善此类的sql性能。但需要注意的是 read_rnd_buffer_size 是每个session独占的，如果默认值设置太大，就会造成内存浪费。 3.3 InnoDB 内存优化 innodb用一块内存区做IO缓存池，该缓存池不仅用来缓存innodb的索引块，而且也用来缓存innodb的数据块。 innodb_buffer_pool_size 该变量决定了 innodb 存储引擎表数据和索引数据的最大缓存区大小。在保证操作系统及其他程序有足够内存可用的情况下，innodb_buffer_pool_size 的值越大，缓存命中率越高，访问InnoDB表需要的磁盘I/O 就越少，性能也就越高。 innodb_buffer_pool_size=512M innodb_log_buffer_size 决定了innodb重做日志缓存的大小，对于可能产生大量更新记录的大事务，增加innodb_log_buffer_size的大小，可以避免innodb在事务提交前就执行不必要的日志写入磁盘操作。 innodb_log_buffer_size=10M 4. Mysql并发参数调整 从实现上来说，MySQL Server 是多线程结构，包括后台线程和客户服务线程。多线程可以有效利用服务器资源，提高数据库的并发性能。在Mysql中，控制并发连接和线程的主要参数包括 max_connections、back_log、thread_cache_size、table_open_cahce。 4.1 max_connections 采用max_connections 控制允许连接到MySQL数据库的最大数量，默认值是 151。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接数已达到允许最大值而失败，这是可以考虑增大max_connections 的值。 Mysql 最大可支持的连接数，取决于很多因素，包括给定操作系统平台的线程库的质量、内存大小、每个连接的负荷、CPU的处理速度，期望的响应时间等。在Linux 平台下，性能好的服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。 4.2 back_log back_log 参数控制MySQL监听TCP端口时设置的积压请求栈大小。如果MySql的连接数达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源，将会报错。5.6.6 版本之前默认值为 50 ， 之后的版本默认为 50 + （max_connections / 5）， 但最大不超过900。 如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大back_log 的值。 4.3 table_open_cache 该参数用来控制所有SQL语句执行线程可打开表缓存的数量， 而在执行SQL语句时，每一个SQL执行线程至少要打开 1 个表缓存。该参数的值应该根据设置的最大连接数 max_connections 以及每个连接执行关联查询中涉及的表的最大数量来设定 ： ​ max_connections x N ； 4.4 thread_cache_size 为了加快连接数据库的速度，MySQL 会缓存一定数量的客户服务线程以备重用，通过参数 thread_cache_size 可控制 MySQL 缓存客户服务线程的数量。 4.5 innodb_lock_wait_timeout 该参数是用来设置InnoDB 事务等待行锁的时间，默认值是50ms ， 可以根据需要进行动态设置。对于需要快速反馈的业务系统来说，可以将行锁的等待时间调小，以避免事务长时间挂起； 对于后台运行的批量处理程序来说， 可以将行锁的等待时间调大， 以避免发生大的回滚操作。 5. Mysql锁问题 5.1 锁概述 锁是计算机协调多个进程或线程并发访问某一资源的机制（避免争抢）。 在数据库中，除传统的计算资源（如 CPU、RAM、I/O 等）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。 5.2 锁分类 从对数据操作的粒度分 ： 1） 表锁：操作时，会锁定整个表。 2） 行锁：操作时，会锁定当前操作行。 从对数据操作的类型分： 1） 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响。 2） 写锁（排它锁）：当前操作没有完成之前，它会阻断其他写锁和读锁。 5.3 Mysql 锁 相对其他数据库而言，MySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。下表中罗列出了各存储引擎对锁的支持情况： 存储引擎 表级锁 行级锁 页面锁 MyISAM 支持 不支持 不支持 InnoDB 支持 支持 不支持 MEMORY 支持 不支持 不支持 BDB 支持 不支持 支持 MySQL这3种锁的特性可大致归纳如下 ： 锁类型 特点 表级锁 偏向MyISAM 存储引擎，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。 行级锁 偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 页面锁 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 从上述特点可见，很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理（OLTP）系统。 5.2 MyISAM 表锁 MyISAM 存储引擎只支持表锁，这也是MySQL开始几个版本中唯一支持的锁类型。 5.2.1 如何加表锁 MyISAM 在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。 显示加表锁语法： 加读锁 ： lock table table_name read; 加写锁 ： lock table table_name write； 5.2.2 读锁案例 准备环境 create database demo_03 default charset=utf8mb4; use demo_03; CREATE TABLE `tb_book` ( `id` INT(11) auto_increment, `name` VARCHAR(50) DEFAULT NULL, `publish_time` DATE DEFAULT NULL, `status` CHAR(1) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=myisam DEFAULT CHARSET=utf8 ; INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,'java编程思想','2088-08-01','1'); INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,'solr编程思想','2088-08-08','0'); CREATE TABLE `tb_user` ( `id` INT(11) auto_increment, `name` VARCHAR(50) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=myisam DEFAULT CHARSET=utf8 ; INSERT INTO tb_user (id, name) VALUES(NULL,'令狐冲'); INSERT INTO tb_user (id, name) VALUES(NULL,'田伯光'); 客户端 一 ： 1）获得tb_book 表的读锁 lock table tb_book read; 2） 执行查询操作 select * from tb_book; 可以正常执行 ， 查询出数据。 客户端 二 ： 3） 执行查询操作 select * from tb_book; 客户端 一 ： 4）查询未锁定的表 select name from tb_seller; 客户端 二 ： 5）查询未锁定的表 select name from tb_seller; 可以正常查询出未锁定的表； 客户端 一 ： 6） 执行插入操作 insert into tb_book values(null,'Mysql高级','2088-01-01','1'); 执行插入， 直接报错 ， 由于当前tb_book 获得的是 读锁， 不能执行更新操作。 客户端 二 ： 7） 执行插入操作 insert into tb_book values(null,'Mysql高级','2088-01-01','1'); 当在客户端一中释放锁指令 unlock tables 后 ， 客户端二中的 inesrt 语句 ， 立即执行 ； 5.2.3 写锁案例 客户端 一 : 1）获得tb_book 表的写锁 lock table tb_book write ; 2）执行查询操作 select * from tb_book ; 查询操作执行成功； 3）执行更新操作 update tb_book set name = 'java编程思想（第二版）' where id = 1; 更新操作执行成功 ； 客户端 二 : 4）执行查询操作 select * from tb_book ; 当在客户端一中释放锁指令 unlock tables 后 ， 客户端二中的 select 语句 ， 立即执行 ； 5.2.4 结论 锁模式的相互兼容性如表中所示： 由上表可见： ​ 1） 对MyISAM 表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求； ​ 2） 对MyISAM 表的写操作，则会阻塞其他用户对同一表的读和写操作； ​ 简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁，则既会阻塞读，又会阻塞写。 此外，MyISAM 的读写锁调度是写优先，这也是MyISAM不适合做写为主的表的存储引擎的原因。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。 5.2.5 查看锁的争用情况 show open tables； In_user : 表当前被查询使用的次数。如果该数为零，则表是打开的，但是当前没有被使用。 Name_locked：表名称是否被锁定。名称锁定用于取消表或对表进行重命名等操作。 show status like 'Table_locks%'; Table_locks_immediate ： 指的是能够立即获得表级锁的次数，每立即获取锁，值加1。 Table_locks_waited ： 指的是不能立即获取表级锁而需要等待的次数，每等待一次，该值加1，此值高说明存在着较为严重的表级锁争用情况。 5.3 InnoDB 行锁 5.3.1 行锁介绍 行锁特点 ：偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 InnoDB 与 MyISAM 的最大不同有两点：一是支持事务；二是 采用了行级锁。 5.3.2 背景知识 事务及其ACID属性 事务是由一组SQL语句组成的逻辑处理单元。 事务具有以下4个特性，简称为事务ACID属性。 ACID属性 含义 原子性（Atomicity） 事务是一个原子操作单元，其对数据的修改，要么全部成功，要么全部失败。 一致性（Consistent） 在事务开始和完成时，数据都必须保持一致状态。 隔离性（Isolation） 数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的 “独立” 环境下运行。 持久性（Durable） 事务完成之后，对于数据的修改是永久的。 并发事务处理带来的问题 问题 含义 丢失更新（Lost Update） 当两个或多个事务选择同一行，最初的事务修改的值，会被后面的事务修改的值覆盖。 脏读（Dirty Reads） 当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 不可重复读（Non-Repeatable Reads） 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现和以前读出的数据不一致。 幻读（Phantom Reads） 一个事务按照相同的查询条件重新读取以前查询过的数据，却发现其他事务插入了满足其查询条件的新数据。 事务隔离级别 为了解决上述提到的事务并发问题，数据库提供一定的事务隔离机制来解决这个问题。数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使用事务在一定程度上“串行化” 进行，这显然与“并发” 是矛盾的。 数据库的隔离级别有4个，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏写、脏读、不可重复读、幻读这几类问题。 隔离级别 丢失更新 脏读 不可重复读 幻读 Read uncommitted × √ √ √ Read committed × × √ √ Repeatable read（默认） × × × √ Serializable × × × × 备注 ： √ 代表可能出现 ， × 代表不会出现 。 Mysql 的数据库的默认隔离级别为 Repeatable read ， 查看方式： show variables like 'tx_isolation'; 5.3.3 InnoDB 的行锁模式 InnoDB 实现了以下两种类型的行锁。 共享锁（S）：又称为读锁，简称S锁，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。 排他锁（X）：又称为写锁，简称X锁，排他锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行读取和修改。 对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)； 对于普通SELECT语句，InnoDB不会加任何锁； 可以通过以下语句显示给记录集加共享锁或排他锁 。 共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE 排他锁（X) ：SELECT * FROM table_name WHERE ... FOR UPDATE 5.3.4 案例准备工作 create table test_innodb_lock( id int(11), name varchar(16), sex varchar(1) )engine = innodb default charset=utf8; insert into test_innodb_lock values(1,'100','1'); insert into test_innodb_lock values(3,'3','1'); insert into test_innodb_lock values(4,'400','0'); insert into test_innodb_lock values(5,'500','1'); insert into test_innodb_lock values(6,'600','0'); insert into test_innodb_lock values(7,'700','0'); insert into test_innodb_lock values(8,'800','1'); insert into test_innodb_lock values(9,'900','1'); insert into test_innodb_lock values(1,'200','0'); create index idx_test_innodb_lock_id on test_innodb_lock(id); create index idx_test_innodb_lock_name on test_innodb_lock(name); 5.3.5 行锁基本演示 Session-1 Session-2 关闭自动提交功能 可以正常的查询出全部的数据 获取id为3的数据 ； 更新id为3 的数据， 出于等待状态 解除阻塞，更新正常进行 以上， 操作的都是同一行的数据，接下来，演示不同行的数据 ： 由于与Session-1 操作不是同一行，获取当前行锁，执行更新； 5.3.6 无索引行锁升级为表锁 如果不通过索引条件检索数据，那么InnoDB将对表中的所有记录加锁，实际效果跟表锁一样。 查看当前表的索引 ： show index from test_innodb_lock ; Session-1 Session-2 关闭事务的自动提交 执行更新语句 ： 提交事务： 执行提交操作 ： 由于 执行更新时 ， name字段本来为varchar类型， 我们是作为数组类型使用，存在类型转换，索引失效，最终行锁变为表锁 ； 5.3.7 间隙锁危害 当我们用范围条件，而不是使用相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据进行加锁； 对于键值在条件范围内但并不存在的记录，叫做 &quot;间隙（GAP）&quot; ， InnoDB也会对这个 &quot;间隙&quot; 加锁，这种锁机制就是所谓的 间隙锁（Next-Key锁） 。 示例 ： Session-1 Session-2 关闭事务自动提交 根据id范围更新数据 插入id为2的记录， 出于阻塞状态 提交事务 ； 解除阻塞 ， 执行插入操作 ： 提交事务 ： 5.3.8 InnoDB 行锁争用情况 show status like 'innodb_row_lock%'; Innodb_row_lock_current_waits: 当前正在等待锁定的数量 Innodb_row_lock_time: 从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg:每次等待所花平均时长 Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间 Innodb_row_lock_waits: 系统启动后到现在总共等待的次数 当等待的次数很高，而且每次等待的时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。 5.3.9 总结 InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面带来了性能损耗可能比表锁会更高一些，但是在整体并发处理能力方面要远远由于MyISAM的表锁的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势。 但是，InnoDB的行级锁同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。 优化建议： 尽可能让所有数据检索都能通过索引来完成，避免无索引行锁升级为表锁。 合理设计索引，尽量缩小锁的范围 尽可能减少索引条件，及索引范围，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可使用低级别事务隔离（但是需要业务层面满足需求） 6. 常用SQL技巧 6.1 SQL执行顺序 编写顺序 SELECT DISTINCT &lt;select list&gt; FROM &lt;left_table&gt; &lt;join_type&gt; JOIN &lt;right_table&gt; ON &lt;join_condition&gt; WHERE &lt;where_condition&gt; GROUP BY &lt;group_by_list&gt; HAVING &lt;having_condition&gt; ORDER BY &lt;order_by_condition&gt; LIMIT &lt;limit_params&gt; 执行顺序 FROM &lt;left_table&gt; ON &lt;join_condition&gt; &lt;join_type&gt; JOIN &lt;right_table&gt; WHERE &lt;where_condition&gt; GROUP BY &lt;group_by_list&gt; HAVING &lt;having_condition&gt; SELECT DISTINCT &lt;select list&gt; ORDER BY &lt;order_by_condition&gt; LIMIT &lt;limit_params&gt; 6.2 正则表达式使用 正则表达式（Regular Expression）是指一个用来描述或者匹配一系列符合某个句法规则的字符串的单个字符串。 符号 含义 ^ 在字符串开始处进行匹配 $ 在字符串末尾处进行匹配 . 匹配任意单个字符, 包括换行符 [...] 匹配出括号内的任意字符 [^...] 匹配不出括号内的任意字符 a* 匹配零个或者多个a(包括空串) a+ 匹配一个或者多个a(不包括空串) a? 匹配零个或者一个a a1|a2 匹配a1或a2 a(m) 匹配m个a a(m,) 至少匹配m个a a(m,n) 匹配m个a 到 n个a a(,n) 匹配0到n个a (...) 将模式元素组成单一元素 select * from emp where name regexp '^T'; select * from emp where name regexp '2$'; select * from emp where name regexp '[uvw]'; 6.3 MySQL 常用函数 数字函数 函数名称 作 用 ABS 求绝对值 SQRT 求二次方根 MOD 求余数 CEIL 和 CEILING 两个函数功能相同，都是返回不小于参数的最小整数，即向上取整 FLOOR 向下取整，返回值转化为一个BIGINT RAND 生成一个0~1之间的随机数，传入整数参数是，用来产生重复序列 ROUND 对所传参数进行四舍五入 SIGN 返回参数的符号 POW 和 POWER 两个函数的功能相同，都是所传参数的次方的结果值 SIN 求正弦值 ASIN 求反正弦值，与函数 SIN 互为反函数 COS 求余弦值 ACOS 求反余弦值，与函数 COS 互为反函数 TAN 求正切值 ATAN 求反正切值，与函数 TAN 互为反函数 COT 求余切值 字符串函数 函数名称 作 用 LENGTH 计算字符串长度函数，返回字符串的字节长度 CONCAT 合并字符串函数，返回结果为连接参数产生的字符串，参数可以使一个或多个 INSERT 替换字符串函数 LOWER 将字符串中的字母转换为小写 UPPER 将字符串中的字母转换为大写 LEFT 从左侧字截取符串，返回字符串左边的若干个字符 RIGHT 从右侧字截取符串，返回字符串右边的若干个字符 TRIM 删除字符串左右两侧的空格 REPLACE 字符串替换函数，返回替换后的新字符串 SUBSTRING 截取字符串，返回从指定位置开始的指定长度的字符换 REVERSE 字符串反转（逆序）函数，返回与原始字符串顺序相反的字符串 日期函数 函数名称 作 用 CURDATE 和 CURRENT_DATE 两个函数作用相同，返回当前系统的日期值 CURTIME 和 CURRENT_TIME 两个函数作用相同，返回当前系统的时间值 NOW 和 SYSDATE 两个函数作用相同，返回当前系统的日期和时间值 MONTH 获取指定日期中的月份 MONTHNAME 获取指定日期中的月份英文名称 DAYNAME 获取指定曰期对应的星期几的英文名称 DAYOFWEEK 获取指定日期对应的一周的索引位置值 WEEK 获取指定日期是一年中的第几周，返回值的范围是否为 0〜52 或 1〜53 DAYOFYEAR 获取指定曰期是一年中的第几天，返回值范围是1~366 DAYOFMONTH 获取指定日期是一个月中是第几天，返回值范围是1~31 YEAR 获取年份，返回值范围是 1970〜2069 TIME_TO_SEC 将时间参数转换为秒数 SEC_TO_TIME 将秒数转换为时间，与TIME_TO_SEC 互为反函数 DATE_ADD 和 ADDDATE 两个函数功能相同，都是向日期添加指定的时间间隔 DATE_SUB 和 SUBDATE 两个函数功能相同，都是向日期减去指定的时间间隔 ADDTIME 时间加法运算，在原始时间上添加指定的时间 SUBTIME 时间减法运算，在原始时间上减去指定的时间 DATEDIFF 获取两个日期之间间隔，返回参数 1 减去参数 2 的值 DATE_FORMAT 格式化指定的日期，根据参数返回指定格式的值 WEEKDAY 获取指定日期在一周内的对应的工作日索引 聚合函数 函数名称 作用 MAX 查询指定列的最大值 MIN 查询指定列的最小值 COUNT 统计查询结果的行数 SUM 求和，返回指定列的总和 AVG 求平均值，返回指定列数据的平均值 Mysql高级-day04 1. MySql中常用工具 1.1 mysql 该mysql不是指mysql服务，而是指mysql的客户端工具。 语法 ： mysql [options] [database] 1.1.1 连接选项 参数 ： -u, --user=name 指定用户名 -p, --password[=name] 指定密码 -h, --host=name 指定服务器IP或域名 -P, --port=# 指定连接端口 示例 ： mysql -h 127.0.0.1 -P 3306 -u root -p mysql -h127.0.0.1 -P3306 -uroot -p2143 1.1.2 执行选项 -e, --execute=name 执行SQL语句并退出 此选项可以在Mysql客户端执行SQL语句，而不用连接到MySQL数据库再执行，对于一些批处理脚本，这种方式尤其方便。 示例： mysql -uroot -p2143 db01 -e &quot;select * from tb_book&quot;; 1.2 mysqladmin mysqladmin 是一个执行管理操作的客户端程序。可以用它来检查服务器的配置和当前状态、创建并删除数据库等。 可以通过 ： mysqladmin --help 指令查看帮助文档 示例 ： mysqladmin -uroot -p2143 create 'test01'; mysqladmin -uroot -p2143 drop 'test01'; mysqladmin -uroot -p2143 version; 1.3 mysqlbinlog 由于服务器生成的二进制日志文件以二进制格式保存，所以如果想要检查这些文本的文本格式，就会使用到mysqlbinlog 日志管理工具。 语法 ： mysqlbinlog [options] log-files1 log-files2 ... 选项： -d, --database=name : 指定数据库名称，只列出指定的数据库相关操作。 -o, --offset=# : 忽略掉日志中的前n行命令。 -r,--result-file=name : 将输出的文本格式日志输出到指定文件。 -s, --short-form : 显示简单格式， 省略掉一些信息。 --start-datatime=date1 --stop-datetime=date2 : 指定日期间隔内的所有日志。 --start-position=pos1 --stop-position=pos2 : 指定位置间隔内的所有日志。 1.4 mysqldump mysqldump 客户端工具用来备份数据库或在不同数据库之间进行数据迁移。备份内容包含创建表，及插入表的SQL语句。 语法 ： mysqldump [options] db_name [tables] mysqldump [options] --database/-B db1 [db2 db3...] mysqldump [options] --all-databases/-A 1.4.1 连接选项 参数 ： -u, --user=name 指定用户名 -p, --password[=name] 指定密码 -h, --host=name 指定服务器IP或域名 -P, --port=# 指定连接端口 1.4.2 输出内容选项 参数： --add-drop-database 在每个数据库创建语句前加上 Drop database 语句 --add-drop-table 在每个表创建语句前加上 Drop table 语句 , 默认开启 ; 不开启 (--skip-add-drop-table) -n, --no-create-db 不包含数据库的创建语句 -t, --no-create-info 不包含数据表的创建语句 -d --no-data 不包含数据 -T, --tab=name 自动生成两个文件：一个.sql文件，创建表结构的语句； 一个.txt文件，数据文件，相当于select into outfile 示例 ： mysqldump -uroot -p2143 db01 tb_book --add-drop-database --add-drop-table &gt; a mysqldump -uroot -p2143 -T /tmp test city 1.5 mysqlimport/source mysqlimport 是客户端数据导入工具，用来导入mysqldump 加 -T 参数后导出的文本文件。 语法： mysqlimport [options] db_name textfile1 [textfile2...] 示例： mysqlimport -uroot -p2143 test /tmp/city.txt 如果需要导入sql文件,可以使用mysql中的source 指令 : source /root/tb_book.sql 1.6 mysqlshow mysqlshow 客户端对象查找工具，用来很快地查找存在哪些数据库、数据库中的表、表中的列或者索引。 语法： mysqlshow [options] [db_name [table_name [col_name]]] 参数： --count 显示数据库及表的统计信息（数据库，表 均可以不指定）-i 显示指定数据库或者指定表的状态信息 示例： #查询每个数据库的表的数量及表中记录的数量mysqlshow -uroot -p2143 --count#查询test库中每个表中的字段书，及行数mysqlshow -uroot -p2143 test --count#查询test库中book表的详细情况mysqlshow -uroot -p2143 test book --count 2. Mysql 日志 在任何一种数据库中，都会有各种各样的日志，记录着数据库工作的方方面面，以帮助数据库管理员追踪数据库曾经发生过的各种事件。MySQL 也不例外，在 MySQL 中，有 4 种不同的日志，分别是错误日志、二进制日志（BINLOG 日志）、查询日志和慢查询日志，这些日志记录着数据库在不同方面的踪迹。 2.1 错误日志 错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，可以首先查看此日志。 该日志是默认开启的 ， 默认存放目录为 mysql 的数据目录（var/lib/mysql）, 默认的日志文件名为 hostname.err（hostname是主机名）。 查看日志位置指令 ： show variables like 'log_error%'; 查看日志内容 ： tail -f /var/lib/mysql/xaxh-server.err 2.2 二进制日志 2.2.1概述 二进制日志（BINLOG）记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句，但是不包括数据查询语句。此日志对于灾难时的数据恢复起着极其重要的作用，MySQL的主从复制， 就是通过该binlog实现的。 二进制日志，默认情况下是没有开启的，需要到MySQL的配置文件中开启，并配置MySQL日志的格式。 配置文件位置 : /usr/my.cnf 日志存放位置 : 配置时，给定了文件名但是没有指定路径，日志默认写入Mysql的数据目录。 #配置开启binlog日志， 日志的文件前缀为 mysqlbin -----&gt; 生成的文件名如 : mysqlbin.000001,mysqlbin.000002 log_bin=mysqlbin #配置二进制日志的格式 binlog_format=STATEMENT 2.2.2 日志格式 STATEMENT 该日志格式在日志文件中记录的都是SQL语句（statement），每一条对数据进行修改的SQL都会记录在日志文件中，通过Mysql提供的mysqlbinlog工具，可以清晰的查看到每条语句的文本。主从复制的时候，从库（slave）会将日志解析为原文本，并在从库重新执行一次。 ROW 该日志格式在日志文件中记录的是每一行的数据变更，而不是记录SQL语句。比如，执行SQL语句 ： update tb_book set status='1' , 如果是STATEMENT 日志格式，在日志中会记录一行SQL文件； 如果是ROW，由于是对全表进行更新，也就是每一行记录都会发生变更，ROW 格式的日志中会记录每一行的数据变更。 MIXED 这是目前MySQL默认的日志格式，即混合了STATEMENT 和 ROW两种格式。默认情况下采用STATEMENT，但是在一些特殊情况下采用ROW来进行记录。MIXED 格式能尽量利用两种模式的优点，而避开他们的缺点。 2.2.3 日志读取 由于日志以二进制方式存储，不能直接读取，需要用mysqlbinlog工具来查看，语法如下 ： mysqlbinlog log-file； 查看STATEMENT格式日志 执行插入语句 ： insert into tb_book values(null,'Lucene','2088-05-01','0'); 查看日志文件 ： mysqlbin.index : 该文件是日志索引文件 ， 记录日志的文件名； mysqlbing.000001 ：日志文件 查看日志内容 ： mysqlbinlog mysqlbing.000001； 查看ROW格式日志 配置 : #配置开启binlog日志， 日志的文件前缀为 mysqlbin -----&gt; 生成的文件名如 : mysqlbin.000001,mysqlbin.000002 log_bin=mysqlbin #配置二进制日志的格式 binlog_format=ROW 插入数据 : insert into tb_book values(null,'SpringCloud实战','2088-05-05','0'); 如果日志格式是 ROW , 直接查看数据 , 是查看不懂的 ; 可以在mysqlbinlog 后面加上参数 -vv mysqlbinlog -vv mysqlbin.000002 2.2.4 日志删除 对于比较繁忙的系统，由于每天生成日志量大 ，这些日志如果长时间不清楚，将会占用大量的磁盘空间。下面我们将会讲解几种删除日志的常见方法 ： 方式一 通过 Reset Master 指令删除全部 binlog 日志，删除之后，日志编号，将从 xxxx.000001重新开始 。 查询之前 ，先查询下日志文件 ： 执行删除日志指令： Reset Master 执行之后， 查看日志文件 ： 方式二 执行指令 purge master logs to 'mysqlbin.******' ，该命令将删除 ****** 编号之前的所有日志。 方式三 执行指令 purge master logs before 'yyyy-mm-dd hh24:mi:ss' ，该命令将删除日志为 &quot;yyyy-mm-dd hh24:mi:ss&quot; 之前产生的所有日志 。 方式四 设置参数 --expire_logs_days=# ，此参数的含义是设置日志的过期天数， 过了指定的天数后日志将会被自动删除，这样将有利于减少DBA 管理日志的工作量。 配置如下 ： 2.3 查询日志 查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的SQL语句。 默认情况下， 查询日志是未开启的。如果需要开启查询日志，可以设置以下配置 ： #该选项用来开启查询日志 ， 可选值 ： 0 或者 1 ； 0 代表关闭， 1 代表开启 general_log=1 #设置日志的文件名 ， 如果没有指定， 默认的文件名为 host_name.log general_log_file=file_name 在 mysql 的配置文件 /usr/my.cnf 中配置如下内容 ： 配置完毕之后，在数据库执行以下操作 ： select * from tb_book; select * from tb_book where id = 1; update tb_book set name = 'lucene入门指南' where id = 5; select * from tb_book where id &lt; 8; 执行完毕之后， 再次来查询日志文件 ： 2.4 慢查询日志 慢查询日志记录了所有执行时间超过参数 long_query_time 设置值并且扫描记录数不小于 min_examined_row_limit 的所有的SQL语句的日志。long_query_time 默认为 10 秒，最小为 0， 精度可以到微秒。 2.4.1 文件位置和格式 慢查询日志默认是关闭的 。可以通过两个参数来控制慢查询日志 ： # 该参数用来控制慢查询日志是否开启， 可取值： 1 和 0 ， 1 代表开启， 0 代表关闭slow_query_log=1 # 该参数用来指定慢查询日志的文件名slow_query_log_file=slow_query.log# 该选项用来配置查询的时间限制， 超过这个时间将认为值慢查询， 将需要进行日志记录， 默认10slong_query_time=10 2.4.2 日志的读取 和错误日志、查询日志一样，慢查询日志记录的格式也是纯文本，可以被直接读取。 1） 查询long_query_time 的值。 2） 执行查询操作 select id, title,price,num ,status from tb_item where id = 1; 由于该语句执行时间很短，为0s ， 所以不会记录在慢查询日志中。 select * from tb_item where title like '%阿尔卡特 (OT-927) 炭黑 联通3G手机 双卡双待165454%' ; 该SQL语句 ， 执行时长为 26.77s ，超过10s ， 所以会记录在慢查询日志文件中。 3） 查看慢查询日志文件 直接通过cat 指令查询该日志文件 ： 如果慢查询日志内容很多， 直接查看文件，比较麻烦， 这个时候可以借助于mysql自带的 mysqldumpslow 工具， 来对慢查询日志进行分类汇总。 3. Mysql复制 3.1 复制概述 复制是指将主数据库的DDL 和 DML 操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。 MySQL支持一台主库同时向多台从库进行复制， 从库同时也可以作为其他从服务器的主库，实现链状复制。 3.2 复制原理 MySQL 的主从复制原理如下。 从上层来看，复制分成三步： Master 主库在事务提交时，会把数据变更作为时间 Events 记录在二进制日志文件 Binlog 中。 主库推送二进制日志文件 Binlog 中的日志事件到从库的中继日志 Relay Log 。 slave重做中继日志中的事件，将改变反映它自己的数据。 3.3 复制优势 MySQL 复制的有点主要包含以下三个方面： 主库出现问题，可以快速切换到从库提供服务。 可以在从库上执行查询操作，从主库中更新，实现读写分离，降低主库的访问压力。 可以在从库中执行备份，以避免备份期间影响主库的服务。 3.4 搭建步骤 3.4.1 master 1） 在master 的配置文件（/usr/my.cnf）中，配置如下内容： #mysql 服务ID,保证整个集群环境中唯一 server-id=1 #mysql binlog 日志的存储路径和文件名 log-bin=/var/lib/mysql/mysqlbin #错误日志,默认已经开启 #log-err #mysql的安装目录 #basedir #mysql的临时目录 #tmpdir #mysql的数据存放目录 #datadir #是否只读,1 代表只读, 0 代表读写 read-only=0 #忽略的数据, 指不需要同步的数据库 binlog-ignore-db=mysql #指定同步的数据库 #binlog-do-db=db01 2） 执行完毕之后，需要重启Mysql： service mysql restart ； 3） 创建同步数据的账户，并且进行授权操作： grant replication slave on *.* to 'itcast'@'192.168.192.131' identified by 'itcast'; flush privileges; 4） 查看master状态： show master status; 字段含义： File : 从哪个日志文件开始推送日志文件 Position ： 从哪个位置开始推送日志Binlog_Ignore_DB : 指定不需要同步的数据库 3.4.2 slave 1） 在 slave 端配置文件中，配置如下内容： #mysql服务端ID,唯一 server-id=2 #指定binlog日志 log-bin=/var/lib/mysql/mysqlbin 2） 执行完毕之后，需要重启Mysql： service mysql restart； 3） 执行如下指令 ： change master to master_host= '192.168.192.130', master_user='itcast', master_password='itcast', master_log_file='mysqlbin.000001', master_log_pos=413; 指定当前从库对应的主库的IP地址，用户名，密码，从哪个日志文件开始的那个位置开始同步推送日志。 4） 开启同步操作 start slave; show slave status; 5） 停止同步操作 stop slave; 3.4.3 验证同步操作 1） 在主库中创建数据库，创建表，并插入数据 ： create database db01; user db01; create table user( id int(11) not null auto_increment, name varchar(50) not null, sex varchar(1), primary key (id) )engine=innodb default charset=utf8; insert into user(id,name,sex) values(null,'Tom','1'); insert into user(id,name,sex) values(null,'Trigger','0'); insert into user(id,name,sex) values(null,'Dawn','1'); 2） 在从库中查询数据，进行验证 ： 在从库中，可以查看到刚才创建的数据库： 在该数据库中，查询user表中的数据： 4. 综合案例 4.1 需求分析 在业务系统中，需要记录当前业务系统的访问日志，该访问日志包含：操作人，操作时间，访问类，访问方法，请求参数，请求结果，请求结果类型，请求时长 等信息。记录详细的系统访问日志，主要便于对系统中的用户请求进行追踪，并且在系统 的管理后台可以查看到用户的访问记录。 记录系统中的日志信息，可以通过Spring 框架的AOP来实现。具体的请求处理流程，如下： 4.2 搭建案例环境 4.2.1 数据库表 CREATE DATABASE mysql_demo DEFAULT CHARACTER SET utf8mb4 ； CREATE TABLE `brand` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL COMMENT '品牌名称', `first_char` varchar(1) DEFAULT NULL COMMENT '品牌首字母', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; CREATE TABLE `item` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '商品id', `title` varchar(100) NOT NULL COMMENT '商品标题', `price` double(10,2) NOT NULL COMMENT '商品价格，单位为：元', `num` int(10) NOT NULL COMMENT '库存数量', `categoryid` bigint(10) NOT NULL COMMENT '所属类目，叶子类目', `status` varchar(1) DEFAULT NULL COMMENT '商品状态，1-正常，2-下架，3-删除', `sellerid` varchar(50) DEFAULT NULL COMMENT '商家ID', `createtime` datetime DEFAULT NULL COMMENT '创建时间', `updatetime` datetime DEFAULT NULL COMMENT '更新时间', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='商品表'; CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(45) NOT NULL, `password` varchar(96) NOT NULL, `name` varchar(45) NOT NULL, `birthday` datetime DEFAULT NULL, `sex` char(1) DEFAULT NULL, `email` varchar(45) DEFAULT NULL, `phone` varchar(45) DEFAULT NULL, `qq` varchar(32) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; CREATE TABLE `operation_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'ID', `operate_class` varchar(200) DEFAULT NULL COMMENT '操作类', `operate_method` varchar(200) DEFAULT NULL COMMENT '操作方法', `return_class` varchar(200) DEFAULT NULL COMMENT '返回值类型', `operate_user` varchar(20) DEFAULT NULL COMMENT '操作用户', `operate_time` varchar(20) DEFAULT NULL COMMENT '操作时间', `param_and_value` varchar(500) DEFAULT NULL COMMENT '请求参数名及参数值', `cost_time` bigint(20) DEFAULT NULL COMMENT '执行方法耗时, 单位 ms', `return_value` varchar(200) DEFAULT NULL COMMENT '返回值', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 4.2.2 pom.xml &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.16&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;${log4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;8080&lt;/port&gt; &lt;path&gt;/&lt;/path&gt; &lt;uriEncoding&gt;utf-8&lt;/uriEncoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 4.2.3 web.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; version=&quot;2.5&quot;&gt; &lt;!-- 解决post乱码 --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;log-datalist.html&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;/web-app&gt; 4.2.4 db.properties jdbc.driver=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://192.168.142.128:3306/mysql_demo jdbc.username=root jdbc.password=itcast 4.2.5 applicationContext.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 加载配置文件 --&gt; &lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt; &lt;!-- 配置 spring 创建容器时要扫描的包 --&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;&gt; &lt;/context:exclude-filter&gt; &lt;/context:component-scan&gt; &lt;!-- 配置 MyBatis 的 Session 工厂 --&gt; &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;cn.itcast.pojo&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;driverClass&quot; value=&quot;${jdbc.driver}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;${jdbc.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;${jdbc.username}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${jdbc.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置 Mapper 扫描器 --&gt; &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;cn.itcast.mapper&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置事务的注解驱动 --&gt; &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;&gt;&lt;/tx:annotation-driven&gt; &lt;/beans&gt; 4.2.6 springmvc.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;cn.itcast.controller&quot;&gt;&lt;/context:component-scan&gt; &lt;mvc:annotation-driven&gt;&lt;/mvc:annotation-driven&gt; &lt;aop:aspectj-autoproxy /&gt; &lt;/beans&gt; 4.2.7 导入基础工程 4.3 通过AOP记录操作日志 4.3.1 自定义注解 通过自定义注解，来标示方法需不需要进行记录日志，如果该方法在访问时需要记录日志，则在该方法上标示该注解既可。 @Inherited @Documented @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface OperateLog { } 4.3.2 定义通知类 @Component @Aspect public class OperateAdvice { private static Logger log = Logger.getLogger(OperateAdvice.class); @Autowired private OperationLogService operationLogService; @Around(&quot;execution(* cn.itcast.controller.*.*(..)) &amp;&amp; @annotation(operateLog)&quot;) public Object insertLogAround(ProceedingJoinPoint pjp , OperateLog operateLog) throws Throwable{ System.out.println(&quot; ************************ 记录日志 [start] ****************************** &quot;); OperationLog op = new OperationLog(); DateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); op.setOperateTime(sdf.format(new Date())); op.setOperateUser(DataUtils.getRandStr(8)); op.setOperateClass(pjp.getTarget().getClass().getName()); op.setOperateMethod(pjp.getSignature().getName()); //获取方法调用时传递的参数 Object[] args = pjp.getArgs(); op.setParamAndValue(Arrays.toString(args)); long start_time = System.currentTimeMillis(); //放行 Object object = pjp.proceed(); long end_time = System.currentTimeMillis(); op.setCostTime(end_time - start_time); if(object != null){ op.setReturnClass(object.getClass().getName()); op.setReturnValue(object.toString()); }else{ op.setReturnClass(&quot;java.lang.Object&quot;); op.setParamAndValue(&quot;void&quot;); } log.error(JsonUtils.obj2JsonString(op)); operationLogService.insert(op); System.out.println(&quot; ************************** 记录日志 [end] *************************** &quot;); return object; } } 4.3.3 方法上加注解 在需要记录日志的方法上加上注解@OperateLog。 @OperateLog @RequestMapping(&quot;/insert&quot;) public Result insert(@RequestBody Brand brand){ try { brandService.insert(brand); return new Result(true,&quot;操作成功&quot;); } catch (Exception e) { e.printStackTrace(); return new Result(false,&quot;操作失败&quot;); } } 4.4 日志查询后端代码实现 4.4.1 Mapper接口 public interface OperationLogMapper { public void insert(OperationLog operationLog); public List&lt;OperationLog&gt; selectListByCondition(Map dataMap); public Long countByCondition(Map dataMap); } 4.4.2 Mapper.xml 映射配置文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt; &lt;mapper namespace=&quot;cn.itcast.mapper.OperationLogMapper&quot; &gt; &lt;insert id=&quot;insert&quot; parameterType=&quot;operationLog&quot;&gt; INSERT INTO operation_log(id,return_value,return_class,operate_user,operate_time,param_and_value, operate_class,operate_method,cost_time) VALUES(NULL,#{returnValue},#{returnClass},#{operateUser},#{operateTime},#{paramAndValue}, #{operateClass},#{operateMethod},#{costTime}) &lt;/insert&gt; &lt;select id=&quot;selectListByCondition&quot; parameterType=&quot;map&quot; resultType=&quot;operationLog&quot;&gt; select id , operate_class as operateClass , operate_method as operateMethod, return_class as returnClass, operate_user as operateUser, operate_time as operateTime, param_and_value as paramAndValue, cost_time as costTime, return_value as returnValue from operation_log &lt;include refid=&quot;oplog_where&quot;/&gt; limit #{start},#{size} &lt;/select&gt; &lt;select id=&quot;countByCondition&quot; resultType=&quot;long&quot; parameterType=&quot;map&quot;&gt; select count(*) from operation_log &lt;include refid=&quot;oplog_where&quot;/&gt; &lt;/select&gt; &lt;sql id=&quot;oplog_where&quot;&gt; &lt;where&gt; &lt;if test=&quot;operateClass != null and operateClass != '' &quot;&gt; and operate_class = #{operateClass} &lt;/if&gt; &lt;if test=&quot;operateMethod != null and operateMethod != '' &quot;&gt; and operate_method = #{operateMethod} &lt;/if&gt; &lt;if test=&quot;returnClass != null and returnClass != '' &quot;&gt; and return_class = #{returnClass} &lt;/if&gt; &lt;if test=&quot;costTime != null&quot;&gt; and cost_time = #{costTime} &lt;/if&gt; &lt;/where&gt; &lt;/sql&gt; &lt;/mapper&gt; 4.4.3 Service @Service @Transactional public class OperationLogService { //private static Logger logger = Logger.getLogger(OperationLogService.class); @Autowired private OperationLogMapper operationLogMapper; //插入数据 public void insert(OperationLog operationLog){ operationLogMapper.insert(operationLog); } //根据条件查询 public PageResult selectListByCondition(Map dataMap, Integer pageNum , Integer pageSize){ if(paramMap ==null){ paramMap = new HashMap(); } paramMap.put(&quot;start&quot; , (pageNum-1)*rows); paramMap.put(&quot;rows&quot;,rows); Object costTime = paramMap.get(&quot;costTime&quot;); if(costTime != null){ if(&quot;&quot;.equals(costTime.toString())){ paramMap.put(&quot;costTime&quot;,null); }else{ paramMap.put(&quot;costTime&quot;,new Long(paramMap.get(&quot;costTime&quot;).toString())); } } System.out.println(dataMap); long countStart = System.currentTimeMillis(); Long count = operationLogMapper.countByCondition(dataMap); long countEnd = System.currentTimeMillis(); System.out.println(&quot;Count Cost Time : &quot; + (countEnd-countStart)+&quot; ms&quot;); List&lt;OperationLog&gt; list = operationLogMapper.selectListByCondition(dataMap); long queryEnd = System.currentTimeMillis(); System.out.println(&quot;Query Cost Time : &quot; + (queryEnd-countEnd)+&quot; ms&quot;); return new PageResult(count,list); } } 4.4.4 Controller @RestController @RequestMapping(&quot;/operationLog&quot;) public class OperationLogController { @Autowired private OperationLogService operationLogService; @RequestMapping(&quot;/findList&quot;) public PageResult findList(@RequestBody Map dataMap, Integer pageNum , Integer pageSize){ PageResult page = operationLogService.selectListByCondition(dataMap, pageNum, pageSize); return page; } } 4.5 日志查询前端代码实现 前端代码使用 BootStrap + AdminLTE 进行布局， 使用Vuejs 进行视图层展示。 4.5.1 js &lt;script&gt; var vm = new Vue({ el: '#app', data: { dataList:[], searchEntity:{ operateClass:'', operateMethod:'', returnClass:'', costTime:'' }, page: 1, //显示的是哪一页 pageSize: 10, //每一页显示的数据条数 total: 150, //记录总数 maxPage:8 //最大页数 }, methods: { pageHandler: function (page) { this.page = page; this.search(); }, search: function () { var _this = this; this.showLoading(); axios.post('/operationLog/findList.do?pageNum=' + _this.page + &quot;&amp;pageSize=&quot; + _this.pageSize, _this.searchEntity).then(function (response) { if (response) { _this.dataList = response.data.dataList; _this.total = response.data.total; _this.hideLoading(); } }) }, showLoading: function () { $('#loadingModal').modal({backdrop: 'static', keyboard: false}); }, hideLoading: function () { $('#loadingModal').modal('hide'); }, }, created:function(){ this.pageHandler(1); } }); &lt;/script&gt; 4.5.2 列表数据展示 &lt;tr v-for=&quot;item in dataList&quot;&gt; &lt;td&gt;&lt;input name=&quot;ids&quot; type=&quot;checkbox&quot;&gt;&lt;/td&gt; &lt;td&gt;{{item.id}}&lt;/td&gt; &lt;td&gt;{{item.operateClass}}&lt;/td&gt; &lt;td&gt;{{item.operateMethod}}&lt;/td&gt; &lt;td&gt;{{item.returnClass}}&lt;/td&gt; &lt;td&gt;{{item.returnValue}}&lt;/td&gt; &lt;td&gt;{{item.operateUser}}&lt;/td&gt; &lt;td&gt;{{item.operateTime}}&lt;/td&gt; &lt;td&gt;{{item.costTime}}&lt;/td&gt; &lt;td class=&quot;text-center&quot;&gt; &lt;button type=&quot;button&quot; class=&quot;btn bg-olive btn-xs&quot;&gt;详情&lt;/button&gt; &lt;button type=&quot;button&quot; class=&quot;btn bg-olive btn-xs&quot;&gt;删除&lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; 4.5.3 分页插件 &lt;div class=&quot;wrap&quot; id=&quot;wrap&quot;&gt; &lt;zpagenav v-bind:page=&quot;page&quot; v-bind:page-size=&quot;pageSize&quot; v-bind:total=&quot;total&quot; v-bind:max-page=&quot;maxPage&quot; v-on:pagehandler=&quot;pageHandler&quot;&gt; &lt;/zpagenav&gt; &lt;/div&gt; 4.6 联调测试 可以通过postman来访问业务系统，再查看数据库中的日志信息，验证能不能将用户的访问日志记录下来。 4.7 分析性能问题 系统中用户访问日志的数据量，随着时间的推移，这张表的数据量会越来越大，因此我们需要根据业务需求，来对日志查询模块的性能进行优化。 1） 分页查询优化 由于在进行日志查询时，是进行分页查询，那也就意味着，在查看时，至少需要查询两次： A. 查询符合条件的总记录数。--&gt; count 操作 B. 查询符合条件的列表数据。--&gt; 分页查询 limit 操作 通常来说，count() 都需要扫描大量的行（意味着需要访问大量的数据）才能获得精确的结果，因此是很难对该SQL进行优化操作的。如果需要对count进行优化，可以采用另外一种思路，可以增加汇总表，或者redis缓存来专门记录该表对应的记录数，这样的话，就可以很轻松的实现汇总数据的查询，而且效率很高，但是这种统计并不能保证百分之百的准确 。对于数据库的操作，“快速、精确、实现简单”，三者永远只能满足其二，必须舍掉其中一个。 2） 条件查询优化 针对于条件查询,需要对查询条件,及排序字段建立索引。 3） 读写分离 通过主从复制集群，来完成读写分离，使写操作走主节点， 而读操作，走从节点。 4） MySQL服务器优化 5） 应用优化 4.8 性能优化 - 分页 4.8.1 优化count 创建一张表用来记录日志表的总数据量： create table log_counter( logcount bigint not null )engine = innodb default CHARSET = utf8; 在每次插入数据之后，更新该表 ： &lt;update id=&quot;updateLogCounter&quot; &gt; update log_counter set logcount = logcount + 1 &lt;/update&gt; 在进行分页查询时, 获取总记录数，从该表中查询既可。 &lt;select id=&quot;countLogFromCounter&quot; resultType=&quot;long&quot;&gt; select logcount from log_counter limit 1&lt;/select&gt; 4.8.2 优化 limit 在进行分页时，一般通过创建覆盖索引，能够比较好的提高性能。一个非常常见，而又非常头疼的分页场景就是 &quot;limit 1000000,10&quot; ，此时MySQL需要搜索出前1000010 条记录后，仅仅需要返回第 1000001 到 1000010 条记录，前1000000 记录会被抛弃，查询代价非常大。 当点击比较靠后的页码时，就会出现这个问题，查询效率非常慢。 优化SQL： select * from operation_log limit 3000000 , 10; 将上述SQL优化为 : select * from operation_log t , (select id from operation_log order by id limit 3000000,10) b where t.id = b.id ; &lt;select id=&quot;selectListByCondition&quot; parameterType=&quot;map&quot; resultType=&quot;operationLog&quot;&gt; select id , operate_class as operateClass , operate_method as operateMethod, return_class as returnClass, operate_user as operateUser, operate_time as operateTime, param_and_value as paramAndValue, cost_time as costTime, return_value as returnValue from operation_log t, (select id from operation_log &lt;where&gt; &lt;include refid=&quot;oplog_where&quot;/&gt; &lt;/where&gt; order by id limit #{start},#{rows}) b where t.id = b.id &lt;/select&gt; 4.9 性能优化 - 索引 当根据操作人进行查询时， 查询的效率很低，耗时比较长。原因就是因为在创建数据库表结构时，并没有针对于 操作人 字段建立索引。 CREATE INDEX idx_user_method_return_cost ON operation_log(operate_user,operate_method,return_class,cost_time); 同上 ， 为了查询效率高，我们也需要对 操作方法、返回值类型、操作耗时 等字段进行创建索引，以提高查询效率。 CREATE INDEX idx_optlog_method_return_cost ON operation_log(operate_method,return_class,cost_time); CREATE INDEX idx_optlog_return_cost ON operation_log(return_class,cost_time); CREATE INDEX idx_optlog_cost ON operation_log(cost_time); 4.10 性能优化 - 排序 在查询数据时，如果业务需求中需要我们对结果内容进行了排序处理 , 这个时候,我们还需要对排序的字段建立适当的索引, 来提高排序的效率 。 4.11 性能优化 - 读写分离 4.11.1 概述 在Mysql主从复制的基础上，可以使用读写分离来降低单台Mysql节点的压力，从而来提高访问效率，读写分离的架构如下： 对于读写分离的实现，可以通过Spring AOP 来进行动态的切换数据源，进行操作 ： 4.11.2 实现方式 db.properties jdbc.write.driver=com.mysql.jdbc.Driver jdbc.write.url=jdbc:mysql://192.168.142.128:3306/mysql_demo jdbc.write.username=root jdbc.write.password=itcast jdbc.read.driver=com.mysql.jdbc.Driver jdbc.read.url=jdbc:mysql://192.168.142.129:3306/mysql_demo jdbc.read.username=root jdbc.read.password=itcast applicationContext-datasource.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 配置数据源 - Read --&gt; &lt;bean id=&quot;readDataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot; destroy-method=&quot;close&quot; lazy-init=&quot;true&quot;&gt; &lt;property name=&quot;driverClass&quot; value=&quot;${jdbc.read.driver}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;${jdbc.read.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;${jdbc.read.username}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${jdbc.read.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置数据源 - Write --&gt; &lt;bean id=&quot;writeDataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot; destroy-method=&quot;close&quot; lazy-init=&quot;true&quot;&gt; &lt;property name=&quot;driverClass&quot; value=&quot;${jdbc.write.driver}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;${jdbc.write.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;${jdbc.write.username}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${jdbc.write.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置动态分配的读写 数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;cn.itcast.aop.datasource.ChooseDataSource&quot; lazy-init=&quot;true&quot;&gt; &lt;property name=&quot;targetDataSources&quot;&gt; &lt;map key-type=&quot;java.lang.String&quot; value-type=&quot;javax.sql.DataSource&quot;&gt; &lt;entry key=&quot;write&quot; value-ref=&quot;writeDataSource&quot;/&gt; &lt;entry key=&quot;read&quot; value-ref=&quot;readDataSource&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=&quot;defaultTargetDataSource&quot; ref=&quot;writeDataSource&quot;/&gt; &lt;property name=&quot;methodType&quot;&gt; &lt;map key-type=&quot;java.lang.String&quot;&gt; &lt;entry key=&quot;read&quot; value=&quot;,get,select,count,list,query,find&quot;/&gt; &lt;entry key=&quot;write&quot; value=&quot;,add,create,update,delete,remove,insert&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; ChooseDataSource public class ChooseDataSource extends AbstractRoutingDataSource { public static Map&lt;String, List&lt;String&gt;&gt; METHOD_TYPE_MAP = new HashMap&lt;String, List&lt;String&gt;&gt;(); /** * 实现父类中的抽象方法，获取数据源名称 * @return */ protected Object determineCurrentLookupKey() { return DataSourceHandler.getDataSource(); } // 设置方法名前缀对应的数据源 public void setMethodType(Map&lt;String, String&gt; map) { for (String key : map.keySet()) { List&lt;String&gt; v = new ArrayList&lt;String&gt;(); String[] types = map.get(key).split(&quot;,&quot;); for (String type : types) { if (!StringUtils.isEmpty(type)) { v.add(type); } } METHOD_TYPE_MAP.put(key, v); } System.out.println(&quot;METHOD_TYPE_MAP : &quot;+METHOD_TYPE_MAP); } } DataSourceHandler public class DataSourceHandler { // 数据源名称 public static final ThreadLocal&lt;String&gt; holder = new ThreadLocal&lt;String&gt;(); /** * 在项目启动的时候将配置的读、写数据源加到holder中 */ public static void putDataSource(String datasource) { holder.set(datasource); } /** * 从holer中获取数据源字符串 */ public static String getDataSource() { return holder.get(); } } DataSourceAspect @Aspect @Component @Order(-9999) @EnableAspectJAutoProxy(proxyTargetClass = true) public class DataSourceAspect { protected Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 配置前置通知,使用在方法aspect()上注册的切入点 */ @Before(&quot;execution(* cn.itcast.service.*.*(..))&quot;) @Order(-9999) public void before(JoinPoint point) { String className = point.getTarget().getClass().getName(); String method = point.getSignature().getName(); logger.info(className + &quot;.&quot; + method + &quot;(&quot; + Arrays.asList(point.getArgs())+ &quot;)&quot;); try { for (String key : ChooseDataSource.METHOD_TYPE_MAP.keySet()) { for (String type : ChooseDataSource.METHOD_TYPE_MAP.get(key)) { if (method.startsWith(type)) { System.out.println(&quot;key : &quot; + key); DataSourceHandler.putDataSource(key); break; } } } } catch (Exception e) { e.printStackTrace(); } } } 通过 @Order(-9999) 注解来控制事务管理器, 与该通知类的加载顺序 , 需要让通知类 , 先加载 , 来判定使用哪个数据源 . 4.11.3 验证 在主库和从库中，执行如下SQL语句，来查看是否读的时候， 从从库中读取 ； 写入操作的时候，是否写入到主库。 show status like 'Innodb_rows_%' ; 4.11.4 原理 4.12 性能优化 - 应用优化 4.12.1 缓存 可以在业务系统中使用redis来做缓存，缓存一些基础性的数据，来降低关系型数据库的压力，提高访问效率。 4.12.2 全文检索 如果业务系统中的数据量比较大（达到千万级别），这个时候，如果再对数据库进行查询，特别是进行分页查询，速度将变得很慢（因为在分页时首先需要count求合计数），为了提高访问效率，这个时候，可以考虑加入Solr 或者 ElasticSearch全文检索服务，来提高访问效率。 4.13.3 非关系数据库 也可以考虑将非核心（重要）数据，存在 MongoDB 中，这样可以提高插入以及查询的效率。 ","link":"https://memorykki.github.io/Mysql-hm/"},{"title":"Redis","content":"Redis（Remote Dictionary Server )，即远程字典服务，是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库。 一、Nosql概述 为什么使用Nosql 1、单机Mysql时代 90年代,一个网站的访问量一般不会太大，单个数据库完全够用。随着用户增多，网站出现以下问题 数据量增加到一定程度，单机数据库就放不下了 数据的索引（B+ Tree）,一个机器内存也存放不下 访问量变大后（读写混合），一台服务器承受不住。 2、Memcached(缓存) + Mysql + 垂直拆分（读写分离） 网站80%的情况都是在读，每次都要去查询数据库的话就十分的麻烦！所以说我们希望减轻数据库的压力，我们可以使用缓存来保证效率！ 优化过程经历了以下几个过程： 优化数据库的数据结构和索引(难度大) 文件缓存，通过IO流获取比每次都访问数据库效率略高，但是流量爆炸式增长时候，IO流也承受不了 MemCache,当时最热门的技术，通过在数据库和数据库访问层之间加上一层缓存，第一次访问时查询数据库，将结果保存到缓存，后续的查询先检查缓存，若有直接拿去使用，效率显著提升。 3、分库分表 + 水平拆分 + Mysql集群 4、如今最近的年代 如今信息量井喷式增长，各种各样的数据出现（用户定位数据，图片数据等），大数据的背景下关系型数据库（RDBMS）无法满足大量数据要求。Nosql数据库就能轻松解决这些问题。 目前一个基本的互联网项目 为什么要用NoSQL ？ 用户的个人信息，社交网络，地理位置。用户自己产生的数据，用户日志等等爆发式增长！ 这时候我们就需要使用NoSQL数据库的，Nosql可以很好的处理以上的情况！ 什么是Nosql NoSQL = Not Only SQL（不仅仅是SQL） Not Only Structured Query Language 关系型数据库：列+行，同一个表下数据的结构是一样的。 非关系型数据库：数据存储没有固定的格式，并且可以进行横向扩展。 NoSQL泛指非关系型数据库，随着web2.0互联网的诞生，传统的关系型数据库很难对付web2.0时代！尤其是超大规模的高并发的社区，暴露出来很多难以克服的问题，NoSQL在当今大数据环境下发展的十分迅速，Redis是发展最快的。 Nosql特点 方便扩展（数据之间没有关系，很好扩展！） 大数据量高性能（Redis一秒可以写8万次，读11万次，NoSQL的缓存记录级，是一种细粒度的缓存，性能会比较高！） 数据类型是多样型的！（不需要事先设计数据库，随取随用） 传统的 RDBMS 和 NoSQL 传统的 RDBMS(关系型数据库) - 结构化组织 - SQL - 数据和关系都存在单独的表中 row col - 操作，数据定义语言 - 严格的一致性 - 基础的事务 - ... Nosql - 不仅仅是数据 - 没有固定的查询语言 - 键值对存储，列存储，文档存储，图形数据库（社交关系） - 最终一致性 - CAP定理和BASE - 高性能，高可用，高扩展 - ... 了解：3V + 3高 大数据时代的3V ：主要是描述问题的 海量Velume 多样Variety 实时Velocity 大数据时代的3高 ： 主要是对程序的要求 高并发 高可扩 高性能 真正在公司中的实践：NoSQL + RDBMS 一起使用才是最强的。 阿里巴巴演进分析 推荐阅读：阿里云的这群疯子https://yq.aliyun.com/articles/653511 # 商品信息 - 一般存放在关系型数据库：Mysql,阿里巴巴使用的Mysql都是经过内部改动的。 # 商品描述、评论(文字居多) - 文档型数据库：MongoDB # 图片 - 分布式文件系统 FastDFS - 淘宝：TFS - Google: GFS - Hadoop: HDFS - 阿里云: oss # 商品关键字 用于搜索 - 搜索引擎：solr,elasticsearch - 阿里：Isearch 多隆 # 商品热门的波段信息 - 内存数据库：Redis，Memcache # 商品交易，外部支付接口 - 第三方应用 Nosql的四大分类 KV键值对 新浪：Redis 美团：Redis + Tair 阿里、百度：Redis + Memcache 文档型数据库（bson数据格式）： MongoDB(掌握) 基于分布式文件存储的数据库。C++编写，用于处理大量文档。 MongoDB是RDBMS和NoSQL的中间产品。MongoDB是非关系型数据库中功能最丰富的，NoSQL中最像关系型数据库的数据库。 ConthDB 列存储数据库 HBase(大数据必学) 分布式文件系统 图关系数据库 用于广告推荐，社交网络 Neo4j、InfoGrid 分类 Examples举例 典型应用场景 数据模型 优点 缺点 键值对（key-value） Tokyo Cabinet/Tyrant, Redis, Voldemort, Oracle BDB 内容缓存，主要用于处理大量数据的高访问负载，也用于一些日志系统等等。 Key 指向 Value 的键值对，通常用hash table来实现 查找速度快 数据无结构化，通常只被当作字符串或者二进制数据 列存储数据库 Cassandra, HBase, Riak 分布式的文件系统 以列簇式存储，将同一列数据存在一起 查找速度快，可扩展性强，更容易进行分布式扩展 功能相对局限 文档型数据库 CouchDB, MongoDb Web应用（与Key-Value类似，Value是结构化的，不同的是数据库能够了解Value的内容） Key-Value对应的键值对，Value为结构化数据 数据结构要求不严格，表结构可变，不需要像关系型数据库一样需要预先定义表结构 查询性能不高，而且缺乏统一的查询语法。 图形(Graph)数据库 Neo4J, InfoGrid, Infinite Graph 社交网络，推荐系统等。专注于构建关系图谱 图结构 利用图结构相关算法。比如最短路径寻址，N度关系查找等 很多时候需要对整个图做计算才能得出需要的信息，而且这种结构不太好做分布式的集群 二、Redis入门 概述 Redis是什么？ Redis（Remote Dictionary Server )，即远程字典服务。 是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 Redis能该干什么？ 内存存储、持久化，内存是断电即失的，所以需要持久化（RDB、AOF） 高效率、用于高速缓冲 发布订阅系统 地图信息分析 计时器、计数器(eg：浏览量) 。。。 特性 多样的数据类型 持久化 集群 事务 … 环境搭建 官网：https://redis.io/ 推荐使用Linux服务器学习。 windows版本的Redis已经停更很久了… Windows安装 https://github.com/dmajkic/redis 解压安装包 开启redis-server.exe 启动redis-cli.exe测试 Linux安装 下载安装包！redis-5.0.8.tar.gz 解压Redis的安装包！程序一般放在 /opt 目录下 基本环境安装 yum install gcc-c++ # 然后进入redis目录下执行 make # 然后执行 make install redis默认安装路径 /usr/local/bin 将redis的配置文件复制到 程序安装目录 /usr/local/bin/kconfig下 redis默认不是后台启动的，需要修改配置文件！ 通过制定的配置文件启动redis服务 使用redis-cli连接指定的端口号测试，Redis的默认端口6379 查看redis进程是否开启 关闭Redis服务 shutdown 再次查看进程是否存在 后面我们会使用单机多Redis启动集群测试 测试性能 **redis-benchmark：**Redis官方提供的性能测试工具，参数选项如下： 简单测试： # 测试：100个并发连接 100000请求 redis-benchmark -h localhost -p 6379 -c 100 -n 100000 12 基础知识 redis默认有16个数据库 默认使用的第0个; 16个数据库为：DB 0~DB 15 默认使用DB 0 ，可以使用select n切换到DB n，dbsize可以查看当前数据库的大小，与key数量相关。 127.0.0.1:6379&gt; config get databases # 命令行查看数据库数量databases 1) &quot;databases&quot; 2) &quot;16&quot; 127.0.0.1:6379&gt; select 8 # 切换数据库 DB 8 OK 127.0.0.1:6379[8]&gt; dbsize # 查看数据库大小 (integer) 0 # 不同数据库之间 数据是不能互通的，并且dbsize 是根据库中key的个数。 127.0.0.1:6379&gt; set name sakura OK 127.0.0.1:6379&gt; SELECT 8 OK 127.0.0.1:6379[8]&gt; get name # db8中并不能获取db0中的键值对。 (nil) 127.0.0.1:6379[8]&gt; DBSIZE (integer) 0 127.0.0.1:6379[8]&gt; SELECT 0 OK 127.0.0.1:6379&gt; keys * 1) &quot;counter:__rand_int__&quot; 2) &quot;mylist&quot; 3) &quot;name&quot; 4) &quot;key:__rand_int__&quot; 5) &quot;myset:__rand_int__&quot; 127.0.0.1:6379&gt; DBSIZE # size和key个数相关 (integer) 5 keys * ：查看当前数据库中所有的key。 flushdb：清空当前数据库中的键值对。 flushall：清空所有数据库的键值对。 Redis是单线程的，Redis是基于内存操作的。 所以Redis的性能瓶颈不是CPU,而是机器内存和网络带宽。 那么为什么Redis的速度如此快呢，性能这么高呢？QPS达到10W+。 Redis 选择使用单线程模型处理客户端的请求主要还是因为 CPU 不是 Redis 服务器的瓶颈，所以使用多线程模型带来的性能提升并不能抵消它带来的开发成本和维护成本，系统的性能瓶颈也主要在网络 I/O 操作上；而 Redis 6.0引入多线程操作也是出于性能上的考虑，对于一些大键值对的删除操作，通过多线程非阻塞地释放内存空间也能减少对 Redis 主线程阻塞的时间，提高执行的效率。 Redis为什么单线程还这么快？ 误区1：高性能的服务器一定是多线程的？ 误区2：多线程（CPU上下文会切换！）一定比单线程效率高！ 核心：Redis是将所有的数据放在内存中的，所以说使用单线程去操作效率就是最高的，多线程（CPU上下文会切换：耗时的操作！），对于内存系统来说，如果没有上下文切换效率就是最高的，多次读写都是在一个CPU上的，在内存存储数据情况下，单线程就是最佳的方案。 Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。 三、五大数据类型 Redis是一个开源（BSD许可），内存存储的数据结构服务器，可用作数据库，高速缓存和消息队列代理。它支持字符串、哈希表、列表、集合、有序集合，位图，hyperloglogs等数据类型。内置复制、Lua脚本、LRU收回、事务以及不同级别磁盘持久化功能，同时通过Redis Sentinel提供高可用，通过Redis Cluster提供自动分区。 Redis-key 在redis中无论什么数据类型，在数据库中都是以key-value形式保存，通过进行对Redis-key的操作，来完成对数据库中数据的操作。 下面学习的命令： exists key：判断键是否存在 del key：删除键值对 move key db：将键值对移动到指定数据库 expire key second：设置键值对的过期时间 type key：查看value的数据类型 127.0.0.1:6379&gt; keys * # 查看当前数据库所有key (empty list or set) 127.0.0.1:6379&gt; set name qinjiang # set key OK 127.0.0.1:6379&gt; set age 20 OK 127.0.0.1:6379&gt; keys * 1) &quot;age&quot; 2) &quot;name&quot; 127.0.0.1:6379&gt; move age 1 # 将键值对移动到指定数据库 (integer) 1 127.0.0.1:6379&gt; EXISTS age # 判断键是否存在 (integer) 0 # 不存在 127.0.0.1:6379&gt; EXISTS name (integer) 1 # 存在 127.0.0.1:6379&gt; SELECT 1 OK 127.0.0.1:6379[1]&gt; keys * 1) &quot;age&quot; 127.0.0.1:6379[1]&gt; del age # 删除键值对 (integer) 1 # 删除个数 127.0.0.1:6379&gt; set age 20 OK 127.0.0.1:6379&gt; EXPIRE age 15 # 设置键值对的过期时间 (integer) 1 # 设置成功 开始计数 127.0.0.1:6379&gt; ttl age # 查看key的过期剩余时间 (integer) 13 127.0.0.1:6379&gt; ttl age (integer) 11 127.0.0.1:6379&gt; ttl age (integer) 9 127.0.0.1:6379&gt; ttl age (integer) -2 # -2 表示key过期，-1表示key未设置过期时间 127.0.0.1:6379&gt; get age # 过期的key 会被自动delete (nil) 127.0.0.1:6379&gt; keys * 1) &quot;name&quot; 127.0.0.1:6379&gt; type name # 查看value的数据类型 string 关于TTL命令 Redis的key，通过TTL命令返回key的过期时间，一般来说有3种： 当前key没有设置过期时间，所以会返回-1. 当前key有设置过期时间，而且key已经过期，所以会返回-2. 当前key有设置过期时间，且key还没有过期，故会返回key的正常剩余时间. 关于重命名RENAME和RENAMENX RENAME key newkey修改 key 的名称 RENAMENX key newkey仅当 newkey 不存在时，将 key 改名为 newkey 。 更多命令学习：https://www.redis.net.cn/order/ [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-wBVZtGVm-1597890996517)(狂神说 Redis.assets/image-20200813114228439.png)] String(字符串) 普通的set、get直接略过。 命令 描述 示例 APPEND key value 向指定的key的value后追加字符串 127.0.0.1:6379&gt; set msg hello OK 127.0.0.1:6379&gt; append msg &quot; world&quot; (integer) 11 127.0.0.1:6379&gt; get msg “hello world” DECR/INCR key 将指定key的value数值进行+1/-1(仅对于数字) 127.0.0.1:6379&gt; set age 20 OK 127.0.0.1:6379&gt; incr age (integer) 21 127.0.0.1:6379&gt; decr age (integer) 20 INCRBY/DECRBY key n 按指定的步长对数值进行加减 127.0.0.1:6379&gt; INCRBY age 5 (integer) 25 127.0.0.1:6379&gt; DECRBY age 10 (integer) 15 INCRBYFLOAT key n 为数值加上浮点型数值 127.0.0.1:6379&gt; INCRBYFLOAT age 5.2 “20.2” STRLEN key 获取key保存值的字符串长度 127.0.0.1:6379&gt; get msg “hello world” 127.0.0.1:6379&gt; STRLEN msg (integer) 11 GETRANGE key start end 按起止位置获取字符串（闭区间，起止位置都取） 127.0.0.1:6379&gt; get msg “hello world” 127.0.0.1:6379&gt; GETRANGE msg 3 9 “lo worl” SETRANGE key offset value 用指定的value 替换key中 offset开始的值 127.0.0.1:6379&gt; SETRANGE msg 2 hello (integer) 7 127.0.0.1:6379&gt; get msg “tehello” GETSET key value 将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 127.0.0.1:6379&gt; GETSET msg test “hello world” SETNX key value 仅当key不存在时进行set 127.0.0.1:6379&gt; SETNX msg test (integer) 0 127.0.0.1:6379&gt; SETNX name sakura (integer) 1 SETEX key seconds value set 键值对并设置过期时间 127.0.0.1:6379&gt; setex name 10 root OK 127.0.0.1:6379&gt; get name (nil) MSET key1 value1 [key2 value2..] 批量set键值对 127.0.0.1:6379&gt; MSET k1 v1 k2 v2 k3 v3 OK MSETNX key1 value1 [key2 value2..] 原子性，批量设置键值对，仅当参数中所有的key都不存在时执行 127.0.0.1:6379&gt; MSETNX k1 v1 k4 v4 (integer) 0 MGET key1 [key2..] 批量获取多个key保存的值 127.0.0.1:6379&gt; MGET k1 k2 k3 1) “v1” 2) “v2” 3) “v3” PSETEX key milliseconds value 和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间， getset key value 如果不存在值，则返回nil，如果存在值，返回原来的值，并设置新的值,cas String类似的使用场景：value除了是字符串还可以是数字，用途举例： 计数器 统计多单位的数量：uid:123666：follow 0 粉丝数 对象存储缓存 setnx 分布式锁 key -&gt; user:{id}:{field} List(列表) Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边） 一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)。 首先我们列表，可以经过规则定义将其变为队列、栈、双端队列等 正如图Redis中List是可以进行双端操作的，所以命令也就分为了LXXX和RLLL两类，有时候L也表示List例如LLEN 命令 描述 LPUSH/RPUSH key value1[value2..] 从左边/右边向列表中PUSH值(一个或者多个)。 LRANGE key start end 获取list 起止元素==（索引从左往右 递增）== LPUSHX/RPUSHX key value 向已存在的列名中push值（一个或者多个） `LINSERT key BEFORE AFTER pivot value` LLEN key 查看列表长度 LINDEX key index 通过索引获取列表元素 LSET key index value 通过索引为元素设值 LPOP/RPOP key 从最左边/最右边移除值 并返回 RPOPLPUSH source destination 将列表的尾部(右)最后一个值弹出，并返回，然后加到另一个列表的头部 LTRIM key start end 通过下标截取指定范围内的列表 LREM key count value List中是允许value重复的 count &gt; 0：从头部开始搜索 然后删除指定的value 至多删除count个 count &lt; 0：从尾部开始搜索… count = 0：删除列表中所有的指定value。 BLPOP/BRPOP key1[key2] timout 移出并获取列表的第一个/最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 BRPOPLPUSH source destination timeout 和RPOPLPUSH功能相同，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 ---------------------------LPUSH---RPUSH---LRANGE-------------------------------- 127.0.0.1:6379&gt; LPUSH mylist k1 # LPUSH mylist=&gt;{1} (integer) 1 127.0.0.1:6379&gt; LPUSH mylist k2 # LPUSH mylist=&gt;{2,1} (integer) 2 127.0.0.1:6379&gt; RPUSH mylist k3 # RPUSH mylist=&gt;{2,1,3} (integer) 3 127.0.0.1:6379&gt; get mylist # 普通的get是无法获取list值的 (error) WRONGTYPE Operation against a key holding the wrong kind of value 127.0.0.1:6379&gt; LRANGE mylist 0 4 # LRANGE 获取起止位置范围内的元素 1) &quot;k2&quot; 2) &quot;k1&quot; 3) &quot;k3&quot; 127.0.0.1:6379&gt; LRANGE mylist 0 2 1) &quot;k2&quot; 2) &quot;k1&quot; 3) &quot;k3&quot; 127.0.0.1:6379&gt; LRANGE mylist 0 1 1) &quot;k2&quot; 2) &quot;k1&quot; 127.0.0.1:6379&gt; LRANGE mylist 0 -1 # 获取全部元素 1) &quot;k2&quot; 2) &quot;k1&quot; 3) &quot;k3&quot; ---------------------------LPUSHX---RPUSHX----------------------------------- 127.0.0.1:6379&gt; LPUSHX list v1 # list不存在 LPUSHX失败 (integer) 0 127.0.0.1:6379&gt; LPUSHX list v1 v2 (integer) 0 127.0.0.1:6379&gt; LPUSHX mylist k4 k5 # 向mylist中 左边 PUSH k4 k5 (integer) 5 127.0.0.1:6379&gt; LRANGE mylist 0 -1 1) &quot;k5&quot; 2) &quot;k4&quot; 3) &quot;k2&quot; 4) &quot;k1&quot; 5) &quot;k3&quot; ---------------------------LINSERT--LLEN--LINDEX--LSET---------------------------- 127.0.0.1:6379&gt; LINSERT mylist after k2 ins_key1 # 在k2元素后 插入ins_key1 (integer) 6 127.0.0.1:6379&gt; LRANGE mylist 0 -1 1) &quot;k5&quot; 2) &quot;k4&quot; 3) &quot;k2&quot; 4) &quot;ins_key1&quot; 5) &quot;k1&quot; 6) &quot;k3&quot; 127.0.0.1:6379&gt; LLEN mylist # 查看mylist的长度 (integer) 6 127.0.0.1:6379&gt; LINDEX mylist 3 # 获取下标为3的元素 &quot;ins_key1&quot; 127.0.0.1:6379&gt; LINDEX mylist 0 &quot;k5&quot; 127.0.0.1:6379&gt; LSET mylist 3 k6 # 将下标3的元素 set值为k6 OK 127.0.0.1:6379&gt; LRANGE mylist 0 -1 1) &quot;k5&quot; 2) &quot;k4&quot; 3) &quot;k2&quot; 4) &quot;k6&quot; 5) &quot;k1&quot; 6) &quot;k3&quot; ---------------------------LPOP--RPOP-------------------------- 127.0.0.1:6379&gt; LPOP mylist # 左侧(头部)弹出 &quot;k5&quot; 127.0.0.1:6379&gt; RPOP mylist # 右侧(尾部)弹出 &quot;k3&quot; ---------------------------RPOPLPUSH-------------------------- 127.0.0.1:6379&gt; LRANGE mylist 0 -1 1) &quot;k4&quot; 2) &quot;k2&quot; 3) &quot;k6&quot; 4) &quot;k1&quot; 127.0.0.1:6379&gt; RPOPLPUSH mylist newlist # 将mylist的最后一个值(k1)弹出，加入到newlist的头部 &quot;k1&quot; 127.0.0.1:6379&gt; LRANGE newlist 0 -1 1) &quot;k1&quot; 127.0.0.1:6379&gt; LRANGE mylist 0 -1 1) &quot;k4&quot; 2) &quot;k2&quot; 3) &quot;k6&quot; ---------------------------LTRIM-------------------------- 127.0.0.1:6379&gt; LTRIM mylist 0 1 # 截取mylist中的 0~1部分 OK 127.0.0.1:6379&gt; LRANGE mylist 0 -1 1) &quot;k4&quot; 2) &quot;k2&quot; # 初始 mylist: k2,k2,k2,k2,k2,k2,k4,k2,k2,k2,k2 ---------------------------LREM-------------------------- 127.0.0.1:6379&gt; LREM mylist 3 k2 # 从头部开始搜索 至多删除3个 k2 (integer) 3 # 删除后：mylist: k2,k2,k2,k4,k2,k2,k2,k2 127.0.0.1:6379&gt; LREM mylist -2 k2 #从尾部开始搜索 至多删除2个 k2 (integer) 2 # 删除后：mylist: k2,k2,k2,k4,k2,k2 ---------------------------BLPOP--BRPOP-------------------------- mylist: k2,k2,k2,k4,k2,k2 newlist: k1 127.0.0.1:6379&gt; BLPOP newlist mylist 30 # 从newlist中弹出第一个值，mylist作为候选 1) &quot;newlist&quot; # 弹出 2) &quot;k1&quot; 127.0.0.1:6379&gt; BLPOP newlist mylist 30 1) &quot;mylist&quot; # 由于newlist空了 从mylist中弹出 2) &quot;k2&quot; 127.0.0.1:6379&gt; BLPOP newlist 30 (30.10s) # 超时了 127.0.0.1:6379&gt; BLPOP newlist 30 # 我们连接另一个客户端向newlist中push了test, 阻塞被解决。 1) &quot;newlist&quot; 2) &quot;test&quot; (12.54s) 小结 list实际上是一个链表，before Node after , left, right 都可以插入值 如果key不存在，则创建新的链表 如果key存在，新增内容 如果移除了所有值，空链表，也代表不存在 在两边插入或者改动值，效率最高！修改中间元素，效率相对较低 应用： 消息排队！消息队列（Lpush Rpop）,栈（Lpush Lpop） Set(集合) Redis的Set是string类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 Redis 中 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员)。 命令 描述 SADD key member1[member2..] 向集合中无序增加一个/多个成员 SCARD key 获取集合的成员数 SMEMBERS key 返回集合中所有的成员 SISMEMBER key member 查询member元素是否是集合的成员,结果是无序的 SRANDMEMBER key [count] 随机返回集合中count个成员，count缺省值为1 SPOP key [count] 随机移除并返回集合中count个成员，count缺省值为1 SMOVE source destination member 将source集合的成员member移动到destination集合 SREM key member1[member2..] 移除集合中一个/多个成员 SDIFF key1[key2..] 返回所有集合的差集 key1- key2 - … SDIFFSTORE destination key1[key2..] 在SDIFF的基础上，将结果保存到集合中==(覆盖)==。不能保存到其他类型key噢！ SINTER key1 [key2..] 返回所有集合的交集 SINTERSTORE destination key1[key2..] 在SINTER的基础上，存储结果到集合中。覆盖 SUNION key1 [key2..] 返回所有集合的并集 SUNIONSTORE destination key1 [key2..] 在SUNION的基础上，存储结果到及和张。覆盖 SSCAN KEY [MATCH pattern] [COUNT count] 在大量数据环境下，使用此命令遍历集合中元素，每次遍历部分 ---------------SADD--SCARD--SMEMBERS--SISMEMBER-------------------- 127.0.0.1:6379&gt; SADD myset m1 m2 m3 m4 # 向myset中增加成员 m1~m4 (integer) 4 127.0.0.1:6379&gt; SCARD myset # 获取集合的成员数目 (integer) 4 127.0.0.1:6379&gt; smembers myset # 获取集合中所有成员 1) &quot;m4&quot; 2) &quot;m3&quot; 3) &quot;m2&quot; 4) &quot;m1&quot; 127.0.0.1:6379&gt; SISMEMBER myset m5 # 查询m5是否是myset的成员 (integer) 0 # 不是，返回0 127.0.0.1:6379&gt; SISMEMBER myset m2 (integer) 1 # 是，返回1 127.0.0.1:6379&gt; SISMEMBER myset m3 (integer) 1 ---------------------SRANDMEMBER--SPOP---------------------------------- 127.0.0.1:6379&gt; SRANDMEMBER myset 3 # 随机返回3个成员 1) &quot;m2&quot; 2) &quot;m3&quot; 3) &quot;m4&quot; 127.0.0.1:6379&gt; SRANDMEMBER myset # 随机返回1个成员 &quot;m3&quot; 127.0.0.1:6379&gt; SPOP myset 2 # 随机移除并返回2个成员 1) &quot;m1&quot; 2) &quot;m4&quot; # 将set还原到{m1,m2,m3,m4} ---------------------SMOVE--SREM---------------------------------------- 127.0.0.1:6379&gt; SMOVE myset newset m3 # 将myset中m3成员移动到newset集合 (integer) 1 127.0.0.1:6379&gt; SMEMBERS myset 1) &quot;m4&quot; 2) &quot;m2&quot; 3) &quot;m1&quot; 127.0.0.1:6379&gt; SMEMBERS newset 1) &quot;m3&quot; 127.0.0.1:6379&gt; SREM newset m3 # 从newset中移除m3元素 (integer) 1 127.0.0.1:6379&gt; SMEMBERS newset (empty list or set) # 下面开始是多集合操作,多集合操作中若只有一个参数默认和自身进行运算 # setx=&gt;{m1,m2,m4,m6}, sety=&gt;{m2,m5,m6}, setz=&gt;{m1,m3,m6} -----------------------------SDIFF------------------------------------ 127.0.0.1:6379&gt; SDIFF setx sety setz # 等价于setx-sety-setz 1) &quot;m4&quot; 127.0.0.1:6379&gt; SDIFF setx sety # setx - sety 1) &quot;m4&quot; 2) &quot;m1&quot; 127.0.0.1:6379&gt; SDIFF sety setx # sety - setx 1) &quot;m5&quot; -------------------------SINTER--------------------------------------- # 共同关注（交集） 127.0.0.1:6379&gt; SINTER setx sety setz # 求 setx、sety、setx的交集 1) &quot;m6&quot; 127.0.0.1:6379&gt; SINTER setx sety # 求setx sety的交集 1) &quot;m2&quot; 2) &quot;m6&quot; -------------------------SUNION--------------------------------------- 127.0.0.1:6379&gt; SUNION setx sety setz # setx sety setz的并集 1) &quot;m4&quot; 2) &quot;m6&quot; 3) &quot;m3&quot; 4) &quot;m2&quot; 5) &quot;m1&quot; 6) &quot;m5&quot; 127.0.0.1:6379&gt; SUNION setx sety # setx sety 并集 1) &quot;m4&quot; 2) &quot;m6&quot; 3) &quot;m2&quot; 4) &quot;m1&quot; 5) &quot;m5&quot; Hash（哈希） Redis hash 是一个string类型的field和value的映射表，hash特别适合用于存储对象。 Set就是一种简化的Hash,只变动key,而value使用默认值填充。可以将一个Hash表作为一个对象进行存储，表中存放对象的信息。 命令 描述 HSET key field value 将哈希表 key 中的字段 field 的值设为 value 。重复设置同一个field会覆盖,返回0 HMSET key field1 value1 [field2 value2..] 同时将多个 field-value (域-值)对设置到哈希表 key 中。 HSETNX key field value 只有在字段 field 不存在时，设置哈希表字段的值。 HEXISTS key field 查看哈希表 key 中，指定的字段是否存在。 HGET key field value 获取存储在哈希表中指定字段的值 HMGET key field1 [field2..] 获取所有给定字段的值 HGETALL key 获取在哈希表key 的所有字段和值 HKEYS key 获取哈希表key中所有的字段 HLEN key 获取哈希表中字段的数量 HVALS key 获取哈希表中所有值 HDEL key field1 [field2..] 删除哈希表key中一个/多个field字段 HINCRBY key field n 为哈希表 key 中的指定字段的整数值加上增量n，并返回增量后结果 一样只适用于整数型字段 HINCRBYFLOAT key field n 为哈希表 key 中的指定字段的浮点数值加上增量 n。 HSCAN key cursor [MATCH pattern] [COUNT count] 迭代哈希表中的键值对。 ------------------------HSET--HMSET--HSETNX---------------- 127.0.0.1:6379&gt; HSET studentx name sakura # 将studentx哈希表作为一个对象，设置name为sakura (integer) 1 127.0.0.1:6379&gt; HSET studentx name gyc # 重复设置field进行覆盖，并返回0 (integer) 0 127.0.0.1:6379&gt; HSET studentx age 20 # 设置studentx的age为20 (integer) 1 127.0.0.1:6379&gt; HMSET studentx sex 1 tel 15623667886 # 设置sex为1，tel为15623667886 OK 127.0.0.1:6379&gt; HSETNX studentx name gyc # HSETNX 设置已存在的field (integer) 0 # 失败 127.0.0.1:6379&gt; HSETNX studentx email 12345@qq.com (integer) 1 # 成功 ----------------------HEXISTS-------------------------------- 127.0.0.1:6379&gt; HEXISTS studentx name # name字段在studentx中是否存在 (integer) 1 # 存在 127.0.0.1:6379&gt; HEXISTS studentx addr (integer) 0 # 不存在 -------------------HGET--HMGET--HGETALL----------- 127.0.0.1:6379&gt; HGET studentx name # 获取studentx中name字段的value &quot;gyc&quot; 127.0.0.1:6379&gt; HMGET studentx name age tel # 获取studentx中name、age、tel字段的value 1) &quot;gyc&quot; 2) &quot;20&quot; 3) &quot;15623667886&quot; 127.0.0.1:6379&gt; HGETALL studentx # 获取studentx中所有的field及其value 1) &quot;name&quot; 2) &quot;gyc&quot; 3) &quot;age&quot; 4) &quot;20&quot; 5) &quot;sex&quot; 6) &quot;1&quot; 7) &quot;tel&quot; 8) &quot;15623667886&quot; 9) &quot;email&quot; 10) &quot;12345@qq.com&quot; --------------------HKEYS--HLEN--HVALS-------------- 127.0.0.1:6379&gt; HKEYS studentx # 查看studentx中所有的field 1) &quot;name&quot; 2) &quot;age&quot; 3) &quot;sex&quot; 4) &quot;tel&quot; 5) &quot;email&quot; 127.0.0.1:6379&gt; HLEN studentx # 查看studentx中的字段数量 (integer) 5 127.0.0.1:6379&gt; HVALS studentx # 查看studentx中所有的value 1) &quot;gyc&quot; 2) &quot;20&quot; 3) &quot;1&quot; 4) &quot;15623667886&quot; 5) &quot;12345@qq.com&quot; -------------------------HDEL-------------------------- 127.0.0.1:6379&gt; HDEL studentx sex tel # 删除studentx 中的sex、tel字段 (integer) 2 127.0.0.1:6379&gt; HKEYS studentx 1) &quot;name&quot; 2) &quot;age&quot; 3) &quot;email&quot; -------------HINCRBY--HINCRBYFLOAT------------------------ 127.0.0.1:6379&gt; HINCRBY studentx age 1 # studentx的age字段数值+1 (integer) 21 127.0.0.1:6379&gt; HINCRBY studentx name 1 # 非整数字型字段不可用 (error) ERR hash value is not an integer 127.0.0.1:6379&gt; HINCRBYFLOAT studentx weight 0.6 # weight字段增加0.6 &quot;90.8&quot; Hash变更的数据user name age，尤其是用户信息之类的，经常变动的信息！Hash更适合于对象的存储，Sring更加适合字符串存储！ Zset（有序集合） 不同的是每个元素都会关联一个double类型的分数（score）。redis正是通过分数来为集合中的成员进行从小到大的排序。 score相同：按字典顺序排序 有序集合的成员是唯一的,但分数(score)却可以重复。 命令 描述 ZADD key score member1 [score2 member2] 向有序集合添加一个或多个成员，或者更新已存在成员的分数 ZCARD key 获取有序集合的成员数 ZCOUNT key min max 计算在有序集合中指定区间score的成员数 ZINCRBY key n member 有序集合中对指定成员的分数加上增量 n ZSCORE key member 返回有序集中，成员的分数值 ZRANK key member 返回有序集合中指定成员的索引 ZRANGE key start end 通过索引区间返回有序集合成指定区间内的成员 ZRANGEBYLEX key min max 通过字典区间返回有序集合的成员 ZRANGEBYSCORE key min max 通过分数返回有序集合指定区间内的成员==-inf 和 +inf分别表示最小最大值，只支持开区间()== ZLEXCOUNT key min max 在有序集合中计算指定字典区间内成员数量 ZREM key member1 [member2..] 移除有序集合中一个/多个成员 ZREMRANGEBYLEX key min max 移除有序集合中给定的字典区间的所有成员 ZREMRANGEBYRANK key start stop 移除有序集合中给定的排名区间的所有成员 ZREMRANGEBYSCORE key min max 移除有序集合中给定的分数区间的所有成员 ZREVRANGE key start end 返回有序集中指定区间内的成员，通过索引，分数从高到底 ZREVRANGEBYSCORRE key max min 返回有序集中指定分数区间内的成员，分数从高到低排序 ZREVRANGEBYLEX key max min 返回有序集中指定字典区间内的成员，按字典顺序倒序 ZREVRANK key member 返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 ZINTERSTORE destination numkeys key1 [key2 ..] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中，numkeys：表示参与运算的集合数，将score相加作为结果的score ZUNIONSTORE destination numkeys key1 [key2..] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 ZSCAN key cursor [MATCH pattern\\] [COUNT count] 迭代有序集合中的元素（包括元素成员和元素分值） -------------------ZADD--ZCARD--ZCOUNT-------------- 127.0.0.1:6379&gt; ZADD myzset 1 m1 2 m2 3 m3 # 向有序集合myzset中添加成员m1 score=1 以及成员m2 score=2.. (integer) 2 127.0.0.1:6379&gt; ZCARD myzset # 获取有序集合的成员数 (integer) 2 127.0.0.1:6379&gt; ZCOUNT myzset 0 1 # 获取score在 [0,1]区间的成员数量 (integer) 1 127.0.0.1:6379&gt; ZCOUNT myzset 0 2 (integer) 2 ----------------ZINCRBY--ZSCORE-------------------------- 127.0.0.1:6379&gt; ZINCRBY myzset 5 m2 # 将成员m2的score +5 &quot;7&quot; 127.0.0.1:6379&gt; ZSCORE myzset m1 # 获取成员m1的score &quot;1&quot; 127.0.0.1:6379&gt; ZSCORE myzset m2 &quot;7&quot; --------------ZRANK--ZRANGE----------------------------------- 127.0.0.1:6379&gt; ZRANK myzset m1 # 获取成员m1的索引，索引按照score排序，score相同索引值按字典顺序顺序增加 (integer) 0 127.0.0.1:6379&gt; ZRANK myzset m2 (integer) 2 127.0.0.1:6379&gt; ZRANGE myzset 0 1 # 获取索引在 0~1的成员 1) &quot;m1&quot; 2) &quot;m3&quot; 127.0.0.1:6379&gt; ZRANGE myzset 0 -1 # 获取全部成员 1) &quot;m1&quot; 2) &quot;m3&quot; 3) &quot;m2&quot; #testset=&gt;{abc,add,amaze,apple,back,java,redis} score均为0 ------------------ZRANGEBYLEX--------------------------------- 127.0.0.1:6379&gt; ZRANGEBYLEX testset - + # 返回所有成员 1) &quot;abc&quot; 2) &quot;add&quot; 3) &quot;amaze&quot; 4) &quot;apple&quot; 5) &quot;back&quot; 6) &quot;java&quot; 7) &quot;redis&quot; 127.0.0.1:6379&gt; ZRANGEBYLEX testset - + LIMIT 0 3 # 分页 按索引显示查询结果的 0,1,2条记录 1) &quot;abc&quot; 2) &quot;add&quot; 3) &quot;amaze&quot; 127.0.0.1:6379&gt; ZRANGEBYLEX testset - + LIMIT 3 3 # 显示 3,4,5条记录 1) &quot;apple&quot; 2) &quot;back&quot; 3) &quot;java&quot; 127.0.0.1:6379&gt; ZRANGEBYLEX testset (- [apple # 显示 (-,apple] 区间内的成员 1) &quot;abc&quot; 2) &quot;add&quot; 3) &quot;amaze&quot; 4) &quot;apple&quot; 127.0.0.1:6379&gt; ZRANGEBYLEX testset [apple [java # 显示 [apple,java]字典区间的成员 1) &quot;apple&quot; 2) &quot;back&quot; 3) &quot;java&quot; -----------------------ZRANGEBYSCORE--------------------- 127.0.0.1:6379&gt; ZRANGEBYSCORE myzset 1 10 # 返回score在 [1,10]之间的的成员 1) &quot;m1&quot; 2) &quot;m3&quot; 3) &quot;m2&quot; 127.0.0.1:6379&gt; ZRANGEBYSCORE myzset 1 5 1) &quot;m1&quot; 2) &quot;m3&quot; --------------------ZLEXCOUNT----------------------------- 127.0.0.1:6379&gt; ZLEXCOUNT testset - + (integer) 7 127.0.0.1:6379&gt; ZLEXCOUNT testset [apple [java (integer) 3 ------------------ZREM--ZREMRANGEBYLEX--ZREMRANGBYRANK--ZREMRANGEBYSCORE-------------------------------- 127.0.0.1:6379&gt; ZREM testset abc # 移除成员abc (integer) 1 127.0.0.1:6379&gt; ZREMRANGEBYLEX testset [apple [java # 移除字典区间[apple,java]中的所有成员 (integer) 3 127.0.0.1:6379&gt; ZREMRANGEBYRANK testset 0 1 # 移除排名0~1的所有成员 (integer) 2 127.0.0.1:6379&gt; ZREMRANGEBYSCORE myzset 0 3 # 移除score在 [0,3]的成员 (integer) 2 # testset=&gt; {abc,add,apple,amaze,back,java,redis} score均为0 # myzset=&gt; {(m1,1),(m2,2),(m3,3),(m4,4),(m7,7),(m9,9)} ----------------ZREVRANGE--ZREVRANGEBYSCORE--ZREVRANGEBYLEX----------- 127.0.0.1:6379&gt; ZREVRANGE myzset 0 3 # 按score递减排序，然后按索引，返回结果的 0~3 1) &quot;m9&quot; 2) &quot;m7&quot; 3) &quot;m4&quot; 4) &quot;m3&quot; 127.0.0.1:6379&gt; ZREVRANGE myzset 2 4 # 返回排序结果的 索引的2~4 1) &quot;m4&quot; 2) &quot;m3&quot; 3) &quot;m2&quot; 127.0.0.1:6379&gt; ZREVRANGEBYSCORE myzset 6 2 # 按score递减顺序 返回集合中分数在[2,6]之间的成员 1) &quot;m4&quot; 2) &quot;m3&quot; 3) &quot;m2&quot; 127.0.0.1:6379&gt; ZREVRANGEBYLEX testset [java (add # 按字典倒序 返回集合中(add,java]字典区间的成员 1) &quot;java&quot; 2) &quot;back&quot; 3) &quot;apple&quot; 4) &quot;amaze&quot; -------------------------ZREVRANK------------------------------ 127.0.0.1:6379&gt; ZREVRANK myzset m7 # 按score递减顺序，返回成员m7索引 (integer) 1 127.0.0.1:6379&gt; ZREVRANK myzset m2 (integer) 4 # mathscore=&gt;{(xm,90),(xh,95),(xg,87)} 小明、小红、小刚的数学成绩 # enscore=&gt;{(xm,70),(xh,93),(xg,90)} 小明、小红、小刚的英语成绩 -------------------ZINTERSTORE--ZUNIONSTORE----------------------------------- 127.0.0.1:6379&gt; ZINTERSTORE sumscore 2 mathscore enscore # 将mathscore enscore进行合并 结果存放到sumscore (integer) 3 127.0.0.1:6379&gt; ZRANGE sumscore 0 -1 withscores # 合并后的score是之前集合中所有score的和 1) &quot;xm&quot; 2) &quot;160&quot; 3) &quot;xg&quot; 4) &quot;177&quot; 5) &quot;xh&quot; 6) &quot;188&quot; 127.0.0.1:6379&gt; ZUNIONSTORE lowestscore 2 mathscore enscore AGGREGATE MIN # 取两个集合的成员score最小值作为结果的 (integer) 3 127.0.0.1:6379&gt; ZRANGE lowestscore 0 -1 withscores 1) &quot;xm&quot; 2) &quot;70&quot; 3) &quot;xg&quot; 4) &quot;87&quot; 5) &quot;xh&quot; 6) &quot;93&quot; 应用案例： set排序 存储班级成绩表 工资表排序！ 普通消息，1.重要消息 2.带权重进行判断 排行榜应用实现，取Top N测试 四、三种特殊数据类型 Geospatial(地理位置) 使用经纬度定位地理坐标并用一个有序集合zset保存，所以zset命令也可以使用 命令 描述 geoadd key longitud(经度) latitude(纬度) member [..] 将具体经纬度的坐标存入一个有序集合 geopos key member [member..] 获取集合中的一个/多个成员坐标 geodist key member1 member2 [unit] 返回两个给定位置之间的距离。默认以米作为单位。 `georadius key longitude latitude radius m km GEORADIUSBYMEMBER key member radius... 功能与GEORADIUS相同，只是中心位置不是具体的经纬度，而是使用结合中已有的成员作为中心点。 geohash key member1 [member2..] 返回一个或多个位置元素的Geohash表示。使用Geohash位置52点整数编码。 有效经纬度 有效的经度从-180度到180度。 有效的纬度从-85.05112878度到85.05112878度。 指定单位的参数 unit 必须是以下单位的其中一个： m 表示单位为米。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 关于GEORADIUS的参数 通过georadius就可以完成 附近的人功能 withcoord:带上坐标 withdist:带上距离，单位与半径单位相同 COUNT n : 只显示前n个(按距离递增排序) ----------------georadius--------------------- 127.0.0.1:6379&gt; GEORADIUS china:city 120 30 500 km withcoord withdist # 查询经纬度(120,30)坐标500km半径内的成员 1) 1) &quot;hangzhou&quot; 2) &quot;29.4151&quot; 3) 1) &quot;120.20000249147415&quot; 2) &quot;30.199999888333501&quot; 2) 1) &quot;shanghai&quot; 2) &quot;205.3611&quot; 3) 1) &quot;121.40000134706497&quot; 2) &quot;31.400000253193539&quot; ------------geohash--------------------------- 127.0.0.1:6379&gt; geohash china:city yichang shanghai # 获取成员经纬坐标的geohash表示 1) &quot;wmrjwbr5250&quot; 2) &quot;wtw6ds0y300&quot; Hyperloglog(基数统计) Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。 因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 其底层使用string数据类型 什么是基数？ 数据集中不重复的元素的个数。 应用场景： 网页的访问量（UV）：一个用户多次访问，也只能算作一个人。 传统实现，存储用户的id,然后每次进行比较。当用户变多之后这种方式及其浪费空间，而我们的目的只是计数，Hyperloglog就能帮助我们利用最小的空间完成。 命令 描述 PFADD key element1 [elememt2..] 添加指定元素到 HyperLogLog 中 PFCOUNT key [key] 返回给定 HyperLogLog 的基数估算值。 PFMERGE destkey sourcekey [sourcekey..] 将多个 HyperLogLog 合并为一个 HyperLogLog ----------PFADD--PFCOUNT--------------------- 127.0.0.1:6379&gt; PFADD myelemx a b c d e f g h i j k # 添加元素 (integer) 1 127.0.0.1:6379&gt; type myelemx # hyperloglog底层使用String string 127.0.0.1:6379&gt; PFCOUNT myelemx # 估算myelemx的基数 (integer) 11 127.0.0.1:6379&gt; PFADD myelemy i j k z m c b v p q s (integer) 1 127.0.0.1:6379&gt; PFCOUNT myelemy (integer) 11 ----------------PFMERGE----------------------- 127.0.0.1:6379&gt; PFMERGE myelemz myelemx myelemy # 合并myelemx和myelemy 成为myelemz OK 127.0.0.1:6379&gt; PFCOUNT myelemz # 估算基数 (integer) 17 如果允许容错，那么一定可以使用Hyperloglog ! 如果不允许容错，就使用set或者自己的数据类型即可 ！ BitMaps(位图) 使用位存储，信息状态只有 0 和 1 Bitmap是一串连续的2进制数字（0或1），每一位所在的位置为偏移(offset)，在bitmap上可执行AND,OR,XOR,NOT以及其它位操作。 应用场景 签到统计、状态统计 命令 描述 setbit key offset value 为指定key的offset位设置值 getbit key offset 获取offset位的值 bitcount key [start end] 统计字符串被设置为1的bit数，也可以指定统计范围按字节 bitop operration destkey key[key..] 对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 BITPOS key bit [start] [end] 返回字符串里面第一个被设置为1或者0的bit位。start和end只能按字节,不能按位 ------------setbit--getbit-------------- 127.0.0.1:6379&gt; setbit sign 0 1 # 设置sign的第0位为 1 (integer) 0 127.0.0.1:6379&gt; setbit sign 2 1 # 设置sign的第2位为 1 不设置默认 是0 (integer) 0 127.0.0.1:6379&gt; setbit sign 3 1 (integer) 0 127.0.0.1:6379&gt; setbit sign 5 1 (integer) 0 127.0.0.1:6379&gt; type sign string 127.0.0.1:6379&gt; getbit sign 2 # 获取第2位的数值 (integer) 1 127.0.0.1:6379&gt; getbit sign 3 (integer) 1 127.0.0.1:6379&gt; getbit sign 4 # 未设置默认是0 (integer) 0 -----------bitcount---------------------------- 127.0.0.1:6379&gt; BITCOUNT sign # 统计sign中为1的位数 (integer) 4 bitmaps的底层 [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9PlszjhS-1597890996519)(D:\\我\\MyBlog\\狂神说 Redis.assets\\image-20200803234336175.png)] 这样设置以后你能get到的值是：\\xA2\\x80，所以bitmaps是一串从左到右的二进制串 五、事务 Redis的单条命令是保证原子性的，但是redis事务不能保证原子性 Redis事务本质：一组命令的集合。 ----------------- 队列 set set set 执行 ------------------- 事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。 一次性：一次性执行 顺序性：队列依次执行 排他性：不允许被干扰 Redis事务没有隔离级别的概念 Redis单条命令是保证原子性的，但是事务不保证原子性，且没有回滚！ Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性。 Redis事务其他实现 基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行，其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完 基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐 Redis事务操作过程 开启事务（multi） 命令入队 执行事务（exec） 所以事务中的命令在加入时都没有被执行，直到提交时才会开始执行(Exec)一次性完成。 127.0.0.1:6379&gt; multi # 开启事务 OK 127.0.0.1:6379&gt; set k1 v1 # 命令入队 QUEUED 127.0.0.1:6379&gt; set k2 v2 # .. QUEUED 127.0.0.1:6379&gt; get k1 QUEUED 127.0.0.1:6379&gt; set k3 v3 QUEUED 127.0.0.1:6379&gt; keys * QUEUED 127.0.0.1:6379&gt; exec # 事务执行 1) OK 2) OK 3) &quot;v1&quot; 4) OK 5) 1) &quot;k3&quot; 2) &quot;k2&quot; 3) &quot;k1&quot; 取消事务(discurd) 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; set k1 v1 QUEUED 127.0.0.1:6379&gt; set k2 v2 QUEUED 127.0.0.1:6379&gt; DISCARD # 放弃事务 OK 127.0.0.1:6379&gt; EXEC (error) ERR EXEC without MULTI # 当前未开启事务 127.0.0.1:6379&gt; get k1 # 被放弃事务中命令并未执行 (nil) 事务错误 代码语法错误（编译时异常）所有的命令都不执行 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; set k1 v1 QUEUED 127.0.0.1:6379&gt; set k2 v2 QUEUED 127.0.0.1:6379&gt; error k1 # 这是一条语法错误命令 (error) ERR unknown command `error`, with args beginning with: `k1`, # 会报错但是不影响后续命令入队 127.0.0.1:6379&gt; get k2 QUEUED 127.0.0.1:6379&gt; EXEC (error) EXECABORT Transaction discarded because of previous errors. # 执行报错 127.0.0.1:6379&gt; get k1 (nil) # 其他命令并没有被执行 代码逻辑错误 (运行时异常) **其他命令可以正常执行 ** &gt;&gt;&gt; 所以不保证事务原子性 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; set k1 v1 QUEUED 127.0.0.1:6379&gt; set k2 v2 QUEUED 127.0.0.1:6379&gt; INCR k1 # 这条命令逻辑错误（对字符串进行增量） QUEUED 127.0.0.1:6379&gt; get k2 QUEUED 127.0.0.1:6379&gt; exec 1) OK 2) OK 3) (error) ERR value is not an integer or out of range # 运行时报错 4) &quot;v2&quot; # 其他命令正常执行 # 虽然中间有一条命令报错了，但是后面的指令依旧正常执行成功了。 # 所以说Redis单条指令保证原子性，但是Redis事务不能保证原子性。 监控 悲观锁： 很悲观，认为什么时候都会出现问题，无论做什么都会加锁 乐观锁： 很乐观，认为什么时候都不会出现问题，所以不会上锁！更新数据的时候去判断一下，在此期间是否有人修改过这个数据 获取version 更新的时候比较version 使用watch key监控指定数据，相当于乐观锁加锁。可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。 正常执行 127.0.0.1:6379&gt; set money 100 # 设置余额:100 OK 127.0.0.1:6379&gt; set use 0 # 支出使用:0 OK 127.0.0.1:6379&gt; watch money # 监视money (上锁) OK 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; DECRBY money 20 QUEUED 127.0.0.1:6379&gt; INCRBY use 20 QUEUED 127.0.0.1:6379&gt; exec # 监视值没有被中途修改，事务正常执行 1) (integer) 80 2) (integer) 20 测试多线程修改值，使用watch可以当做redis的乐观锁操作（相当于getversion，） 提交事务时比对之前watch时get到的值，发现和现在不同，就执行失败，相当于加锁。 我们启动另外一个客户端模拟插队线程。 线程1： 127.0.0.1:6379&gt; watch money # money上锁 OK 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; DECRBY money 20 QUEUED 127.0.0.1:6379&gt; INCRBY use 20 QUEUED 127.0.0.1:6379&gt; # 此时事务并没有执行 模拟线程插队，线程2： 127.0.0.1:6379&gt; INCRBY money 500 # 修改了线程一中监视的money (integer) 600 12 回到线程1，执行事务： 127.0.0.1:6379&gt; EXEC # 执行之前，另一个线程修改了我们的值，这个时候就会导致事务执行失败 (nil) # 没有结果，说明事务执行失败 127.0.0.1:6379&gt; get money # 线程2 修改生效 &quot;600&quot; 127.0.0.1:6379&gt; get use # 线程1事务执行失败，数值没有被修改 &quot;0&quot; 失败了怎么办呢？解锁获取最新值，然后再加锁进行事务。 unwatch进行解锁。 注意：每次提交执行exec后都会自动释放锁，不管是否成功 六、Jedis 使用Java来操作Redis，Jedis是Redis官方推荐使用的Java连接redis的客户端。 导入依赖 &lt;!--导入jredis的包--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--fastjson--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.70&lt;/version&gt; &lt;/dependency&gt; 编码测试 连接数据库 修改redis的配置文件 vim /usr/local/bin/myconfig/redis.conf 1 将只绑定本地注释 [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-4IRUFJ95-1597890996520)(狂神说 Redis.assets/image-20200813161921480.png)] 保护模式改为 no [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-oKjIVapw-1597890996521)(狂神说 Redis.assets/image-20200813161939847.png)] 允许后台运行 [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-c2IMvpZL-1597890996522)(狂神说 Redis.assets/image-20200813161954567.png)] 开放端口6379 firewall-cmd --zone=public --add-port=6379/tcp --permanet 1 重启防火墙服务 systemctl restart firewalld.service 1 阿里云服务器控制台配置安全组 重启redis-server [root@AlibabaECS bin]# redis-server myconfig/redis.conf 1 操作命令 TestPing.java public class TestPing { public static void main(String[] args) { Jedis jedis = new Jedis(&quot;192.168.xx.xxx&quot;, 6379); String response = jedis.ping(); System.out.println(response); // PONG } } 断开连接 事务 public class TestTX { public static void main(String[] args) { Jedis jedis = new Jedis(&quot;39.99.xxx.xx&quot;, 6379); JSONObject jsonObject = new JSONObject(); jsonObject.put(&quot;hello&quot;, &quot;world&quot;); jsonObject.put(&quot;name&quot;, &quot;kuangshen&quot;); // 开启事务 Transaction multi = jedis.multi(); String result = jsonObject.toJSONString(); // jedis.watch(result) try { multi.set(&quot;user1&quot;, result); multi.set(&quot;user2&quot;, result); // 执行事务 multi.exec(); }catch (Exception e){ // 放弃事务 multi.discard(); } finally { // 关闭连接 System.out.println(jedis.get(&quot;user1&quot;)); System.out.println(jedis.get(&quot;user2&quot;)); jedis.close(); } } } 七、SpringBoot整合 导入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; springboot 2.x后 ，原来使用的 Jedis 被 lettuce 替换。 jedis：采用的直连，多个线程操作的话，是不安全的。如果要避免不安全，使用jedis pool连接池！更像BIO模式 lettuce：采用netty，实例可以在多个线程中共享，不存在线程不安全的情况！可以减少线程数据了，更像NIO模式 我们在学习SpringBoot自动配置的原理时，整合一个组件并进行配置一定会有一个自动配置类xxxAutoConfiguration,并且在spring.factories中也一定能找到这个类的完全限定名。Redis也不例外。 那么就一定还存在一个RedisProperties类 之前我们说SpringBoot2.x后默认使用Lettuce来替换Jedis，现在我们就能来验证了。 先看Jedis: @ConditionalOnClass注解中有两个类是默认不存在的，所以Jedis是无法生效的 然后再看Lettuce： 完美生效。 现在我们回到RedisAutoConfiguratio 只有两个简单的Bean RedisTemplate StringRedisTemplate 当看到xxTemplate时可以对比RestTemplat、SqlSessionTemplate,通过使用这些Template来间接操作组件。那么这俩也不会例外。分别用于操作Redis和Redis中的String数据类型。 在RedisTemplate上也有一个条件注解，说明我们是可以对其进行定制化的 说完这些，我们需要知道如何编写配置文件然后连接Redis，就需要阅读RedisProperties 这是一些基本的配置属性。 还有一些连接池相关的配置。注意使用时一定使用Lettuce的连接池。 编写配置文件 # 配置redis spring.redis.host=39.99.xxx.xx spring.redis.port=6379 使用RedisTemplate @SpringBootTest class Redis02SpringbootApplicationTests { @Autowired private RedisTemplate redisTemplate; @Test void contextLoads() { // redisTemplate 操作不同的数据类型，api和我们的指令是一样的 // opsForValue 操作字符串 类似String // opsForList 操作List 类似List // opsForHah // 除了基本的操作，我们常用的方法都可以直接通过redisTemplate操作，比如事务和基本的CRUD // 获取连接对象 //RedisConnection connection = redisTemplate.getConnectionFactory().getConnection(); //connection.flushDb(); //connection.flushAll(); redisTemplate.opsForValue().set(&quot;mykey&quot;,&quot;kuangshen&quot;); System.out.println(redisTemplate.opsForValue().get(&quot;mykey&quot;)); } } 测试结果 此时我们回到Redis查看数据时候，惊奇发现全是乱码，可是程序中可以正常输出： 这时候就关系到存储对象的序列化问题，在网络中传输的对象也是一样需要序列化，否者就全是乱码。 我们转到看那个默认的RedisTemplate内部什么样子： 在最开始就能看到几个关于序列化的参数。 默认的序列化器是采用JDK序列化器 而默认的RedisTemplate中的所有序列化器都是使用这个序列化器： 后续我们定制RedisTemplate就可以对其进行修改。 RedisSerializer提供了多种序列化方案： 直接调用RedisSerializer的静态方法来返回序列化器，然后set 自己new 相应的实现类，然后set 定制RedisTemplate的模板： 我们创建一个Bean加入容器，就会触发RedisTemplate上的条件注解使默认的RedisTemplate失效。 @Configuration public class RedisConfig { @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { // 将template 泛型设置为 &lt;String, Object&gt; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate(); // 连接工厂，不必修改 template.setConnectionFactory(redisConnectionFactory); /* * 序列化设置 */ // key、hash的key 采用 String序列化方式 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); // value、hash的value 采用 Jackson 序列化方式 template.setValueSerializer(RedisSerializer.json()); template.setHashValueSerializer(RedisSerializer.json()); template.afterPropertiesSet(); return template; } } 这样一来，只要实体类进行了序列化，我们存什么都不会有乱码的担忧了。 [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-oc8kJP08-1597890996523)(狂神说 Redis.assets/image-20200817175638086.png)] 八、自定义Redis工具类 使用RedisTemplate需要频繁调用.opForxxx然后才能进行对应的操作，这样使用起来代码效率低下，工作中一般不会这样使用，而是将这些常用的公共API抽取出来封装成为一个工具类，然后直接使用工具类来间接操作Redis,不但效率高并且易用。 工具类参考博客： https://www.cnblogs.com/zeng1994/p/03303c805731afc9aa9c60dbbd32a323.html https://www.cnblogs.com/zhzhlong/p/11434284.html 九、Redis.conf 容量单位不区分大小写，G和GB有区别 可以使用 include 组合多个配置问题 网络配置 日志输出级别 日志输出文件 持久化规则 由于Redis是基于内存的数据库，需要将数据由内存持久化到文件中 持久化方式： RDB AOF RDB文件相关 主从复制 Security模块中进行密码设置 客户端连接相关 maxclients 10000 最大客户端数量 maxmemory &lt;bytes&gt; 最大内存限制 maxmemory-policy noeviction # 内存达到限制值的处理策略 redis 中的默认的过期策略是 volatile-lru 。 设置方式 config set maxmemory-policy volatile-lru 1 过期键的删除策略（3种） Redis的过期策略就是指当Redis中缓存的key过期了，Redis如何处理。 定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。 惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。 定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。 (expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。) Redis中同时使用了惰性过期和定期过期两种过期策略。 maxmemory-policy 内存淘汰策略六种方式 MySQL里有2000w数据，redis中只存20w的数据，保证redis中的数据都是热点数据。 Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。 **volatile-lru：**只对设置了过期时间的key进行LRU（默认值） **volatile-random：**随机删除即将过期key volatile-ttl ： 有更早过期时间的key优先移除 allkeys-lru ： 删除lru算法的key **allkeys-random：**随机删除 noeviction ： 永不过期，返回错误 AOF相关部分 十、持久化—RDB RDB：Redis Databases [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-C0mm1D4A-1597890996524)(狂神说 Redis.assets/image-20200818122236614.png)] 什么是RDB 在指定时间间隔后，将内存中的数据集快照写入数据库 ；在恢复时候，直接读取快照文件，进行数据的恢复 ； 默认情况下， Redis 将数据库快照保存在名字为 dump.rdb的二进制文件中。文件名可以在配置文件中进行自定义。 工作原理 在进行 RDB 的时候，redis 的主线程是不会做 io 操作的，主线程会 fork 一个子线程来完成该操作； Redis 调用forks。同时拥有父进程和子进程。 子进程将数据集写入到一个临时 RDB 文件中。 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益(因为是使用子进程进行写操作，而父进程依然可以接收来自客户端的请求。) 触发机制 save的规则满足的情况下，会自动触发rdb原则 执行flushall命令，也会触发我们的rdb原则 退出redis，也会自动产生rdb文件 save 使用 save 命令，会立刻对当前内存中的数据进行持久化 ,但是会阻塞，也就是不接受其他操作了； 由于 save 命令是同步命令，会占用Redis的主进程。若Redis数据非常多时，save命令执行速度会非常慢，阻塞所有客户端的请求。 flushall命令 flushall 命令也 会触发持久化 ； 触发持久化规则 满足配置条件中的触发条件 ； 可以通过配置文件对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动进行数据集保存操作。 bgsave bgsave 是异步进行，进行持久化的时候，redis 还可以将继续响应客户端请求 ； bgsave和save对比 命令 save bgsave IO类型 同步 异步 阻塞？ 是 是（阻塞发生在fock()，通常非常快） 复杂度 O(n) O(n) 优点 不会消耗额外的内存 不阻塞客户端命令 缺点 阻塞客户端命令 需要fock子进程，消耗内存 优缺点 优点： 适合大规模的数据恢复 对数据的完整性要求不高 缺点： 需要一定的时间间隔进行操作，如果redis意外宕机了，这个最后一次修改的数据就没有了。 fork进程的时候，会占用一定的内容空间。 十一、持久化AOF Append Only File 将我们所有的命令都记录下来，history，恢复的时候就把这个文件全部再执行一遍 以日志的形式来记录每个写的操作，将Redis执行过的所有指令记录下来（读操作不记录），只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 什么是AOF 快照功能（RDB）并不是非常耐久（durable)： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、以及未保存到快照中的那些数据。 从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化。 如果要使用AOF，需要修改配置文件： appendonly no yes则表示启用AOF 默认是不开启的，我们需要手动配置，然后重启redis，就可以生效了！ 如果这个aof文件有错位，这时候redis是启动不起来的，我需要修改这个aof文件 redis给我们提供了一个工具redis-check-aof --fix 优点和缺点 appendonly yes # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分的情况下，rdb完全够用 appendfilename &quot;appendonly.aof&quot; # 判断是否需要将 AOF 缓存区中的内容写入和同步到 AOF 文件中 # appendfsync always # 每次修改都会sync 消耗性能 appendfsync everysec # 每秒执行一次 sync 可能会丢失这一秒的数据 # appendfsync no # 不执行 sync ,这时候操作系统自己同步数据，速度最快,Redis 在每一个事件循环都要将 AOF 缓冲区中的所有内容写入到 AOF 文件。而 AOF 文件的同步由操作系统控制。这种模式下速度最快，但是同步的时间间隔较长，出现故障时可能会丢失较多数据。 优点 每一次修改都会同步，文件的完整性会更加好 没秒同步一次，可能会丢失一秒的数据 从不同步，效率最高 缺点 相对于数据文件来说，aof远远大于rdb，修复速度比rdb慢！ Aof运行效率也要比rdb慢，所以我们redis默认的配置就是rdb持久化 重写 4.1重写机制介绍 AOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制, 当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof 4.2重写的原理 AOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件最后再rename)，遍历新进程的内存中数据，每条记录有一条的Set语句。重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似 4.3重写的三种触发机制 ​ 1.手动调用 bgrewriteaof 命令，如果当前有正在运行的 rewrite 子进程，则本次rewrite 会推迟执行，否则，直接触发一次 rewrite。 ​ 2.通过配置指令手动开启 AOF 功能，如果没有 RDB 子进程的情况下，会触发一次 rewrite，将当前数据库中的数据写入 rewrite 文件。 ​ 3.在 Redis 定时器中，如果有需要退出执行的 rewrite 并且没有正在运行的 RDB 或者 rewrite 子进程时，触发一次或者 AOF 文件大小已经到达配置的 rewrite 条件也会自动触发一次。 ​ 4.Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M是也会触发 十二、RDB和AOP选择 RDB 和 AOF 对比 RDB AOF 启动优先级 低 高 体积 小 大 -&gt; 重写 恢复速度 快 慢 数据安全性 丢数据 根据策略决定 更加完整，持续的IO 如何选择使用哪种持久化方式？ 一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快。 十三、Redis发布与订阅 Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客 户端： 命令 命令 描述 PSUBSCRIBE pattern [pattern..] 订阅一个或多个符合给定模式的频道。 PUNSUBSCRIBE pattern [pattern..] 退订一个或多个符合给定模式的频道。 PUBSUB subcommand [argument[argument]] 查看订阅与发布系统状态。 PUBLISH channel message 向指定频道发布消息 SUBSCRIBE channel [channel..] 订阅给定的一个或多个频道。 SUBSCRIBE channel [channel..] 退订一个或多个频道 示例 ------------订阅端---------------------- 127.0.0.1:6379&gt; SUBSCRIBE sakura # 订阅sakura频道 Reading messages... (press Ctrl-C to quit) # 等待接收消息 1) &quot;subscribe&quot; # 订阅成功的消息 2) &quot;sakura&quot; 3) (integer) 1 1) &quot;message&quot; # 接收到来自sakura频道的消息 &quot;hello world&quot; 2) &quot;sakura&quot; 3) &quot;hello world&quot; 1) &quot;message&quot; # 接收到来自sakura频道的消息 &quot;hello i am sakura&quot; 2) &quot;sakura&quot; 3) &quot;hello i am sakura&quot; --------------消息发布端------------------- 127.0.0.1:6379&gt; PUBLISH sakura &quot;hello world&quot; # 发布消息到sakura频道 (integer) 1 127.0.0.1:6379&gt; PUBLISH sakura &quot;hello i am sakura&quot; # 发布消息 (integer) 1 -----------------查看活跃的频道------------ 127.0.0.1:6379&gt; PUBSUB channels 1) &quot;sakura&quot; 原理 每个 Redis 服务器进程都维持着一个表示服务器状态的 redis.h/redisServer 结构， 结构的 pubsub_channels 属性是一个字典， 这个字典就用于保存订阅频道的信息，其中，字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端。 客户端订阅，就被链接到对应频道的链表的尾部，退订则就是将客户端节点从链表中移除。 缺点 如果一个客户端订阅了频道，但自己读取消息的速度却不够快的话，那么不断积压的消息会使redis输出缓冲区的体积变得越来越大，这可能使得redis本身的速度变慢，甚至直接崩溃。 这和数据传输可靠性有关，如果在订阅方断线，那么他将会丢失所有在断线期间发布者发布的消息。 应用 消息订阅：公众号订阅，微博关注等等（起始更多是使用消息队列来进行实现） 多人在线聊天室。 稍微复杂的场景，我们就会使用消息中间件MQ处理。 十四、Redis主从复制 概念 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点（Master/Leader）,后者称为从节点（Slave/Follower）， 数据的复制是单向的！只能由主节点复制到从节点（主节点以写为主、从节点以读为主）。 默认情况下，每台Redis服务器都是主节点，一个主节点可以有0个或者多个从节点，但每个从节点只能由一个主节点。 作用 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余的方式。 故障恢复：当主节点故障时，从节点可以暂时替代主节点提供服务，是一种服务冗余的方式 负载均衡：在主从复制的基础上，配合读写分离，由主节点进行写操作，从节点进行读操作，分担服务器的负载；尤其是在多读少写的场景下，通过多个从节点分担负载，提高并发量。 高可用基石：主从复制还是哨兵和集群能够实施的基础。 为什么使用集群 单台服务器难以负载大量的请求 单台服务器故障率高，系统崩坏概率大 单台服务器内存容量有限。 环境配置 我们在讲解配置文件的时候，注意到有一个replication模块 (见Redis.conf中第8条) 查看当前库的信息：info replication 127.0.0.1:6379&gt; info replication # Replication role:master # 角色 connected_slaves:0 # 从机数量 master_replid:3b54deef5b7b7b7f7dd8acefa23be48879b4fcff master_replid2:0000000000000000000000000000000000000000 master_repl_offset:0 second_repl_offset:-1 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 既然需要启动多个服务，就需要多个配置文件。每个配置文件对应修改以下信息： 端口号 pid文件名 日志文件名 rdb文件名 启动单机多服务集群： 一主二从配置 ==默认情况下，每台Redis服务器都是主节点；==我们一般情况下只用配置从机就好了！ 认老大！一主（79）二从（80，81） 使用SLAVEOF host port就可以为从机配置主机了。 然后主机上也能看到从机的状态： 我们这里是使用命令搭建，是暂时的，==真实开发中应该在从机的配置文件中进行配置，==这样的话是永久的。 使用规则 从机只能读，不能写，主机可读可写但是多用于写。 127.0.0.1:6381&gt; set name sakura # 从机6381写入失败 (error) READONLY You can't write against a read only replica. 127.0.0.1:6380&gt; set name sakura # 从机6380写入失败 (error) READONLY You can't write against a read only replica. 127.0.0.1:6379&gt; set name sakura OK 127.0.0.1:6379&gt; get name &quot;sakura&quot; 当主机断电宕机后，默认情况下从机的角色不会发生变化 ，集群中只是失去了写操作，当主机恢复以后，又会连接上从机恢复原状。 当从机断电宕机后，若不是使用配置文件配置的从机，再次启动后作为主机是无法获取之前主机的数据的，若此时重新配置称为从机，又可以获取到主机的所有数据。这里就要提到一个同步原理。 第二条中提到，默认情况下，主机故障后，不会出现新的主机，有两种方式可以产生新的主机： 从机手动执行命令slaveof no one,这样执行以后从机会独立出来成为一个主机 使用哨兵模式（自动选举） 如果没有老大了，这个时候能不能选择出来一个老大呢？手动！ 如果主机断开了连接，我们可以使用SLAVEOF no one让自己变成主机！其他的节点就可以手动连接到最新的主节点（手动）！如果这个时候老大修复了，那么久重新连接！ 十五、哨兵模式 更多信息参考博客：https://www.jianshu.com/p/06ab9daf921d 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式。 单机单个哨兵 哨兵的作用： 通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。 当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知其他的从服务器，修改配置文件，让它们切换主机。 多哨兵模式 哨兵的核心配置 sentinel monitor mymaster 127.0.0.1 6379 1 数字1表示 ：当一个哨兵主观认为主机断开，就可以客观认为主机故障，然后开始选举新的主机。 测试 redis-sentinel xxx/sentinel.conf 成功启动哨兵模式 此时哨兵监视着我们的主机6379，当我们断开主机后： 哨兵模式优缺点 优点： 哨兵集群，基于主从复制模式，所有主从复制的优点，它都有 主从可以切换，故障可以转移，系统的可用性更好 哨兵模式是主从模式的升级，手动到自动，更加健壮 缺点： Redis不好在线扩容，集群容量一旦达到上限，在线扩容就十分麻烦 实现哨兵模式的配置其实是很麻烦的，里面有很多配置项 哨兵模式的全部配置 完整的哨兵模式配置文件 sentinel.conf # Example sentinel.conf # 哨兵sentinel实例运行的端口 默认26379 port 26379 # 哨兵sentinel的工作目录 dir /tmp # 哨兵sentinel监控的redis主节点的 ip port # master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成。 # quorum 当这些quorum个数sentinel哨兵认为master主节点失联 那么这时 客观上认为主节点失联了 # sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; sentinel monitor mymaster 127.0.0.1 6379 1 # 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码 # 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码 # sentinel auth-pass &lt;master-name&gt; &lt;password&gt; sentinel auth-pass mymaster MySUPER--secret-0123passw0rd # 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒 # sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt; sentinel down-after-milliseconds mymaster 30000 # 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步， 这个数字越小，完成failover所需的时间就越长， 但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。 可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。 # sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt; sentinel parallel-syncs mymaster 1 # 故障转移的超时时间 failover-timeout 可以用在以下这些方面： #1. 同一个sentinel对同一个master两次failover之间的间隔时间。 #2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。 #3.当想要取消一个正在进行的failover所需要的时间。 #4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了 # 默认三分钟 # sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt; sentinel failover-timeout mymaster 180000 # SCRIPTS EXECUTION #配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。 #对于脚本的运行结果有以下规则： #若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10 #若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。 #如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。 #一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。 #通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本， #这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数， #一个是事件的类型， #一个是事件的描述。 #如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。 #通知脚本 # sentinel notification-script &lt;master-name&gt; &lt;script-path&gt; sentinel notification-script mymaster /var/redis/notify.sh # 客户端重新配置主节点参数脚本 # 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。 # 以下参数将会在调用脚本时传给脚本: # &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt; # 目前&lt;state&gt;总是“failover”, # &lt;role&gt;是“leader”或者“observer”中的一个。 # 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的 # 这个脚本应该是通用的，能被多次调用，不是针对性的。 # sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt; sentinel client-reconfig-script mymaster /var/redis/reconfig.sh 十六、缓存穿透与雪崩 缓存穿透（查不到，根本没有这个key） 概念 在默认情况下，用户请求数据时，会先在缓存(Redis)中查找，若没找到即缓存未命中，再在数据库中进行查找，数量少可能问题不大，可是一旦大量的请求数据（例如秒杀场景）缓存都没有命中的话，就会全部转移到数据库上，造成数据库极大的压力，就有可能导致数据库崩溃。网络安全中也有人恶意使用这种手段进行攻击被称为洪水攻击。 解决方案 布隆过滤器 对所有可能查询的参数以Hash的形式存储，以便快速确定是否存在这个值，在控制层先进行拦截校验，校验不通过直接打回，减轻了存储系统的压力。 对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。 Bitmap： 典型的就是哈希表 缺点是，Bitmap对于每个元素只能记录1bit信息，如果还想完成额外的功能，恐怕只能靠牺牲更多的空间、时间来完成了。 布隆过滤器（推荐） 就是引入了k(k&gt;1)k(k&gt;1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程。 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 Bloom-Filter算法的核心思想就是利用多个不同的Hash函数来解决“冲突”。 Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突，我们可以多引入几个Hash，**如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。**这便是Bloom-Filter的基本思想。 Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。 缓存空对象 一次请求若在缓存和数据库中都没找到，就在缓存中方一个空对象用于处理后续这个请求。 这样做有一个缺陷：存储空对象也需要空间，大量的空对象会耗费一定的空间，存储效率并不高。解决这个缺陷的方式就是设置较短过期时间 接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截； 即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。 缓存击穿（一条数据过期） 概念 相较于缓存穿透，缓存击穿的目的性更强，一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。这就是缓存被击穿，只是针对其中某个key的缓存不可用而导致击穿，但是其他的key依然可以使用缓存响应。 比如热搜排行上，一个热点新闻被同时大量访问就可能导致缓存击穿。 解决方案 设置热点数据永不过期 这样就不会出现热点数据过期的情况，但是当Redis内存空间满的时候也会清理部分数据，而且此种方案会占用空间，一旦热点数据多了起来，就会占用部分空间。 加互斥锁(分布式锁) 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。保证同时刻只有一个线程访问。这样对锁的要求就十分高。 缓存雪崩（大量数据过期） 概念 大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。 解决方案 redis高可用 - 集群 这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续工作，其实就是搭建的集群 限流降级 - 加锁 这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。 数据预热 数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同（随机）的过期时间，让缓存失效的时间点尽量均匀。 失效标记更新 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。 集群方案 1、哨兵模式 2、Redis Cluster（服务端路由） Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询请求发送到正确的节点上执行。 方案说明 通过哈希的方式，将数据分片，每个节点均分存储一定哈希槽(哈希值)区间的数据，默认分配了16384 个槽位 每份数据分片会存储在多个互为主从的多节点上，数据写入先写主节点，再同步到从节点(支持配置为阻塞同步)，同一分片多个节点间的数据不保持一致性 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点 扩容时时需要需要把旧节点的数据迁移一部分到新节点 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 闲聊协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制 集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。 3、Redis Sharding（客户端分配） Redis Sharding是Redis Cluster出来之前，业界普遍使用的多Redis实例集群方法。其主要思想是采用哈希算法将key进行散列，通过hash函数，特定的key会映射到特定的Redis节点上。 优点 优势在于非常简单，服务端的Redis实例彼此独立，相互无关联，每个Redis实例像单服务器一样运行，非常容易线性扩展，系统的灵活性很强 缺点 由于sharding处理放到客户端，规模进一步扩大时给运维带来挑战。 客户端sharding不支持动态增删节点。服务端Redis实例群拓扑结构有变化时，每个客户端都需要更新调整。连接不能共享，当应用规模增大时，资源浪费制约优化 4、代理服务器 客户端发送请求到一个代理组件，代理解析客户端的数据，并将请求转发至正确的节点，最后将结果回复给客户端 透明接入，业务程序不用关心后端Redis实例，切换成本低 Proxy 的逻辑和存储的逻辑是隔离的 代理层多了一次转发，性能有所损耗 Redis 主从架构 单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 redis replication 的核心机制 redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量； 一个 master node 是可以配置多个 slave node 的； slave node 也可以连接其他的 slave node； slave node 做复制的时候，不会 block master node 的正常工作； slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了； slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。 注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。 核心原理 当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。 如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件， 同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中， 接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。 slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。 缺点 所有的slave节点数据的复制和同步都由master节点来处理，会照成master节点压力太大，使用主从从结构来解决 哈希槽、一致性 Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。 集群最大节点个数是16384个 Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。 Redis集群目前无法做数据库选择，默认在0数据库。 分区 Redis是单线程的，提高多核CPU的利用率 可以在同一个服务器部署多个Redis的实例，并把他们当作不同的服务器来使用，在某些时候，无论如何一个服务器是不够的， 所以，如果你想使用多个CPU，你可以考虑一下分片（shard）。就是集群！ 为什么要做Redis分区？ 分区可以让Redis管理更大的内存，Redis将可以使用所有机器的内存。如果没有分区，你最多只能使用一台机器的内存。分区使Redis的计算能力通过简单地增加计算机得到成倍提升，Redis的网络带宽也会随着计算机和网卡的增加而成倍增长。 Redis分区实现方案 客户端分区就是在客户端就已经决定数据会被存储到哪个redis节点或者从哪个redis节点读取。大多数客户端已经实现了客户端分区。 代理分区意味着客户端将请求发送给代理，然后代理决定去哪个节点写数据或者读数据。代理根据分区规则决定请求哪些Redis实例，然后根据Redis的响应结果返回给客户端。redis和memcached的一种代理实现就是Twemproxy 服务端查询路由(Query routing) 的意思是客户端随机地请求任意一个redis实例，然后由Redis将请求转发给正确的Redis节点。Redis Cluster实现了一种混合形式的查询路由，但并不是直接将请求从一个redis节点转发到另一个redis节点，而是在客户端的帮助下直接redirected到正确的redis节点。 Redis分区缺点 涉及多个key的操作通常不会被支持。例如你不能对两个集合求交集，因为他们可能被存储到不同的Redis实例（实际上这种情况也有办法，但是不能直接使用交集指令）。 同时操作多个key,则不能使用Redis事务. 分区使用的粒度是key，不能使用一个非常长的排序key存储一个数据集 当使用分区的时候，数据处理会非常复杂，例如为了备份你必须从不同的Redis实例和主机同时收集RDB / AOF文件。 分区时动态扩容或缩容可能非常复杂。Redis集群在运行时增加或者删除Redis节点，能做到最大程度对用户透明地数据再平衡，但其他一些客户端分区或者代理分区方法则不支持这种特性。然而，有一种预分片的技术也可以较好的解决这个问题。 分布式锁 Redis Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系Redis中可以使用SETNX命令实现分布式锁。 当且仅当 key 不存在，将 key 的值设为 value。 若给定的 key 已经存在，则 SETNX 不做任何动作 流程 使用SETNX命令获取锁，若返回0（key已存在，锁已存在）则获取失败，反之获取成功 为了防止获取锁后程序出现异常，导致其他线程/进程调用SETNX命令总是返回0而进入死锁状态，需要为该key设置一个“合理”的过期时间 释放锁，使用DEL命令将锁数据删除 Zookeeper 每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 以可靠性为主首推Zookeeper。 RedLock Redis 官方站提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性： 安全特性：互斥访问，即永远只有一个 client 能拿到锁 避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区 容错性：只要大部分 Redis 节点存活就可以正常提供服务 Redis与Memcached 数据类型、持久化、原生集群模式、单线程/多线程 双写一致性 先更新数据库，然后再删除缓存。 大量数据插入 Redis2.6开始redis-cli支持一种新的被称之为pipe mode的新模式用于执行大量数据插入工作。 查找key 假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？ 使用keys指令可以扫出指定模式的key列表。 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？ redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 延时队列 使用sortedset，使用时间戳做score, 消息内容作为key,调用zadd来生产消息，消费者使用zrangbyscore获取n秒之前的数据做轮询处理。 异步队列 使用list类型保存数据信息，rpush生产消息，lpop消费消息，当lpop没有消息时，可以sleep一段时间，然后再检查有没有信息，如果不想sleep的话，可以使用blpop, 在没有信息的时候，会一直阻塞，直到信息的到来。redis可以通过pub/sub主题订阅模式实现一个生产者，多个消费者，当然也存在一定的缺点，当消费者下线时，生产的消息会丢失。 ","link":"https://memorykki.github.io/Redis/"},{"title":"Mybatis","content":"MyBatis 是一个半自动化的ORM框架 0.1. MyBatis与Hibernate有哪些不同？ 0.2. 模糊查询like 0.3. 引入资源方式 0.4. namespace 0.5. Dao接口和XML文件里的SQL是如何建立关系的？ 0.5.1. 一、解析XML 0.5.1.1. 、创建SqlSource 0.5.1.2. 、创建MappedStatement 0.5.2. 二、Dao接口代理 0.5.3. 三、执行 0.6. 作用域 0.7. 属性名和字段名不一致 0.8. ResultMap 0.9. 分页 0.10. 在mapper中如何传递多个参数? 0.11. Mapper 编写有哪几种方式？ 0.12. 注解 0.13. Mybatis详细的执行流程 0.14. MyBatis的工作原理以及核心流程介绍 0.14.1. 、MyBatis的工作原理以及核心流程详解 0.15. @Param 0.16. #与$的区别 0.17. 数据库链接中断如何处理 0.18. 数据库插入重复如何处理 0.19. 一个Connection在MySQL中对应一个线程？ 0.20. 预编译的过程 0.20.1. 、JDBC的预编译用法 0.20.2. 、预编译的好处 0.20.2.1. 、预编译能避免SQL注入 0.20.2.2. 、预编译能提高SQL执行效率 0.20.3. 、预编译的实现过程 0.21. 0.22. 多对一 0.22.1. 按查询嵌套处理 0.22.2. 按结果嵌套处理 0.23. 一对多 0.23.1. 按结果嵌套处理 0.23.2. 按查询嵌套处理 0.24. 动态 SQL 0.25. Dao接口的工作原理 0.26. 缓存 0.26.1. 一级缓存 0.26.2. 二级缓存 0.27. 、#{}和${}的区别是什么？ 0.28. 、Xml 映射文件中，除了常见的 select|insert|update|delete 标签之外，还有哪些标签？ 0.29. 、最佳实践中，通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应，请问，这个 Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？ 0.29.1. ==补充：== 0.30. 、MyBatis 是如何进行分页的？分页插件的原理是什么？ 0.31. 、简述 MyBatis 的插件运行原理，以及如何编写一个插件。 0.32. 、MyBatis 执行批量插入，能返回数据库主键列表吗？ 0.33. 、MyBatis 动态 sql 是做什么的？都有哪些动态 sql？能简述一下动态 sql 的执行原理不？ 0.34. 、MyBatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？ 0.35. 、MyBatis 能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别。 0.36. 、MyBatis 是否支持延迟加载？如果支持，它的实现原理是什么？ 0.37. 、MyBatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复？ 0.38. 、MyBatis 中如何执行批处理？ 0.39. 、MyBatis 都有哪些 Executor 执行器？它们之间的区别是什么？ 0.40. 、MyBatis 中如何指定使用哪一种 Executor 执行器？ 0.41. 、MyBatis 是否可以映射 Enum 枚举类？ 0.42. 、MyBatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？ 0.43. 、简述 MyBatis 的 Xml 映射文件和 MyBatis 内部数据结构之间的映射关系？ 0.44. 、为什么说 MyBatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？ MyBatis 是一个半自动化的ORM框架 所有的增删改操作都需要提交事务！ 接口所有的普通参数，尽量都写上@Param参数，尤其是多个参数时，必须写上！ 有时候根据业务的需求，可以考虑使用map传递参数！ 为了规范操作，在SQL的配置文件中，我们尽量将Parameter参数和resultType都写上！ Caching缓存 Simple Batch批量 Reuse复用 http://www.mybatis.cn/category/interview/ maven静态资源过滤问题 &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; 0.1. MyBatis与Hibernate有哪些不同？ 1、Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。 2、Mybatis直接编写原生态sql，可以严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需求变化要求迅速输出成果。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件，则需要自定义多套sql映射文件，工作量大。 3、Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件，如果用hibernate开发可以节省很多代码，提高效率。 0.2. 模糊查询like 第1种：在Java代码中添加sql通配符。 string wildcardname = “%smi%”; list&lt;name&gt; names = mapper.selectlike(wildcardname); &lt;select id=”selectlike”&gt; select * from foo where bar like #{value} &lt;/select&gt; 第2种：在sql语句中拼接通配符，会引起sql注入 string wildcardname = “smi”; list&lt;name&gt; names = mapper.selectlike(wildcardname); &lt;select id=”selectlike”&gt; select * from foo where bar like &quot;%&quot;#{value}&quot;%&quot; &lt;/select&gt; 或是利用sql的contact函数。 &lt;select id=&quot;selectLike&quot;&gt; select * from users where name like contact(&quot;%&quot;, #{value}, &quot;%&quot;) &lt;/select&gt; 0.3. 引入资源方式 &lt;!-- 使用相对于类路径的资源引用 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;org/mybatis/builder/PostMapper.xml&quot;/&gt; &lt;/mappers&gt; &lt;!-- 使用完全限定资源定位符（URL） --&gt; &lt;mappers&gt; &lt;mapper url=&quot;file:///var/mappers/AuthorMapper.xml&quot;/&gt; &lt;/mappers&gt; &lt;!-- 使用映射器接口实现类的完全限定类名 需要配置文件名称和接口名称一致，并且位于同一目录下 --&gt; &lt;mappers&gt; &lt;mapper class=&quot;org.mybatis.builder.AuthorMapper&quot;/&gt; &lt;/mappers&gt; &lt;!-- 将包内的映射器接口实现全部注册为映射器 但是需要配置文件名称和接口名称一致，并且位于同一目录下 --&gt; &lt;mappers&gt; &lt;package name=&quot;org.mybatis.builder&quot;/&gt; &lt;/mappers&gt; &lt;!--优先级由上向下降低--&gt; 0.4. namespace namespace中文意思：命名空间，作用如下： namespace的命名必须跟某个接口同名 接口中的方法与映射文件中sql语句id应该一一对应 namespace和子元素的id联合保证唯一 , 区别不同的mapper 绑定DAO接口 namespace命名规则 : 包名+类名 0.5. Dao接口和XML文件里的SQL是如何建立关系的？ 通过Dao接口生成的动态代理调用查询，根据绑定的namespace确定唯一id，在注册中心里找到mappedStatement，通过sqlSource生成SQL语句，jdbc执行返回。 0.5.1. 一、解析XML 首先，Mybatis在初始化SqlSessionFactoryBean的时候，找到mapperLocations路径去解析里面所有的XML文件，这里我们重点关注两部分。 0.5.1.1. 、创建SqlSource Mybatis会把每个SQL标签封装成SqlSource对象，然后根据SQL语句的不同，又分为动态SQL和静态SQL。其中，静态SQL包含一段String类型的sql语句；而动态SQL则是由一个个SqlNode组成。 0.5.1.2. 、创建MappedStatement XML文件中的每一个SQL标签就对应一个MappedStatement对象，这里面有两个属性很重要。 id：全限定类名+方法名组成的ID。 sqlSource：当前SQL标签对应的SqlSource对象。 创建完MappedStatement对象，将它缓存到Configuration#mappedStatements中。 Configuration对象就是Mybatis中的大管家，基本所有的配置信息都维护在这里。把所有的XML都解析完成之后，Configuration就包含了所有的SQL信息。 到目前为止，XML就解析完成了。当我们执行Mybatis方法的时候，就通过全限定类名+方法名找到MappedStatement对象，然后解析里面的SQL内容，执行即可。 0.5.2. 二、Dao接口代理 我们的Dao接口并没有实现类，那么，我们在调用它的时候，它是怎样最终执行到我们的SQL语句的呢？ 首先，我们在Spring配置文件中，一般会这样配置： &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.viewscenes.netsupervisor.dao&quot; /&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 或者你的项目是基于SpringBoot的，那么肯定也见过这种： @MapperScan(&quot;com.xxx.dao&quot;) 它们的作用是一样的。将包路径下的所有类注册到Spring Bean中，并且将它们的beanClass设置为MapperFactoryBean。MapperFactoryBean实现了FactoryBean接口，俗称工厂Bean。那么，当我们通过@Autowired注入这个Dao接口的时候，返回的对象就是MapperFactoryBean这个工厂Bean中的getObject()方法对象。 简单来说，它就是通过JDK动态代理，返回了一个Dao接口的代理对象，这个代理对象的处理器是MapperProxy对象。所有，我们通过@Autowired注入Dao接口的时候，注入的就是这个代理对象，我们调用到Dao接口的方法时，则会调用到MapperProxy对象的invoke方法。 曾经有个朋友问过这样一个问题： 对于有实现的dao接口，mapper还会用代理么？ 答案是肯定，只要你配置了MapperScan，它就会去扫描，然后生成代理。但是，如果你的dao接口有实现类，并且这个实现类也是一个Spring Bean，那就要看你在Autowired的时候，去注入哪一个了。 会报错，因为在注入的时候，找到了两个UserDao的实例对象。其实我们通过名字注入就可以了。 0.5.3. 三、执行 如上所述，当我们调用Dao接口方法的时候，实际调用到代理对象的invoke方法。 在这里，实际上调用的就是SqlSession里面的东西了。 public class DefaultSqlSession implements SqlSession { public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) { try { MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } } } 是通过statement全限定类型+方法名拿到MappedStatement 对象，然后通过执行器Executor去执行具体SQL并返回。 0.6. 作用域 作用域理解 SqlSessionFactoryBuilder 的作用在于创建 SqlSessionFactory，创建成功后，SqlSessionFactoryBuilder 就失去了作用，所以它只能存在于创建 SqlSessionFactory 的方法中，而不要让其长期存在。因此 SqlSessionFactoryBuilder 实例的最佳作用域是方法作用域（也就是局部方法变量）。 SqlSessionFactory 可以被认为是一个数据库连接池，它的作用是创建 SqlSession 接口对象。因为 MyBatis 的本质就是 Java 对数据库的操作，所以 SqlSessionFactory 的生命周期存在于整个 MyBatis 的应用之中，所以一旦创建了 SqlSessionFactory，就要长期保存它，直至不再使用 MyBatis 应用，所以可以认为 SqlSessionFactory 的生命周期就等同于 MyBatis 的应用周期。 由于 SqlSessionFactory 是一个对数据库的连接池，所以它占据着数据库的连接资源。如果创建多个 SqlSessionFactory，那么就存在多个数据库连接池，这样不利于对数据库资源的控制，也会导致数据库连接资源被消耗光，出现系统宕机等情况，所以尽量避免发生这样的情况。 因此在一般的应用中我们往往希望 SqlSessionFactory 作为一个单例，让它在应用中被共享。所以说 SqlSessionFactory 的最佳作用域是应用作用域。 如果说 SqlSessionFactory 相当于数据库连接池，那么 SqlSession 就相当于一个数据库连接（Connection 对象），你可以在一个事务里面执行多条 SQL，然后通过它的 commit、rollback 等方法，提交或者回滚事务。所以它应该存活在一个业务请求中，处理完整个请求后，应该关闭这条连接，让它归还给 SqlSessionFactory，否则数据库资源就很快被耗费精光，系统就会瘫痪，所以用 try...catch...finally... 语句来保证其正确关闭。 所以 SqlSession 的最佳的作用域是请求或方法作用域。 0.7. 属性名和字段名不一致 方案一：为列名指定别名 , 别名和java实体类的属性名一致 . &lt;select id=&quot;selectUserById&quot; resultType=&quot;User&quot;&gt; select id , name , pwd as password from user where id = #{id} &lt;/select&gt; 方案二：使用结果集映射-&gt;ResultMap 【推荐】 &lt;resultMap id=&quot;UserMap&quot; type=&quot;User&quot;&gt; &lt;!-- id为主键 --&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;!-- column是数据库表的列名 , property是对应实体类的属性名 --&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt; &lt;result column=&quot;pwd&quot; property=&quot;password&quot;/&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectUserById&quot; resultMap=&quot;UserMap&quot;&gt; select id , name , pwd from user where id = #{id} &lt;/select&gt; 0.8. ResultMap 自动映射 简单地将所有的列映射到 HashMap 的键上，这由 resultType 属性指定。虽然在大部分情况下都够用，但是 HashMap 不是一个很好的模型。你的程序更可能会使用 JavaBean 或 POJO（Plain Old Java Objects，普通老式 Java 对象）作为模型。 手动映射 返回值类型为resultMap 0.9. 分页 limit物理分页 #语法 SELECT * FROM table LIMIT stratIndex，pageSize SELECT * FROM table LIMIT 5,10; // 检索记录行 6-15 #为了检索从某一个偏移量到记录集的结束所有的记录行，可以指定第二个参数为 -1： SELECT * FROM table LIMIT 95,-1; // 检索记录行 96-last. #如果只给定一个参数，它表示返回最大的记录行数目： SELECT * FROM table LIMIT 5; //检索前 5 个记录行 #换句话说，LIMIT n 等价于 LIMIT 0,n。 RowBounds逻辑分页 除了使用Limit在SQL层面实现分页，也可以使用RowBounds在Java代码层面实现分页 SqlSession session = MybatisUtils.getSession(); int currentPage = 2; //第几页 int pageSize = 2; //每页显示几个 RowBounds rowBounds = new RowBounds((currentPage-1)*pageSize,pageSize); //通过session.**方法进行传递rowBounds，[此种方式现在已经不推荐使用了] List&lt;User&gt; users = session.selectList(&quot;com.kuang.mapper.UserMapper.getUserByRowBounds&quot;, null, rowBounds); PageHelper 0.10. 在mapper中如何传递多个参数? 1、第一种： DAO层的函数 2、第二种： 使用 @param 注解: 然后,就可以在xml像下面这样使用(推荐封装为一个map,作为单个参数传递给mapper): 3、第三种：多个参数封装成map 0.11. Mapper 编写有哪几种方式？ 接口实现类继承 SqlSessionDaoSupport：使用此种方法需要编写mapper 接口，mapper 接口实现类、mapper.xml 文件 1、在 sqlMapConfig.xml 中配置 mapper.xml 的位置 &lt;mappers&gt; &lt;mapper resource=&quot;mapper.xml 文件的地址&quot; /&gt; &lt;mapper resource=&quot;mapper.xml 文件的地址&quot; /&gt; &lt;/mappers&gt; 2、定义 mapper 接口 3、实现类集成 SqlSessionDaoSupport，mapper 方法中可以 this.getSqlSession()进行数据增删改查。 4、spring 配置 &lt;bean id=&quot; &quot; class=&quot;mapper 接口的实现&quot;&gt; &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 使用 org.mybatis.spring.mapper.MapperFactoryBean ： 1、在 sqlMapConfig.xml 中配置 mapper.xml 的位置，如果 mapper.xml 和 mappre 接口的名称相同且在同一个目录，这里可以不用配置 &lt;mapper resource=&quot;mapper.xml 文件的地址&quot; /&gt; &lt;mapper resource=&quot;mapper.xml 文件的地址&quot; /&gt; &lt;/mappers&gt; 2、定义 mapper 接口： 2.1、mapper.xml 中的 namespace 为 mapper 接口的地址 2.2、mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一 致 3、Spring 中定义 使用 mapper 扫描器： 1、mapper.xml 文件编写： mapper.xml 中的 namespace 为 mapper 接口的地址； mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一致； 如果将 mapper.xml 和 mapper 接口的名称保持一致则不用在 sqlMapConfig.xml 中进行配置 2、定义 mapper 接口： 注意 mapper.xml 的文件名和 mapper 的接口名称保持一致，且放在同一个目录 3、配置 mapper 扫描器： &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;mapper 接口包地址&quot;&gt;&lt;/property&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;/&gt; &lt;/bean&gt; 4、使用扫描器后从 spring 容器中获取 mapper 的实现对象。 0.12. 注解 1、我们在我们的接口中添加注解 //查询全部用户 @Select(&quot;select id,name,pwd password from user&quot;) public List&lt;User&gt; getAllUser(); 2、在mybatis的核心配置文件中注入 &lt;!--使用class绑定接口--&gt; &lt;mappers&gt; &lt;mapper class=&quot;com.kuang.mapper.UserMapper&quot;/&gt; &lt;/mappers&gt; 0.13. Mybatis详细的执行流程 0.14. MyBatis的工作原理以及核心流程介绍 JDBC有四个核心对象： （1）DriverManager，用于注册数据库连接 （2）Connection，与数据库连接对象 （3）Statement/PrepareStatement，操作数据库SQL语句的对象 （4）ResultSet，结果集或一张虚拟表 MyBatis也有四大核心对象： （1）SqlSession对象，该对象中包含了执行SQL语句的所有方法。类似于JDBC里面的Connection （2）Executor接口，它将根据SqlSession传递的参数动态地生成需要执行的SQL语句，同时负责查询缓存的维护。类似于JDBC里面的Statement/PrepareStatement。 （3）MappedStatement对象，该对象是对映射SQL的封装，用于存储要映射的SQL语句的id、参数等信息。 （4）ResultHandler对象，用于对返回的结果进行处理，最终得到自己想要的数据格式或类型。可以自定义返回类型。 0.14.1. 、MyBatis的工作原理以及核心流程详解 MyBatis的工作原理如下图所示： （1）读取MyBatis的配置文件。mybatis-config.xml为MyBatis的全局配置文件，用于配置数据库连接信息。 （2）加载映射文件。映射文件即SQL映射文件，该文件中配置了操作数据库的SQL语句，需要在MyBatis配置文件mybatis-config.xml中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表。 （3）构造会话工厂。通过MyBatis的环境配置信息构建会话工厂SqlSessionFactory。 （4）创建会话对象。由会话工厂创建SqlSession对象，该对象中包含了执行SQL语句的所有方法。 （5）Executor执行器。MyBatis底层定义了一个Executor接口来操作数据库，它将根据SqlSession传递的参数动态地生成需要执行的SQL语句，同时负责查询缓存的维护。 （6）MappedStatement对象。在Executor接口的执行方法中有一个MappedStatement类型的参数，该参数是对映射信息的封装，用于存储要映射的SQL语句的id、参数等信息。 （7）输入参数映射。输入参数类型可以是Map、List等集合类型，也可以是基本数据类型和POJO类型。输入参数映射过程类似于JDBC对preparedStatement对象设置参数的过程。 （8）输出结果映射。输出结果类型可以是Map、List等集合类型，也可以是基本数据类型和POJO类型。输出结果映射过程类似于JDBC对结果集的解析过程。 0.15. @Param @Param注解用于给方法参数起一个名字。以下是总结的使用原则： 在方法只接受一个参数的情况下，可以不使用@Param。 在方法接受多个参数的情况下，建议一定要使用@Param注解给参数命名。 如果参数是 JavaBean ， 则不能使用@Param。 不使用@Param注解时，参数只能有一个，并且是Javabean。 0.16. #与$的区别 /#{} 的作用主要是替换预编译语句(PrepareStatement)中的占位符? 【推荐使用】 INSERT INTO user (name) VALUES (#{name}); INSERT INTO user (name) VALUES (?); ${} 的作用是直接进行字符串替换 INSERT INTO user (name) VALUES ('${name}'); INSERT INTO user (name) VALUES ('kuangshen'); （3）使用#{}可以有效的防止SQL注入，提高系统安全性。原因在于：预编译机制。预编译完成之后，SQL的结构已经固定，即便用户输入非法参数，也不会对SQL的结构产生影响，从而避免了潜在的安全风险。 （4）预编译是提前对SQL语句进行预编译，而其后注入的参数将不会再进行SQL编译。我们知道，SQL注入是发生在编译的过程中，因为恶意注入了某些特殊字符，最后被编译成了恶意的执行操作。而预编译机制则可以很好的防止SQL注入。 0.17. 数据库链接中断如何处理 数据库的访问底层是通过tcp实现的，如果数据库链接中断，那么应用程序是不知道的，跟时间有关的设置有：max_idle_time，connect_timeout。max_idle_time表明最大的空闲时间，超过这个时间socket就会关闭，timeout。 0.18. 数据库插入重复如何处理 插入的过程一般都是分两步的：先判断是否存在记录，没有存在则插入否则不插入。如果存在并发操作，那么同时进行了第一步，然后大家都发现没有记录，最后在第二步的时候都插入了数据从而造成数据的重复。解决插入重复的思路可以是这样的： 下面场景，假设同时有三个线程：线程a、线程b、线程c，进行插入操作。 （1）判断数据库是否有数据，有的话则无所作为。没有数据的话，则进行下面第2步。 （2）大家都要去竞争锁，用redis当锁，即：redis set key，其中只有一个操作a会成功，其他并发的线程b和c会失败的。 （3）上面set key 成功的线程a，开始执行插入数据操作，无论是否插入数据成功，都在最后del key。【注】插入不成功可以多尝试几次，增加成功的概率。 （4）如果拿到锁的线程a没有插入成功，即便是尝试了数次也没有插入成功，此时定是系统出现了bug，应该搞一个短信报警机制，让研发人员及时发现问题。 0.19. 一个Connection在MySQL中对应一个线程？ 在高性能服务器端端开发底层往往靠io复用来处理，这种模式就是：单线程+事件处理机制。在MySQL里面往往有一个主线程，这是单线程（与Java中处处强调多线程的思想有点不同哦），它不断的循环查看是否有socket是否有读写事件，如果有读写事件，再从线程池里面找个工作线程处理这个socket的读写事件，完事之后工作线程会回到线程池。所以：Java客户端中的一个Connection不是在MySQL中就对应一个线程来处理这个链接，而是由监听socket的主线程+线程池里面固定数目的工作线程来处理的。 0.20. 预编译的过程 0.20.1. 、JDBC的预编译用法 相信每个人都应该了解JDBC中的PreparedStatement接口，它是用来实现SQL预编译的功能。其用法是这样的： Class.forName(&quot;com.mysql.jdbc.Driver&quot;); String url = &quot;jdbc:mysql://127.0.0.1:3306/mybatis&quot;; String user = &quot;root&quot;; String password = &quot;123456&quot;; //建立数据库连接 Connection conn = DriverManager.getConnection(url, user, password); String sql = &quot;insert into user(username, sex, address) values(?,?,?)&quot;; PreparedStatement ps = conn.preparedStatement(sql); ps.setString(1, &quot;张三&quot;); //为第一个问号赋值 ps.setInt(2, 2); //为第二个问号赋值 ps.setString(3, &quot;北京&quot;); //为第三个问号赋值 ps.executeUpdate(); conn.close(); 0.20.2. 、预编译的好处 0.20.2.1. 、预编译能避免SQL注入 预编译功能可以避免SQL注入，因为SQL已经编译完成，其结构已经固定，用户的输入只能当做参数传入进去，不能再破坏SQL的结果，无法造成曲解SQL原本意思的破坏。 0.20.2.2. 、预编译能提高SQL执行效率 预编译功能除了避免SQL注入，还能提高SQL执行效率。当客户发送一条SQL语句给服务器后，服务器首先需要校验SQL语句的语法格式是否正确，然后把SQL语句编译成可执行的函数，最后才是执行SQL语句。其中校验语法，和编译所花的时间可能比执行SQL语句花的时间还要多。 如果我们需要执行多次insert语句，但只是每次插入的值不同，MySQL服务器也是需要每次都去校验SQL语句的语法格式以及编译，这就浪费了太多的时间。如果使用预编译功能，那么只对SQL语句进行一次语法校验和编译，所以效率要高。 0.20.3. 、预编译的实现过程 预编译功能如此重要，那么数据库是如何实现预编译的呢？这个问题其实可以当做一个面试题，能很好的考察面试者对预编译的理解。下面以MySQL为例说明一下预编译的过程： MySQL执行预编译分为如三步： 第一步：执行预编译语句，例如：prepare myperson from 'select * from t_person where name=?' 第二步：设置变量，例如：set @name='Jim' 第三步：执行语句，例如：execute myperson using @name 如果需要再次执行myperson，那么就不再需要第一步，即不需要再编译语句了： 设置变量，例如：set @name='Tom' 执行语句，例如：execute myperson using @name 0.21. 0.22. 多对一 多对一的理解： 多个学生对应一个老师 如果对于学生这边，就是一个多对一的现象，即从学生这边关联一个老师！ 0.22.1. 按查询嵌套处理 1、给StudentMapper接口增加方法 //获取所有学生及对应老师的信息 public List&lt;Student&gt; getStudents(); 2、编写对应的Mapper文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.kuang.mapper.StudentMapper&quot;&gt; &lt;!-- 需求：获取所有学生及对应老师的信息 思路： 1. 获取所有学生的信息 2. 根据获取的学生信息的老师ID-&gt;获取该老师的信息 3. 思考问题，这样学生的结果集中应该包含老师，该如何处理呢，数据库中我们一般使用关联查询？ 1. 做一个结果集映射：StudentTeacher 2. StudentTeacher结果集的类型为 Student 3. 学生中老师的属性为teacher，对应数据库中为tid。 多个 [1,...）学生关联一个老师=&gt; 一对一，一对多 4. 查看官网找到：association – 一个复杂类型的关联；使用它来处理关联查询 --&gt; &lt;select id=&quot;getStudents&quot; resultMap=&quot;StudentTeacher&quot;&gt; select * from student &lt;/select&gt; &lt;resultMap id=&quot;StudentTeacher&quot; type=&quot;Student&quot;&gt; &lt;!--association关联属性 property属性名 javaType属性类型 column在多的一方的表中的列名--&gt; &lt;association property=&quot;teacher&quot; column=&quot;tid&quot; javaType=&quot;Teacher&quot; select=&quot;getTeacher&quot;/&gt; &lt;/resultMap&gt; &lt;!-- 这里传递过来的id，只有一个属性的时候，下面可以写任何值 association中column多参数配置： column=&quot;{key=value,key=value}&quot; 其实就是键值对的形式，key是传给下个sql的取值名称，value是片段一中sql查询的字段名。 --&gt; &lt;select id=&quot;getTeacher&quot; resultType=&quot;teacher&quot;&gt; select * from teacher where id = #{id} &lt;/select&gt; &lt;/mapper&gt; 3、编写完毕去Mybatis配置文件中，注册Mapper！ 4、注意点说明： &lt;resultMap id=&quot;StudentTeacher&quot; type=&quot;Student&quot;&gt; &lt;!--association关联属性 property属性名 javaType属性类型 column在多的一方的表中的列名--&gt; &lt;association property=&quot;teacher&quot; column=&quot;{id=tid,name=tid}&quot; javaType=&quot;Teacher&quot; select=&quot;getTeacher&quot;/&gt; &lt;/resultMap&gt; &lt;!-- 这里传递过来的id，只有一个属性的时候，下面可以写任何值 association中column多参数配置： column=&quot;{key=value,key=value}&quot; 其实就是键值对的形式，key是传给下个sql的取值名称，value是片段一中sql查询的字段名。 --&gt; &lt;select id=&quot;getTeacher&quot; resultType=&quot;teacher&quot;&gt; select * from teacher where id = #{id} and name = #{name} &lt;/select&gt; 0.22.2. 按结果嵌套处理 1、接口方法编写 public List&lt;Student&gt; getStudents2(); 2、编写对应的mapper文件 &lt;!-- 按查询结果嵌套处理 思路： 1. 直接查询出结果，进行结果集的映射 --&gt; &lt;select id=&quot;getStudents2&quot; resultMap=&quot;StudentTeacher2&quot; &gt; select s.id sid, s.name sname , t.name tname from student s,teacher t where s.tid = t.id &lt;/select&gt; &lt;resultMap id=&quot;StudentTeacher2&quot; type=&quot;Student&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;sid&quot;/&gt; &lt;result property=&quot;name&quot; column=&quot;sname&quot;/&gt; &lt;!--关联对象property 关联对象在Student实体类中的属性--&gt; &lt;association property=&quot;teacher&quot; javaType=&quot;Teacher&quot;&gt; &lt;result property=&quot;name&quot; column=&quot;tname&quot;/&gt; &lt;/association&gt; &lt;/resultMap&gt; 0.23. 一对多 对多的理解： 一个老师拥有多个学生 如果对于老师这边，就是一个一对多的现象，即从一个老师下面拥有一群学生（集合）！ 0.23.1. 按结果嵌套处理 1、TeacherMapper接口编写方法 //获取指定老师，及老师下的所有学生 public Teacher getTeacher(int id); 2、编写接口对应的Mapper配置文件 &lt;mapper namespace=&quot;com.kuang.mapper.TeacherMapper&quot;&gt; &lt;!-- 思路: 1. 从学生表和老师表中查出学生id，学生姓名，老师姓名 2. 对查询出来的操作做结果集映射 1. 集合的话，使用collection！ JavaType和ofType都是用来指定对象类型的 JavaType是用来指定pojo中属性的类型 ofType指定的是映射到list集合属性中pojo的类型。 --&gt; &lt;select id=&quot;getTeacher&quot; resultMap=&quot;TeacherStudent&quot;&gt; select s.id sid, s.name sname , t.name tname, t.id tid from student s,teacher t where s.tid = t.id and t.id=#{id} &lt;/select&gt; &lt;resultMap id=&quot;TeacherStudent&quot; type=&quot;Teacher&quot;&gt; &lt;result property=&quot;name&quot; column=&quot;tname&quot;/&gt; &lt;collection property=&quot;students&quot; ofType=&quot;Student&quot;&gt; &lt;result property=&quot;id&quot; column=&quot;sid&quot; /&gt; &lt;result property=&quot;name&quot; column=&quot;sname&quot; /&gt; &lt;result property=&quot;tid&quot; column=&quot;tid&quot; /&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;/mapper&gt; 0.23.2. 按查询嵌套处理 1、TeacherMapper接口编写方法 public Teacher getTeacher2(int id); 2、编写接口对应的Mapper配置文件 &lt;select id=&quot;getTeacher2&quot; resultMap=&quot;TeacherStudent2&quot;&gt; select * from teacher where id = #{id} &lt;/select&gt; &lt;resultMap id=&quot;TeacherStudent2&quot; type=&quot;Teacher&quot;&gt; &lt;!--column是一对多的外键 , 写的是一的主键的列名--&gt; &lt;collection property=&quot;students&quot; javaType=&quot;ArrayList&quot; ofType=&quot;Student&quot; column=&quot;id&quot; select=&quot;getStudentByTeacherId&quot;/&gt; &lt;/resultMap&gt; &lt;select id=&quot;getStudentByTeacherId&quot; resultType=&quot;Student&quot;&gt; select * from student where tid = #{id} &lt;/select&gt; 小结 1、关联-association 2、集合-collection 3、所以association是用于一对一和多对一，而collection是用于一对多的关系 4、JavaType和ofType都是用来指定对象类型的 JavaType是用来指定pojo中属性的类型 ofType指定的是映射到list集合属性中pojo的类型。 0.24. 动态 SQL 动态SQL指的是根据不同的查询条件 , 生成不同的Sql语句. ------------------------------- - if - choose (when, otherwise) - trim (where, set) - foreach ------------------------------- 0.25. Dao接口的工作原理 Dao接口即Mapper接口。接口的全限名，就是映射文件中的namespace的值；接口的方法名，就是映射文件中Mapper的Statement的id值；接口方法内的参数，就是传递给sql的参数。 Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MapperStatement。在Mybatis中，每一个 、、、标签，都会被解析为一个MapperStatement对象。 举例来说：cn.mybatis.mappers.StudentDao.findStudentById，可以唯一找到namespace为 com.mybatis.mappers.StudentDao下面 id 为 findStudentById 的 MapperStatement。 Mapper接口里的方法，是不能重载的，因为是使用 全限名+方法名 的保存和寻找策略。Mapper 接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Mapper接口生成代理对象proxy，代理对象会拦截接口方法，转而执行MapperStatement所代表的sql，然后将sql执行结果返回。 0.26. 缓存 1、什么是缓存 [ Cache ]？ 存在内存中的临时数据。 将用户经常查询的数据放在缓存（内存）中，用户去查询数据就不用从磁盘上(关系型数据库数据文件)查询，从缓存中查询，从而提高查询效率，解决了高并发系统的性能问题。 2、为什么使用缓存？ 减少和数据库的交互次数，减少系统开销，提高系统效率。 3、什么样的数据能使用缓存？ 经常查询并且不经常改变的数据。 Mybatis的缓存实际上就是一个HashMap，key是真正执行的sql语句，value是缓存的结果。 0.26.1. 一级缓存 一级缓存也叫本地缓存： 与数据库同一次会话期间查询到的数据会放在本地缓存中。 以后如果需要获取相同的数据，直接从缓存中拿，没必须再去查询数据库； 一级缓存失效的四种情况： 一级缓存是SqlSession级别的缓存，是一直开启的，我们关闭不了它； 一级缓存失效情况：没有使用到当前的一级缓存，效果就是，还需要再向数据库中发起一次查询请求！ 1、sqlSession不同。每个sqlSession中的缓存相互独立 2、sqlSession相同，查询条件不同。当前缓存中，不存在这个数据 3、sqlSession相同，两次查询之间执行了增删改操作！因为增删改操作可能会对当前数据产生影响 4、sqlSession相同，手动清除一级缓存。session.clearCache(); 0.26.2. 二级缓存 二级缓存也叫全局缓存，一级缓存作用域太低了，所以诞生了二级缓存 基于namespace级别的缓存，一个名称空间，对应一个二级缓存； 工作机制 一个会话查询一条数据，这个数据就会被放在当前会话的一级缓存中； 如果当前会话关闭了，这个会话对应的一级缓存就没了；但是我们想要的是，会话关闭了，一级缓存中的数据被保存到二级缓存中； 新的会话查询信息，就可以从二级缓存中获取内容； 不同的mapper查出的数据会放在自己对应的缓存（map）中； 总结 只要开启了二级缓存，我们在同一个Mapper中的查询，可以在二级缓存中拿到数据 查出的数据都会被默认先放在一级缓存中 只有会话提交或者关闭以后，一级缓存中的数据才会转到二级缓存中 查询时顺序：二级 -&gt; 一级 -&gt; 数据库 对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。 0.27. 、#{}和${}的区别是什么？ 注：这道题是面试官面试我同事的。 答： ${}是 Properties 文件中的变量占位符，它可以用于标签属性值和 sql 内部，属于静态文本替换，比如${driver}会被静态替换为com.mysql.jdbc.Driver。 #{}是 sql 的参数占位符，MyBatis 会将 sql 中的#{}替换为?号，在 sql 执行前会使用 PreparedStatement 的参数设置方法，按序给 sql 的?号占位符设置参数值，比如 ps.setInt(0, parameterValue)，#{item.name} 的取值方式为使用反射从参数对象中获取 item 对象的 name 属性值，相当于 param.getItem().getName()。 0.28. 、Xml 映射文件中，除了常见的 select|insert|update|delete 标签之外，还有哪些标签？ 注：这道题是京东面试官面试我时问的。 答：还有很多其他的标签，&lt;resultMap&gt;、&lt;parameterMap&gt;、&lt;sql&gt;、&lt;include&gt;、&lt;selectKey&gt;，加上动态 sql 的 9 个标签，trim|where|set|foreach|if|choose|when|otherwise|bind等，其中为 sql 片段标签，通过&lt;include&gt;标签引入 sql 片段，&lt;selectKey&gt;为不支持自增的主键生成策略标签。 0.29. 、最佳实践中，通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应，请问，这个 Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？ 注：这道题也是京东面试官面试我被问的。 答：Dao 接口，就是人们常说的 Mapper接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中MappedStatement的 id 值，接口方法内的参数，就是传递给 sql 的参数。Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为 key 值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到 namespace 为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在 MyBatis 中，每一个&lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt;标签，都会被解析为一个MappedStatement对象。 Dao 接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。 Dao 接口里的方法可以重载，但是Mybatis的XML里面的ID不允许重复。 Mybatis版本3.3.0，亲测如下： /** * Mapper接口里面方法重载 */public interface StuMapper { List&lt;Student&gt; getAllStu(); List&lt;Student&gt; getAllStu(@Param(&quot;id&quot;) Integer id);} 然后在 StuMapper.xml 中利用Mybatis的动态sql就可以实现。 &lt;select id=&quot;getAllStu&quot; resultType=&quot;com.pojo.Student&quot;&gt; select * from student &lt;where&gt; &lt;if test=&quot;id != null&quot;&gt; id = #{id} &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 能正常运行，并能得到相应的结果，这样就实现了在Dao接口中写重载方法。 Mybatis 的 Dao 接口可以有多个重载方法，但是多个接口对应的映射必须只有一个，否则启动会报错。 相关 issue ：更正：Dao 接口里的方法可以重载，但是Mybatis的XML里面的ID不允许重复！。 Dao 接口的工作原理是 JDK 动态代理，MyBatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行MappedStatement所代表的 sql，然后将 sql 执行结果返回。 0.29.1. 补充： Dao接口方法可以重载，但是需要满足以下条件： 仅有一个无参方法和一个有参方法 多个有参方法时，参数数量必须一致。且使用相同的 @Param ，或者使用 param1 这种 测试如下： PersonDao.java Person queryById();Person queryById(@Param(&quot;id&quot;) Long id);Person queryById(@Param(&quot;id&quot;) Long id, @Param(&quot;name&quot;) String name); PersonMapper.xml &lt;select id=&quot;queryById&quot; resultMap=&quot;PersonMap&quot;&gt; select id, name, age, address from person &lt;where&gt; &lt;if test=&quot;id != null&quot;&gt; id = #{id} &lt;/if&gt; &lt;if test=&quot;name != null and name != ''&quot;&gt; name = #{name} &lt;/if&gt; &lt;/where&gt; limit 1&lt;/select&gt; org.apache.ibatis.scripting.xmltags.DynamicContext.ContextAccessor#getProperty方法用于获取&lt;if&gt;标签中的条件值 public Object getProperty(Map context, Object target, Object name) { Map map = (Map) target; Object result = map.get(name); if (map.containsKey(name) || result != null) { return result; } Object parameterObject = map.get(PARAMETER_OBJECT_KEY); if (parameterObject instanceof Map) { return ((Map)parameterObject).get(name); } return null;} parameterObject为map，存放的是Dao接口中参数相关信息。 ((Map)parameterObject).get(name)方法如下 public V get(Object key) { if (!super.containsKey(key)) { throw new BindingException(&quot;Parameter '&quot; + key + &quot;' not found. Available parameters are &quot; + keySet()); } return super.get(key);} queryById()方法执行时，parameterObject为null，getProperty方法返回null值，&lt;if&gt;标签获取的所有条件值都为null，所有条件不成立，动态sql可以正常执行。 queryById(1L)方法执行时，parameterObject为map，包含了id和param1两个key值。当获取&lt;if&gt;标签中name的属性值时，进入((Map)parameterObject).get(name)方法中，map中key不包含name，所以抛出异常。 queryById(1L,&quot;1&quot;)方法执行时，parameterObject中包含id,param1,name,param2四个key值，id和name属性都可以获取到，动态sql正常执行。 0.30. 、MyBatis 是如何进行分页的？分页插件的原理是什么？ 注：我出的。 答：(1) MyBatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页；(2) 可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，(3) 也可以使用分页插件来完成物理分页。 分页插件的基本原理是使用 MyBatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。 举例：select _ from student，拦截 sql 后重写为：select t._ from （select \\* from student）t limit 0，10 0.31. 、简述 MyBatis 的插件运行原理，以及如何编写一个插件。 注：我出的。 答：MyBatis 仅可以编写针对 ParameterHandler、ResultSetHandler、StatementHandler、Executor 这 4 种接口的插件，MyBatis 使用 JDK 的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 实现 MyBatis 的 Interceptor 接口并复写 intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。 0.32. 、MyBatis 执行批量插入，能返回数据库主键列表吗？ 注：我出的。 答：能，JDBC 都能，MyBatis 当然也能。 0.33. 、MyBatis 动态 sql 是做什么的？都有哪些动态 sql？能简述一下动态 sql 的执行原理不？ 注：我出的。 答：MyBatis 动态 sql 可以让我们在 Xml 映射文件内，以标签的形式编写动态 sql，完成逻辑判断和动态拼接 sql 的功能，MyBatis 提供了 9 种动态 sql 标签 trim|where|set|foreach|if|choose|when|otherwise|bind。 其执行原理为，使用 OGNL 从 sql 参数对象中计算表达式的值，根据表达式的值动态拼接 sql，以此来完成动态 sql 的功能。 0.34. 、MyBatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？ 注：我出的。 答：第一种是使用&lt;resultMap&gt;标签，逐一定义列名和对象属性名之间的映射关系。第二种是使用 sql 列的别名功能，将列别名书写为对象属性名，比如 T_NAME AS NAME，对象属性名一般是 name，小写，但是列名不区分大小写，MyBatis 会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成 T_NAME AS NaMe，MyBatis 一样可以正常工作。 有了列名与属性名的映射关系后，MyBatis 通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 0.35. 、MyBatis 能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别。 注：我出的。 答：能，MyBatis 不仅可以执行一对一、一对多的关联查询，还可以执行多对一，多对多的关联查询，多对一查询，其实就是一对一查询，只需要把 selectOne()修改为 selectList()即可；多对多查询，其实就是一对多查询，只需要把 selectOne()修改为 selectList()即可。 关联对象查询，有两种实现方式，一种是单独发送一个 sql 去查询关联对象，赋给主对象，然后返回主对象。另一种是使用嵌套查询，嵌套查询的含义为使用 join 查询，一部分列是 A 对象的属性值，另外一部分列是关联对象 B 的属性值，好处是只发一个 sql 查询，就可以把主对象和其关联对象查出来。 那么问题来了，join 查询出来 100 条记录，如何确定主对象是 5 个，而不是 100 个？其去重复的原理是&lt;resultMap&gt;标签内的&lt;id&gt;子标签，指定了唯一确定一条记录的 id 列，MyBatis 根据列值来完成 100 条记录的去重复功能，&lt;id&gt;可以有多个，代表了联合主键的语意。 同样主对象的关联对象，也是根据这个原理去重复的，尽管一般情况下，只有主对象会有重复记录，关联对象一般不会重复。 举例：下面 join 查询出来 6 条记录，一、二列是 Teacher 对象列，第三列为 Student 对象列，MyBatis 去重复处理后，结果为 1 个老师 6 个学生，而不是 6 个老师 6 个学生。 t_id t_name s_id 1 teacher 38 1 teacher 39 1 teacher 40 1 teacher 41 1 teacher 42 1 teacher 43 0.36. 、MyBatis 是否支持延迟加载？如果支持，它的实现原理是什么？ 注：我出的。 答：MyBatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 MyBatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。 它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName()方法的调用。这就是延迟加载的基本原理。 当然了，不光是 MyBatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。 0.37. 、MyBatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复？ 注：我出的。 答：不同的 Xml 映射文件，如果配置了 namespace，那么 id 可以重复；如果没有配置 namespace，那么 id 不能重复；毕竟 namespace 不是必须的，只是最佳实践而已。 原因就是 namespace+id 是作为 Map&lt;String, MappedStatement&gt;的 key 使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然也就不同。 0.38. 、MyBatis 中如何执行批处理？ 注：我出的。 答：使用 BatchExecutor 完成批处理。 0.39. 、MyBatis 都有哪些 Executor 执行器？它们之间的区别是什么？ 注：我出的 答：MyBatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。 **SimpleExecutor：**每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。 **ReuseExecutor：**执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map&lt;String, Statement&gt;内，供下一次使用。简言之，就是重复使用 Statement 对象。 **BatchExecutor：**执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。 作用范围：Executor 的这些特点，都严格限制在 SqlSession 生命周期范围内。 0.40. 、MyBatis 中如何指定使用哪一种 Executor 执行器？ 注：我出的 答：在 MyBatis 配置文件中，可以指定默认的 ExecutorType 执行器类型，也可以手动给 DefaultSqlSessionFactory 的创建 SqlSession 的方法传递 ExecutorType 类型参数。 0.41. 、MyBatis 是否可以映射 Enum 枚举类？ 注：我出的 答：MyBatis 可以映射枚举类，不单可以映射枚举类，MyBatis 可以映射任何对象到表的一列上。映射方式为自定义一个 TypeHandler，实现 TypeHandler 的 setParameter()和 getResult()接口方法。TypeHandler 有两个作用，一是完成从 javaType 至 jdbcType 的转换，二是完成 jdbcType 至 javaType 的转换，体现为 setParameter()和 getResult()两个方法，分别代表设置 sql 问号占位符参数和获取列查询结果。 0.42. 、MyBatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？ 注：我出的 答：虽然 MyBatis 解析 Xml 映射文件是按照顺序解析的，但是，被引用的 B 标签依然可以定义在任何地方，MyBatis 都可以正确识别。 原理是，MyBatis 解析 A 标签，发现 A 标签引用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时，MyBatis 会将 A 标签标记为未解析状态，然后继续解析余下的标签，包含 B 标签，待所有标签解析完毕，MyBatis 会重新解析那些被标记为未解析的标签，此时再解析 A 标签时，B 标签已经存在，A 标签也就可以正常解析完成了。 0.43. 、简述 MyBatis 的 Xml 映射文件和 MyBatis 内部数据结构之间的映射关系？ 注：我出的 答：MyBatis 将所有 Xml 配置信息都封装到 All-In-One 重量级对象 Configuration 内部。在 Xml 映射文件中，&lt;parameterMap&gt;标签会被解析为 ParameterMap 对象，其每个子元素会被解析为 ParameterMapping 对象。&lt;resultMap&gt;标签会被解析为 ResultMap 对象，其每个子元素会被解析为 ResultMapping 对象。每一个&lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt;标签均会被解析为 MappedStatement 对象，标签内的 sql 会被解析为 BoundSql 对象。 0.44. 、为什么说 MyBatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？ 注：我出的 答：Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而 MyBatis 在查询关联对象或关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自动 ORM 映射工具。 面试题看似都很简单，但是想要能正确回答上来，必定是研究过源码且深入的人，而不是仅会使用的人或者用的很熟的人，以上所有面试题及其答案所涉及的内容，在我的 MyBatis 系列博客中都有详细讲解和原理分析。 ","link":"https://memorykki.github.io/Mybatis/"},{"title":"SpringBoot","content":"Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。 SpringBoot 创建独立Spring应用 内嵌web服务器 自动starter依赖，简化构建配置 自动配置Spring以及第三方功能 提供生产级别的监控、健康检查及外部化配置 无代码生成、无需编写XML 避免大量的 Maven 导入和各种版本冲突。 @SpringBootApplication @SpringBootApplication == @SpringBootConfiguration + @EnableAutoConfiguration + @ComponentScan 主程序所在包及其下面的所有子包里面的组件都会被默认扫描进来 @Configuration 配置类本身也是组件 Full模式：配置类组件之间有依赖关系，方法会被调用得到之前单实例组件，用Full模式；proxyBeanMethods = true Lite模式：配置类组件之间无依赖关系用Lite模式加速容器启动过程，减少判断；proxyBeanMethods = false @Bean、@Component、@Controller、@Service、@Repository、@ComponentScan、@Import @Import：自动以类的NoArgsConstrutor创建实例放入IoC @Conditional @ConditionalOnMissingBean(name = &quot;tom&quot;)：满足指定条件则进行组件注入。 @ImportResource：引入原生的配置文件，比如beans.xml。 配置绑定 两种方式： @Component @ConfigurationProperties(prefix = &quot;mycar&quot;)声明在要绑定的类的上方 @EnableConfigurationProperties(Car.class)，开启对应类的配置绑定功能，把Car这个组件自动注入到容器中； 如果@ConfigurationProperties是在第三方包中，那么@component是不能注入到容器的。只有@EnableConfigurationProperties才可以注入到容器。 @ConfigurationProperties(prefix = &quot;mycar&quot;)：配置绑定。读取到properties文件中的内容，并且把它封装到JavaBean中。 自动配置原理 @SpringBootApplication == @SpringBootConfiguration + @EnableAutoConfiguration + @ComponentScan @SpringBootConfiguration 相当于@Configuration， 两者之间的唯一区别是@SpringBootConfiguration允许自动找到配置。 @EnableAutoConfiguration @EnableAutoConfiguration == @AutoConfigurationPackage + @Import(AutoConfigurationImportSelector.class) @AutoConfigurationPackage 使用@import将AutoConfigurationPackages包下的Registrar类作为组件导入到容器中，然后使用Registrar中的方法批量完成组件的注册。 @Import(AutoConfigurationImportSelector.class) 1、利用getAutoConfigurationEntry(annotationMetadata);给容器中批量导入一些组件 2、调用List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes)获取到所有需要导入到容器中的配置类 3、利用工厂加载 Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader)；得到所有的组件 4、**从META-INF/spring.factories位置来加载一个文件。** 默认扫描我们当前系统里面所有META-INF/spring.factories位置的文件 文件里面写死了spring-boot一启动就要给容器中加载的127个配置类 虽然我们127个场景的所有自动配置启动的时候默认全部加载。xxxxAutoConfiguration 按照条件装配规则（@Conditional），最终会按需配置。 SpringBoot默认会在底层配好所有的组件。但是如果用户自己配置了以用户的优先，约定大于配置。 @ConditionalOnMissingBean，如果没有存在这个bean，那么springboot就会自动帮你配置 总结 SpringBoot先加载所有的自动配置类 xxxxxAutoConfiguration 每个自动配置类按照条件进行生效，默认都会绑定配置文件指定的值。xxxxProperties里面拿。xxxProperties和配置文件进行了绑定 生效的配置类就会给容器中装配很多组件 只要容器中有这些组件，相当于这些功能就有了 定制化配置 用户直接自己@Bean替换底层的组件 用户去看这个组件是获取的配置文件什么值就去修改。 xxxxxAutoConfiguration ---&gt; 组件 ---&gt; xxxxProperties里面拿值 ----&gt; application.properties 静态资源配置原理 SpringBoot启动默认加载 xxxAutoConfiguration 类（自动配置类） SpringMVC功能的自动配置类 WebMvcAutoConfiguration，生效。 请求 -&gt; Controller -&gt; 静态资源处理器 -&gt; 404 配置文件的相关属性和xxx进行了绑定。WebMvcProperties - spring.mvc、ResourceProperties - spring.resources REST 表单提交会带上_method=PUT 请求过来被HiddenHttpMethodFilter拦截 请求是否正常，并且是POST 获取到_method的值。 兼容以下请求；PUT.DELETE.PATCH 原生request（post），包装模式requesWrapper重写了getMethod方法，返回的是传入的值。 过滤器链放行的时候用wrapper。以后的方法调用getMethod是调用requesWrapper的。 请求映射原理 1、处理http，重写servlet、doGet、doPost，实际调用doService 2、所有的请求映射都在HandlerMapping中。RequestMappingHandlerMapping：保存了所有@RequestMapping 和handler的映射规则。 3、请求进来，挨个尝试所有的HandlerMapping看是否有请求信息。 如果有就找到这个请求对应的handler 如果没有就是下一个 HandlerMapping 请求参数注解 @PathVariable、@RequestHeader、@ModelAttribute、@RequestParam、@MatrixVariable、@CookieValue、@RequestBody 参数处理原理 HandlerMapping中找到能处理请求的Handler 为当前Handler 找一个适配器 HandlerAdapter 适配器执行目标方法并确定方法参数的每一个值 参数解析器-HandlerMethodArgumentResolver SpringMVC目标方法能写多少种参数类型。取决于26个参数解析器。 ●当前解析器是否支持解析这种参数 ●支持就调用 resolveArgument 返回值处理器 15个遍历 handlerMapping中找到适合的handler(1/4) -&gt; 为Handler找一个HandlerAdapter -&gt; 参数处理（参数解析器）-&gt; 执行目标方法 -&gt; 返回值处理器 -&gt; 处理派发结果（将所有的数据都放在 ModelAndViewContainer；包含要去的页面地址View。还包含Model数据） POJO封装参数 1、判断参数是不是简单参数 2、创建空的实体类 3、绑定数据 4、类型转换器 -&gt; javaBean 响应JSON 返回值处理器判断是否支持这种类型返回值 supportsReturnType 返回值处理器调用 handleReturnValue 进行处理 RequestResponseBodyMethodProcessor 可以处理返回值标了@ResponseBody 注解的。 利用 MessageConverters 进行处理 将数据写为json 1、内容协商（浏览器默认会以请求头的方式告诉服务器他能接受什么样的内容类型）（Accept字段） 2、服务器最终根据自己自身的能力，决定服务器能生产出什么样内容类型的数据， 3、SpringMVC会挨个遍历所有容器底层的 HttpMessageConverter ，看谁能处理？ 1、得到MappingJackson2HttpMessageConverter可以将对象写为json 2、利用MappingJackson2HttpMessageConverter将对象转为json再写出去。 内容协商 根据客户端接收能力不同，返回不同媒体类型的数据。 1、判断当前响应头中是否已经有确定的媒体类型。MediaType 2、获取客户端（PostMan、浏览器）支持接收的内容类型。（获取客户端Accept请求头字段）【application/xml】 ○contentNegotiationManager 内容协商管理器 默认使用基于请求头的策略 3、遍历循环所有当前系统的 MessageConverter，看谁支持操作这个对象（Person） 4、找到支持操作Person的converter，把converter支持的媒体类型统计出来。 5、客户端需要【application/xml】。服务端能力【10种、json、xml】 6、进行内容协商的最佳匹配媒体类型 7、用 支持 将对象转为 最佳匹配媒体类型 的converter。调用它进行转化 。 视图解析 1、目标方法处理的过程中，所有数据都会被放在 ModelAndViewContainer 里面。包括数据和视图地址 2、方法的参数是一个自定义类型对象（从请求参数中确定的），把他重新放在 ModelAndViewContainer 3、任何目标方法执行完成以后都会返回 ModelAndView（数据和视图地址）。 4、processDispatchResult 处理派发结果（页面改如何响应） 1、render(mv, request, response); 进行页面渲染逻辑 ○1、根据方法的String返回值得到 View 对象【定义了页面的渲染逻辑】 1、所有的视图解析器尝试是否能根据当前返回值得到View对象 2、得到了 redirect:/main.html --&gt; Thymeleaf new RedirectView() 3、ContentNegotiationViewResolver 里面包含了下面所有的视图解析器，内部还是利用下面所有视图解析器得到视图对象。 4、view.render(mv.getModelInternal(), request, response); 视图对象调用自定义的render进行页面渲染工作 RedirectView 如何渲染【重定向到一个页面】 1、获取目标url地址 2、response.sendRedirect(encodedURL); 拦截器 implements HandlerInterceptor，重写preHandle、postHandle、afterCompletion。 1、根据当前请求，找到HandlerExecutionChain【可以处理请求的handler以及handler的所有 拦截器】 2、先来顺序执行 所有拦截器的 preHandle方法 1、如果当前拦截器prehandler返回为true。则执行下一个拦截器的preHandle 2、如果当前拦截器返回为false。直接倒序执行所有已经执行了的拦截器的 afterCompletion； 3、如果任何一个拦截器返回false。直接跳出不执行目标方法 4、所有拦截器都返回True。执行目标方法 5、倒序执行所有拦截器的postHandle方法 6、前面的步骤有任何异常都会直接倒序触发 afterCompletion 7、页面成功渲染完成以后，也会倒序触发 afterCompletion Spring Security 和 Shiro 由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，包括 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。当然这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。Shiro 和 Spring Security 相比，主要有如下一些特点： Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架 Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单 Spring Security 功能强大；Shiro 功能简单 跨域问题 跨域可以在前端通过 JSONP 来解决，但是 JSONP 只可以发送 GET 请求，无法发送其他类型的请求，在 RESTful 风格的应用中，就显得非常鸡肋，因此我们推荐在后端通过 （CORS，Cross-origin resource sharing） 来解决跨域问题。 在传统的 SSM 框架中，就可以通过 CORS 来解决跨域问题，只不过之前我们是在 XML 文件中配置 CORS ，现在可以通过实现WebMvcConfigurer接口然后重写addCorsMappings方法解决跨域问题。 starter 首先它提供了一个自动化配置类，一般命名为 XXXAutoConfiguration ，在这个配置类中通过条件注解来决定一个配置是否生效（条件注解就是 Spring 中原本就有的），然后它还会提供一系列的默认配置，也允许开发者根据实际情况自定义相关配置，然后通过类型安全的属性注入将这些配置属性注入进来，新注入的属性会代替掉默认属性。正因为如此，很多第三方框架，我们只需要引入依赖就可以直接使用了。当然，开发者也可以自定义 Starter spring-boot-starter-parent 我们都知道，新创建一个 Spring Boot 项目，默认都是有 parent 的，这个 parent 就是 spring-boot-starter-parent ，spring-boot-starter-parent 主要有如下作用： 定义了 Java 编译版本为 1.8 。 使用 UTF-8 格式编码。 继承自 spring-boot-dependencies，这个里边定义了依赖的版本，也正是因为继承了这个依赖，所以我们在写依赖时才不需要写版本号。 执行打包操作的配置。 自动化的资源过滤。 自动化的插件配置。 针对 application.properties 和 application.yml 的资源过滤，包括通过 profile 定义的不同环境的配置文件，例如 application-dev.properties 和 application-dev.yml。 Spring Boot 打成的 jar 和普通的 ja Spring Boot 项目最终打包成的 jar 是可执行 jar ，这种 jar 可以直接通过 java -jar xxx.jar 命令来运行，这种 jar 不可以作为普通的 jar 被其他项目依赖，即使依赖了也无法使用其中的类。 Spring Boot 的 jar 无法被其他项目依赖，主要还是他和普通 jar 的结构不同。普通的 jar 包，解压后直接就是包名，包里就是我们的代码，而 Spring Boot 打包成的可执行 jar 解压后，在 \\BOOT-INF\\classes 目录下才是我们的代码，因此无法被直接引用。如果非要引用，可以在 pom.xml 文件中增加配置，将 Spring Boot 项目打包成两个 jar ，一个可执行，一个可引用。 ","link":"https://memorykki.github.io/SpringBoot/"},{"title":"SpringMVC","content":"Spring MVC 是 Spring 提供的一个基于 MVC 设计模式的轻量级 Web 开发框架,本质上相当于 Servlet。 SpringMVC Spring MVC是一个基于Java的实现了MVC设计模式的请求驱动类型的轻量级Web框架，通过把模型-视图-控制器分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。 核心组件 前端控制器 DispatcherServlet（不需要程序员开发）：处理所有的HTTP请求和响应，减少了其它组件之间的耦合度。 处理器映射器HandlerMapping（不需要程序员开发）：根据请求的URL来查找Handler 处理器适配器HandlerAdapter 处理器Handler（需要程序员开发） 视图解析器 ViewResolver（不需要程序员开发）：进行视图的解析，根据视图逻辑名解析成真正的视图（view） 视图View（需要程序员开发jsp）：View是一个接口， 它的实现类支持不同的视图类型（jsp，freemarker，pdf等等） 控制器 解析用户输入并将其转换为一个由视图呈现给用户的模型。Spring用一个非常抽象的方式实现了一个控制层，允许用户创建多种用途的控制器。 单例模式，所以在多线程访问的时候有线程安全问题，同步会影响性能，解决方案是在控制器里面不能写字段。 工作原理 request -&gt; DispatcherServlet -&gt; HandlerMapping -&gt; HandlerAdapter -&gt; Handler -&gt; ModelAndView -&gt; ViewResolver -&gt; View -&gt; reponse （1）用户发送请求至前端控制器DispatcherServlet； （2） DispatcherServlet收到请求后，调用HandlerMapping处理器映射器，请求获取Handle； （3）处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet； （4）DispatcherServlet 调用 HandlerAdapter处理器适配器； （5）HandlerAdapter 经过适配调用 具体处理器(Handler，也叫后端控制器)； （6）Handler执行完成返回ModelAndView； （7）HandlerAdapter将Handler执行结果ModelAndView返回给DispatcherServlet； （8）DispatcherServlet将ModelAndView传给ViewResolver视图解析器进行解析； （9）ViewResolver解析后返回具体View； （10）DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中） （11）DispatcherServlet响应用户。 MVC设计模式 模型（model）- 视图（view）- 控制器（controller），三层架构的设计模式。用于实现前端页面的展现与后端业务数据处理的分离。 分层设计，实现了业务系统各个组件之间的解耦，有利于业务系统的可扩展性，可维护性。 有利于系统的并行开发，提升开发效率。 注解 @RequestMapping：用于处理请求 url 映射的注解，可用于类或方法上，用于类上表示类父路径。 @RequestBody：接收http请求的json数据，将json转换为java对象。 @ResponseBody：将controller方法返回对象转化为json对象响应给客户。 @RestController == @ResponseBody ＋ @Controller @Controller 控制器Controller 负责处理由DispatcherServlet 分发的请求，它把用户请求的数据经过业务处理层处理之后封装成一个Model ，然后再把该Model 返回给对应的View 进行展示。 @Controller 只是定义了一个控制器类，而使用@RequestMapping 注解的方法才是真正处理请求的处理器。 @PathVariable @RequestParam @PathVariable来获取 @RequestMapping(value = “/page/{id}”, method = RequestMethod.GET) @RequestParam用来获得静态的URL请求入参 spring注解时action里用到。 Spring MVC与Struts2 相同点 都是基于mvc的表现层框架，都用于web项目的开发。 不同点 前端控制器不一样。Spring MVC的前端控制器是servlet：DispatcherServlet。struts2的前端控制器是filter：StrutsPreparedAndExcutorFilter。 请求参数的接收方式不一样。Spring MVC是使用方法的形参接收请求的参数，基于方法的开发，线程安全，可以设计为单例或者多例的开发，推荐使用单例模式的开发（执行效率更高），默认就是单例开发模式。struts2是通过类的成员变量接收请求的参数，是基于类的开发，线程不安全，只能设计为多例的开发。 Struts采用值栈存储请求和响应的数据，通过OGNL存取数据，Spring MVC通过参数解析器是将request请求内容解析，并给方法形参赋值，将数据和视图封装成ModelAndView对象，最后又将ModelAndView中的模型数据通过reques域传输到页面。Jsp视图解析器默认使用jstl。 重定向和转发 转发：在返回值前面加&quot;forward:&quot;，譬如&quot;forward:user.do?name=method4&quot; 重定向：在返回值前面加&quot;redirect:&quot;，譬如&quot;redirect:http://www.baidu.com&quot; 中文乱码 POST 在web.xml中配置一个CharacterEncodingFilter过滤器，设置成utf-8； &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; GET 修改tomcat配置文件添加编码与工程编码一致 &lt;ConnectorURIEncoding=&quot;utf-8&quot; connectionTimeout=&quot;20000&quot; port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; redirectPort=&quot;8443&quot;/&gt; 对参数进行重新编码： String userName = new String(request.getParamter(“userName”).getBytes(“ISO8859-1”),“utf-8”) 异常处理 可以将异常抛给Spring框架，由Spring框架来处理；我们只需要配置简单的异常处理器，在异常处理器中添视图页面即可。 拦截GET方式提交的方法 可以在@RequestMapping注解里面加上method=RequestMethod.GET。 获取 Request、Session 直接在方法的形参中声明request，Spring MVC就自动把request对象传入。 拦截的方法里面得到入参 直接在形参里面声明这个参数就可以,但必须名字和传过来的参数一样。 接收对象属性的参数 直接在方法中声明这个对象，Spring MVC就自动会把属性赋值到这个对象里面。 后台向前台传递数据 通过ModelMap对象,可以在这个对象里面调用put方法,把对象加到里面,前台就可以通过el表达式拿到。 ModelMap数据放入Session 可以在类上面加上@SessionAttributes注解，里面包含的字符串就是要放入session里面的key。 拦截器 Spring MVC中的拦截器（Interceptor）类似于Servlet中的过滤器（Filter），它主要用于拦截用户请求并作相应的处理。例如通过拦截器可以进行权限验证、记录请求信息的日志、判断用户是否登录等。 通过实现HandlerInterceptor接口； 实现 preHandle（之前）、postHandle（之后）、afterCompletion（该方法会在整个请求完成，即视图渲染结束之后执行。可以通过此方法实现一些资源清理、记录日志信息等工作。） 通过实现WebRequestInterceptor接口。 单个拦截器 多个拦截器 preHandle()方法会按照配置文件中拦截器的配置顺序执行，postHandle()方法和afterCompletion()方法则会按照配置顺序的反序执行。 WebApplicationContext WebApplicationContext 继承了ApplicationContext 并增加了一些WEB应用必备的特有功能，它不同于一般的ApplicationContext ，因为它能处理主题，并找到被关联的servlet。 ","link":"https://memorykki.github.io/SpringMVC/"},{"title":"Spring","content":"Spring 是最受欢迎的企业级 Java 应用程序开发框架，数以百万的来自世界各地的开发人员使用 Spring 框架来创建性能好、易于测试、可重用的代码。 spring 基于POJO的轻量级和最小侵入性编程； 通过依赖注入和面向接口实现松耦合； 基于切面和惯例进行声明式编程； 通过切面和模板减少样板式代码。 优缺点 方便解耦，简化开发 Spring就是一个大工厂，可以将所有对象的创建和依赖关系的维护，交给Spring管理。 AOP编程的支持 Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能。 声明式事务的支持 只需要通过配置就可以完成对事务的管理，而无需手动编程。 方便程序的测试 Spring对Junit4支持，可以通过注解方便的测试Spring程序。 方便集成各种优秀框架 Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架的直接支持（如：Struts、Hibernate、MyBatis等）。 降低JavaEE API的使用难度 Spring对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低。 Spring依赖反射，反射影响性能 spring模块 &lt;img src=&quot;https://img-blog.csdnimg.cn/2019102923475419.png) 设计模式 工厂模式：BeanFactory就是简单工厂模式的体现，用来创建对象的实例； 单例模式：Bean默认为单例模式。 代理模式：Spring的AOP功能用到了JDK的动态代理和CGLIB字节码生成技术； 模板方法：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。 观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知被制动更新，如Spring中listener的实现–ApplicationListener。 spring context 提供spring 框架的基础功能，BeanFactory 是 任何以spring为基础的应用的核心。 Bean 工厂是工厂模式的一个实现，提供了控制反转功能，用来把应用的配置和依赖从真正的应用代码中分离。最常用的就是XmlBeanFactory ，它根据XML文件中的定义加载beans。该容器从XML 文件读取配置元数据并用它去创建一个完全配置的系统或应用。 事件 上下文更新事件：在调用ConfigurableApplicationContext 接口中的refresh()方法时被触发。 上下文开始事件：当容器调用ConfigurableApplicationContext的Start()方法开始/重新开始容器时触发该事件。 上下文停止事件：当容器调用ConfigurableApplicationContext的Stop()方法停止容器时触发该事件。 上下文关闭事件：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。 请求处理事件：在Web应用中，当一个http请求（request）结束触发该事件。如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。 IOC 控制反转把传统上由程序代码直接操控的对象的调用权交给容器，通过容器来实现对象组件的装配和管理。所谓的“控制反转”概念就是对组件对象控制权的转移，从程序代码本身转移到了外部容器。 Spring IOC 负责创建对象，管理对象（通过依赖注入（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。 DI是IOC的一个功能 优点 代码量降到最低。 最小的代价和最小的侵入性使松散耦合得以实现。 支持加载服务时的饿汉式初始化和懒加载。 实现原理：工厂模式+反射 ApplicationContext的实现 FileSystemXmlApplicationContext ：一个XML文件中加载beans的定义，XML Bean 配置文件的全路径名必须提供给它的构造函数。 ClassPathXmlApplicationContext：一个XML文件中加载beans的定义，你需要正确设置classpath因为这个容器将在classpath里找bean配置。 WebXmlApplicationContext：一个XML文件定义了一个WEB应用的所有bean。 BeanFactory 和 ApplicationContext BeanFactory和ApplicationContext是Spring的两大核心接口，都可以当做Spring的容器。其中ApplicationContext是BeanFactory的子接口。 BeanFactory：是Spring里面最底层的接口，包含了各种Bean的定义，读取bean配置文档，管理bean的加载、实例化，控制bean的生命周期，维护bean之间的依赖关系。 ApplicationContext接口作为BeanFactory的派生，除了提供BeanFactory所具有的功能外，还提供了更完整的框架功能： 继承MessageSource，因此支持国际化。 统一的资源文件访问方式。 提供在监听器中注册bean的事件。 同时加载多个配置文件。 加载方式 BeanFactroy采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化。这样，我们就不能发现一些存在的Spring的配置问题。如果Bean的某一个属性没有注入，BeanFacotry加载后，直至第一次使用调用getBean方法才会抛出异常。 ApplicationContext，它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误，这样有利于检查所依赖属性是否注入。 ApplicationContext启动后预载入所有的单实例Bean，通过预载入单实例bean ,确保当你需要的时候，你就不用等待，因为它们已经创建好了。 相对于基本的BeanFactory，ApplicationContext 唯一的不足是占用内存空间。当应用程序配置Bean较多时，程序启动较慢。 创建方式 BeanFactory通常以编程的方式被创建，ApplicationContext还能以声明的方式创建，如使用ContextLoader。 注册方式 BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。 低级容器 BeanFactory 加载配置文件，解析成 BeanDefinition 放在 Map 里。 调用 getBean 的时候，从 BeanDefinition 所属的 Map 里，拿出 Class 对象进行实例化，同时，如果有依赖关系，将递归调用 getBean 方法 —— 完成依赖注入。 高级容器 ApplicationContext 他包含了低级容器的功能，当他执行 refresh 模板方法的时候，将刷新整个容器的 Bean。同时其作为高级容器，包含了太多的功能。一句话，他不仅仅是 IoC。他支持不同信息源头，支持 BeanFactory 工具类，支持层级容器，支持访问文件资源，支持事件发布通知，支持接口回调等等。 DI IoC是一个很大的概念，可以用不同的方式来实现：依赖注入和依赖查找 依赖注入：组件之间的依赖关系由容器在应用系统运行期来决定，也就是由容器动态地将某种依赖关系的目标对象实例注入到应用系统中的各个关联的组件之中。 让容器全权负责依赖查询，受管组件只需要暴露JavaBean的setter方法或者带参数的构造器或者接口，使容器可以在初始化时组装对象的依赖关系。 DI 实现 接口注入由于在灵活性和易用性比较差，现在从Spring4开始已被废弃。 构造器依赖注入：构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖。 Setter方法注入：Setter方法注入是容器通过调用无参构造器或无参static工厂 方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。 构造函数注入 setter 注入 没有部分注入 有部分注入 不会覆盖 setter 属性 会覆盖 setter 属性 任意修改都会创建一个新实例 任意修改不会创建一个新实例 适用于设置很多属性 适用于设置少量属性 最好的解决方案是用构造器参数实现强制依赖，setter方法实现可选依赖。 bean属性赋值（创建bean） setter constructor 索引值index指定参数位置 类型不同区分重载的构造器 工厂 静态工厂：将对象创建的过程封装到静态方法中。当客户端需要对象时，只需要简单地调用静态方法，而不用关心创建对象的细节。 实例工厂：将对象的创建过程封装到另外一个对象实例的方法里。当客户端需要请求对象时，只需要简单的调用该实例方法而不需要关心对象的创建细节。 FactoryBean 工厂bean：跟普通bean不同，其返回的对象不是指定类的一个实例，其返回的是该工厂bean的getObject方法所返回的对象。 作用域scope singleton : bean在每个Spring ioc 容器中只有一个实例。 prototype：一个bean的定义可以有多个实例。 request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。 session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 注意： 缺省的Spring bean 的作用域是Singleton。使用 prototype 作用域需要慎重的思考，因为频繁创建和销毁 bean 会带来很大的性能开销。 bean线程安全 Spring框架中的单例bean不是线程安全的。 spring 中的 bean 默认是单例模式，spring 框架并没有对单例 bean 进行多线程的封装处理。 实际上大部分时候 spring bean 无状态的（比如 dao 类），所有某种程度上来说 bean 也是安全的，但如果 bean 有状态的话，那就要开发者自己去保证线程安全了，最简单的就是改变 bean 的作用域，把“singleton”变更为“prototype”，这样请求 bean 相当于 new Bean()了，所以就可以保证线程安全了。 有状态就是有数据存储功能，无状态就是不会保存数据。 线程并发问题 在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域，因为Spring对一些Bean中非线程安全状态采用ThreadLocal进行处理，解决线程安全问题。 解决多线程中相同变量的访问冲突问题： 同步机制采用了“时间换空间”的方式，仅提供一份变量，不同的线程在访问前需要获取锁，没获得锁的线程则需要排队。 ThreadLocal采用了“空间换时间”的方式。 ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。 bean生命周期 在传统的Java应用中，bean的生命周期很简单。使用Java关键字new进行bean实例化，然后该bean就可以使用了。一旦该bean不再被使用，则由Java自动进行垃圾回收。 ①Spring IOC容器可以管理bean的生命周期，Spring允许在bean生命周期内特定的时间点执行指定的任务。 ②Spring IOC容器对bean的生命周期进行管理的过程： ​ [1]通过构造器或工厂方法创建bean实例 ​ [2]为bean的属性设置值和对其他bean的引用 ​ [3]调用bean的初始化方法 ​ [4]bean可以使用了 ​ [5]当容器关闭时，调用bean的销毁方法 ③在配置bean时，通过init-method和destroy-method 属性为bean指定初始化和销毁方法 ④bean的后置处理器 ​ [1]bean后置处理器允许在调用初始化方法前后对bean进行额外的处理 ​ [2]bean后置处理器对IOC容器里的所有bean实例逐一处理，而非单一实例。其典型应用是：检查bean属性的正确性或根据特定的标准更改bean的属性。 ​ [3] bean后置处理器时需要实现接口：org.springframework.beans.factory.config.BeanPostProcessor。在初始化方法被调用前后，Spring将把每个bean实例分别传递给上述接口的以下两个方法： ​ postProcessBeforeInitialization(Object, String) ​ postProcessAfterInitialization(Object, String) ⑤添加bean后置处理器后bean的生命周期 ​ [1]通过构造器或工厂方法创建bean实例 ​ [2]为bean的属性设置值和对其他bean的引用 ​ [3]将bean实例传递给bean后置处理器的**postProcessBeforeInitialization()**方法 ​ [4]调用bean的初始化方法 ​ [5]将bean实例传递给bean后置处理器的**postProcessAfterInitialization()**方法 ​ [6]bean可以使用了 ​ [7]当容器关闭时调用bean的销毁方法 bean生命周期方法 setup：在容器加载bean的时候被调用。 teardown：在容器卸载类的时候被调用。 bean 标签有两个重要的属性（ init-method 和 destroy-method ）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct和@PreDestroy）。 内部bean 当一个bean仅被用作另一个bean的属性时，它能被声明为一个内部bean。 内部bean可以用setter注入“属性”和构造方法注入“构造参数”的方式来实现，内部bean通常是匿名的，它们的Scope一般是prototype。 注入集合 ，、、 自动装配 手动装配：以value或ref的方式明确指定属性值都是手动装配。 自动装配：根据指定的装配规则，不需要明确指定，Spring自动将匹配的属性值注入bean中。 方式： no：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean。 byName：通过bean的名称进行自动装配，如果一个bean的 property 与另一bean 的name 相同，就进行自动装配。 byType：通过参数的数据类型进行自动装配。 constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。 autodetect：自动探测，若有构造方法则construct，否则 byType。 @Autowired @Autowired可用于：构造函数、属性、Setter方法 使用之前需要在Spring配置文件进行配置，&lt;context:annotation-config /&gt;。 在启动spring IoC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowied、@Resource或@Inject时，就会在IoC容器自动查找需要的bean，并装配给该对象的属性。 在使用@Autowired时，首先在容器中查询对应类型的bean： 如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据； 如果查询的结果不止一个，那么@Autowired会根据名称来查找； 如果上述查找的结果为空，那么会抛出异常。解决方法时，使用required=false。 注解 @Component：这将 java 类标记为 bean，通用构造型。 @Controller：标记为 Spring Web MVC 控制器。 @Service：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。 @Repository：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IoC 容器，并使未经检查的异常有资格转换为 Spring DataAccessException。 @Required：表明bean的属性必须在配置的时候设置，通过一个bean定义的显式的属性值或通过自动装配，未配置未被设置，容器将抛出异常； @Autowired和@Resource @Autowired：默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。 @Resource：默认是按照名称来装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入。 @Inject和：按类型注入匹配的bean，但没有reqired属性。 @Qualifier 当您创建多个相同类型的 bean 并希望仅使用属性装配其中一个 bean 时，您可以使用@Qualifier 注解和 @Autowired 通过指定应该装配哪个确切的 bean 来消除歧义。 JdbcTemplate JdbcTemplate 类提供了很多便利的方法解决诸如把数据库数据转变成基本数据类型或对象，执行写好的或可调用的数据库操作语句，提供自定义的数据错误处理。 事务管理类型 编程式事务管理：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。 声明式事务管理：这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。 使用原生的JDBC API实现事务管理是所有事务管理方式的基石，同时也是最典型的编程式事务管理。编程式事务管理需要将事务管理代码嵌入到业务方法中来控制事务的提交和回滚。在使用编程的方式管理事务时，必须在每个事务操作中包含额外的事务管理代码。相对于核心业务而言，事务管理的代码显然属于非核心业务，如果多个模块都使用同样模式的代码进行事务管理，显然会造成较大程度的代码冗余。 大多数情况下声明式事务比编程式事务管理更好：它将事务管理代码从业务方法中分离出来，以声明的方式来实现事务管理。唯一不足地方是，最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。 事务管理代码的固定模式作为一种横切关注点，可以通过AOP方法模块化，进而借助Spring AOP框架实现声明式事务管理。 实现原理 Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。 事务传播行为 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。 Spring定义了7种类传播行为。 事务传播属性可以在@Transactional注解的propagation属性中定义。 事务隔离级别可以在@Transactional注解的isolation属性中定义。 脏读 不可重复读 幻读 READ UNCOMMITTED 有 有 有 READ COMMITTED 无 有 有 REPEATABLE READ 无 无 有 SERIALIZABLE 无 无 无 AOP 一般称为面向切面编程，作为面向对象的一种补充，用于将那些与业务无关，但却对多个对象产生影响的公共行为和逻辑，抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。可用于权限认证、日志、事务处理等。 AspectJ AOP AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。 静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。 AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。 Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。 静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。 JDK动态代理和CGLIB动态代理的区别 Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理： JDK动态代理只提供接口的代理，不支持类的代理。核心InvocationHandler接口和Proxy类，InvocationHandler 通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy利用 InvocationHandler动态创建一个符合某一接口的的实例, 生成目标类的代理对象。 如果代理类没有实现 InvocationHandler 接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。 CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。 InvocationHandler 的 invoke(Object proxy,Method method,Object[] args)：proxy是最终生成的代理实例; method 是被代理目标实例的某个具体方法; args 是被代理目标实例某个方法的具体入参, 在方法反射调用时使用。 几个名词 &lt;![](https://memorykki.github.io/post-images/Spring/aop.png&quot; style=&quot;zoom: 50%;&quot; /&gt; 横切关注点：从每个方法中抽取出来的同一类非核心业务。 切面（Aspect）：封装横切关注点信息的类，每个关注点体现为一个通知方法。 通知（Advice)：切面必须要完成的各个具体工作。 连接点（Joinpoint)：横切关注点在程序代码中的具体体现，对应程序执行的某个特定位置。例如：类某个方法调用前、调用后、方法捕获到异常后等。在应用程序中可以使用横纵两个坐标来定位一个具体的连接点。 切入点（pointcut)：定位连接点的方式。每个类的方法中都包含多个连接点，所以连接点是类中客观存在的事物。如果把连接点看作数据库中的记录，那么切入点就是查询条件——AOP可以通过切入点定位到特定的连接点。切点通过Pointcut 接口进行描述，它使用类和方法作为连接点的查询条件。 目标（Target)：被通知的对象 代理（Proxy)：向目标对象应用通知之后创建的代理对象 织入（Weaving）：织入是把切面应用到目标对象并创建新的代理对象的过程。在目标对象的生命周期里有多少个点可以进行织入： 编译期：切面在目标类编译时被织入。AspectJ的织入编译器是以这种方式织入切面的。java -&gt; class 类加载期：切面在目标类加载到JVM时被织入。需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ5的加载时织入就支持以这种方式织入切面。class -&gt; 内存 运行期：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。SpringAOP就是以这种方式织入切面。 关注点和横切关注点 关注点（concern）是应用中一个模块的行为，一个关注点可能会被定义成一个我们想实现的一个功能。 横切关注点（cross-cutting concern）是一个关注点，此关注点是整个应用都会使用的功能，并影响整个应用，比如日志，安全和数据传输，几乎应用的每个模块都需要的功能。因此这些都属于横切关注点。 运行时通知对象 通过在代理类中包裹切面，Spring在运行期把切面织入到Spring管理的bean中。代理封装了目标类，并拦截被通知方法的调用，再把调用转发给真正的目标bean。当代理拦截到方法调用时，在调用目标bean方法之前，会执行切面逻辑。 直到应用需要被代理的bean时，Spring才创建代理对象。如果使用的是ApplicationContext的话，在ApplicationContext从BeanFactory中加载所有bean的时候，Spring才会创建被代理的对象。因为Spring运行时才创建代理对象，所以我们不需要特殊的编译器来织入SpringAOP的切面。 Spring只支持方法级别的连接点 因为Spring基于动态代理，所以Spring只支持方法连接点。**Spring缺少对字段连接点的支持，而且它不支持构造器连接点。**方法之外的连接点拦截功能，我们可以利用Aspect来补充。 通知类型 前置通知（Before）：调用之前 后置通知（After）：方法完成之后 返回通知（After-returning ）：方法成功执行之后 异常通知（After-throwing）：方法抛出异常后 环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。 环绕通知是所有通知类型中功能最为强大的，能够全面地控制连接点，甚至可以控制是否执行连接点。 明确调用ProceedingJoinPoint的proceed()方法来执行被代理的方法。如果忘记这样做就会导致通知被执行了，但目标方法没有被执行。 同一个aspect，不同advice的执行顺序 around 优先于其他 没有异常 around before before target method 执行 around after after afterReturning 有异常 around before before target method 执行 around after after afterThrowing:异常发生 java.lang.RuntimeException: 异常发生 切面 Aspect aspect 由 pointcount 和 advice 组成，切面是通知和切点的结合。 它既包含了横切逻辑的定义, 也包括了连接点的定义. Spring AOP 就是负责实施切面的框架, 它将切面所定义的横切逻辑编织到切面所指定的连接点中. AOP 的工作重心在于如何将增强编织目标对象的连接点上, 这里包含两个工作: 如何通过 pointcut 和 advice 定位到特定的 joinpoint 上 如何在 advice 中编写切面代码. 可以简单地认为, 使用 @Aspect 注解的类就是切面. ","link":"https://memorykki.github.io/Spring/"},{"title":"操作系统笔记","content":"概述、启动中断异常和系统调用、连续内存分配、非连续内存分配、虚拟内存、页面置换算法、进程、处理器调度、同步互斥问题、死锁问题。 5.5万字，252张图，150页，超长总结。 目录 1. 操作系统概述 1.1. 操作系统的启动 1.2. 操作系统的结构 2. 启动、中断、异常和系统调用 2.1. 操作系统的启动 2.2. 中断、异常、和系统调用 3. 连续内存分配 3.1. 计算机体系结构及内存分层体系 3.2. 连续内存分配：压缩式与交换式碎片整理 4. 非连续内存分配 4.1. 分段 4.2. 分页 4.3. 页表、TLB 4.4. 多级页表 4.5. 反向页表 5. 虚拟内存 5.1. 起因 5.2. 覆盖技术 5.3. 交换技术 5.4. 虚存技术 6. 页面置换算法 6.1. 最优页面置换算法 6.2. 先进先出算法 6.3. 最近最久未使用算法 6.4. 时钟页面置换算法 6.5. 二次机会法 6.6. 最不常用算法 6.7. BELADY现象、比较 6.8. 问题、工作集模型 6.9. 缺页率页面置换算法 6.10. 抖动问题 7. 进程 7.1. 进程的定义 7.2. 进程的组成 7.3. 进程的特点 7.4. 进程控制结构 7.5. 进程的生命期管理 7.6. 进程状态变化模型 7.7. 进程的挂起 7.8. 为什么使用线程 7.9. 什么是线程 7.10. 线程的实现 7.11. 上下文切换 7.12. 进程控制—创建进程 7.13. 进程控制—加载和执行过程 7.14. 进程控制—等待和终止进程 8. 处理器调度 8.1. 背景 8.2. 调度原则 8.3. 调度算法 8.4. 实时调度 9. 同步互斥问题 9.1. 背景知识 9.2. 一些概念PART1 9.3. 一些概念PART2 9.4. 一些概念PART3 9.5. 临界区 9.6. 禁用硬件中断 9.7. 基于软件的解决方案 9.8. 更高级的抽象 — 基于原子操作 10. 信号量与管程 10.1. 背景 10.2. 信号量 10.3. 信号量的使用 10.4. 信号量的实现 10.5. 管程 10.6. 经典同步问题1----读者优先读写者问题 10.7. 经典同步问题1—写者优先读写者问题 10.8. 经典同步问题2—哲学家就餐问题 11. 死锁 11.1. 死锁问题 11.2. 系统模型 11.3. 死锁的特征 11.4. 死锁处理办法 11.5. 死锁预防和死锁避免 11.6. 死锁检测和死锁恢复 11.7. IPC概述 11.8. 信号，管道，消息队列和共享内存 12. 面试总结 12.1. 用户态和内核态 12.2. 缓冲区溢出 12.3. 中断与轮询 12.4. 临界区 12.5. 进程和线程的区别 12.6. 程序与进程的区别 12.7. 进程和线程的上下文切换代价 12.8. 进程同步的原则 12.9. 进程同步 12.9.1. 硬件同步机制 12.9.2. 信号量同步机制 12.9.3. 管程机制 12.10. 进程通信 12.11. 线程同步的方式 12.12. 死锁 12.12.1. 死锁的概念 12.12.2. 死锁产生的四个必要条件 12.12.3. 死锁的处理 12.13. 进程状态 12.14. 线程状态 12.15. 进程调度 12.16. 虚拟内存 12.17. 分页和分段 12.18. 页面置换算法 12.19. 局部性原理 13. 参考链接 1. 操作系统概述 1.1. 操作系统的启动 用户角度：操作系统是一个控制软件 管理应用程序 为应用程序提供服务 杀死应用程序 资源管理 管理外设/分配资源 在操作系统下，进程&lt;-&gt;CPU, 文件&lt;-&gt;磁盘，地址空间&lt;-&gt;内存。 操作系统的架构层次：硬件之上，应用软件之下(为应用软件提供服务支持)。 Linux，Windows界面属于外壳shell(与User交互)，而不是内核kernel，而kernel是研究重点，在shell之下。 Kernel包括： CPU调度器 物理内存管理 虚拟内存管理 文件系统管理 中断处理和IO设备驱动 (底层硬件) OS Kernel的特征： 并发(指一段时间内多个程序运行；而并行是指一个时间点上多个程序运行，要求多个CPU):计算机系统中同时存在多个运行的程序，需要OS管理和调度 共享：“同时”访问 或 互斥共享 虚拟：利用多道程序设计技术，让每一个用户都觉得有一个计算机专门为他服务 异步：程序的执行不是一步到底的，而是走走停停，向前推进的速度不可预知 但只要运行环境相同，OS要保证程序运行的结果也相同 1.2. 操作系统的结构 简单的操作系统：MS-DOS 不分模块的单体内核 (内部通过函数调用访问,缺点，复杂，紧耦合，易受攻击) 微内核，尽可能把内核功能移植到用户空间，缺点性能低。 外核，内核分为一块，一块负责和硬件打交道，另一部分和应用打交道。 虚拟机,VMs(虚拟机)-&gt;VMM(虚拟机监视器)-&gt;物理机硬件，多操作系统共享硬件资源。 2. 启动、中断、异常和系统调用 2.1. 操作系统的启动 CPU, I/O, 内存通过总线连接。 DISK:存放OS； BIOS：基本I/O处理系统( basic I/O system); Bootloader: 加载OS到内存中。 当电脑通电时，段寄存器CS和指令寄存器IP能够确定一个内存地址，例如CS:IP = 0xf000:fff0. POST(加电自检)，寻找显卡和执行BIOS。(显示器，键盘…是否正常)。 步骤： BIOS: 将Bootloader从磁盘的磁盘的引导扇区(512字节)加载到0x7c00；跳转到CS:IP=0000:7c00的内存区域(以便下一步) Bootloader：将操作系统的代码和数据从硬盘加载到内存中；跳转到操作系统的起始地址。 系统调用：(来源于应用程序)应用程序主动向操作系统发出服务请求。 异常：(来源于不良的应用程序)非法指令或其它花的处理状态(e.g.内存出错)。 中断：(来源于外设)来自不同的硬件设备的计时器和网络的中断。 为什么应用程序不能直接访问硬件而是通过操作系统？ 计算机运行时，内核是被信任的第三方。 只有内核可以执行特权指令。 为了方便应用程序。 讨论的问题：操作系统如何设计和实现中断/异常和系统调用；他们三者的区别和特点。 产生的源头 中断：外设(键盘/鼠标/网卡/声卡/显卡，可以产生各种事件) 异常：应用程序意想不到的行为(e.g.异常，恶意程序，应用程序需要的资源未得到满足) 系统调用(system call)：应用程序请求操作提供服务(e.g.打开/关闭/读写文件，发送网络包) 处理时间 中断：异步； 异常：同步； 系统调用：同步或异步。 响应 中断：持续，对用户应用程序时透明的 异常：杀死或者重新执行意想不到的应用程序指令 系统调用：等待和持续 2.2. 中断、异常、和系统调用 中断和异常的处理机制 中断是外设的事件 异常是内部迫使cpu访问一些被中断和异常服务访问的功能 中断和异常都一个硬件的处理过程和软件的处理过程，两者和在一起才构成操作系统的具体服务。 将中断和异常编号容易区分，每一个编号有一个对应的地址。 这些中断号会构成一个表，当发生中断或者是异常的时候，只需要去查找这个表，就可以容易查找到对应是哪一个。 中断的处理过程：（包括软件和硬件） 硬件：设置中断标记[cpu初始化] 将内部、外部事件设置中断标记 中断事件的ID 软件： 保存当前的处理状态。便于后续从打断的点继续完成。 中断服务程序处理 清楚中断标记 恢复之前保存的处理状态 异常的处理过程：（异常也会有一个异常的编号） 保存现场 异常处理 杀死产生了异常的程序 重新执行异常指令，重新执行这个指令，程序可以继续的执行。 恢复现场 系统调用： 程序访问主要是通过高层次的API接口，而不是直接进行系统调用。 这些API定义了可以提供哪些系统调用 通常情况下，与每个系统调用相关的序号 系统调用接口根据这些序号来维护表的索引。 系统调用接口调用内核态中预期的系统调用 并返回系统调用的状态和其他任何返回值 用户不需要知道系统调用是如何实现的 只需要获取API和了解操作新系统将什么作为返回结果 操作系统接口的细节大部分都隐藏在API中 通过运行程序支持的库来管理（用包含编译器的库来创建函数集） 两个概念：用户态和内核态 用户态： 应用程序在执行的过程中，cpu所处于的一个特权级的状态，其特权级特别低，不能访问某些特殊的机器指令和io 内核态： 操作系统运行过程中cpu所处于的一个状态，cpu可以执行任何的一条特权指令和io，可以完全的控制这个计算机系统 ps当一个应用程序调用一个系统调用的时候，会完成从用户态到内核态的转换，从而使控制权从应用程序交到了操作系统来。操作系统就是可以对系统调用识别来完成具体的服务。 函数的调用和系统调用的区别： 函数的调用只是简单的在一个栈空间里完成函数的调用和返回。而在系统调用过程中，由于应用程序和内核都有各自的堆栈，所以这回涉及到一个堆栈的切换，还会涉及特权级的转换，从用户态转换到内核态。这个是有消耗的，但是会换来安全和可靠。 跨越操作系统边界的开销： 建立中断、异常、系统调用号与对应服务例程映射关系的初始化开销，并且会有一个映射的表，需要对这个表进行维护。 操作系统有自己堆栈，需要对这个堆栈进行维护有消耗。（当然，应用程序也有自己的堆栈） 操作系统不信任应用程序，会对参数进行检查，会有一个时间是上的开销。 数据的内存拷贝，从内核到用户空间，会有一个拷贝的开销。 3. 连续内存分配 3.1. 计算机体系结构及内存分层体系 计算机体系结构/内存分层体系内容： 计算机系统结构 内存分层体系 在操作系统的内存管理范例 一、计算机系统结构主要包含了三大内容： cpu：完成对整个程序的控制 内存：放置了程序的代码和管理的数据 外设：配合程序发挥更大的作用 二、内存的层次机构：cpu要访问的指令和数据所处的位置在什么地方 cpu寄存器，cache：都是处于cpu内部，速度很快，容量很少，可以放的数据有限 主存，物理内存：容量大，但是速度小 硬盘：需要永久保存的数据就放在硬盘中，掉电也不会丢失，速度更慢，但是容量更大 三、操作系统到底要完成的重点事情 可以抽象出来，只需要考虑连续的地址空间，而不需要考虑细节 保护进程空间，有一个隔离的机制，避免应用程序破坏别人 进程空间的通信，共享的空间，使数据的传递安全，有效的 让正在运行的程序，放在内存汇总，让暂时不需要的访问的数据可以临时的放在硬盘中。（例如p4） 四、两种不同的空间 主存硬盘的物理地址空间 运行程序锁看见的空间是逻辑地址空间 五、在操作系统中管理内存的不同方法 程序重定位，分段，分页，虚拟内存，按需分页虚拟内存 ps：其实现高度依赖于硬件，必须知道内存架构，MMU（内存管理单元）：硬件组件负责处理cpu的内存访问请求。 3.2地址空间与地址生成 涉及到几点： 地址空间定义 地址生成 地址的安全检查 一、地址空间的定义 物理地址空间—与硬件直接对于（例如内存条所代表的主存） 逻辑地址空间—程序所看见的地址空间 像这条指令一样，其具体的映射关系，需要操作系统来处理的 二、逻辑地址生成 执行文件放在内存中去，还是一个逻辑的地址 三、完成逻辑地址到物理地址的映射过程 当cpu需要执行这条指令的过程如下： ALU运算器需要这条指令的内容 cpu里面的mmu（内存管理单元）查找逻辑地址的映射表，找出逻辑地址和物理地址之间的映射 cpu控制器会从总线发送物理地址的内存内容的请求（就是指令的内容） 主存会把总线拿到的物理地址内存的内容传给cpu 其中，操作系统的作用是建立起逻辑地址和物理地址之间的映射。 四、地址的安全监测的过程 操作系统的另一个目标是放在内存中的程序相互之间不能互相的干扰 操作系统首先要确保每一个程序可以有效的访问地址空间。（包括起始地址和地址的长度）。 map会指出逻辑地址是否满足映射关系，然后就去到相应的物理地址，将指令数据取回来。 如果不满足，cpu将会产生一个memory异常（内存访问异常） 3.3连续内存的分配：内存碎片与分区的动态分配 连续内存分配所涉及的问题： 内存碎片问题 分区的动态分配（第一分配，最佳适配，最差适配） 压缩式碎片整理 交换式碎片整理 一、内存碎片问题 可以理解为当给一个运行的程序分配一个空间之后，会出现一些无法进一步利用的空间。 1.外部碎片：分配单元之间无法利用的空间 2.内部碎片：运行的程序无法对所分配好的空间进一步的使用 二、分区的动态分配 什么时候需要分配连续的内存： 1.当一个程序准许运行在内存中时候，需要在内存中分配一个连续的区间 2.分配一个连续的内存切间给运行的程序以访问数据 操作系统中会有一些数据结构和算法对空余的内存空间进行有效的管理。 以下有三个简单的内存分配的算法。 首次适配（first fit） 最优适配（best fit） 最差适配（worst fit） 以下分别做简单的介绍： 首次适配算法（first fit）–第一个内存块 基本原理和实现： 需求： 按地址排序的空闲块列表（从0地址开始），分配需要寻找一个合适的分区重分配需要检查，看是否自由分区能合并于相邻的空闲分区（若有）。 优点： 简单，并且容易产生更大的空闲块（没有被破坏），向这地址空间的结尾 缺点： 容易产生外碎片，两个空闲块的空间因为比较小，就可以不会被使用，并且随着时间这个特性会加剧。 最优适配算法（best fit）–最适合分配请求的size 基本原理和实现： 按尺寸排列空闲块列表 分配需要寻找一个合适的分区 重分配需要搜索及合并于相邻的空闲分区（若有） 优点： 对于大多数小内存分配的情况比较合适，比较简单。避免了分割大的空闲块，并且最小化外部碎片产生的尺寸 缺点： 将外碎片分配得比较细，重分配慢，而且容易产生很多没用的微小碎片（不怎么好） 最差适配算法（worst fit）—与请求差距最大的size分配 大块变成了小块，小块进行保留 基本原理和实现： 按差距的尺寸最大进行排列空闲块列表 分配很快（获得更大的分区） 重分配需要合并相邻的空间分区（若有），然后调整空闲块列表 优点： 假如分配是中等尺寸效果最好 缺点： 易于破坏大的空闲块以致于大分区无法被分配 小结：应用请求的需求是随机的和可变的，这些算法都不可能满足全部的应用请求的。 3.2. 连续内存分配：压缩式与交换式碎片整理 以下两种方法减少碎片的产生 1.压缩式碎片整理 2.交互式碎片整理 一、压缩式（compression）碎片整理 重置程序以合并孔洞 要求所有程序是动态可重置的 内存拷贝前思考两个问题： 什么时候考虑内存的重定位是合适的 当程序处于等待的状态之中可以开始内存的重定位 考虑开销大不大 仅仅利用软件的移动实现开销是很大的 二、交换式（swap）碎片整理 考虑几个问题 考虑将那一个程序拷贝到磁盘中去？ 什么时候做这个换入和换出的操作？ 换入换出的开销？ 4. 非连续内存分配 4.1. 分段 分段的管理机制分为两点： 在分段情况下，内存地址空间如何寻址的问题 如何去实现分段的寻址方案 一、分段 计算机程序是由各种各样的段来存储的 分段：更好的分离和共享 通过分段，可以有效的隔离开来，相应的分离出来，更加有效进行管理，分配和保护。这中间需要一种映射机制来实现相关联。 映射之后：位置不一样，大小也不一样 二、段的访问机制 将一个一维的地址分成两块： 一个是段号的寻址，另一个是偏移的寻址 段号+段内偏移何合在一起就形成了一段机制来寻址的方式。 分两种情况： 段寄存器+地址寄存器实现方案（x86） 将段和段内偏移合在一起，单地址实现方案 三、将段的映射机制映射起来 过程： 1、通过段号找到段所在物理内存的起始地址 2、但是这个映射关系需要存储–段表，段表中存储中逻辑地址的段号和物理地址的段号的映射关系。 3、段表中存储着两个重要的信息：一个是段表的起始地址，另外一个是段长度的限制，两者合在一起就形成了一个物理地址。 4、 这样形成了物理地址之后，根据这个地址来查找在物理内存的位置，然后把相应的数据取出来，交给cpu做进一步的处理 段表有操作系统来建立，此时段机制就可以正常的工作了。 而且段机制用得比较少，现在大多数的cpu用的是分页机制。 4.2. 分页 两个内容： 分页地址空间 页寻址方案 段需要一个段号和段内的偏移，而页也一样，需要页号和页内的偏移。 主要区别在于在段的机制里面，段的尺寸是可变的，而分页机制中页的大小是固定的，这个是最大的区别。 一、页的分类 划分物理内存至固定大小的帧 大小是2的幂，eg：512,4096（4k）,8192 划分逻辑地址空间至相同大小的页 大小是2的幂，eg：512,4096,8192 ps：页的大小是不变的，这样便于硬件对其实现 页帧（frame）是物理页 页（page）逻辑页 我们需要建立一个逻辑页地址和物理页地址的一个映射关系。 建立方案：转换逻辑地址为物理地址（page to frame） 页表 MMU（内存管理单元）/TLB（块表） 二、页帧（frame）—物理地址 定义：物理内存的组织和布局方式 页帧也有两部分组成： 页帧号（frame number） 页帧偏移（frame offset） 页帧号占 F 位，页帧本身的大小占 S 位 在整个的寻址空间中有 2^F 这么多个页帧的个数 页帧而每一页的总大小是 2^S 解析： 一帧包含了9位，所以没一页帧的大小都是2 ^9 这么大小，而页帧号是3，所以代表了有3个这么大的一个页，所以也就是2^9 * 3，最后，再加上偏移量o，为6，所以最后的结果是 2^9 * 3 + 6 = 1542. 所以地址就是0x1542 三、页（page）—逻辑地址 和页帧的区别是其页号和页帧号的szie大小可能不一样。但是每一个页的大小和每一个页帧的大小都是一样的。 其逻辑地址的计算方法与页帧的计算方法是一样的。 四、地址的转换 过程如下： 1、首先cpu会去寻址（逻辑地址或者是物理地址），这个地址会分为另两个内容，一个是offect偏移量，一个是页号。 2、将也号作为一个索引，查一个页表（Page table），其实以页号为索引的一向内容，可以根据其查找出页帧号。而且还有知道其基地址，就形成了页帧号和页帧偏移量大小的物理地址。（所以页的偏移大小和页帧的偏移大小是一样的） 3、这样就知道了对应的物理地址的所在位置。这个整个的大致过程。 ps：其中page table是操作系统在内存初始化的时候建立起来的。 4.3. 页表、TLB 一、页表的结构 在页表中，有一系列的属性，eg：可读可写，是否存在等等… 二、页表的地址转换的例子 逻辑地址空间和物理地址空间大小是不对等的，但是每一个页内的偏移都是相等的。 其中，resident 位为0 代表 内存不存在，为1表示存在。如果cpu访问了为0的地址，这是会产生一个异常，就是内存访问异常。 如图所示： 页为（4,0）的逻辑地址，由于resident = 0，所以真是的物理内存不存在 页为（3，1024）的逻辑地址，由于resident = 1，地址存在，查表可知，frame物理地址的页帧号是4，偏移量与页的偏移量相同，一样为1023，所以结果地址便为（4,1023） （页表的建立过程是有操作系统完成的） 三、分页机制的性能问题 1、空间的代价问题 2、时间的开销问题 （希望时间越短越好，效率越大越好） 问题一：页表可能非常大 64位机器如果每页是1k，那么一个页表的大小会是多少呢？ 问题二：页表可能开销大 每一个应用程序都要生成一个自己的页表，开销比较大 如何处理？ 缓存（Caching） 将一些常用的数据缓存到黎cpu非常近的地方，提高访问的速度 间接（Indirection）访问 通过间接的方法，将一个很大的空间，拆分为一个很小的空间。通过多级的页表机制，可以缓解页表占用空间过大的问题。 时间问题 —TLB 缓存近期访问的页帧转换表项 TLB是一个特殊的区域，位于CPU的内部。 Key和Value两个形成了TLB的表相，而这个表相是由相关存储器来实现的，这个是一种快速查询的存储器，速度很快，可以并发的查找，但是容量是有限的。所以可以将一些经常使用的页表项放在TLB中。可以通过查询TLB，避免了一次页表的访问。 当出现TLB访问不到的情况，这个情况叫做TLB miss，这是cpu就不得不查页表。 而对于TLB miss这个情况，将新的页帧加载到TLB中，部分是有cpu硬件来完成的，而部分是有操作系统完成的，也就是两种情况都存在。 4.4. 多级页表 一、空间问题 — 二级页表解决 一级页表里面存储的是二级页表的地址，二级页表知道之后就会知道frame number页帧号。 通过这种方式可以极大的减少空间的消耗，因为如果一级页表中的resident = 0的话。就没有必要再二级页表中添加其索引的，比单级页表大大的减小了空间的开销。 二、多级页表 多级页表可以表示一个更大的地址空间，形成一个树状的结构。这个是以时间换取空间，但是时间问题也可以通过TLB方法来解决。 4.5. 反向页表 一、反向页表： 以物理地址的页帧号（frame number）方向查找逻辑页的页号（page number） 这样使得寄存器的容量，只与物理地址空间的大小相关，与逻辑地址空间大小无关。 但是有一个主要的问题：如何将页号和页帧号建立起一个映射关系。 页存储器方案的权衡： 优点： 转换表的大小相对于物理内存来说很小 转换表的大小跟逻辑地址空间的大小无关 缺点： 需要的信息对调了，既根据帧号可找到页号 如何转换回来？既根据页号找到帧号 在需要在反向表中搜索想要的页号 二、关联存储器方案 可以并行的查找页号所对应的帧号，其key是他的页号，value是页帧号 存在的问题： 设计成本太大，硬件处理很复杂 内存访问的开销问题 大量的关联内存非常昂贵，难以在单个时钟周期内完成且耗电 三、基于哈希（hash）计算的反向页表 只需要建立好一个哈希的函数，输入一个值，就会得到一个输出。而输入的值是page number，输出的值是frame number。 为了能提高加速，需要硬件的加速。 为了提高效率，加一个PID的标识 可以有效的缓解完成映射的开销。 在反向页表中通过哈希算法来搜索一个页对应的帧号 对页号做哈希计算，为了在“帧表”（每一帧用于一个表项）中获取对应的帧号 页i被放置在表中f(i)位置，其中f是设定的哈希函数 为了查找页i，执行下列操作： 计算哈希函数f(i)并且使用它作为页寄存器表的索引，获取对应的页寄存器，检查寄存器标签是否包含i，如果包含，则代表成功，否则失败。 5. 虚拟内存 5.1. 起因 理想中的存储器： 更大，更快，更便宜的非易性存储器 硬盘的速度远远的慢于内存的执行。 磁带比硬盘的存储容量更加的庞大。 现有的物理内存掉电之后数据还是会丢失的。 以上设计了三种技术： 手动覆盖技术：只把指令和数据保存在内存中 自动交换技术：将程序导出内存到硬盘上 虚拟内存技术（前两种是虚拟内存还没出现的情况下诞生的）：以更小的力度把数据导出导入到内存中来，充分的利用了内存空间的手段 5.2. 覆盖技术 一、覆盖技术的基础 目标： 是在较小的可用内存中运行较大（相对而言的）的程序。常用与多道程序系统，与分区存储管理配合使用。 原理： 把程序按照其自身的逻辑结构，划分为若干个功能上相对独立的程序模块，那些不会同时执行的模块共享同一块内存区域，按时间先后来运行。 必要部分（常用功能）的代码和数据常驻内存； 可选部分（不常用内存）在其他程序模块中实现，平时存放在外存中，在需要用到时才装入内存； 不存在调用关系的模块不必同时装入到内存，从而可以相互覆盖，既这些模块共有一个分区。 二、应用例子 例子一： bc是对等的，相互之间不会调用，所以分在一个区；A调用b的时候，c是不会执行的，所以只需要将b放在内存中即可。 def也是对等的，相互之间也不会调用，所以也分在一个区；当C调用e的时候，df通用是不会被调用的，所以也只需要将e放在内存中即可。 例子二： 所以覆盖技术是可以有多种方式选择的。 三、覆盖技术的优缺点 优点： 将一个大程序可以放在一个很小的内存里面通过交换技术执行。 缺点： 由程序员来把一个大的程序划分为若干个小功能模块，并确定各个模块之间的覆盖技术，费时费力，增加了编程的复杂度。 覆盖模块从外存装入内存，实际上是以时间延长来换取空间节省 5.3. 交换技术 一、交换技术的基础 目标： 多道程序在内存中时，让正在运行的程序或需要运行的程序获得更多的内存资源。 方法： 可将暂时不能运行的程序送到外存，从而获得空闲内存空间 操作系统把一个进程的整个地址空间的内容保存到外存中（换出swap out），而将外存中的某个进程的地址空间读入到内存中（换入swap in）。换入换出内容的大小为整个程序的地址空间。 这个换入换出的交换技术是操作系统内存管理的重要组成部分。 二、交换技术实现的几个问题： 交换时机的确定：何时小发生交换？只当内存空间不够或有不够危险的时候才换出。 交换区的大小：必须足够大以存放所以用户进程的所有内存映射的拷贝；必须能对这些内存映像进行直接存取。 程序换入时的重定位：换出后再换入的内存位置一定要在原来的位置上吗，寻址可能会出现问题？最好采用动态地址映射的方法，建好页表就行。 ps：交换技术是可以由操作系统来完成的，对于程序员来说是透明的。 三、小结–覆盖与交换的比较 覆盖只能发生在那些相互之间没有调用关系的程序模块之间，因此程序员必须给出程序内的各个模块之间的逻辑覆盖结构。 交换技术是以内存中的程序大小为单位来进行的，它不需要程序员给出各个模块之间的逻辑覆盖结构。换言之，交换发生在内存中程序与管理程序或操作系统之间，而覆盖则发生在运行程序内部。 5.4. 虚存技术 （虚拟内存管理技术） 一、前诉 在内存不够用的情形下，可以采用覆盖技术和交换技术，但是： 覆盖技术：需要程序员自己把整个程序划分为若干个小的功能模块，并确定各个模块之间的覆盖关系，增加了程序员的负担； 交换技术：以进程作为交换的单位，需要把进程的整个地址空间都换进换出，增加了处理器的开销。 希望通过一种更好的办法，能够充分的解决交换技术和覆盖技术出现的问题。 二、虚拟内存的基础 目标： 像覆盖技术那样，不是把程序的所有内容都放在内存中，因而能够运行比当前的空闲内存空间还要大的程序。但做得更好，由操作系统自动来完成，无须程序员的干涉； 像交换技术那样，能够实现进程在内存与外存之间的交换，因而获得更多的空闲内存空间。但做得更好，只对进程的部分内容在内存和外存之间进行交换。 二、虚拟技术–程序的局部性原理 定义： 程序的局部性原理（principle of locality），指程序在执行过程中的一个较短时期，所执行的指令地址和指令的操作数地址，分别局限于一定的区域，这个可以表现为： 时间局部性：一条指令的一次执行和下次执行，一个数据的一次访问和下一次访问都集中在一个较短时期内。 空间局部性：当前指令和邻近的几条指令，当前访问的数据和邻近的几个数据都集中在一个较小区域内。 程序的局部性原理表明，从理论上说，虚拟存储技术是能够实现的，而且在实现了以后应该是能够取得一个满意的效果。访问的速度更快，并且可以提供一个很多的空间。 三、程序局部性的例子 可见：程序1是按照行来访问的，而程序2的按照列来访问的 结果分析： 由于程序1的没相邻的两次访问的地址差距较大，不满足时间局部性和空间局部性，所以会产生多次的缺页中断，对系统的开销较大。而程序2两个数据是相邻的，具有良好的时间局部性和空间局部性。 如果程序不具有局部性，这个高效的机制就很难的实现。 四、虚存技术的大致流程 前提： 操作系统有了硬件支持分段/分页机制，在此内存管理基础之上来实现一个以页或者是段为单位的虚存管理。 过程： 在装入程序的时候，不必将所有的程序和数据装入内存中去，而只需将当前需要执行的部分的代码数据放在相关的段或者是页中，这样可以是的一小部分的代码放在内存中去了。 在程序执行过程中，如果需要执行的指令或访问的数据尚未在内存中（称为缺页或缺段），则由处理器通知操作系统将相应的页面或段调入到内存，然后继续执行程序。 另一方面。操作系统将内存中暂时不使用的页面或段调出保存在外存上，从而腾出更多空闲空间存放将要装入的程序以及将要调入的页面或段。 如图所示：程序只有少部分在内存中，而大部分都是在外存中存储。 五、虚拟内存的基本特征 较大的用户空间： 通过把物理内存与外存结合，提供给用户的虚拟内存空间通常大于实际的物理内存，既实现了这两者的分离。如32位的虚拟地址理论上可以访问4GB，而可能计算机上仅有256M的物理内存，但硬盘的容量大于4GB 部分交换： 与交换技术相比较，虚拟存储的调入和调出是对部分虚拟地址空间进行的，其每次的换入换出是非常规整的，要么是段或者是页。不需要将整个程序交换出去。力度更小，但是效率更高。 不连续性： 物理内存分配不连续，虚拟地址空间使用也是不连续的。本来所有的数据都是连续的放在虚拟内存中的，但是操作系统要把某些数据换出去，而造成的不连续。操作系统会弥补好，正常的访问。 六、虚拟内存技术的具体实现 基于页式虚拟内存管理，与前诉的一样。 大部分虚拟存储系统都采用虚拟页式存储管理技术，既在页式存储管理的基础上，增加请求调页和页面置换功能。 基本思路： 当一个用户程序要调入内存运行时，不是将该程序的所以页面都装入内存，而是只装入部分的页面，就可以启动程序运行。 在运行的过程中，如果发现要运行的程序或要访问数据不在内存，则向系统发出缺页中断请求，系统在处理这个中断时，将外存中相应的页面调入内存，使得该程序能够继续运行。 七、页表表项的设定 驻留位：表示该页是在内存还是外存。如果该位等于1，表示该页位于内存当中，既该页表项是有效的，可以使用；如果改为为0，表示该页当前还在外存当中，如果访问该页表项，将导致缺页中断。 保护位：表示允许对该页做何种类型的访问，如只读，可读写，可执行等。 修改位：表面此页面在内存中是否被修改过，当系统回收该物理页面时，根据此位来决定是否把他的内容写回外存。位为0就表示数据一样的，不需要写回外存中。 访问位：如果该页面被访问过（包括读操作或写操作），则设定词尾，用于页面的置换算法。 示例： （左边是虚存页表的映射关系，每一个页表项有4K的物理页，X代表驻留位为0，如果是一个具体的数，就代表驻留位为1，映射关系有效。） 示例1： MOV REG, 0 //将0地址的内容赋值给一个寄存器 可以看见，0地址是在最底下，有一个2，这表示驻留位是1，且页帧号是2。而一个页的大小是4096,既4k，所以2*4k为8k。也就是所将对应的8k的地址8192的内容给寄存器。 MOV REG, 0 ---------&gt; MOV REG, 8192 示例2： MOV REG, 32780 可以看见32780对应的页表项是32k，其驻留位的设置是0，没有对应的一个页帧号，意味着访问这一页会产生缺页（缺页异常） MOV REG, 32780 ---------&gt; MOV REG, 缺页中断 八、缺页中断处理过程 当cpu执行一条指令load一个内存地址，如果这个内存地址没有一个对应的关系，也就是说没有一个存在位，这时就会产生一个缺页异常，接来下操作系统就会完成一些列缺页中断的处理： 如果在内存中有空闲的物理页面，则分配一空闲的物理页帧f，然后转第4步；否者转第2步。 采用某种页面置换算法，选择一个将被替换的物理页帧f，他所对应的逻辑页为q。如果该页在内存期间被修改过，则需把他写回外存。 对q所对应的页表项进行修改，把驻留位置置0。 将需要访问的页p装入到物理页面f当中。也就是把页所需要访问的地址对应的硬盘中的数据，以页为单位，从硬盘读到内存中去，读到刚分配到的那个内存地址。 修改p所对应的页表项的内容，把驻留位置1，把物理页帧号置为f。 重新运行被中断的指令。 九、后备存储（Backing Store） 在何处保存未被映射的页？ 能够简单地识别在二级存储器中的页 交换空间（磁盘或者文件）：特殊格式，用于存储未被映射的页面 概念： 一个虚拟地址空间的页面可以被映射到一个文件（在二级存储中）中的某个位置 代码段：映射到可执行二进制文件 动态加载的共享程序段：映射到动态调用的库文件 其他段：可能被映射到交换文件（swap file） 有了这个后背存储/二级存储，可以充分保证了虚存空间的有效性。 十、虚拟内存性能 为了便于理解分页的开销，使用有效存储器访问时间effective memory access 其实取决参数p，如果p足够小，就可以使平均的执行时间接近10nm，但是如果程序有局部性特点，就表面产生缺页的次数会很少，这是效率就会很高。 6. 页面置换算法 1、局部页面置换算法 最优页面置换算法（OPT、optimal） 先进先出算法（FIFO） 最近最久未使用算法（LRU,Least Recently Used） 时钟页面置换算法（Clock） 最不常用算法（LFU，Least Frequently Used） Belady现象 LRU、FIFO和Clock的比较 2、全局页面置换算法 工作集模型 工作集页置换算法 缺页率置换算法 功能： 当缺页中断发生，需要调入新的页面而内存已满时，选择内存当中哪个物理页面被置换。 目标： 尽可能地减少页面的换进换出次数（既缺页中断的次数）。具体来说，把未来不再使用的或短期内较少使用的页面换出，通常只能在局部性原理指导下依据过去的统计数据来进行预测。 页面锁定（frame locking）： 用于描述必须常驻内存的操作系统的关键部分或时间关键（time-critical）的应用程序。实现的方法是L在页表中添加锁定标志位（lock bit）。使其不在页面置换算法范围之内，也就说不会被换入换出。 通常只需要考虑页号，因为偏移号一般不起作用。只保留页号。基于这个list来设计各种的页面替换算法。 通过模拟一个页面置换的行为并且记录产生页缺失数的数量。一般情况下，产生的缺页次数越少，性能就越高。 6.1. 最优页面置换算法 一、基础 基本思路： 当一个缺页中断发生时，对于保存在内存当中的每一个逻辑页面，计算在它的下一次访问之前，还需要等待多长时间，从中选择等待时间最长的那个，作为被置换的页面。 结论： 这只是一个理想的情况，在实际系统中是无法实现的，因为操作系统无从知道每一个页面要等待多长时间以后才会再次被访问。 可用作其他算法的性能评价的依据（在一个模拟器上运行某个程序，并记录每一次的页面访问情况，在第二遍运行时即可使用最优算法。然后以此为基础，评价其他的算法。） 二、示例 在上帝视角中，可以看出由于d是最长时间没有被使用，所以d将会被e所替换，如上所示。 6.2. 先进先出算法 一、基础 先进先出算法（First-In First-Out，FIFO）： 1、基本思路： 选择在内存中驻留时间最长的页面并淘汰之。具体来说，系统维护着一个链表，记录了所有位于内存当中的逻辑页面。从链表的排列顺序来看，链首页面的驻留时间最长，链尾页面的驻留时间最短。当发生一个缺页中断时，把链首页面淘汰出局，并把新的页面添加到链表的末尾。 2、评价： 性能较差，调出的页面有可能是经常要访问的页面，并且有Belady现象（给的物理页帧越多，产生缺少的次数越大）。FIFO算法很少单独使用。 二、示例 实现简单，但是产生的缺页次数比较多 6.3. 最近最久未使用算法 一、基础 最近最久未使用算法（LRU,Least Recently Used） 1、基本思路： 当一个缺页中断发生时，选择最久未使用的那个页面，并淘汰之。 2、评价： 它是对最优页面置换算法的一个近似，其依据是程序的局部性原理，既在最近一小段时间（最近几条指令）内，如果某些页面被频繁地访问，那么在将来的一小段时间内。他们还可能会再一次地频繁地访问。反过来说，如果在过去某些页面长时间未被访问，那么在将来他们还可能会长时间地得不到访问。也就是根据过去推算出未来。 二、示例 LRU算法需要记录各个页面使用时间的先后顺序。 开销比较大。两种可能的实现方法是： 方法一： 系统维护一个页面链表，最近各个使用过的页面作为首节点，最久未使用的页面作为尾节点。每一次访问内存时，找到相应的页面，把它从链表中摘下来，在移动到链表之首。每次缺页中断发生时，也就是没有这个页表，所以会把新的页表查到链表头，然后淘汰链表末尾的页面。 方法二： 设置一个活动页面栈，当访问某页时，将此页号压入栈顶，然后考察栈内是否有与页面相同的页号，若有则抽出。然后压入栈顶。当需要淘汰一个页面时，总是选择栈底的页面，它就是最久未使用的。 效果比较好，但是系统的开销比较大 6.4. 时钟页面置换算法 一、基础 Clock页面置换算法，LRU的近视，对FIFO的一种改进 1、基本思路 需要用到页表项当中的访问位，当一个页面被装入内存时，把该位初始化为0。然后如果这个页面被访问（读/写），则把该位置置1。 把各个页面组织形成环形链表（类似钟表面），把指针指向最老的页面（最先进来）。 当发生一个缺页中断时，考察指针所指向的最老页面。若它的访问位为0，立即淘汰；若访问位为1，则把该位置为0，然后指针往下移动一格。如此下去，直到找到被淘汰的页面，然后把指针移动到它的下一格。 二、具体实现 resident bit：存在位，代表是否存在。如果是1，代表在物理内存是存在的，表示映射的关系是正常的。如果是0，就不能正常的映射。 used bit：如果是1代表当前的页被访问过一次，硬件支持将其置为1。（这个位可以硬件自动的操作，同时也可以由软件操作） frame number：页帧号 时钟页面置换算法的依据就是第二个位------used bit 三、示例 其中的置1操作是由硬件自动实现的。 替换的情况是产生缺页中断时候才会执行的。如果本来就有此内存，则指针是不需要向下移动寻找最老页面。也就是说如果存在，则指针保持不动，只需要置1操作既可。 6.5. 二次机会法 一、基础 resident bit：存在位，代表是否存在。如果是1，代表在物理内存是存在的，表示映射的关系是正常的。如果是0，就不能正常的映射 used bit：如果是1代表当前的页被访问过一次，硬件支持将其置为1。（这个位可以硬件自动的操作，同时也可以由软件操作） dirty bit：如果执行了一个写操作，那么这个位会置为1；如果只是读操作，那么这个位是0。这个bit的设置也是由硬件来完成的。 当某一个运行的程序，对某一页进行访问之后。 如果是写操作，硬件会将used bit和dirty bit都置为1. 如果是读操作。硬件会将used bit置为1，而dirty bit还是0. 这个bit可以区分读和写，但是对我们的置换算法有什么帮助呢？ 解析： 因为我们的算法是换入换出算法，所以如果当应用程序对内存进行读操作的时候，这个内存与磁盘的内容是一样的，所以只需要将其释放掉就可以了，不需要进行换入换出的操作。 而如果应用程序对内存进行了写操作的时候，这时表面与磁盘的内容不一样，替换的时候就需要把内容换入换出。 这时，两个bit都用上了，来减少硬盘的访问也就是减少写回操作的次数。 由于used=1，dirty=1的页会循环两次才会被替换出去，所以很形象生动的称之为二次机会法。 通过这种方式，可以把经常使用dirty bit的这个页有更多的机会留着内存中来。而不会被换到内存中去。对硬盘的访问次数也会减少。 二、示例 带有w表示对此页进行的是写操作而不是读操作，读操作是不带w 此时考虑两个位，used bit和dirty bit 比较接近LRU算法，优先的把只读的页换出去了，对于可写的页减少了换出去的概率，对于可以减少回写的概率。 6.6. 最不常用算法 一、基础 最不常用算法（Least Frequently Used，LFU） 基本思路： 当一个缺页中断发生时，选着访问次数最少的那个页面，并淘汰之。被访问的次数也会很少。 实现方法： 对每个页面设置一个访问计数器，每当一个页面被访问时，该页面的访问计数器加1。每当反生缺页中断时，淘汰计数器最小的那个页面。 问题： 增加计数器会消耗硬件资源，会浪费空间，而选择次数最少的那个意味在要遍历整个链表，耗费时间，实现比较费时费力。而且当一个页面在进程开始的时使用得很多，但是以后就不再使用了，LFU还是会保留。（根据该点的解决方法：定时的把次数寄存器右移一位） LRU和LFU的区别： LRU考察的是多久未访问，时间越短越好；而LFU考察的是访问的次数或频度，访问次数越多越好。 二、示例 以上操作是将访问次数最多的替换出去。 6.7. Belady现象、比较 一、Belady现象 （Belady是一个科学家的名字，不必纠结） 定义： 在采用FIFO算法时，有时会出现分配的物理页面数增加，缺页率反而提高的异常现象。 Belady现象的原因： FIFO算法的置换特征与进程访问内存的动态特征是矛盾的，与置换算法的目标是不一致的（既替换较少使用的页面），因此，被它替换出去的页面并不一定是进程不会访问的。 二、Belady现象示例 1、当分3个物理页的情况—出现9次中断缺失 2、当分4个物理页的情况—出现10次中断缺失 结果： 出现了物理页，给了更多的物理页，但是出现页缺失的情况更多 相比之下，LRU算法是符合预期情况的，给的硬件资源越多，产生中断页缺失的情况就会越少。 原因： LRU算法满足某种栈的属性，而FIFO算法不满足某种栈的属性，所以会导致Belady现象。 三、LRU、FIFO、Clock的比较 1、性质的比较 2、性能的比较 6.8. 问题、工作集模型 局部页面置换算法都是针对一个正在运行的程序来讲的，但是操作系统支持多个应用程序。 以上可见，只是仅仅增加了一个物理页帧，就对整个页面置换算法造成很大的效果影响。如果对一个程序固定一个物理页帧，其实是在某一个程度上限制了这个程序产生缺页的特点。因为其对物理内存的需求是动态可变的。 而前面所诉的前提是物理页帧是假设为固定的。这样就限制了灵活性。但是可以根据不同的运行阶段，动态分配调整物理页帧的大小，这点就是全局页面置换算法要考虑的问题。 一、工作集模型 前面介绍的各种页面置换算法都是基于一个前提的，既程序的局部性原理。 如果局部性原理不成立，那么各种页面置换算法就没有什么分别，也没有什么意义。例如：假设进程对逻辑页面的访问顺序是1,2,3,4,5,6,7,8,9…,即单调递增，那么在物理页面数有限的前提下，不管采用何种置换算法，每次的页面访问都必然导致缺页中断。 如果局部性原理是成立的，那么如何来证明它的存在，如何来对它进行定量地分析？这就是工作集模型。 1、工作集的定义： 一个进程当前正在使用的逻辑页面集合，可以用一个二元函数W(t，△)来表示。 二元函数W(t，△) 其中参数如下： 例子： 这表明t2具有良好的局部性，t1有一定的局部性，但是整体的局部性不如t2的效果好。 2、工作集大小的变化： 进程开始执行后，随着访问新页面逐步建立较稳定的工作集。当内存访问的局部性区域的位置大致稳定时，工作集带下也大致稳定；局部性区域的位置改变时，工作集快速扩张和收缩过渡到下一个稳定值。 二、常驻集模型 常驻集是指在当前时刻，进程实际驻留在内存当中的页面集合。 工作集是进程运行过程中固有的性质，而常驻集取决于系统分配给进程的物理页面数目（物理空间的大小），以及所采用的页面置换算法。来决定到底把哪些页面放在内存中来。 常驻集是当前运行的程序访问的页在哪些在内存中；而工作集指的是程序运行中所需要访问的页是哪些，这表示有些页是不在内存中的，只有部分页是在内存中的。 如果一个进程的整个工作集读在内存当中，既常驻集属于工作集，那么进程将很顺利地进行运行，而不会造成太多的缺页中断（直到工作集发生剧烈变动，从而过渡到另一个状态）。 当进程常驻集的大小达到某个数目之后，再给它分配更多的物理页面，缺页率也不会明显下降，可以给其他运行的程序，使总的缺页比较少。 6.9两个全局置换算法 一、工作集页置换算法 1、基本思想 有一个size，代表了其过去形成工作集的大小。窗口里面的页是当前时间内被访问到的页。随着时间的挪动平移，如果某一个不在这个时间的窗口之内，这个页也会被丢到，而并不是说要等到缺页的时候才会丢页。也就是这个页不属于这个窗口了，就会被替换。 2、示例 结果如下： 1----edac----abcd 6----dbce----bcde 2----dacc----acd 7----bcec----bce 3----accd----acd 8----cece----ce 4----ccdb----bcd 9----ecea----ace 5----cdbc----bcd 10—cead----acde 分析： 并不是因为缺页而丢弃，而是因为不在这个窗口当中的所以老页都会被换出去。这样可以确保物理内存中有足够的页存在，可以减少页面置换降低，这个是站着整个系统层面上看的。 6.9. 缺页率页面置换算法 1、可变分配策略： 常驻集大小可变。例如：每个进程在刚开始运行的时候，先根据程序大小给它分配一定数目的物理页面，然后在进程运行过程中，再动态地调整常驻集的大小。根据缺页率来改变，缺页率高，可以增加常驻集；缺页率降低，可以减小常驻集。 缺页率算法（PFF，page fault frequency） 2、缺页率 定义： 表示“缺页次数/内存访问次数”（比率）或“缺页的平均时间间隔的倒数”。 影响缺页率的因素： 页面置换算法 分配给进程的物理页面数目 页面本身的大小 程序的编写方法 使整个系统保持一个平衡，使所有的程序到保持一个较低的缺页率。 一个交替的工作集计算明确的试图最小化页缺失 当缺页率高的时候-增加工作集 当缺页率低的时候-减少工作集 3、算法的实现 4、示例 分析： 当前的阈值是2，也就是如果两次产生中断的时间大于2的话，话增加工作；而如果中断的时间小于等于2的话，就会动态的减少工作集。 在时间1时刻，产生一个缺失中断。 在时间4时刻，由于没有b，所以也产生了一次缺失中断。并且，由于1-4之间的时刻大于2，所以会动态的去除在这段时刻中没有读取的页，也是就是ae，所以此时只有bcd三个工作页。 在时间6时刻，由于没有e，产生了一次缺失中断。并且，由于4-6之间的时刻等于2，所以会动态的增加所需要的页。 在时间9时刻，由于没有页a，所以产生了一次缺失中断。并且，由于6-9之间的时刻大于2，所以也会动态的去除在这段时间中没有读取的页，也就是bd，因为这段时间只有ec的页需要操作，此时就只有ace三个工作页。 5、小结 这两个算法是根据工作集的大小动态的调整的，前面只是满的时候才调整，这个是他们之间的主要区别。所有对于操作系统而言，为了应对多个应用程序，采用全局的页面置换算法更加的合适。 6.10. 抖动问题 抖动问题是对工作集和常驻集做进一步的讲解。 1、抖动的定义 如果分配给一个进程的物理页面太少，不能包含整个的工作集，既常驻集属于工作集，那么进程将会造成很多的缺页中断，需要频繁地在内存与外存之间替换页面，从而使进程的运行速度变得很慢，将这种状态称为“抖动”。 2、产生抖动的原因 随着驻留内存的进程的数目增加，分配给每个进程的物理页面数不断减小，缺页率不断上升。所以操作系统要选择一个适当的进程数目和进程需要的帧数，以便在并发水平和缺页率之间达到一个平衡。 3、解决 当运行的程序过多时，cpu要执行多次的换入换出换出io操作，而导致程序没有执行，导致cpu的利用率降低，造成了电脑的卡顿。 蓝线的比值越大，表示缺页的频率很低，cpu利用率较高。（其中页缺失的服务时间是不变的） 当平均页缺失时间 = 页缺失服务时间 的时候，这时候的效率就接近最完美的点。 7. 进程 7.1. 进程的定义 在某种程度上， 可以将应用程序看成是一个进程，其将会消耗耕种各样的计算机资源。 定义： 一个具有一定独立功能的程序在一个数据集合上的一次动态执行过程。 只有当操作系统把执行程序调入到我们的内存之后，让这个程序可以执行起来。（能够让通过cpu对这个程序执行一条条的指令，读取数据完成一定的功能）。也就是静态的执行程序，通过cpu变成一个动态的执行过程，而这个动态的执行过程就是进程。 整个的功能是由程序的代码决定的。 7.2. 进程的组成 1、一个进程应该包括 程序的代码 程序处理的数据 程序计数器中的值，指示下一条将运行的指令 一组通用的寄存器的当前值，堆，栈 一组系统资源（如打开的文件） 总之，进程包含了正在运行的一个程序的所以状态信息。 2、进程与程序的联系 程序是产生进程的基础 程序是静态的代码，代码限制了进程完成是什么样的功能 程序的每次运行构成不同的进程 程序多次运行过程中输入的数据不一样，产生的结果是不一样的，所以构成了不一样的进程 进程是程序功能的体现 尽管输出可能不同，但是这个程序的功能是一样的 通过多次执行，一个程序可对应多个进程；通过调用关系，一个进程可包括多个程序。 进程和程序之间是一个多对多的关系。 3、进程与程序的区别 进程是动态的，程序是静态的；程序是有序代码的集合；进程是程序的执行，进程有核心态/用户态。 进程是暂时的，程序是永久的；进程是一个状态变化的过程，程序可长久保存。 进程与程序的组成不同；进程的组成包括程序、数据和进程控制块（既进程状态信息） 一个很有趣进程与cpu的类比： cpu在工作中会存在切换处理不同程序的 7.3. 进程的特点 动态性：可动态地创建，结束进程； 并发性：进程间可以被独立调度并占有处理机运行；（并发与并行的区别，前者是可以为1个cpu，后者必须要多个cpu） 独立性：不同进程的工作不互相影响，也就是进程不会破坏代码数据的正常执行。（页表的支持） 制约性：因访问共享数据/资源或进程间同步而产生制约 a–体现了动态性 / b–体现了独立性 / c–体现了制约性 描述进程的数据结构：进程控制块（Process Control Block，PCB） 操作系统为每一个进程都维护了一个PCB，用来保存与该进程有关的各种状态信息和需要资源的情况等等。 7.4. 进程控制结构 1、进程控制块：操作系统管理控制进程运行所用的信息集合。 操作系统用PCB来描述进程的基本情况以及运行变化的过程，PCB是进程存在的唯一标志，如果进程消失了那么其对应的PCB也会消失，是一一对应的关系。 2、使用进程控制块 进程的创建：为该进程生产一个PCB； 进程的终止：回收它的PCB； 进程的组织管理：通过对PCB的组织管理来实现； 问题：PCB具体包含什么信息？如何组织的？进程的状态转换…? 3、PCB包含的信息 进程标识信息。 如本进程的标识（进程号，执行的次数…），本进程的产生者标识（父进程标识）；用户标识。 处理机状态信息保存区 保存进程的运行现场信息： 用户可见寄存器：用户程序可以使用的数据，地址等寄存器。 控制和状态寄存器：如程序计数器（PC），程序状态字（PSW） 栈指针：过程调用/系统调用/中断处理和返回时需要用到它。找到当前运行的位置。 进程控制信息 操作系统需要对这个进程进行管理和控制调度和 调度和状态信息：用于操作系统调度进程并占用处理机使用。 进程间通信信息：为支持进程间与通信相关的各种标识，信号，信件等。这些信息存在接受方的进程控制块中 存储管理信息：包含有指向本进程映射存储空间的数据结构。 进程所用资源：说明有进程打开、使用的系统资源，如打开的文件等。 有关数据结构连接信息：进程可以连接到一个进程队列中，或连接到相关的其他进程的PCB 4、PCB的组织方式 链表：同一状态的进程其PCB称一链表，多个状态对应多个不同的链表。各个状态的进程形成不同的链表：就绪链表，阻塞链表。 索引表：同一个状态的进程归入一个index表（由index指向PCB），多个状态对应多个不同的index表。各个状态的进行形成不同的索引表：就绪索引表、阻塞索引表 一般来说会采取链表，因为进程的控制是动态的插入和删除的，所以链表组织方式比较方便，而索引开销比较大。当然如果一开始就固定住了进程的数目，索引也不失为一个选择。 以上是围绕进程静态部分说明，组成，特点等等。 一下是围绕进程动态的状态特点说明，有3个方面的内容： 进程的生命周期管理 进程状态变化模型 进程挂起模型 7.5. 进程的生命期管理 进程的生命是指进程的创建到结束这么一整个的生命期。 进程的生命期管理有以下几个时期： 进程创建 进程运行：正在占用cpu，执行这个进程 进程等待：由于某种特殊原因需要等待 进程唤醒：当等待的条件满足，需要唤醒 进程结束 1、进程创建 引起进程创建的3个主要事件： 系统初始化时 用户请求创建一个新进程 正在运行的进程执行了创建进程的系统调用 但是创建了新的进程不一定可以执行。 2、进程运行 内核选择一个就绪的进程，让它占用处理机并执行 就绪态-----&gt;执行态 其中涉及两个问题： 为何选择？ 如何选择？（涉及调度算法） 3、进程等待 在以下情况下，进程等待（阻塞）: 请求并等待系统服务，无法马上完成 启动某种操作，无法马上完成 需要的数据没有到达 ps：进程等待事件的发起是有自己发起的。因为进程只能自己阻塞自己，因为只有进程自身才能知道何时需要等待某种事件的发生。 4、进程唤醒 唤醒进程的原因： 被阻塞进程需要的资源可被满足 被阻塞进程等待的事件到达 将该进程的PCB插入到就绪队列 ps：进程只能被别的进程或者操作系统唤醒 5、进程结束 在以下四种情况下，进程结束 正常退出（自愿的） 错误退出（自愿的） 致命错误（操作系统强制性的） 被其他进程所杀（强制性的） 7.6. 进程状态变化模型 1、进程的三种基本状态： 进程在生命结束前处于且仅处于三种基本状态之一，不同系统设置的进程状态数目不同。 运行状态（Running）：当一个进程正在处理机上运行时。 就绪状态（Ready）：一个进程获得了除处理机之外的一切所需资源。一旦得到处理机即可运行。 等待状态（又称阻塞状态Blocked）：一个进程正在等待某一事件而暂停运行时。如等待某资源，等待输入/输出完成。 2、三状态变化图： 进程其他的基本状态： 创建状态（New）：一个进程正在被创建，还没被转到就绪状态之前的状态。 结束状态（Exit）：一个进程正在从系统中消失时的状态，这是因为进程结束或由于其他原因所导致。 3、五状态变化图： 就绪态的出现是由于调度机制的存在。 4、可能的状态变化如下： NULL-&gt;New: 一个新进程被产生出来执行一个程序。 New-&gt;Ready: 当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态。（不会持续很久，也就只是一个PCB的初始化。） Ready-&gt;Running ：处于就绪状态的进程被进程调度程序选中后，就分配到处理机上来运行。 Running-&gt;Exit ：当进程表示它已经完成或出现错误，当前运行进程会有操作系统作结束处理。 Running-&gt;Ready ：处于运行状态的进程在其运行过程中，由于分配给它的处理机时间片用完而让出处理机。（操作系统完成） Runing-&gt;Blocked ：当进程请求某样东西切必须等待时。（例如等待一个定时器的到达，读写文件，因为过程比较慢） Blocked-&gt;Ready ：当进程要等待某事件到来时，它从阻塞状态变到就绪状态。（同样由操纵系统完成） 7.7. 进程的挂起 进程挂起和进程阻塞的不一样的。 进程在挂起状态意味着进程没有占有内存空间。处于挂起状态的进程影像在磁盘上。 1、挂起状态 阻塞挂起状态（Blocked-suspend）：进程在外存并等待某事件的出现； 就绪挂起状态（Ready-suspend）：进程在外存，但只要进入内存，即可运行。 2、与挂起中相关的状态转换 挂起（Suspend）：把一个进程从内存转到外存；可能有以下几种情况： 阻塞到阻塞挂起：没有进程处于就绪状态或就绪进程要求更多内存资源时，会进行这种转换，以提高新进程或运行就绪进程； 就绪到就绪挂起：当有高优先级阻塞（系统认为会很快就绪的）进程和低优先级就绪进程时，系统会选择挂起低优先级就绪进程； 运行到就绪挂起：对抢先式分时系统，当有高优先级阻塞挂起进程因事件出现（空间不够）而进入就绪挂起，系统可能会把运行进程转到就绪挂起状态。 在外存时的状态转换： 阻塞挂起到就绪挂起：当有阻塞挂起进程相关事件出现时（也就是条件满足），系统会把阻塞挂起进程转换到就绪挂起进程。 解挂/激活（Activate）：把一个进程从外存转到内存；可能有以下几种情况： 就绪挂起到就绪：没有就绪进程或挂起就绪进程优先级高于就绪进程时，会进行这种转换。 阻塞挂起到阻塞：当一个进程释放足够内存时，系统会把一个高优先级阻塞挂起（系统认为会很快出现所等待的事件）进程转换为阻塞进程。 问题：操作系统怎么通过PCB的定义的进程状态来管理PCB，帮助完成进程的调度过程？ 以进程为基本结构的os，选择某一个进程变成某一种状态都是有操作系统来完成的。最底层为CPU调度程序（包括中断处理等）。上面一层为一组各式各样的进程。 3、状态队列 状态队列是操作系统管理进程的一个很重要的数据结构 由操作系统来维护一组队列，用来表示系统当中所以进程的当前状态； 不同的状态分别用不同的队列来表示（就绪队列，各种类型的阻塞队列）； 每个进程的PCB都根据它的状态加入到相应的队列当中，当一个进程的状态发生变化时，它的PCB从一个状态队列中脱离出来，加入到另外一个队列。 4、状态表示方法 要注意，如果事件1只能满足一个进程，那么只能把这一个进程从阻塞态变成就绪态。如果事件1产生之后，所以等待事件1的进程都等到满足，那么这些进程都会挂到就绪队列里面去。 线程管理： 很久之前，操作系统一直以进程作为独立运行的基本单位，直到80年代中期，人们有提出了更小独立运行的基本单位—线程。 为什么使用线程？ 什么是线程 线程的实现 多线程编程接口举例 7.8. 为什么使用线程 例子： 1、单进程的实现方法 可能出现的问题： 播放出来的声音能否连续？ 各个函数之间不是并发执行，影响资源的使用效率。 2、多进程的实现方法 可能出现的问题： 进程之前如何通信，共享数据？ 维护进程的系统开销比较大： 创建进程时，分配资源，建立PCB；撤销进程时，回收资源，撤销PCB；进程切换时，保存当前进程的状态信息。 根据以上问题，提出一个新的实体，满足一下特性： 实体之间可以并发地执行； 实体之间共享相同的地址空间； 这个实体就是：线程（Thread） 7.9. 什么是线程 1、定义：进程当中的一条执行流程 2、从两个方面来重新理解进程 从资源组合的角度： 进程把一组相关的资源组合起来，构成了一个资源平台（环境），包括地址空间（代码段，数据段）、打开的文件等各种资源； 从运行的角度： 代码在这个资源平台上的一条执行流程（线程）。 ps：进程中的堆，代码段，数据段是线程所共享的内容。而各自又有独特的内容，比如所堆栈，程序计数器，寄存器（不同的执行留和控制流）。所以其有独立拥有的部分，也有公有的部分。 3、线程 = 进程 - 共享资源 线程的优点： 一个进程中可以同时存在多个线程 各个线程之间可以并发地执行 各个线程之间可以共享地址空间和文件等资源 线程的缺点： 一个线程奔溃，会导致其所属进程的所以线程奔溃，安全没有一定的保障。 4、不同操作系统对线程的支持 5、线程所需的资源 6、线程与进程的比较 进程是资源分配单位，线程是CPU调度； 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈； 线程同样具有就绪、阻塞和执行三种基本状态，同样具有状态之间的转换关系； 线程能减少并发执行的时间和空间开销； 线程的创建时间比进程短； 线程的终止时间比进程短； 同一进程内的线程切换时间比进程短； 由于同一进程的各线程间共享内存和文件资源，可直接进行不通过内核的通信。 （切换进程的时候，需要把页表也切换掉，切换页表的开销比较大，因为硬件的信息无效，需要重新加载） 7.10. 线程的实现 主要有三种线程的实现方式： 用户线程：在用户空间实现； 内核线程：在内核中实现； 轻量级进程：在内核汇总实现，支持用户线程 用户线程：操作系统看不到的线程称为用户线程 内核线程：操作系统管理起来（能够看见）的线程称为内核线程 1、用户线程与内核线程的对应关系 多对一 一对一 多对多 2、用户进程 线程控制块（TCB）是在库里面实现的，对于操作系统而言，其看不见TCB，只能看见进程的信息，但是进程里面的线程信息，是有线程管理的库来实现的。 在用户空间实现的线程机制，它不依赖与操作系统的内核，由一组用户级的线程库函数来完成线程的管理，包括进程的创建，终止，同步和调度等。 由于用户线程的维护由相应进程来完成（通过线程库函数），不需要操作系统内核了解用户线程的存在，可用于不支持线程技术的多进程操作系统； 每个进程都需要它自己私有的线程控制块（TCB）列表，用来跟踪记录它的各个线程的状态信息（PC，栈指针，寄存器），TCB由线程库函数来维护； 用户线程的切换也是由线程库函数来完成，无需用户态/核心态切换，所以速度特别快； 允许每个进程拥有自定义的线程调度算法。 否则如果进程被操作系统调度为阻塞态，则其下的所有线程都无法允许。 用户线程的缺点： 阻塞性的系统调用如何实现？如果一个线程发起系统调用而阻塞，则整个进程在等待。因为操作系统只能看见进程，所以这个进程阻塞，旗下所以的线程都会阻塞。 当一个线程开始执行后，除非它主动地交出CPU的使用权，否则它所在的进程当中的其他线程将无法运行。 由于时间片分配给进程，故与其他进程比，在多线程执行时，每个线程得到的时间片较少，执行会较慢。 3、内核线程 （操作系统看得见，TCB是放在内核里面的） 内核线程是指在操作系统的内核当中实现的一种线程机制，由操作系统的内核来完成线程的创建，终止和管理。 在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息（PCB和TCB）； 线程的创建，终止和切换都是通过系统调用/内核函数的方式来进行，由内核来完成，因此系统开销较大； 在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不影响其他内核线程的运行； 时间片分配给线程，多线程的进程获得更多的cpu时间； Windows NT和Windows 200/XP支持内核线程 4、轻量级进程（LightWeight Process） 它是内核支持的用户线程，一个进程可有一个或多个轻量级进程，每个轻量级进程有一个单独的内核线程来支持，（Solaris/Linux） 7.11. 上下文切换 1、定义：停止当前运行的进程（从运行状态改变成其他状态）并且调度其他进程（转变成运行状态的）的过程，称为进程的上下文切换（Compress） 必须在切换之前存储许多部分的进程上下文 必须能够在之后恢复他们，所以进程不能显示它曾经被暂停过 必须快速（上下文准换是非常频繁的） 进程的上下文切换所具体切换的进程所用到的寄存器，使用要关注cpu有哪些寄存器（PC程序计数器，SP堆栈指针）。而在做进程切换的时候，需要将这新信息保存到进程控制块的某一个地方上。 2、切换的过程 所用的信息都是和硬件紧密相连的，所用一般是使用汇编代码来完成编写。 操作系统为活跃进程准备了进程控制块（PCB） 操作系统将进程控制块（PCB）放置在一个合适的队列里 就绪队列 等待I/O队列（每个设备的队列） 僵尸队列 7.12. 进程控制—创建进程 window系统下： linux系统下： fork（）创建一个继承的子进程 复制父进程的所有变量和内存 复制父进程的所以CPU寄存器（有一个寄存器除外） fork（）的返回值 子进程的fork()返回0 父进程的fork()返回子进程标识符 fork()返回值可方便后续使用，子进程可使用getpid()获取PID 父子进程的主要区别----childPID不一样： 7.13. 进程控制—加载和执行过程 系统调用exec()加载程序取代当前运行的进程 其中： exec是准备执行一个新的程序，所以当成功的执行了exec之后，后面的printf函数是不会执行到的。 wait(pid); wait返回了，表示子进程就结束了。 当执行exec的时候，代码数据都复制了一份，但是PID没有变化。但是执行的代码改变了，也就是另外程序的执行过程，如下所示。 执行exec之后，程序的整个控制流会放生完全的变化 exec()调用允许一个进程“加载”一个不同的程序并且在main开始执行(事实上_start，系统调用) 它允许1一个进程指定参数的数量(argc)和它字符串参数数组(argv) 代码段，数据段，stack（栈）&amp;heap（堆）都会被覆盖 fork()的简单实现 对子进程分配内存 复制父进程的内存和CPU寄存器到子进程里 在99%的情况里，我们在调用fork()之后调用exec() 在fork()操作中内存复制是没有作业的 子进程将可能关闭打开的文件和链接 对于此情况需要一个优化： vfork(),vfork只是复制了一小部分的进程的内容，绝大多数的内容都没有被复制。但是这会使系统调用变成两个fork，增加了编程人员的开销。 另一个优化： 通过虚存管理科员实现一个高效的fork实现机制。也就是（Copy on Write，COW）技术，就是写的时候再进行复制。 当父进程创建子进程的时候，如果采用COW技术时，我们在做实际的子进程地址空间复制的时候并没有真是的复制，只是复制了父进程所需要的元数据—页表等等，实现按需写的情况来复制不同的页。 7.14. 进程控制—等待和终止进程 wait()系统调用是被父进程用来等待子进程的结束 父进程先与子进程死亡—子进程为孤儿进程 子进程已经死亡，但父进程还没来得及回收—子进程为僵尸进程 状态转换图： ps：执行exec的时候，程序有可能会处于不同的状态。 因为在执行exec有两个步骤，一个是加载执行程序，二个是运行执行程序。加载的时候，所需要的时间比较长，所以会处于阻塞状态。 8. 处理器调度 8.1. 背景 1、上下文切换： 切换CPU的当前任务，从一个进程/线程到另一个 保存当前进程/线程在PCB/TCP中的执行上下文（CPU状态） 读取下一个进程/线程的上下文 2、CPU调度 从就绪队列中挑选一个进程/线程作为CPU将要运行的下一个进程/线程 调度程序：挑选进程/线程的内核函数（通过一些调度策略） 什么时候进程调度？调度算法实现 问题：在进程/线程的生命周期中的什么时候进行调度？ 3、内核运行调度程序的条件（满足一条即可） 一个进程从运行状态切换到等待状态 一个进程被终结了 4、是否可以抢占 不可抢占： 调度程序必须等待事情结束 可以抢占：（常用，针对用户态的） 调度程序在中断被响应后执行 当前的进程从运行切换到就绪，或者一个进程从等待切换到就绪 当前运行的进程可以被换出 抢占使得系统程序更加的灵活和高效。 8.2. 调度原则 根据什么原则去选择一个进程去执行，这个就是调度的原则 1、执行模型 程序在CPU突发和I/O中交替 每个调度决定都是关于在下一个CPU突发时将哪个动作交给CPU 在时间分片机制下，线程可能在结束当前CPU突发前被迫放弃CPU 2、评价的指标 CPU利用率：CPU处于忙碌状态所占时间的百分比 cpu利用率越高，可以认为当前系统的效率比较好。进程调度进行得好。 吞吐量：在单位时间内完成的进程数量 吞吐量越高，说明进程的效率越好。当然希望当操作系统跑一堆进程时，吞吐率都很高。 周转时间：一个进程从初始化到结束，包括所有等待时间所花费的时间。 周转时间越断越好。 等待时间：进程在就绪队列中的总时间 指的是出于就绪态的时间越短，就越快被cpu执行 响应时间：从一个请求被提交到产生第一次响应所花费的总时间 同样也是越短越好 以上可以对cpu调度的指标有一个分析。 人们通常都需要“更快”的服务 什么是更快： 传输文件时的高带宽 玩游戏时的低延迟 这两个因素是独立的 和水管类比： 低延迟：喝水的时候想要一代开水龙头水就流出来 高带宽：给游泳池充水时希望从水龙头里同时流出大量的水，并且不介意是否存在延迟 3、算法需达到的效果 减少响应时间 及时处理用户的输出并且尽快将输出提供给用户 减少平均响应时间的波动 在交互系统中，可虞可预测性比高差异低平均更重要 增加吞吐量–两个方面 减少开销（操作系统开销，上下文切换） 系统资源的高效利用（CPU,I/O设备） 减少等待时间 减少每个进程的等待时间 其实很难满足以上的全部效果，只能泽中或者在特定场合中选择某种特定效果。 4、公平性 公平的定义： 保证每个进程都占用相同的cpu时间 保证每个进程都等待相同的时间 公平通常会增加平均响应时间 8.3. 调度算法 调度算法有三类： 通常操作系统设计的基本调度算法 嵌入式设备实时的调度算法 针对多处理器的调度算法与考虑 一、常用系统的调用算法 FCFS （先来先服务） （First Come， First Served） SPN（SJF）SRT (短进程优先（短作业优先）剩余时间优先) Shortest Process Next(Shortest Job First) Shortest Remaining Time HRRN (最高响应比优先) Highest Response Ratio Next Round Robin （轮循） 使用时间切片和抢占来轮流执行任务 Multilevel Feedback Queues （多级反馈队列） 优先级队列中的轮循 Fair Share Scheduling （公平共享调度） 二、先来先服务调度（FCFS） 如图所示，如果前面的进程越长，后面的进程等待的时间就越长，从而会影响整个系统的周转时间。 优点： 简单 缺点： 平均等待时间波动较大，平均的周转时间也会比较大 花费时间少的任务可能排在花费时间长的任务后面，没有考虑抢占 可能导致I/O和CPU之间的重叠处理（cpu密集型进程会导致I/O设备闲置时，I/O密集型进程也在等待） 三、短任务优先算法 当一个更短时间进程来了之后，有两种策略： 不理，将这个时间更短的程序排在前面，但是继续执行本来在执行的进程，这种是非抢占性的。（SPN） 将这个时间更短的程序与正在执行的程序剩余所需要执行的时间进行比较，如果跟多，这打断正在执行的程序，这种是可抢占的。（SRT） 优点： 平均等待时间最短 缺点： 可能导致饥饿 连续的短任务流会使长任务饥饿 短时间可用时的任何长任务的CPU时间都会增加平均等待时间 需要预知未来 怎么预估下一个CPU突发的持续时间 简单的解决方法：询问用户 如果用户欺骗就杀死进程 如果用户不知道就采用预估方法 根据过去，预测未来，大致的预测方法如下： 结果大致的相同 四、最高响应比优先算法 在SPN调度的基础上改进 R值越高表示等待的时间越长，就会优先的调度这种进程。 优点： 充分的考虑到了进程的等待时间，所有之前的饥饿现象会得到有效的化解。 缺点： 不可抢占 依然需要知道执行的时间是多长，所以还是要预估 五、轮循算法 各个cpu轮流占用cpu去执行，在叫做量子(或时间切片)的离散单元中分配处理器，时间片结束后，切换到下一个准备好的进程。 每一个进程都有机会去被cpu执行 例子： 可见，轮流算法的平均等待时间是比较大的。 特点： 会比较到的切换时间，进程上下文的切换，确保公平 时间片太大（有可能退化为先来先服务） 等待时间过长 极限情况退化成FCFS 时间片太小（切换过于频繁） 吞吐量由于大量的上下文切换开销收到影响 目标： 选择一个合适的时间量子 经验规则：维持上下文切换开销处于1%以内，这样的情况下99%的时间都是在 进程的执行过程中 与先来先服务算法 进行比较： 六、多级反馈队列 首先完成高优先级的进程，待其完成了所以的任务之后，再去完成第优先级的进程，这样通过分层不同级别的队列，可以实现调度的区分，使得调度的策略更加的合适。 而且进程在不同阶段的特点是不同的，所以调度算法可以考虑到进程各阶段的特点来调整其在队列中的级别。这个就是多级反馈队列可以体现。 特点： 时间量子大小随优先级级别增加而增加 如果任务在当前的时间量子中没有完成，则降到下一个优先级 能够区分进程在动态执行过程中动态的调整进程优先级，使得IO密集型的任务可以很快的执行，而CPU密集型的任务放在优先级较低位置。 七、公平共享调度 FFS控制用户对系统资源的访问 一些用户组比其他组更重要 保证不重要的组无法垄断资源 未使用的资源按照每个组所分配的资源比例来分配 没有达到资源使用率目标的组获得更高的优先级 八、小结 FCFS先来先服务 不公平，平均等待时间较差 SPN/SRT段进程优先 不公平，但是平均等待时间最小 需要精确预测计算时间 可能导致饥饿 HRRN最好响应比优先 基于SPN调度改进（考虑了等待时间） 不可抢占 Round Robin轮循 公平，但是平均等待时间较差 每一个进程有固定的时间片，但是上下文开销较大 MLFQ舵机反馈对列 和SPN类似 动态调整进程优先级 公平共享调度 公平是第一要素 8.4. 实时调度 面向的是实时的系统，更多的是工业控制（火车，机床等等） 1、定义 正确性依赖于其时间和功能两个方面的一种操作系统 2、性能指标 时间约束的及时性（deadlines） 速度和平均性能相对不重要 3、主要特征 时间约束的可预测性 4、分类 强实时系统 需要在保证的时间内完成重要的任务，必须完成 弱实时系统 要求重要的进程的优先级更高，尽量完成，并非必须 Released ：让进程处于就绪态的时间 Execution time：执行时间 Absolute deadline：绝对的截止时间，任务的执行不可以操作这个时间 Relative deadline：相对截止时间，因为任务是间隔的，一段时间完成一个任务 （周期是5，执行的时间就是蓝色的区域） 5、特点 硬时限： 如果错过了最后的期限，可能会发生灾难性或非常严重的后果 必须验证：在最坏的情况下也能够满足时限 保证确定性 软时限： 理想情况下，时限应该被最大满足。如果有时限没有被满足，那么就相应地降低要求 尽量大努力去保证 表示一个实时系统是否能够满足deadline要求 决定实时任务执行的顺序 静态优先级调度 动态优先级调度 6、实时系统中的两类调度算法： RM（Rate Monotonic）速率单调调度 最接静态优先级调度 通过周期安排优先级 周期越短优先级越高 执行周期最短的任务 EDF（Earliest Deadline First）最早期限调度 最佳的动态优先级调度 Deadline越早优先级越高 执行Deadline最早的任务 8.5多处理器调度与优先级反转 一、多处理器调度 1、多处理器的cpu调度更加复杂 多个相同的单处理器组成一个多处理器 负载平衡状态 2、对称多处理器（SMP） 每个处理器运行自己的调度程序 需要在调度程序中同步 二、优先级反转 出现的原因： T1的执行时间受制于T2的执行时间，因为T2抢占了T3的cpu时间去执行，而T1的执行有必须等待T3处理完共享内容，所以T1的执行时间被T2延长了。从而导致T1不能及时的完成其任务，导致系统处于不稳定状态而重启。 特点： 可以发生在任何基于优先级的可抢占的调度机制中。 当系统内的环境强制性使高优先级任务等待低优先级任务时发生。 解决方法： 1、优先级继承（将问题发生时，提升T3的优先级） 低优先级继承高优先级任务的优先级依赖于他们共享的资源 2、天花板优先级 “资源”的优先级和“所有可以锁定该资源的任务中优先级最高的那个任务”的优先级相同。 除非优先级高于系统中所有被锁定的资源的优先级上限，否则任务尝试执行临界区的时候会被阻塞 持有最高优先级上限信号量锁的任务，会继承被该锁所阻塞的任务的优先级 9. 同步互斥问题 9.1. 背景知识 1、如果资源处理不当，可能会出现一些意想不到的情况，合作的风险 独立的线程： 不和其他线程共享资源或状态 确定性-&gt;输入状态决定结果 可重现-&gt;能够重现起始条件 调度顺序不重要 合作线程： 在多喝线程中共享状态 不确定性 不可重现（不可重复性） 这些不确定性和不可重复以意味着bug可能是间歇性发生的，也就是合作是有风险的。 2、为什么要合作 共享资源 资源是需要共享的，因为进程可能要访问同一个文件。 加速 通过并行和并发，可以提高系统的效率，实现更有效的资源的利用。相当于把一个大的任务，拆分成多个小的任务，每个任务通过并行的执行提高系统的性能。 模块化 在设计时将一个大的工作，变成一个小的工作，使之具有模块化，使系统便于扩展。 3、问题出现的原因 例子： 以上四条汇编指令的意思是： 把next_pid赋值给寄存器1（Reg1） 再把这个寄存器1存到了new_pid这个内存单元的去。此时new_pid就具有了next_pid这个值。 寄存器1加一操作。 完成next_pid的值增加了一个1的操作。 总的实现过程： 先把new_pid = next_pid，然后next_pid再加1. 但是，如果这时有两个进程，就会出现意想不到的情况： 问题产生的原因： 在第二次进程的上下文切换时候，进程1的寄存器恢复之后依然100的值，是的next的值无法更新称为102。最终产生了切换使得最终的结果不是想要的结果。这是一种典型的异常现象。 9.2. 一些概念part1 由于上述产生的异常现象（称之为竞态条件Race Condition），这就是为什么要引入同步互斥这些机制的原因，就是要解决这种不确定性的问题。 1、系统缺陷：结果依赖于并发执行或者事件的顺序/时间 不确定性 不可重现 2、怎样避免竞态？ 让指令不被打断（比如上述的四条机器指令不被打断） 3、不被打断的方法：原子操作（Atomic Operation）—不可被打断操作 原子操作是指一次不存在任何中断或者失败的执行 该执行成功结束 或者根本没有执行 并且不应该发现任何部分执行的状态 实际上操作往往不是原子的 有些看上去是原子操作，实际不是 连x++这样简单的语句，实际上是由3条指令造成的 有时候甚至连条单条机器指令都不是原子的 例子： 所以需要后续的同步机制，确保或者是A赢或者是B赢。 4、一些基本概念 临界区（Critical section） 临界区是指进程中的一段需要访问共享资源并且当另一个进程处于相应代码区域时便不会被执行的代码区域。简单来说，就是访问共享资源的那段代码就是临界区。 互斥（Mutual exclusion） 当一个进程处于临界区并访问共享资源时，没有其他进程会处于临界区并且访问任何相同的共享资源。 死锁（Dead lock） 两个或以上的进程，在互相等待完成特定任务，而最终没法将自身任务进行下去。 饥饿（Starvation） 一个可执行的进程，被调度器持续忽略，以至于虽然处于可执行状态却不被执行 9.3. 一些概念part2 1、一个有趣的类比： 2、解决的方法和概念 3、更好的解决方法（轻量级） 但是由于进程上下文切换的原因，问题还是会存在。 如果只是将Note往前面提简单的挪动一下还是不会解决问题，变成谁都不会去买面包了。 9.4. 一些概念part3 1、再换一个方法 结果还是有问题的。 需要确保在任何情况下，只有一个进程在临界区中执行，其他的进程需要在外面等待。 一个更加合理的方案解析过程： 程序是有效的，但是导致代码不一样了。 最终的解决方案： 为每一个线程保护了一段“临界区”代码。使用临界区的思想，问题就可以较好的解决。其是讲前诉方法的一个抽象。 有了临界区的代码之后，就可以确保任何时候只有一个进程在临界区中执行，切其他进程在外面等待，知道临界区中的进程离开，其他进程中的一个会进入临界区去执行。这个是比较合理的一个实现。 9.5. 临界区 在临界区中执行所拥有的属性： 1、互斥：同一个时间临界区中最多存在一个线程 2、前进（Progress）：如果一个线程想要进入临界区，那么它最终会成功，不会一直的死等。 3、有限等待：如果一个线程i处于入口区，那么在i的请求被接受之前，其他线程进入临界区的时间是有限制的。如果是无限等待，就会出现饥饿状态，是Progress前进状态的一种进一步补充。 4、忙等（可选属性）：如果一个进程在等待进入临界区，那么在它可以进入之前会被挂起。 基于这些属性，设计一些方法对临界区进行保护： 方法一：禁用硬件中断 方法二：基于软件的解决方法 方法三：更高级的抽象（基于硬件原子操作的指令） 9.6. 禁用硬件中断 一、基本实现 没有中断，也就是没有了上下文切换，因此没有并发。 硬件将中断处理延迟到中断被启用之后 大多数现代计算机体系结构都提供指令来完成 进入临界区时禁用中断，离开临界区时开启中断。这个方法是可以解决问题的。 二、缺点 1、一旦中断被禁用，线程就无法被停止 整个系统都会为你停下来 可能导致其他线程处于饥饿状态 2、要是临界区可以任意长 无法限制响应中断所需的时间（可能存在硬件影响） 需要注意： 执行这种课屏蔽中断的指令，只是把自身的响应中断的能力屏蔽了，并不意味着也将其他cpu的响应中断能力屏蔽，所以其实其他的cpu还是可以继续产生中断，所以在多cpu的情况下是无法解决互斥问题的。 9.7. 基于软件的解决方案 一、思考方案 1、思考方案一： 某一个进程，它想进入临界区，其有一个顺序（次序），根据这个次序决定谁会进入这个临界区。 方法如下所示： 假设这个线程的次序是0，那么当turn=0时，才去继续下面的执行临界区代码，否者在while循环中一直打转。条件满足时，改变使得turn=1。 而进程1的代码也是类似的，只是while循环中的判断条件是不等于0，下面的turn=0. 这个程序的弊端是： 必须进程1执行一次临界区，进程0执行一次临界区，然后两个交替执行，才能保证两继续的执行。一旦其中的一个进程不愿意再做这个事情，那按照之前的属性，其他进程先进去就应该能够进去，但是在这种模式下，就无法完成这个前进的属性。 2、思考方案二： 前面表示了一个turn是不够表示，所以接下来使用一个小数组flag[2]来表示这个进程是否想进入临界区。 flag[0] = 1 //表示进程0想进入临界区执行 flag[0] = 1 //表示进程1想进入临界区执行 方法1如下所示： 但是这个代码是有问题的，不能满足互斥这个属性。 因为在初始的时候，两个进程都不会想进入临界区，所以两个flag都会赋值为0，表面没有这个需求。这样就是的两个进程都会跳出这个循环，然后都会将直接复制为，想要进入临界区，也就出现了多买面包的想象。 方法2如下所示： 满足了互斥，但是倘若两个线程都赋值了1，出现上下文切换的时候，都无法跳出这个循环，也就是出现是死锁的情况。 可见，互斥的解决并没有想象的那么简单~~~ 二、正确实现 1、正确的接法 将以上的两种思考都综合起来使用。三个变量共同的作用。 算法如下： 该算法可以满足互斥，前进和有限等待三个属性。 反证法来证明： 假定，现在两个进程都进入了临界区，都在执行临界区代码，但是turn只是一个值的，所以总会有一个线程会跳出循环的。 2、另外一种算法 所需的变量空间相同，但是更加的复杂 三、拓展 1、n进程解决方法1 （E&amp;M算法） 除了了针对两个进程之外，还可以拓展到n个进程如何保证互斥。 大致的思路： 对于进程i而言，对于其前面的进程而言，如果有进程想进入临界区，或者已经进入了临界区，那么i进程就必须等待。而对于i进程后面的进程，必须要等待i进程执行之后在进入临界区。这样就可以通过一个循环的方式完成n个进程有序的进入临界区。 2、n进程解决方法2（Bakery算法） 大致思路如下： 四、总结 即使是针对两个进程的解决竞态的实现还是比较复杂的。 需要忙等待，浪费cpu时间。 没有硬件包装的情况下无真正的软件解决方案。对硬件的1需求比较低（只需要load操作和store是原子操作即可） 9.8. 更高级的抽象 — 基于原子操作 软件的处理比较复杂有没有更加简单的实现方法？ 一、基础 使用了一些特殊的操作： 1、Test-and-Set测试和置位 （一条机器指令，但是完成了读写操作两条指令的功能） 从内存中读取值 测试该值是否为1（然后返回真或假） 内存值设置为1 2、交换 （交换内存中的两个值） 只要计算机系统提供了这两条的其中一条指令就可以很容易的完成互斥的问题。 二、解决的方法 1、Test-and-Set方式 解决忙等的情况：先让其睡眠，在加一步唤醒操作 两者的区别： 忙等：不需要上下文切换，但是利用率低，适用与临界区执行时间短的情况。 不忙等：需要上下文切换，上下文切换开销比较大大，适用于临界区很长，远远大于上下文切换所需要的开销。 2、交换的方式 解析： 1)当一个进程想要进入临界区的时候，key=1，而且lock的初始值是0，所以当执行到while循环的时候，由于执行了交换，交换执行的过程不会被打断进行上下文切换的操作，而后lock的变成了1，而key变成了0.所以会退出循环，执行临界区的代码。 2)需要注意的是，当进入临界区的时候，load已经是1，当其他进程进入临界区执行的时候，load是1，而key也是1，交换之后还是1，一直会循环的等待，进入不了临界区。知道进入临界区的进程，退出临界区之后，完成一个将load变成0的操作。其他等待的进程才会继续执行exchange。 三、采用这种原子操作的特点 1、优点 简单并且容易证明 适用于单处理器或者共享主存的多处理器中任意数量的进程 可以很容易拓展n个进程，可以用于支持多临界区 开销比较小 2、缺点 忙等待消耗处理器时间 当进程离开临界区并且多个进程在等待的时候可能导致饥饿现象 出现死锁的情况（例子：如果一个低优先级的进程拥有临界区并且一个高优先级进程也需求，那么高优先级进程会获得处理器并且等待临界区 — 需要用优先级反转的方式进行处理） 四、总结 1、锁是更高级的编程抽象 互斥可以使用锁来实现 通常需要一定等级的硬件支持 2、常用的三种实现方法 禁用中断（仅限于单处理器） 软件方法（复杂） 原子操作指令（但处理器或多处理器均可）—更常用 3、可选的实现内容 有忙等待 无忙等待 10. 信号量与管程 10.1. 背景 利用信号量和管程解决同步互斥的问题 1、并发问题：竞争条件（竞态条件） 多程序并发存在大的问题 2、同步 线程共享公共数据的协调条件 包括互斥与条件同步 互斥：在同一时间只有一个线程可以执行临界区 3、解决同步问题正确比较难 需要高层次的编程抽象（如：锁） 从底层硬件支持编译 解决的过程图如下所示： 10.2. 信号量 （与信号灯有类似之处） 1、抽象数据类型 一个整形（sem），两个原子操作 p操作：sem减一 如果信号量sem&lt;0，认为执行p操作的进程需要睡眠 如果信号量sem&gt;0，认为执行p操作的进程可以继续执行，可以进入临界区 如果挡住了，就不能执行后续的程序，起到了一个阻挡的作用。 v操作：sem加一 如果信号量sem&lt;=0，认为当前的进程等待在这一个信号量上面，然后会唤醒这个进程（一个或多个） 2、信号量的图解机制 如果再来一个列车，信号量就不够了，直到一个列车离开了这个临界区之后，会执行一个v操作，而进入临界区之前会执行一个p操作。 离开这个临界区执行v操作之后，这个进程将道空出来之后，还会通知等待的列车去执行 3、由来 10.3. 信号量的使用 一、基础 1、属性 信号量是整数（有符号数） 一开始通常会设定为一个大于0的数，所以一开始执行p操作不会被阻塞。但是多次执行p操作之后，执行p操作的进程就会等待在上面。这时需要起床进程执行v操作，然后唤醒等在这个上面的进程。（如果只能唤醒一个进程，一般是唤醒第一个等待的进程，FIFO对列） 信号量是被保护的变量 初始化完成后，唯一改变一个信号量的值的办法是通过p操作或者是v操作 操作必须是原子 p操作（信号量减一操作）能够阻塞，v操作（信号量加一操作）不会阻塞 假定信号量是公平的 没有线程被阻塞在p操作仍然阻塞如果v操作被无限频繁地调用（在同一个信号量） 在实践中，FIFO经常被使用 2、两种类型信号量 二进制信号量：可以是0或1（与前面的lock达到同样的效果） 一般/计数信号量：可取任何非负值 两者互相表现（给定一个可以实现另一个） 3、信号量可以用在两个方面 互斥 条件同步（调度约束–一个线程等待另一个线程的事件发生） 二、信号量的使用 1、思想介绍 用二进制信号量实现的互斥 解析： 一开始要设置一个初始值，为了模仿lock操作，实质了初始值为1。然后在临界区之前，作一个信号量的p操作，在临界区执行之后，作一个信号量的v操作。这个就是二进制信号量的最常用法，完全可以代替前面的lock操作。 用二进制信号量完成同步操作 其他复杂的问题 一个线程等待另一个线程处理事情 比如生产东西或消费东西 互斥（锁机制）是不够的 2、正确性要求： buffer是有限的 任何一个时间只能有一个线程操作缓冲区（互斥） 允许一个或多个生产者往buffer中写数据，但是这时候不允许消费者读数据 允许一个或多个消费者往buffer中读数据，但是这时候不允许生产者写数据 当缓冲区为空时，消费者要休眠，消费者必须等待生产者（调度/同步约束） 当缓冲区已满时，生产者必须等待消费者（调度/同步约束） 3、使用分析 每个约束用一个单独的信号量 二进制信号量互斥 对buffer做添加或者取出的保障 一般信号量fullBuffers 代表一开始buffer的数据多少，如果为0，则表示一开始的buffer是空的 一般信号量emptyBuffers 代表当前生产者可以往这个buffer塞多少个数据 以上两个技术信号量用于同步操作，当buffer还有空间时，应唤醒生产者继续生产。 4、代码操作 数据可初始化如下： 生产者的操作： 调用这个函数实现生产者不停的添加数据 消费者的操作： 调用这个函数实现消费者不停的取出数据 解决互斥同步总实现代码分析 解析： 对于生产者来说，由于一开始的buffer设置允许塞进的数据是n，所以生产者可以往下执行。进行buffer的生产操作。 但是在生产之前，需要对mutex进程减操作，使之为0。。生产操作完成之后，将mutex加1操作。这次就保证了buffer的互斥问题，确保之间只有一个线程可以执行。两个操作确保了add buffer是一个互斥的操作，确保互斥性。 互斥操作完成之后，在将fullbuffer进行一个v操作，加1，提醒生产者可以正常的消费。 而对消费者来说，fullbuffer一开始初始值为0，所以是没有数据的。消费者不可能取到数据，所以在等待。所以刚刚生产者唤醒了消费者，和生产者fullbuffer的v操作相匹配。而后进行互斥的取数据操作 取出数据之后，会将emptybuffer进行v操作，表示唤醒生产者可以继续生产，也就是生产的进程可以继续执行。 以上就运用了互斥机制和同步机制来实现了一个完成的消费者和生产者的问题。需要注意好初值的确定。 问题：p，v操作的顺序有影响吗？ v操作是加一操作，所以没有影响 p操作是减一操作，会导致阻塞，所以会产生严重的影响，比如死锁的情况 10.4. 信号量的实现 不仅要会用信号量，还需要知道信号量使用的细节 1、伪代码实现 2、需要注意的问题 信号量的双用途 互斥和条件同步 但等待条件是独立的互斥 读/开发代码比较困难 程序员必须非常精彩信号量 容易出错 使用的信号量已经被另一个线程占用 忘记释放信号量 不能处理死锁问题 10.5. 管程 管程的抽象程度更高，更加容易的来完成相应的同步互斥的问题。 一、基础 1、目的：分离互斥和条件同步的关注 （一开始是完成编程语言的设计，而不是操作系统的设计的，所以其整体上是针对语言的并发机制来完成的） 2、什么是管程（moniter） 管程是包含了一系列的共享变量以及针对这些这些变量函数的一个组合或模块。其包括： 一个锁：指定临界区（确保互斥性，只能有一个线程） 0或者多个条件变量：等待/通知信号量用于管理并发访问共享数据 3、一般方法 收集在对象/模块中的相关共享数据 定义方法来访问共享数据 大致的结构图： 一开始，所有进程在右上角的排队队列中，排队完后进行wait()操作，等到signal()操作唤醒后，执行这个进程的代码。 分析： 实现： 解析： 这里的numwaiting代表的是当前等待线程的个数，而之前的sem是代表信号量的个数。 信号量的实现v操作和c操作是一定会执行的，也就是一定会执行加一操作或者是减一操作。 而这里的wait操作是会做加操作，而signal里面，不一定要做减操作。 这里在wait函数中，还没有require lock就要release（lock）的原因下面再进行讲解。 release(lock)之后，会做一次schedule(),表示会选择下一次线程去执行，因为本身这个线程已经处于睡眠状态了。 schedule()完毕再做一个require(lock)的操作，这里又是为什么？这里的release和require和之前的有所不同，下面讲解。 signal函数是作唤醒的操作，从等待队列里面取出一个线程唤醒，与之前的schedule()是对应的。wakeup(t)是对schedule的进一步触动机制。最后waiting再进行减减操作。 如果等待队列为0，则啥操作也不做这里的numwaiting代表的是当前等待线程的个数，而之前的sem是代表信号量的个数。 二、使用 1、对管程进行初始化 需要注意： lock变量是保证互斥的操作。 condition条件变量，这里有两个条件变量，一个是buffer满，一个是buffer空，也就是 count中记录了buffer中的空闲情况，count=0，表面buffer是空的，如果buffer是n，表面buffer是满的。 2、互斥机制 生产者是Deposit()，消费者是Remove()。 需要注意： 这里不仅仅要对buffer做操作，响应的还要在count中记录下来。 信号量互斥的实现是仅仅靠着这个buffer的，而这里的互斥是在函数的头和尾。 buffer空了消费者会去睡眠，而buffer满了生产者会去睡眠。 为什么？ -----这是管程的定义来决定的 ·因为管程定义，进入管程的时候，只有一个线程可以进去，才能执行管程所管理的所以函数。而既然这图中的两个函数是属于管程管理的两个访问共享变量的函数，就要确保其互斥性和唯一性。所以一进入这个函数就是互斥的。 3、同步机制 生产者的buffer未满操作 如何实现：buffer空了消费者会去睡眠，而buffer满了生产者会去睡眠的过程？ 当buffer满的时候，也就是count=n，这时候会作一个 notfull.wait(&amp;lock)操作。notfull是一个条件变量，不需要有一个初始值。notfull.wait(&amp;lock)就表示当前已经满了，我需要睡眠，同时还带有一个lock。而这个lock就是管程的lock。 小插曲 这时先解释前面的问题：为什么是先relase再require一个锁呢？ 解析： release(lock)：实际上说让当前的生产者释放到这个锁，这使得其他的线程才有可能进入管程去执行。因为这时候生产者要休眠了，所以必须要把这把锁释放。而其释放是由于之前其有一个lock-&gt;Acquire(),已经获得了这个锁。所以在wait操作一定要释放，不然所以等待的线程都在等待，系统会停滞。 一旦将来被唤醒了，也意味着可以继续从schedule中继续往下执行，再去完成一次require(lock)。一旦获取了lock之后，就会跳出wait操作，看看count是否等于n。 消费者的buffer未满操作 而在notfull的另一边，需要有一个唤醒机制，所以消费者这边会有一个notfull.signal()操作。一旦count做了一个减减操作，buffer满了消费了一个操作，这是buffer句未满了，所以需要去进行唤醒，去唤醒正在等待在这上面的线程。 消费者的buffer为空操作与生产者的buffer非空唤醒操作 这消费者这边，buffer空的时候，也同样会有一个while操作，会判断count是否等于0，如果是会作一个notempty的wait操作，直到生产有一个notempty.signal的信号唤醒才可以继续去执行。两者合在一起就是完整的管程来解决生产者消费者的问题。 总的实现与和信号量的代码比较 管程实现： 信号量实现： 两者相比，可以看出，与信号量实现的总体功能是一样的，但是实现的细节不一样。 三、两种特别的方式 问题： 管程实现生产者消费者问题中，还需要注意到一点，当线程在管程中执行时，如果线程这时候要执行针对某一个条件变量的signal唤醒操作之后。这时候，是执行等待在这个条件变量上的线程？还是发出唤醒的线程执行完毕后再让那个等待的线程执行？ 1、两种方法 Hoare方法（比较完美） 一旦发出了signal操作之后，就应该让等待的线程继续执行，而其自身去睡眠。直到等待的线程执行了release之后，这个线程才能继续执行。 特点：比较直观，但是实现起来比较困难 执行的流程如下： Hansen方法： 当发出了signal操作之后，不一定要马上放出对cpu的控制权，而是等发出signal的线程执行完release操作之后才转移cpu控制权。 特点：实现起来比较容易 执行的流程如下： 2、比较 Hoare的while操作可以用if操作来实现，而Hansen的不行，这是唤醒机制不同而造成的。 对于Hansen来说，其并没有马上让等待在这上面的线程执行，所以其必须要做relseae才能释放，所以这时会存在多个被唤醒的线程，抢这个继续执行的count。所以当选择自己的时候，count已经不为n了，所以要循环的查询。 对于Hoare来说，执行之后会马上的转移cpu、控制权，而这时只要一个线程被唤醒，不存在多个的问题。而其一定可以往下执行，因为count一定不为n。 四、总结 10.6. 经典同步问题1----读者优先读写者问题 一、读者写者问题 1、动机：共享数据的访问 2、两种类型的使用者 读者：不需要修改数据 写者：读取和修改数据 3、问题的约束 允许同一时间有多个读者，但在任何时候只有一个写者 当没有写者时读者才能访问数据 当没有读者和写者时写者才能访问数据 在任何时候只能有一个线程可以操作共享变量 读者优先，不按时间顺序 4、共享数据的设计 数据集 信号量CountMutex初始化为1 保证count的读写是互斥的 信号量WriteMutex初始化为1 保证写者的互斥保护，因为只允许一个写操作 整数Rcount初始化1 当前读者的数量，因为可以有多个读者同时操作 二、实现的过程 sem_wait：就是p操作，也就是减一操作 sem_post：就是v操作，也就是加一操作 1、写者的互斥保证 分析： 包起来之后确保只有一个线程可以进行写操作。且一旦写者在写，读者就不能读，只能等待。而当读者在读数据的时候，写者也不能写数据。完成了读者写者的互斥操作与写者与写者的互斥操作。 但是没有体现可以允许多个读者读数据 2、多读者体现 分析： rcount=0，代表当时没有读者，所以只要没有写者，就可以继续的执行。 但是当如果rount！=0的时候，表明当前已经有读者线程在读数据了，也意味着接下来的操作，写者是一定进不来的，rcount++操作完成读就好。 当读完的时候，如果rcount=0，也就是说读者已经读完了，这时外面可能存在写者，所以要唤醒。 3、多读者的互斥 分析： 确保不会存在多个读者同时对rcount进行操作，也就是保证rcount数据的互斥性。 4、完整的读者优先的读者写者问题 三、读者优先与写者优先的区别： 10.7. 经典同步问题1—写者优先读写者问题 利用管程实现写者优先的读者写者问题 一、基础 1、方法构思 需要注意： 读者进行读操作时要注意当前是否有写者（两类：正在写数据的写者和正在等待的写者），这两类写者只要有一个存在，那么读者就需要等待。都不存在才有机会进行读操作。 读完之后，检测是否有写者正在等待，其有责任去唤醒。 当当前有读者正在读的读者或者正在写的写者时，需要等待。（正在等待的读者不需等待，写者优先） 写操作之后，唤醒正在等待的写者或者正在等待的读者。 2、数据结构 AR：当前处于读数据库读者的个数 AW：当前正在写的个数 WR：当前正在等待读者的个数 WW：当前正在等待写者的个数 oktoread：表示当前可以去读 oktowrite：表示当前可以去写 lock：确保只有一个函数进入管程去执行 二、实现 1、读者的具体实现 解析： 因为读者读数据的时候，要确保没有正在写的写者和正在等待的写者（写者优先），所以while语句中判断的依据是（AW+WW）&gt;0的时候，都需要等待，并且不断记录被等待的读者，也就是WR++。等到没有写者的时候，被唤醒，其中一个等待的读者可以继续执行，并且WR–。 当完成读数据库的操作时，正在读的读者减一操作。并且当此时已经没有读者而且正在有等待的写者时，进行唤醒写者的操作。但是当还有读者的时候，为了保证读写的互斥，就没有必要唤醒写者了。 2、写者的具体实现 解析： 当一个写者想写数据的时候，首先进行判断当前有无正在读的读者或者是正在写的写，等待的不需考虑。若没有时，说明可以有机会被唤醒去执行后面的操作。否者继续等待，直到被唤醒，然后等待的写者++。 当写完数据时，正在写的写者减一操作（其实我认为AW只有01两个取值，有正在写的写者，或者没有正在写的写者），此时表面没有正在写的写者，而当有等待的写者，既去唤醒其中的一个写者执行。否则，当有正在等待的读者时，去唤醒全部的读者。 需要注意，signal是唤醒等待在这个条件变量上的一个，而broadcast是唤醒等待在这个条件变量上面的全部。 10.8. 经典同步问题2—哲学家就餐问题 一、基础与尝试 1、问题描述 拿叉子，减一的p操作 放叉子，加一的v操作 2、尝试解决（可以跳过） 方案一： 结果：会导致死锁，谁都拿不了右边的叉子 方案二： 结果：会重复过程 方案三：等待随机的时间 结果：等待时间随机变化，可行，但非万全之策。可能等待时间长的哲学家一直在等待。 方案四： 使用信号量的互斥锁来保护 结果： 互斥访问可以实现不会出现死锁的情况，但是每次只允许一个人进餐。本来可以并行两个哲学家同时吃饭，这与问题项背，效率较低。 其将就餐（而不是叉子）看成是必须互斥访问的临界资源，因此回造成（叉子）资源的浪费。 二、实现思路 1、不同的思考 哲学家维度 计算机维度 2、编写 思路 数据结构 操作方法 3、具体的实现 函数take_forks的定义 需要注意： hungry的状态需要互斥保护 拿两把叉子的过程其实也是在互斥的保护之中 函数test_take_left_right_forks的定义 分析： 首先确保自己是出于饥饿状态的，然后判断两旁的人是否是出于eatting状态，如果都不是，意味两边都有叉子，就可以吃饭了。 可以看出，两把叉子到手，没有一个具体的变量来体现，而是说用状态来表示（因为拿一把叉子是没有意义的）。 而在前面赋初值的时候，s[i]的初值是0，v操作之后，自身编变成了1，也就是自己通知自己可以吃饭了。 问题：为什么会通知自己吃饭？ 因为在take_forks函数的最后，会有一个p操作，加1之后会减1操作，所以这里的p操作不会被阻塞。只是使得同步信号量加一操作之后，使这里的减一操作不会被阻塞。 函数put_forks的定义 功能：把两把叉子放回原处，并在需要的时候去唤醒左岭右舍 需要注意： 这里查看自己的左邻居能否进餐的时候，还有看自己左邻居的左邻居的状态。如果自己左邻居的左邻居的状态是进餐状态，这左邻居不可能进餐。自己的右邻居同理。 程序设计的思考过程 以一般的思路分许问题，写出一个伪代码，再将伪代码变成程序。 在这个过程中要设定好变量（同步和互斥的机制） 逐步细化的方式实现这个处理的过程，一般来说是会匹配的（p操作和v操作） 11. 死锁 11.1. 死锁问题 1、死锁现象 出现的原因：进程并发运行 11.2. 系统模型 1、资源概念 资源一旦是被使用状态，则其他的进程就不应该运用这个资源，有互斥性，如果没有互斥性，就不会产生死锁。 进程使用资源的有限的，资源恢复到空闲的情况。 2、可重复使用的资源 在一个时间只能一个进程使用且不能删除 进程获得资源，后来释放有其他进程重用 处理器，io通道，主和副存储器，设备和数据结构，如文件，数据库和信号量都可以看作是资源的一种形式 如果每个进程拥有一个资源并且请求其他资源，死锁可能发生 3、如何使用资源 创建和销毁进行资源管理，内存管理 在op缓冲区中的中断，信号，消息，信息 如果接受消息阻塞可能会发生死锁 可能少见的组合事件会引起死锁 存在进程管理和调度的过程 4、资源分配图 pi-&gt;rj：表示进程i需要j的资源 rj-&gt;pi：表示资源i被j所使用 5、死锁的判断 情况一 不会产生死锁 情况二 会产生死锁，这个图形成了一个环状的结构（一个大环和小环） 情况三 有环状的资源分配图没有死锁 总结： 死锁一定有环，但是有环不一样产生死锁 11.3. 死锁的特征 这个是死锁出现的四个特征： 需要注意：这四个特征出现并不意味着死锁的出现 右图的p2和p4不满足持有并等待资源，所以不满足这四个特征，所以不是死锁。 11.4. 死锁处理办法 以上的四个方法的约束一个比一个弱，死锁预防的约束最强，而死锁恢复的约束最差。 方法一：确保系统永远不会进入死锁状态 操作系统的功能会被限制，应用系统无法重复的利用cpu执行开销也很大 方法二：运行系统进入死锁状态，然后恢复 但是判断死锁的开销非常大 方法三：忽略这个问题，假装系统中从来没有发生死锁；用于绝大多数的操作系统。 靠假设来忽略这个问题，实际操作的常用方法 11.5. 死锁预防和死锁避免 1、死锁的预防 ---- 让死锁不会出现 思路：只要将前诉的四个资源打破其中的一个，那么久不会出现死锁。 针对死锁的四个必要条件，打破死锁进行一开始预防： 互斥 本来资源是互斥的，通过使资源不互斥。 占用并等待 将条件变大，拿资源就拿全部的资源才去执行，否者不能资源去睡眠，这样就不会存在死锁。但是不同的执行过程中，需要的资源不同，导致一直占用资源但是没有使用，所以会导致系统资源的利用率低。 不抢占 直接将进程kill掉，也就将资源抢占过来了，但是手段比较的暴力，不合理。 循环等待 死锁的出现会出现一个环，打破这个环可以实现死锁的预防。如果对资源类型进行排序，进程按资源顺序进行申请，也就是资源只能往上进行申请，这样就不会形成循环的圈。但是前提是要讲资源排好序，但是资源利用还是不合理的。 2、死锁避免 比上诉的约束条件放松一点 思路：当进程在申请资源的过程中，然后判断这个申请合不合理，如果会存在死锁的概率，就会拒绝这个请求。 需要注意： 其中，不安全状态不一定对导致死锁状态，所以不安全状态是包含着死锁状态，我们需要的是安全状态。将是否会形成环来作为判断依据。 问题：什么是安全状态？ 针对所有的进程，存在一个时间序列，按照这个序列执行，先后顺序执行，所以的进程都可以正常的等待所需要的资源，正常的结束。 要避免进入unsafe空间。而在safe状态不会出现一个环。 11.6银行家算法 一、基础 1、算法的背景 2、前提基础 很重要的判断：safe还是unsafe 二、算法设计 1、数据结构的设计 n = 进程数量 m = 资源类型数量 （其中，每一个资源类型还要一个量） Max（某种类型的总需求量）：nxm矩阵。 如果Max[i，j] = k，表示进程Pi最多请求资源类型Rj的k个示例 （可以知道其整个生命周期中共需要该类多少个资源） 其中存在一条关系式： 2、初始化 3、操作 执行之后： 可申请的资源变少，变少了request 已分配的资源变多，变多了request 还需要的资源变少，变少了request 根据返回值做出改变： 以上就是银行家算法的一个大致思路。 三、示例 第一个例子 1、首先系统和进程所拥有的资源如下图所示 需要注意： Max：所有进程需要资源的情况 Need：当前进程需要进程的情况 Available：系统还剩下资源的情况 Allocation：当前进程已经拥有的资源 Resource：当前系统中总资源的个数 2、可见，p2可以满足情况，执行后可返回其所占有的资源 3、回收资源之后，按照顺序，p1所需要的资源是可以满足的，可以执行 4、p1执行完之后，对资源进行回收，接下来剩下的两个进程偶读可以满足要求。可以随便选一个，比如p3，然后再选择p4. 结论： 所以，这样我们就已经找到了一个序列，如果按照p2-p1-p3-p4这个顺序去执行，就可以实现所以的进程都可以正常的执行并结束，其所需要的资源都可以得到满足。这个就是安全的执行序列，safe。 第二个例子： 如果一开始p1提出了一个101请求，执行之后 此时系统所剩余的资源为011，此时不能满足任何的其他进程，会进入一个unsafe的状态。 所以，一开始银行家算法是不会接受p1的101的请求的。 总结： 银行家算法的思路是判断当前的资源分配操作是否安全的，如果安全则可以执行，如果不安全就不能分配出去。 11.6. 死锁检测和死锁恢复 一、基础 1、背景 死锁的检测又将条件放宽了一点。 前面的死锁避免是既是不会导致死锁的现象方法，但是如果会出现不安全状态，也不会执行。 这里的死锁检测允许系统进入unsafe状态，在某一个状态判断当前的系统是否出现死锁，如果是，就启动恢复机制；如果没有，就继续执行，将死锁的检测放在了系统运行中，更往后了。 2、死锁检测的大致思路 允许系统进入死锁状态 死锁检测算法 恢复机制 3、检测原理 将资源分配图中资源的节点简化，只留下进程。从而将资源分配图，变成进程等待图。然后再判断这个等待图是否具有环。有环就代表有可能死锁。 死锁检测算法 死锁检测算法，定期的执行对操作系统运行比较大，更多是起调试的作用。而已银行家算法需要提前知道进程未来所需要的资源，这个是比较难实现的，只能去预估。 二、示例 1、例子一 2、例子二 结果： 没有一个进程的需求可以得到满足，死锁会检测出一个环，与银行家算法是比较类似的。 三、算法是使用 四、死锁的恢复 都存在某种程度上的强制性和不合理性。所以死锁恢复是最后的手段。 11.7. IPC概述 一、基础 IPC的意思就是进程间通信 1、问题：为什么要进行进程间通信？ 进程之间可能要完成一个大的任务，这需要一定的数据的沟通和信息的传递，保存进程独立性的通信，保证其可以有效的沟通。 2、IPC提供2个操作 send message receive message 3、通信的前提 在他们之间建立通信链路 通过send/receive交换消息 4、通信链路实现 物理（例如共享内存，硬件总线） 逻辑（例如，逻辑属性） 二、间接通信与直接通信 直接通信 间接通信 主需要关注在哪里收数据或者将数据丢到哪里去就行了。一般是os中的共享数据。 三、阻塞或是非阻塞的 11.8. 信号，管道，消息队列和共享内存 （这里只是作简单的介绍，没有涉及具体的实现方法） 1、数据的缓冲 需要注意： 无论是哪种情况，当缓冲中没有数据的时候，接收方都必须等待数据的到来 一、信号 1、介绍 关注某一种信号，发生了某一种响应之后，可以编写特定的处理函数。效率比较高。 处理完之后，会回到被打断的函数重新的实现。 2、如何实现 应用程序针对某种新号作定点处理，要完成的操作是： 开始的时候，要针对某种信号的handle，把这个作为系统调用发给操作系统。操作系统就知道当这个进程发出某种信号就会跳转到预先编写的处理函数中。 操作系统将系统调用返回用户空间的堆栈进行修改，使得本来是返回调用语句的后条执行变成到这个信号处理函数的入口，同时在把信号处理函数的之后的地址作为栈帧的返回地址。所以要修改应用程序的堆栈。 二、管道 管道是用来实现数据的交换。其以文件的操作。 思路：将一个文件的输出，重定向到令一个文件的输入，这样就可以完成一系列的操作。（重定向符为“&gt;”） 如何实现： shell进程收到了这条命令之后，会创建两个进程，ls进程和more进程。同时将ls的输出到一个管道中，而不是屏幕上（内存中的一个buffer）。而对于more，不是从键盘接受信息，而是从管道中接受数据，这样就完成了输入输出的重定向功能。这样就完成了分页显示目录的功能。（存在阻塞现象） 特点： 管道是通过父进程帮子进程建立好的一个通道，如果没有父子关系，这样就不能正常工作了。 管道的数据是一种字节流。 有bufffer满和buffer空的限制 三、消息队列 特点： 数据是结构化的数据，而不是字节流，传进去的是一个有意义的数据结构 可以实现多个互不相关的进程完成数据交换 四、共享内存 上面两种都是间接通信。共享内存是直接通信的方式。 （通过内核，读写内存，实现进程的数据的交换） 共享内存的实现机制： 12. 面试总结 12.1. 用户态和内核态 计算机系统中的两类程序：系统程序和应用程序，为了保证系统程序不被应用程序有意或无意地破坏，为计算机设置了两种状态： 系统态(也称为管态或核心态)，操作系统在系统态运行——运行操作系统程序 用户态(也称为目态)，应用程序只能在用户态运行——运行用户程序 将 CPU 的指令集分为特权指令和非特权指令两类： 特权指令——在内核态时运行的指令 对内存空间的访问范围基本不受限制，不仅能访问用户存储空间，也能访问系统存储空间， 特权指令只允许操作系统使用，不允许应用程序使用，否则会引起系统混乱。 特权指令：启动IO、内存清零、修改程序状态字、设置时钟、允许/禁止中断、停机； 非特权指令——在用户态时运行的指令 一般应用程序所使用的都是非特权指令，它只能完成一般性的操作和任务，不能对系统中的硬件和软件直接进行访问，其对内存的访问范围也局限于用户空间。 非特权指令：控制转移、算术运算、访管指令、取数指令 UNIX 系统把进程的执行状态分为两种: 用户态执行，表示进程正处于用户状态中执行； 核心态执行，表示一个应用进程执行系统调用后，或 I/O 中断、时钟中断后，进程便处于核心态执行。 差别在于： 处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所占有的处理机是可被抢占的； 而处于核心态执行中的进程，则能访问所有的内存空间和对象，且所占用的处理机是不允许被抢占的。 注意 用户态切换到内核态的唯一途径——&gt;中断/异常/陷入/系统调用（就是陷入指令） 内核态切换到用户态的途径——&gt;设置程序状态字 注意一条特殊的指令——陷入指令（又称为访管指令，因为内核态也被称为管理态，访管就是访问管理态）。该指令给用户提供接口，用于调用操作系统的服务。 12.2. 缓冲区溢出 缓冲区溢出是指当计算机向缓冲区内填充数据时超过了缓冲区本身的容量，溢出的数据覆盖在合法数据上。 危害 而缓冲区溢出中，最为危险的是堆栈溢出，因为入侵者可以利用堆栈溢出，在函数返回时改变返回程序的地址，让其跳转到任意地址； 带来的危害一种是程序崩溃导致拒绝服务，另外一种就是跳转并且执行一段恶意代码，比如得到shell，然后为所欲为。通过往程序的缓冲区写超出其长度的内容，造成缓冲区的溢出，从而破坏程序的堆栈，使程序转而执行其它指令，以达到攻击的目的。 造成缓冲区溢出的主原因是程序中没有仔细检查用户输入的参数。 12.3. 中断与轮询 中断是指在计算机执行期间，系统内发生任何非寻常的或非预期的急需处理事件，使得CPU暂时中断当前正在执行的程序而转去执行相应的事件处理程序。待处理完毕后又返回原来被中断处继续执行或调度新的进程执行的过程。 中断和轮询的特点 对I/O设备的程序轮询的方式，是早期的计算机系统对I/O设备的一种管理方式。它定时对各种设备轮流询问一遍有无处理要求。轮流询问之后，有要求的，则加以处理。在处理I/O设备的要求之后，处理机返回继续工作。尽管轮询需要时间，但轮询要比I/O设备的速度要快得多，所以一般不会发生不能及时处理的问题。当然，再快的处理机，能处理的输入输出设备的数量也是有一定限度的。而且，程序轮询毕竟占据了CPU相当一部分处理时间，因此，程序轮询是一种效率较低的方式，在现代计算机系统中已很少应用。 轮询——效率低，等待时间很长，CPU利用率不高。 中断——容易遗漏一些问题，CPU利用率高。 12.4. 临界区 资源的共享有两种方式：互斥共享和同时访问。 临界资源：一次仅允许一个进程使用的资源，包括硬件和软件 临界区：每个进程中访问临界资源的那段程序。每次只准许一个进程进入临界区，进入后不允许其他进程进入。 如果有若干进程要求进入空闲的临界区，一次仅允许一个进程进入； 任何时候，处于临界区内的进程不可多于一个。如已有进程进入自己的临界区，则其它所有试图进入临界区的进程必须等待； 进入临界区的进程要在有限时间内退出，以便其它进程能及时进入自己的临界区； 如果进程不能进入自己的临界区，则应让出CPU，避免进程出现“忙等”现象。 12.5. 进程和线程的区别 进程是具有一定功能的程序关于某个数据集合上的一次运行活动，进程是系统进行资源调度和分配的一个独立单位。 线程是进程的实体，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位。 资源分配给进程，同一进程的所有线程共享该进程的所有资源。 同一进程中的多个线程共享代码段(代码和常量)，数据段(全局变量和静态变量)，扩展段(堆存储)。但是每个线程拥有自己的栈段，栈段又叫运行时段，用来存放所有局部变量和临时变量 处理机分给线程，即真正在处理机上运行的是线程 进程是系统进行资源调度和分配的的基本单位，实现了操作系统的并发； 线程是CPU调度和分派的基本单位，用于保证程序的 实时性，实现进程内部的并发； 一个程序至少有一个进程，一个进程至少有一个线程，线程依赖于进程而存在； 进程在执行过程中拥有独立的内存单元，而多个线程共享进程的内存。 通信方式不同。线程之间的通信比较方便，同一进程下的线程共享数据（比如全局变量，静态变量）。而进程之间的通信只能通过进程通信的方式进行 12.6. 程序与进程的区别 进程是一个动态概念，而程序是一个静态概念。 进程具有并行特征，而程序不反映执行所以没有并行特征 进程是竞争计算机系统资源的基本单位，而程序不反映执行也就不会竞争计算机系统资源 不同的进程可以包含同一程序，只要该程序所对应的数据集不同。 12.7. 进程和线程的上下文切换代价 进程切换分两步： 切换页目录以使用新的地址空间（进程） 切换内核栈和硬件上下文（进程、线程） 切换的性能消耗： 线程上下文切换虚拟内存空间依然是相同的，但是进程切换是不同的。这两种上下文切换的处理都是通过操作系统内核来完成的。 上下文的切换会扰乱处理器的缓存机制。简单的说，一旦去切换上下文，处理器中所有已经缓存的内存地址一瞬间都作废了。还有一个显著的区别是当你改变虚拟内存空间的时候，处理的页表缓冲（processor's Translation Lookaside Buffer (TLB)）或者相当的神马东西会被全部刷新，这将导致内存的访问在一段时间内相当的低效。但是在线程的切换中，不会出现这个问题。 12.8. 进程同步的原则 空闲让进； 忙则等待（保证对临界区的互斥访问）； 有限等待（有限代表有限的时间，避免死等）； 让权等待（当进程不能进入自己的临界区时，应该释放处理机，以免陷入忙等状态） 死等状态： 进程在有限时间内根本不能进入临界区，而一直在尝试进入，陷入一种无结果的等待状态。 （没有进入临界区的正在等待的某进程根本无法获得临界资源而进入进程，这种等待是无结果的，是死等状态）-&gt; 这个时候应该放弃这个无结果的事情，保证自己等待的时间是有限的 忙等状态： 当一个进程正处在某临界区内，任何试图进入其临界区的进程都必须进入代码连续循环，陷入忙等状态。连续测试一个变量直到某个值出现为止，称为忙等。 （没有进入临界区的正在等待的某进程不断的在测试循环代码段中的变量的值，占着处理机而不释放，这是一种忙等状态）-&gt; 这个时候应该释放处理机让给其他进程 有限等待：对要求访问临界资源的进程，应保证有限时间内能进入自己的临界区，以免陷入“死等”状态（受惠的是进程自己） 让权等待：当进程不能进入自己的临界区时，应立即释放处理机，以免进程陷入“忙等”状态（受惠的是其他进程） 12.9. 进程同步 在OS中引入进程后，一方面使系统的吞吐量和资源的利用率得到提升，另一方面也使得系统变得复杂，如果没有合理的方式对进程进行妥善的管理，必然会引起进程对系统资源的无序竞争，使系统变得混乱；为了实现对并发进程的有效管理，在多道程序系统中引入了同步机制，常见的同步机制有：硬件同步机制、信号量机制、管程机制等，利用它们确保程序执行的可再现性； 12.9.1. 硬件同步机制 通常计算机会提供一些特殊的硬件指令，允许对一个字中的内容进行检测和修正，或者对两个字的内容进行交换；对临界区的管理，可以视为对“锁”的管理：当“锁”开的时候，就允许进入，然后把“锁”关上；当“锁”关上的时候，就只能在外面等待；显然，对“锁”的检测（相当于进入区代码）和打开“锁”（相当于临界区）的操作必须是连续的；常见的硬件同步机制有： 关中断 是实现互斥的最简单方法之一。在进入锁检测之前，关闭中断，直到完成锁检测并上锁之后才打开中断。这样，进程在临界区执行期间，计算机系统不响应中断，从而不会引发调度，自然不会发生进程或者线程切换。但是关中断的方法有许多缺点： 滥用关中断权利，可能会造成严重后果； 关中断时间过长，会影响系统效率，限制处理器交叉执行程序的能力； 关中断的方法不适合多CPU系统； Test-and-Set TS指令的一般描述如下： boolean TS(boolean *lock){ boolean old; old=*lock; *lock=true; return old; } 相应的进入区代码为： while(TS(&amp;lock)); TS指令中，当lock为false时，就将其设置为true，然后返回false;当lock为true时，就返回true；返回false表示资源可用；返回true表示资源不可用； 上面这段代码实现的功能：如果lock为false，那么设置它为true，但是要返回false；如果lock为true，不做改变，那么仍旧返回true；但实际上，它是这么做的：不论lock是什么，都把它设置为true。而返回它原来的值； Swap void Swap(boolean* lock,boolean* key){ boolean temp=*lock; *lock=*key; *key=temp; } 相应的进入区代码为： key=true; do{ Swap(&amp;lock,&amp;key); }while(key!=false); //进入临界区 Swap指令中，do-while循环中的退出条件是key为false ；而key为false 意味着lock为false，表示资源可用；当lock为true的时候，key就为true；那么循环就会一直进行下去； 利用TS机制和Swap机制，都会让进程处于忙等状态，并不符合同步机制的要求；（准确的说，不是实现不了同步，而是效率不高，不太高效~） 12.9.2. 信号量同步机制 信号量同步机制由Dijkstra（很厉害的大神，单源最短路劲算法就是他提出的）；信号量机制已被广泛应用到单处理机和多处理机系统以及计算机网络中； 整型信号量 整型信号量S表示资源数目，除初始化外，仅能通过两个标准的原子操作进行修改：wait(S)和signal(S)；这两个操作长期以来也别称为P、V操作； wait(S){ while(S&lt;=0); S--； } signal(S){ S++; } 其实问题就是，wait和signal两个原子操作仍旧会产生“忙等”——进程不断测试，一直问，你说烦不烦？ 记录型信号量 记录型信号量机制是一种不存在忙等现象的进程同步机制；但是采取了让权等待策略后，就会有多个进程等待访问统一资源的情况，于是还需要把这些进城组织起来，于是除了S用来表示资源的数量外，还需要一个指针；这也是记录型信号量的名称来源：使用了记录型的数据结构； typedef struct{ int value; sturct process_control_block *list; }semaphore; wait(semaphore *S){ S-&gt;value--; if(S-&gt;value&lt;0){ block(S-&gt;list); } } signal(semaphore *S){ S-&gt;value++; if(S-&gt;value&lt;=0){ wakeup(S-&gt;list); } } 记录型信号量中，value不仅指示资源的数量，由于每次wait操作value都会递减，所以value的值会反映出等待资源的进程有多少个。在signal中，value经过自增后，如果还&lt;=0，说明还有进程在等待该资源，所以需要wakeup一个进程。 AND型信号量 前面所述的进程互斥问题针对的是多个并发进程共享一个临界资源的情况，但是如果多个进程共享多个资源时仍旧采取这样单个的分配方法，就有可能发生死锁现象；为了避免这样的现象，提出来AND型信号量：将进程在整个运行过程中需要的所有资源，要么一次性全部分配给进程，然后使用完后再一起释放。要么一个都不分配，这样便可以避免死锁现象。wait和signal操作要做出相应改变。 Swait(S1,S2,S3,S4,S5....){ while(true){ if(S1&gt;=1&amp;&amp;S2&gt;=1...){ for(i=1;i&lt;=n;i++){ Si--; } break; }else{ //找到第一个小于等于0的Si,然后将进程放置到与其相关的等待队列中 } } } Ssignal(S1,S2...Sn){ while(true){ for(i=0;i&lt;=n;i++){ Si++; //唤醒一个等待Si资源的进程——该进程将进入Swait中的while循环里继续判断其他资源是否可用。 } } } 信号量集 前面介绍的几种信号量同步机制都是对某一资源进行一个单位的申请和释放。当一次需要N个的时候，就需要进行N次请求，这不但低效而且容易发生死锁情况；还有些情况下，为了保证系统的安全性，当所申请的资源低于某个值时，就需要停止对该类资源的分配。解决办法就是当进程申请某类临界资源时，都必须测试资源的数量，判断是否大于可分配的下限值，然后决定是否分配； 基于上述提到的两点问题，需要对AND信号量机制加以扩充，对进程所申请的所有资源以及每类资源不同的资源需求量，再一次PV原语操作中完成申请和释放。对信号量Si的测试值不再是1，而是ti。当Si&lt;=ti时就不再分配；同时，进程需要传递给wait方法每类资源所需要的数目，由此形成一般化的“信号量集”机制； Swait(S1,t1,d1....Sn,tn,dn);表示对Si类资源的需求是di个，当Si的数量小于ti时就不再分配； Ssignal(S1,d1....Sn,dn);表示归还Si类资源的数目是di个； 特殊的，Swait(S,d,d)表示信号量集中只有一个信号量；它允许每次申请d个资源，当资源数量小于d时不予分配； Swait(S,1,1)表示普通的一般记录型信号量； 信号量的应用 实现进程互斥：基本操作； 实现前驱关系：当进程A中的X1操作结束后才进行进程B中的X2操作，像这种需求即为前驱关系。可以设置一种虚拟的资源S，并设置其状态为不可用，然后在X1的后面加上signal(S)，在X2语句前加上wait（S），以此实现这种执行顺序上的控制； 12.9.3. 管程机制 信号量机制虽然是一种既方便又实用的进程同步机制，但是要访问临界资源的进程需要自备同步操作wait（S）和signal(S)，这就使得对共享资源进行访问的代码遍布各个进程，不利于系统管理，还增加系统死锁的风险；管程机制是一种解决该问题的方法； 操作系统的作用之一就是实现对计算机系统资源的抽象，管程机制使用少量的信息和对该资源所执行的操作来表征该资源，所以共享系统资源就变为了共享数据结构，并将对这些共享数据结构的操作定义为一组过程。进程对共享资源的申请、释放和其他操作必须通过这组过程。代表共享资源的数据结构以及由对该共享数据结构实施操作的一组过程所组成的资源管理程序共同构成了一个操作系统的资源管理模块，我们称之为管程； 管程由四部分组成：名称、局部于管程的共享数据结构说明、对该数据结构进行操作的一组过程、对局部于管程的共享数据结构设置初始值的语句； 所有进程访问临界资源时，都只能通过管程间接访问，而管程每次只准许一个进程进入管程，从而实现互斥。管程体现了面向对象程序设计的思想；具有：模块化，即管程是一个独立的基本单位，可单独编译；抽象数据类型，不仅有数据还有对数据的操作；信息隐蔽，管程中的数据结构只能被管程中的过程访问，这些过程也是在管程内部定义的，而管程之外的进程只需调用而无需了解其内部的具体实现细节。（这样，原来遍布系统的共享资源的访问代码，就集中到管程中啦）； 管程和进程的对比 两者都定义了各自的数据结构，但是管程定义的数据结构是对公用资源的抽象，进程定义的是私有数据结构PCB； 两者都有对各自数据结构的操作，但是管程的操作是为了实现同步和初始化，进程是由顺序程序执行有关操作； 进程的目的是在于实现系统的并发，而管程的目的是解决共享资源的互斥访问； 进程是主动工作的，管程需要被其他程序使用，属于被动工作的； 进程有动态性，管程是操作系统中的一个资源管理模块； 管程中还有一个比较重要的概念就是条件变量。当一个进程进入了管程但在管程中被阻塞或者挂起，此时该进程需要释放对管程的占有，并且根据阻塞或者挂起的原因，也就是条件变量，进入相应的等待队列，等待其他进程的唤醒。条件变量x具有两种操作：x.wait()和x.signal()； x.wait()：正在调用管程的进程因x条件而需要被挂起或者阻塞，则调用x.wait()将自己插入到条件变量x的等待队列上并释放管程，直到x条件变化； x.signal()：正在调用管程的进程发现x条件发生了变化，重新启动一个因x而阻塞的进程，如果有多个进程因x而阻塞，也只能选择一个； 如果进程Q因为x条件而处于阻塞状态，当P调用管程时，执行了x.signal()操作后，Q重新启动，此时P和Q到底谁来继续拥有管程呢？答案是两者均可； 12.10. 进程通信 进程通信，是指进程之间的信息交换（信息量少则一个状态或数值，多者则是成千上万个字节）。因此，对于用信号量进行的进程间的互斥和同步，由于其所交换的信息量少而被归结为低级通信 所谓高级进程通信指：用户可以利用操作系统所提供的一组通信命令传送大量数据的一种通信方式。操作系统隐藏了进程通信的实现细节。或者说，通信过程对用户是透明的 高级通信机制可归结为三大类： 共享存储器系统（存储器中划分的共享存储区）；实际操作中对应的是“剪贴板”（windows剪贴板实际上是系统维护管理的一块内存区域）的通信方式。 消息传递系统（进程间的数据交换以消息（message）为单位，当今最流行的微内核操作系统中，微内核与服务器之间的通信，无一例外地都采用了消息传递机制。应用举例：邮槽（MailSlot）是基于广播通信体系设计出来的，它采用无连接的不可靠的数据传输。邮槽是一种单向通信机制，创建邮槽的服务器进程读取数据，打开邮槽的客户机进程写入数据 管道通信系统（管道即：连接读写进程以实现他们之间通信的共享文件（pipe文件，类似先进先出的队列，由一个进程写，另一进程读））。实际操作中，管道分为：匿名管道、命名管道。 匿名管道是一个未命名的、单向管道，通过父进程和一个子进程之间传输数据。匿名管道只能实现本地机器上两个进程之间的通信，而不能实现跨网络的通信。 命名管道不仅可以在本机上实现两个进程间的通信，还可以跨网络实现两个进程间的通信 管道：管道是单向的、先进先出的、无结构的、固定大小的字节流，它把一个进程的标准输出和另一个进程的标准输入连接在一起。写进程在管道的尾端写入数据，读进程在管道的道端读出数据。数据读出后将从管道中移走，其它读进程都不能再读到这些数据。管道提供了简单的流控制机制。进程试图读空管道时，在有数据写入管道前，进程将一直阻塞。同样地，管道已经满时，进程再试图写管道，在其它进程从管道中移走数据之前，写进程将一直阻塞。管道可用于具有亲缘关系的父子进程间的通信，有名管道除了具有管道所具有的功能外，它还允许无亲缘关系进程间的通信； 信号（signal）：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生，调用特定的函数处理； 信号量：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其它进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段 消息队列：是一个在系统内核中用来保存消 息的队列，它在系统内核中是以消息链表的形式出现的。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点 共享内存：共享内存允许两个或多个进程访问同一个逻辑内存。这一段内存可以被两个或两个以上的进程映射至自身的地址空间中，一个进程写入共享内存的信息，可以被其他使用这个共享内存的进程，通过一个简单的内存读取读出，从而实现了进程间的通信。如果某个进程向共享内存写入数据，所做的改动将立即影响到可以访问同一段共享内存的任何其他进程。共享内存是最快的IPC方式，它是针对其它进程间通信方式运行效率低而专门设计的。它往往与其它通信机制（如信号量）配合使用，来实现进程间的同步和通信 套接字：套接字也是一种进程间通信机制，与其它通信机制不同的是，它可用于网络中不同机器间的进程通信 12.11. 线程同步的方式 互斥量 Synchronized/Lock：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问 信号量 Semphare：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量 事件(信号)，Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作 12.12. 死锁 12.12.1. 死锁的概念 在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。通俗的讲，就是两个或多个进程无限期的阻塞、相互等待的一种状态。 12.12.2. 死锁产生的四个必要条件 互斥：至少有一个资源必须属于非共享模式，即一次只能被一个进程使用；若其他申请使用该资源，那么申请进程必须等到该资源被释放为止； 占有并等待：一个进程必须占有至少一个资源，并等待另一个资源，而该资源为其他进程所占有； 非抢占：进程不能被抢占，即资源只能被进程在完成任务后自愿释放 循环等待：若干进程之间形成一种头尾相接的环形等待资源关系 12.12.3. 死锁的处理 解决死锁的基本方法主要有 预防死锁、避免死锁、检测死锁、解除死锁 、鸵鸟策略 等。 死锁预防 死锁预防的基本思想是 只要确保死锁发生的四个必要条件中至少有一个不成立，就能预防死锁的发生，具体方法包括： 打破互斥条件：允许进程同时访问某些资源。但是，有些资源是不能被多个进程所共享的，这是由资源本身属性所决定的，因此，这种办法通常并无实用价值。 打破占有并等待条件：可以实行资源预先分配策略(进程在运行前一次性向系统申请它所需要的全部资源，若所需全部资源得不到满足，则不分配任何资源，此进程暂不运行；只有当系统能满足当前进程所需的全部资源时，才一次性将所申请资源全部分配给该线程)或者只允许进程在没有占用资源时才可以申请资源（一个进程可申请一些资源并使用它们，但是在当前进程申请更多资源之前，它必须全部释放当前所占有的资源）。但是这种策略也存在一些缺点：在很多情况下，无法预知一个进程执行前所需的全部资源，因为进程是动态执行的，不可预知的；同时，会降低资源利用率，导致降低了进程的并发性。 打破非抢占条件：允许进程强行从占有者哪里夺取某些资源。也就是说，但一个进程占有了一部分资源，在其申请新的资源且得不到满足时，它必须释放所有占有的资源以便让其它线程使用。这种预防死锁的方式实现起来困难，会降低系统性能。 打破循环等待条件：实行资源有序分配策略。对所有资源排序编号，所有进程对资源的请求必须严格按资源序号递增的顺序提出，即只有占用了小号资源才能申请大号资源，这样就不回产生环路，预防死锁的发生。 死锁避免 动态地检测资源分配状态，以确保系统处于安全状态，只有处于安全状态时才会进行资源的分配。所谓安全状态是指：即使所有进程突然请求需要的所有资源，也能存在某种对进程的资源分配顺序，使得每一个进程运行完毕。 死锁检测 检测有向图是否存在环；或者使用类似死锁避免的检测算法。 死锁解除 死锁解除的常用两种方法为进程终止和资源抢占。所谓进程终止是指简单地终止一个或多个进程以打破循环等待，包括两种方式：终止所有死锁进程和一次只终止一个进程直到取消死锁循环为止；所谓资源抢占是指从一个或多个死锁进程那里抢占一个或多个资源，此时必须考虑三个问题： 利用抢占：挂起某些进程，并抢占它的资源。但应防止某些进程被长时间挂起而处于饥饿状态； 利用回滚：让某些进程回退到足以解除死锁的地步，进程回退时自愿释放资源。要求系统保持进程的历史信息，设置还原点； 利用杀死进程：强制杀死某些进程直到死锁解除为止，可以按照优先级进行。 12.13. 进程状态 就绪状态：进程已获得除处理机以外的所需资源，等待分配处理机资源； 运行状态：占用处理机资源运行，处于此状态的进程数小于等于CPU数； 阻塞状态： 进程等待某种条件，在条件满足之前无法执行； 12.14. 线程状态 在 Java虚拟机 中，线程从最初的创建到最终的消亡，要经历若干个状态：创建(new)、就绪(runnable/start)、运行(running)、阻塞(blocked)、等待(waiting)、时间等待(time waiting) 和 消亡(dead/terminated)。在给定的时间点上，一个线程只能处于一种状态，各状态的含义如下图所示： 线程各状态之间的转换如下： 12.15. 进程调度 调度种类 高级调度：(High-Level Scheduling)又称为作业调度，它决定把后备作业调入内存运行 中级调度：(Intermediate-Level Scheduling)交换调度。又称为在虚拟存储器中引入，在内、外存对换区进行进程对换 低级调度：(Low-Level Scheduling)又称为进程调度，它决定就绪队列的某进程获得CPU 线程调度 非抢占式调度与抢占式调度 非抢占式：分派程序一旦把处理机分配给某进程后便让它一直运行下去，直到进程完成或发生进程调度进程调度某事件而阻塞时，才把处理机分配给另一个进程 抢占式：操作系统将正在运行的进程强行暂停，由调度程序将CPU分配给其他就绪进程的调度方式 调度算法 FIFO或First Come, First Served (FCFS)先来先服务 调度的顺序就是任务到达就绪队列的顺序 公平、简单(FIFO队列)、非抢占、不适合交互式 未考虑任务特性，平均等待时间可以缩短 Shortest Job First (SJF) 最短的作业(CPU区间长度最小)最先调度 SJF可以保证最小的平均等待时间，但难以知道下一个CPU区间长度 Shortest Remaining Job First (SRJF) SJF的可抢占版本，比SJF更有优势 SJF(SRJF): 如何知道下一CPU区间大小？根据历史进行预测: 指数平均法 优先权调度 每个任务关联一个优先权，调度优先权最高的任务 注意：优先权太低的任务一直就绪，得不到运行，出现“饥饿”现象 Round-Robin(RR)轮转调度算法 设置一个时间片，按时间片来轮转调度（“轮叫”算法） 优点: 定时有响应，等待时间较短；缺点: 上下文切换次数较多 时间片太大，响应时间太长；吞吐量变小，周转时间变长；当时间片过长时，退化为FCFS 多级队列调度 按照一定的规则建立多个进程队列，一个进程根据自身属性被永久地分配到一个队列中。 不同的队列有固定的优先级（高优先级有抢占权） 不同的队列可以给不同的时间片和采用不同的调度方法 存在问题1：没法区分I/O bound和CPU bound 存在问题2：也存在一定程度的“饥饿”现象 多级反馈队列 在多级队列的基础上，任务可以在队列之间移动，更细致的区分任务 可以根据“享用”CPU时间多少来移动队列，阻止“饥饿” 最通用的调度算法，多数OS都使用该方法或其变形，如UNIX、Windows等 若进程使用过多CPU时间，那么它会被转移到更低的优先级队列；在较低优先级队列等待时间过长的进程会被转移到更高优先级队列，以防止饥饿发生。 12.16. 虚拟内存 内存的发展历程 没有内存抽象(单进程，除去操作系统所用的内存之外，全部给用户程序使用) —&gt; 有内存抽象（多进程，进程独立的地址空间，交换技术(内存大小不可能容纳下所有并发执行的进程)—&gt; 连续内存分配(固定大小分区(多道程序的程度受限)，可变分区(首次适应，最佳适应，最差适应)，碎片) —&gt; 不连续内存分配（分段，分页，段页式，虚拟内存） 虚拟内存 虚拟内存允许执行进程不必完全在内存中。虚拟内存的基本思想是：每个进程拥有独立的地址空间，这个空间被分为大小相等的多个块，称为页(Page)，每个页都是一段连续的地址。这些页被映射到物理内存，但并不是所有的页都必须在内存中才能运行程序。当程序引用到一部分在物理内存中的地址空间时，由硬件立刻进行必要的映射；当程序引用到一部分不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的命令。这样，对于进程而言，逻辑上似乎有很大的内存空间，实际上其中一部分对应物理内存上的一块(称为帧，通常页和帧大小相等)，还有一些没加载在内存中的对应在硬盘上，如图5所示。 注意，请求分页系统、请求分段系统和请求段页式系统都是针对虚拟内存的，通过请求实现内存与外存的信息置换。 由图5可以看出，虚拟内存实际上可以比物理内存大。当访问虚拟内存时，会访问MMU（内存管理单元）去匹配对应的物理地址（比如图5的0，1，2）。如果虚拟内存的页并不存在于物理内存中（如图5的3,4），会产生缺页中断，从磁盘中取得缺的页放入内存，如果内存已满，还会根据某种算法将磁盘中的页换出。 3. 虚拟内存的应用与优点 虚拟内存很适合在多道程序设计系统中使用，许多程序的片段同时保存在内存中。当一个程序等待它的一部分读入内存时，可以把CPU交给另一个进程使用。虚拟内存的使用可以带来以下好处： 在内存中可以保留多个进程，系统并发度提高 解除了用户与内存之间的紧密约束，进程可以比内存的全部空间还大 抖动 抖动本质上是指频繁的页调度行为，具体来讲，进程发生缺页中断，这时，必须置换某一页。然而，其他所有的页都在使用，它置换一个页，但又立刻再次需要这个页。因此，会不断产生缺页中断，导致整个系统的效率急剧下降，这种现象称为颠簸（抖动）。 内存颠簸的解决策略包括： 如果是因为页面替换策略失误，可以修改替换算法来解决这个问题； 如果是因为运行的程序太多，造成程序无法同时将所有频繁访问的页面调入内存，则要降低多道程序的数量； 否则，还剩下两个办法：终止该进程或增加物理内存容量。 12.17. 分页和分段 段式存储管理是一种符合用户视角的内存分配管理方案。在段式存储管理中，将程序的地址空间划分为若干段（segment），如代码段，数据段，堆栈段；这样每个进程有一个二维地址空间，相互独立，互不干扰。段式管理的优点是：没有内碎片（因为段大小可变，改变段大小来消除内碎片）。但段换入换出时，会产生外碎片（比如4k的段换5k的段，会产生1k的外碎片） 页式存储管理方案是一种用户视角内存与物理内存相分离的内存分配管理方案。在页式存储管理中，将程序的逻辑地址划分为固定大小的页（page），而物理内存划分为同样大小的帧，程序加载时，可以将任意一页放入内存中任意一个帧，这些帧不必连续，从而实现了离散分离。页式存储管理的优点是：没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满）。 区别 目的不同：分页是由于系统管理的需要而不是用户的需要，它是信息的物理单位；分段的目的是为了能更好地满足用户的需要，它是信息的逻辑单位，它含有一组其意义相对完整的信息； 大小不同：页的大小固定且由系统决定，而段的长度却不固定，由其所完成的功能决定； 地址空间不同： 段向用户提供二维地址空间；页向用户提供的是一维地址空间； 信息共享：段是信息的逻辑单位，便于存储保护和信息的共享，页的保护和共享受到限制； 内存碎片：页式存储管理的优点是没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满）；而段式管理的优点是没有内碎片（因为段大小可变，改变段大小来消除内碎片）。但段换入换出时，会产生外碎片（比如4k的段换5k的段，会产生1k的外碎片）。 12.18. 页面置换算法 在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘中来腾出空间。页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。 最佳页面置换算法OPT（Optimal replacement algorithm）：置换以后不需要或者最远的将来才需要的页面，是一种理论上的算法，是最优策略； 先进先出FIFO：置换在内存中驻留时间最长的页面。缺点：有可能将那些经常被访问的页面也被换出，从而使缺页率升高； 二次机会算法SCR：按FIFO选择某一页面，若其访问位为1，给第二次机会，并将访问位置0； 时钟算法 Clock：SCR中需要将页面在链表中移动（第二次机会的时候要将这个页面从链表头移到链表尾），时钟算法使用环形链表，再使用一个指针指向最老的页面，避免了移动页面的开销； 最近未使用算法NRU（Not Recently Used）：检查访问位R、修改位M，优先置换R=M=0，其次是（R=0, M=1）； 最近最少使用算法LRU（Least Recently Used）：置换出未使用时间最长的一页；实现方式：维护时间戳，或者维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的。 最不经常使用算法NFU：置换出访问次数最少的页面 12.19. 局部性原理 时间上的局部性：最近被访问的页在不久的将来还会被访问； 空间上的局部性：内存中被访问的页周围的页也很可能被访问。 13. 参考链接 CSDN-Clichong 面试/笔试第二弹 —— 操作系统面试问题集锦 【操作系统】死等状态、忙等状态、有限等待、让权等待 操作系统面试题目总结 面试考点——用户态和内核态的区别 操作系统之进程同步和通信——进程同步的基本概念及其实现方式、进程通信及其实现方式 面试CS基础之操作系统 ","link":"https://memorykki.github.io/os-interview/"},{"title":"操作系统引论","content":"准备的书太拉跨，不准备继续总结了 目录 1. 操作系统引论 1.1. 操作系统的目标 1.2. 操作系统的作用 1.3. 操作系统的发展过程 1.3.1. 未配置操作系统的计算机系统 1.3.1.1. 人工操作方式 1.3.1.2. 脱机输入输出方式 1.3.2. 单道批处理系统 1.3.3. 多道批处理系统 1.3.4. 分时系统 1.3.5. 实时系统 1.3.6. 微机操作系统 1.4. 操作系统的基本特性 1.4.1. 并发 1.4.2. 共享 1.4.3. 虚拟 1.4.4. 异步 1.5. 操作系统的主要功能 1.5.1. 处理机管理功能 1.5.2. 存储器管理功能 1.5.3. 设备管理功能 1.5.4. 文件管理功能 1.5.5. OS与用户之间的接口 1.5.6. 现代操作系统的新功能 1.6. OS结构设计 1.6.1. 传统OS结构 1.6.1.1. 无结构OS 1.6.1.2. 模块化结构OS 1.6.1.3. 分层式结构OS 1.6.2. 客户服务器模式 1.6.3. 面向对象的程序设计技术 1.6.4. 微内核OS结构 1.6.4.1. 基本概念 1.6.4.2. 基本功能 1.6.4.3. 优缺点 2. 进程的描述与控制 2.1. 前趋图和程序执行 2.1.1. 前趋图 2.1.2. 程序顺序执行 2.1.3. 程序并发执行 2.2. 进程的描述 2.2.1. 进程的定义 2.2.2. 进程的特征 2.3. 进程的基本状态及转换 2.3.1. 三种基本状态 2.3.2. 状态转换 2.3.3. 创建状态和终止状态 2.4. 挂起操作 1. 操作系统引论 操作系统是一组能有效地组织和管理计算机硬件和软件资源，合理地对各类作业进行调度，以及方便用户使用的程序的集合。 1.1. 操作系统的目标 方便性：裸机上难以使用； 有效性：提高资源利用率，提高系统吞吐量； 可扩充性：适应硬件、体系结构发展； 开放性：软硬件的兼容性。 1.2. 操作系统的作用 OS作为用户与计算机硬件系统之间的接口。三种使用方式：系统调用、命令、图形窗口； OS作为计算机系统资源的管理者。资源分四类： 处理机：分配和控制处理机： 存储器：内存的分配与回收； I/O设备：I/O设备的分配与操纵； 文件：文件的存取、共享、保护； OS实现了对计算机资源的抽象。通过I/O设备管理软件、文件系统、窗口等多层次抽象，使裸机称为更强的虚机器。不仅实现了功能，而且隐藏了细节。 1.3. 操作系统的发展过程 1.3.1. 未配置操作系统的计算机系统 1.3.1.1. 人工操作方式 早期的操作方式是由程序员将事先已穿孔的纸带（或卡片），装入纸带输入机（或卡片输入机），再启动它们将纸带（或卡片）上的程序和数据输入计算机，然后启动计算机运行。仅当程序运行完毕并取走计算结果后，才允许下一个用户上机。这种人工操作方式有以下两方面的缺点： 用户独占全机，即一台计算机的全部资源由上机用户所独占。 CPU等待人工操作。当用户进行装带（卡）、卸带（卡）等人工操作时，CPU及内存等资源是空闲的。 1.3.1.2. 脱机输入输出方式 为了解决人机矛盾及CPU和I/O设备之间速度不匹配的矛盾，出现了脱机IO技术。该技术是事先将装有用户程序和数据的纸带装入纸带输入机，在一台外围机的控制下，把纸带（卡片）上的数据（程序）输入到磁带上。当CPU需要这些程序和数据时，再从磁带上高速地调入内存。 由于程序和数据的输入和输出都是在外围机的控制下完成的，或者说，它们是在脱离主机的情况下进行的，故称为脱机输入输出方式。反之，把在主机的直接控制下进行输入输出的方式称为联机输入/输出方式。 优点是： 减少了CPU的空闲时间：外围机操作并不占用主机时间； 提高了I/O速度。 1.3.2. 单道批处理系统 处理过程 为实现对作业的连续处理，先把一批作业以脱机方式输入到磁带上，配上监督程序。首先由监督程序将磁带上的第一个作业装入内存，并把运行控制权交给该作业；当该作业处理完成时，又把控制权交还给监督程序，再由监督程序把磁带上的第二个作业调入内存。计算机系统就这样自动地一个作业紧接一个作业地进行处理，直至磁带上的所有作业全部完成，这样便形成了早期的批处理系统。虽然系统对作业的处理是成批进行的，但在内存中始终只保持一道作业，故称为单道批处理系统。 旨在解决人机矛盾和CPU与IO设备速度不匹配矛盾的过程中形成的，提高系统资源的利用率和系统吞吐量。 缺点 系统中的资源得不到充分的利用。因为在内存中仅有一道程序，每逢该程序在运行中发出I/O请求后，CPU便处于等待状态，必须在其I/O完成后才继续运行。又因I/O设备的低速性，更使CPU的利用率显著降低。 1.3.3. 多道批处理系统 处理过程 用户所提交的作业先存放在外存上，并排成一个队列，称为“后备队列”。然后由作业调度程序按一定的算法，从后备队列中选择若干个作业调入内存，使它们共享CPU和系统中的各种资源。由于同时在内存中装有若干道程序，这样便可以在运行程序A时，利用其因I/O操作而暂停执行时的CPU空档时间，再调度另一道程序B运行，同样可以利用程序B在I/O操作时的CPU空档时间，再调度程序C运行，使多道程序交替地运行，这样便可以保持CPU处于忙碌状态。 优缺点 资源利用率高。引入多道批处理能使多道程序交替运行，以保持CPU处于忙碌状态；在内存中装入多道程序可提高内存的利用率；此外还可以提高IO设备的利用率。 系统吞吐量大。能提高系统吞吐量的主要原因可归结为：①CPU和其它资源保持“忙碌”状态；仅当作业完成时或运行不下去时才进行切换，系统开销小。 平均周转时间长。由于作业要排队依次进行处理，因而作业的周转时间较长，通常需几个小时，甚至几天。 无交互能力。用户一旦把作业提交给系统后，直至作业完成，用户都不能调试修改。 需要解决的问题 处理机争用问题。既要能满足各道程序运行的需要，又要能提高处理机的利用率。 内存分配和保护问题。系统应能为每道程序分配必要的内存空间，使它们“各得其所”，且不会因某道程序出现异常情况而破坏其它程序。 I/O设备分配问题。系统应采取适当的策略来分配系统中的IO设备，以达到既能方便用户对设备的使用，又能提高设备利用率的目的。 文件的组织和管理问题。系统应能有效地组织存放在系统中的大量的程序和数据，使它们既便于用户使用，又能保证数据的安全性。 作业管理问题。系统中存在着各种作业（应用程序），系统应能对系统中所有的作业进行合理的组织，以满足这些作业用户的不同要求 用户与系统的接口问题。为使用户能方便的使用操作系统，OS还应提供用户与操作系统之间的接口。 1.3.4. 分时系统 为了满足人机交互和共享主机的需求。OS能提供终端，系统及时接收处理命令。 及时接收：配置多路卡，实现分时多路复用； 及时处理： 作业直接进入内存； 采用轮转运行方式。设置时间片避免一个作业长期独占CPU。 特征 多路性、独立性、及时性、交互性。 1.3.5. 实时系统 “实时计算”，则可以定义为这样一类计算：系统的正确性，不仅由计算的逻辑结果来确定，而且还取决于产生结果的时间。实时系统最主要的特征，是将时间作为关键参数。 实时任务的类型 周期性 非周期性：开始截止时间和完成截止时间 硬实时任务：必须 软实时任务：不严格 实时系统与分时系统特征的比较 多路性。分时系统中表现为系统按分时原则为多个终端用户服务：实时控制指系统周期性地对多路现场信息进行采集； 独立性。分时系统每个终端在与系统交互时彼此相互独立互不干扰；实时系统中对信息的采集和对对象的控制也都是彼此互不干扰的 及时性。分时系统对实时性的要求是依据人所能接受的等待时间确定的；实时控制系统的实时性则是以控制对象所要求的截止时间来确定的。 （4）交互性。实时系统交互性仅限于访问系统中某些特定的专用服务程序；分时系统能向终端用户提供数据处理、资源共享等服务。 （5）可靠性。分时系统要求系统可靠，实时系统要求系统高度可靠。 1.3.6. 微机操作系统 单用户单任务操作系统：MS-DOS 单用户多任务操作系统：windows 多用户多任务操作系统：unix，linux 1.4. 操作系统的基本特性 1.4.1. 并发 并行与并发 并行性是指两个或多个事件在同一时刻发生。而并发性是指两个或多个事件在同一时间间隔内发生。在多道程序环境下，并发性是指在一段时间内宏观上有多个程序在同时运行，但在单处理机系统中，每一时刻却仅能有一道程序执行，故微观上这些程序只能是分时地交替执行。 引入进程 在一个未引入进程的系统中，在程序执行IO操作时，计算程序也不能执行。但在为计算程序和IO程序分别建立一个进程后，这两个进程便可并发执行。若对内存中的多个程序都分别建立一个进程，它们就可以并发执行，这样便能极大地提高系统资源的利用率，增加系统的吞吐量。 所谓进程，是指在系统中能独立运行并作为资源分配的基本单位，它是由一组机器指令、数据和堆栈等组成的，是一个能独立运行的活动实体。多个进程之间可以并发执行和交换信息。 1.4.2. 共享 OS环境下的资源共享或称为资源复用，是指系统中的资源可供内存中多个并发执行的进程共同使用。这里在宏观上既限定了时间（进程在内存期间），也限定了地点（内存）。 互斥共享方式：进程A要访问某资源时，必须先提出请求。若此时该资源空闲，系统便可将之分配给请求进程A使用。 把这种在一段时间内只允许一个进程访问的资源，称为临界资源（或独占资源）。 同时访问方式 系统中还有另一类资源，允许在一段时间内由多个进程“同时”对它们进行访问。这里所谓的“同时”，在单处理机环境下是宏观意义上的，而在微观上，这些进程对该资源的访问是交替进行的。例如磁盘。 并发和共享是多用户（多任务）OS的两个最基本的特征。它们又是互为存在的条件。即方面资源共享是以进程的并发执行为条件的，若系统不允许并发执行也就不存在资源共享问题；另一方面，若系统不能对资源共享实施有效管理，以协调好诸进程对共享资源的访问，也必然会影响到诸进程间并发执行的程度，甚至根本无法并发执行。 1.4.3. 虚拟 时分复用技术：虚拟处理机技术：虚拟设备技术。 空分复用技术：利用存储器的空闲空间分区域存放和运行其它的多道程序，以此来提高内存的利用率，但并不能实现在逻辑上扩大存储器容量的功能，还必须引入虚拟存储技术才能达到此目的。虚拟存储技术在本质上是实现内存的分时复用，即它可以通过分时复用内存的方式，使一道程序仅在远小于它的内存空间中运行。 1.4.4. 异步 进程是以人们不可预知的速度向前推进的，此即进程的异步性。 1.5. 操作系统的主要功能 1.5.1. 处理机管理功能 进程控制：为作业创建进程、撤消（终止）已结束的进程，以及控制进程在运行过程中的状态转换； 进程同步：为多个进程（含线程）的运行进行协调 进程互斥方式：诸进程在对临界资源进行访问 进程同步方式：指在相互合作去完成共同任务的诸进程间，由同步机构对它们的执行次序加以协调。例如锁。 进程通信：通常在相互合作的进程之间采用直接通信方式，即由源进程利用发送命令直接将消息挂到目标进程的消息队列上，以后由目标进程利用接收命令从其消息队列中取出消息。 调度： 作业调度：后备队列调入内存 进程调度：就绪队列分配处理机 1.5.2. 存储器管理功能 内存分配； 静态分配：装入时确定； 动态分配：允许运行中申请。 内存保护： 程序之间的内存空间不干扰； 用户程序和OS程序数据不干扰。 地址映射：将程序地址空间中的逻辑地址转物理地址。 内存扩充：借助虚拟存储技术： 请求调入功能：装入部分数据便可运行，中途调入； 置换功能：将暂时不用的程序和数据调至硬盘。 1.5.3. 设备管理功能 缓冲管理：缓和CPU与I/O设备速度不匹配的的矛盾，常见的缓冲区机制：单缓冲、双缓冲机制、公用缓冲池机制 设备分配：根据用户进程的IO请求、系统现有资源情况以及按照某种设备分配策略，为之分配其所需的设备。使用完后由系统回收。 设备处理：设备驱动程序。CPU向设备控制器发出I/O命令，完成指定的I/O操作；由CPU接收中断请求并给予迅速的响应和相应的处理。 1.5.4. 文件管理功能 文件存储空间的管理：为每个文件分配外存空间，回收。 目录管理：按名存取，目录查询，文件共享。 文件的读写管理：利用文件指针从外存检索文件。 文件保护：存取控制。 1.5.5. OS与用户之间的接口 用户接口：直接或间接控制作业的命令接口 联机用户接口：为联机用户提供的，包括一组键盘操作命令及命令解释程序； 脱机用户接口：为批处理作业的用户提供的。把需要对作业进行的控制和干预的命令事先写在作业说明书上，提交系统； 图形用户接口。 程序接口：为用户程序访问系统资源而设置，是用户程序取得操作系统服务的唯一途径。它是由一组系统调用组成的，每一个系统调用都是一个能完成特定功能的子程序。早期的系统调用都是用汇编语言提供的，只有在用汇编语言书写的程序中才能直接使用系统调用。但在高级语言中，往往提供了与各系统调用一一对应的库函数这样。 1.5.6. 现代操作系统的新功能 系统安全 网络功能 多媒体 1.6. OS结构设计 1.6.1. 传统OS结构 1.6.1.1. 无结构OS 在早期开发操作系统时，设计者只是把他的注意力放在功能的实现和获得高的效率上，缺乏首尾一致的设计思想。此时的OS是为数众多的一组过程的集合，每个过程可以任意地相互调用其它过程，致使操作系统内部既复杂又混乱，因此，这种OS是无结构的，也有人把它称为整体系统结构 1.6.1.2. 模块化结构OS 模块化程序设计技术是20世纪60年代出现的一种结构化程序设计技术。该技术“模块化”的原则来控制大型软件的复杂度。OS不再是由众多的过程直接构成的，而是按其功能精心地划分为若干个具有一定独立性和大小的模块。每个模块具有某方面的管理功能，并仔细地规定好各模块间的接口，使各模块之间能通过接口实现交互。这种设计方法称为模块-接口法。 模块的独立性要符合高内聚、低耦合的原则。 缺点： 在OS设计时，对各模块间的接口规定很难满足在模块设计完成后对接口的实际需求。 各模块的设计齐头并进，无法寻找一个可靠的决定顺序，造成各种决定的“无序性”，这将使程序人员很难做到“设计中的每一步决定”都是建立在可靠的基础上，因此模块-接口法又被称为“无序模块法”。 1.6.1.3. 分层式结构OS 为了将模块-接口法中“决定顺序”的无序性变为有序性，引入了有序分层法，分层法的设计任务是，在目标系统An和裸机系统（又称宿主系统）A0之间，铺设若干个层次的软件A1、A2、A3、…、An-1，使An通过An-1、An-2、…、A2、A1层，最终能在A0上运行。 在操作系统中，常采用自底向上法来铺设这些中间层。 自底向上的分层设计的基本原则是：每一步设计都建立在可靠的基础上。 优点： 易保证系统的正确性和可靠性。 易扩充和易维护性。 缺点： 分层结构的主要缺点是系统效率降低。由于层次结构是分层单向依赖的，必须在每层之间都建立层次间的通信机制，OS每执行一个功能，通常要自上而下地穿越多个层次，这无疑会增加系统的通信开销，从而导致系统效率的降低。 1.6.2. 客户服务器模式 组成 客户机：每台客户机都是一个自主计算机，具有一定的处理能力。 服务器：通常是一台规模较大的机器，应能为网上所有的用户提供一种或多种服务。平时它一直处于工作状态，被动地等待来自客户机的请求。 网络系统：是用于连接所有客户机和服务器，实现它们之间通信和网络资源共享的系统。 优点 数据的分布处理和存储。由于客户机具有相当强的处理和存储能力，可进行本地处理和数据的分布存储，摆脱主机瓶颈。 便于集中管理。较好地保障系统重要数据的“可靠”和“安全”。 灵活性和可扩充性。理论上，客户机和服务器的数量不受限制，可以配置多种类型的客户机和服务器。 易于改编应用软件。在客户服务器模式中，对于客户机程序的修改和增删，比传统集中模式要容易得多，必要时也允许由客户进行修改。 缺点 基本客户/服务器模式的不足之处是存在着不可靠性和瓶颈问题。在系统仅有一个服务器时，一旦服务器故障，将导致整个网络瘫痪。当服务器在重负荷下工作时，会因忙不过来而显著地延长对用户请求的响应时间。如果在网络中配置多个服务器，并采取相应的安全措施，则这种不足可加以改善。 1.6.3. 面向对象的程序设计技术 略。 1.6.4. 微内核OS结构 有效地支持多处理机运行，适用于分布式系统环境。 1.6.4.1. 基本概念 足够小的内核：实现核心功能，并非完整功能； 基于C/S模式：核心之外的功能通过进程实现，运行在用户态，CS借助微内核的消息传递机制实现信息交互； 应用“机制与策略分离”原理； 所谓机制是指实现某一功能的具体执行机构，策略则是在机制的基础上借助于某些参数和算法来实现该功能的优化，或达到不同的功能目标。通常，机制处于一个系统的基层，而策略则处于系统的高层。在传统的OS中，将机制放在OS的内核的较低层，把策略放在内核的较高层次中。而在微内核操作系统中，通常将机制放在OS的微内核中。正因为如此，才有可能将内核做得很小。 例如，为实现进程（线程）调度功能，须在进程管理中设置一个或多个进程（线程）优先级队列，能将指定优先级进程（线程）从所在队列中取出，并将其投入执行。由于这一部分属于调度功能的机制部分，应将它放入微内核中。而对于用户（进程）如何进行分类，以及其优先级的确认方式或原则，则都是属于策略问题。可将它们放入微内核外的进程线程）管理服务器中。 由于进程（线程）之间的通信功能是微内核OS最基本的功能，被频繁使用，因此几乎所有的微内核OS都是将进程（线程）之间的通信功能放入微内核中。此外，还将进程的切换、线程的调度，以及多处理机之间的同步等功能也放入微内核中。 采用面向对象技术； 1.6.4.2. 基本功能 要注意，特指的是微内核功能，而微内核实现的是使用最频繁的、最核心的功能，还可将一些功能一分为二，核心放入内核，绝大部分核外实现。 进程（线程）管理：应用“机制与策略分离”原理实现； 低级存储器管理； 中断和陷入处理； 1.6.4.3. 优缺点 优点 可扩展性：开发新的软硬件只需在核外增加专用服务器； 可靠性：严格测试而成；精简的API；服务器运行在用户态，出错不会影响内核和其他服务器； 可移植性强：所有与特定CPU和MO设备硬件有关的代码均放在内核和内核下面的硬件隐藏层中，而各种服务器均与硬件平台无关； 支持分布式：广泛使用消息传递通信机制，有一张进程和服务器的标识符与它们所驻留的机器之间的映射表； 面向对象技术。 缺点 OS运行效率降低。 原因：效率降低最主要的原因是，在完成一次客户对操作系统提出的服务请求时，需要利用消息实现多次交互和进行用户/内核模式与上下文的多次切换。然而，在早期的OS中，用户进程在请求取得OS服务时，一般只需进行两次上下文的切换：一次是在执行系统调用后由用户态转向系统态时；另一次是在系统完成用户请求的服务后，由系统态返回用户态时。 在微内核OS中，由于客户和服务器、服务器和服务器之间的通信都需通过微内核，致使同样的服务请求至少需要进行四次上下文切换。 客户发送请求消息给内核，以请求取得某服务器特定的服务；第二次是发生在由 内核把客户的请求消息发往服务器： 服务器完成客户请求后，把响应消息发送到内核； 内核将响应消息发送给客户。 实际上更多，应为服务器不一定靠自身独立完成功能。 为了改善运行效率，可以重新把一些常用的操作系统基本功能由服务器移入微内核中，但这又会使微内核的容量明显地增大。 2. 进程的描述与控制 2.1. 前趋图和程序执行 2.1.1. 前趋图 为了能更好地描述程序的顺序和并发执行情况,我们先介绍用于描述程序执行先后顺序的前趋图。所谓前趋图是指一个有向无循环图,它用于描述进程之间执行的先后顺序。图中的每个结点可用来表示一个进程或程序段,乃至一条语句,结点间的有向边则表示两个结点之间存在的偏序或前趋关系进程(或程序之间的前趋关系可用“→”来表示,如果进程P和P存在着前趋关系,写成P1→P2,表示在P2开始执行之前P1必须完成。 2.1.2. 程序顺序执行 单道程序系统。 通常,一个应用程序由若干个程序段组成,每一个程序段完成特定的功能,它们在执行时,都需要按照某种先后次序顺序执行,仅当前一程序段执行完后,才运行后一程序段。 特征 顺序性：指处理机严格地按照程序所规定的顺序执行,即每一操作必须在下一个操作开始之前结東； 封闭性：指程序在封闭的环境下运行,即程序运行时独占全机资源,资源的状态(除初始状态外)只有本程序才能改变它,程序一旦开始执行,其执行结果不受外界因素影响； 可再现性:指只要程序执行时的环境和初始条件相同,当程序重复执行时,不论它是从头到尾不停顿地执行,还是“停停走走”地执行,都可获得相同的结果。 2.1.3. 程序并发执行 多道程序技术。 只有在不存在前趋关系的程序之间才有可能并发执行,否则无法并发执行。 对于具有下述四条语句的程序段: S1:a:=x+2 S2:b:=y+4 S3:c:=a+b S4:d:=c+b 可以看出:S3必须在a和b被赋值后方能执行:S4必须在S3之后执行;但S1和S2则可以并发执行,因为它们彼此互不依赖。 特征 间断性。程序在并发执行时,由于它们共享系统资源,以及为完成同一项任务而相互合作,致使在这些并发执行的程序之间形成了相互制约的关系。 失去封闭性。当系统中存在着多个可以并发执行的程序时,系统中的各种资源将为它们所共享,而这些资源的状态也由这些程序来改变,致使其中任一程序在运行时,其环境都必然会受到其它程序的影响。 不可再现性。程序在并发执行时,由于失去了封闭性,也将导致其又失去可再现性。 2.2. 进程的描述 2.2.1. 进程的定义 为了使参与并发执行的每个程序(含数据)都能独立地运行,在操作系统中必须为之配置一个专门的数据结构,称为进程控制块( Process Control block,PCB)。系统利用PCB来描述进程的基本情况和活动过程,进而控制和管理进程。这样,由程序段、相关的数据段和pCB三部分便构成了进程实体(又称进程映像)。一般情况下,我们把进程实体就简称为进程。 传统OS中的进程定义为:“进程是进程实体的运行过程,是系统进行资源分配和调度的一个独立单位。” 2.2.2. 进程的特征 动态性：进程的实质是进程实体的执行过程,进程实体有一定的生命期,而程序则只是一组有序指令的集合,并存放于某种介质上,其本身并不具有活动的含义,因而是静态的。 并发性。是指多个进程实体同存于内存中,且能在一段时间内同时运行。引入进 程的目的也正是为了使其进程实体能和其它进程实体并发执行。程序(没有建立PCB)是不能参与并发执行的。 独立性。在传统的OS中,独立性是指进程实体是一个能独立运行、独立获得资源和独立接受调度的基本单位。凡未建立PCB的程序都不能作为一个独立的单位参与运行。 异步性,是指进程是按异步方式运行的,即按各自独立的、不可预知的速度向前推进。正是源于此因,才导致了传统意义上的程序若参与并发执行,会产生其结果的不可再现性。 2.3. 进程的基本状态及转换 2.3.1. 三种基本状态 就绪( Ready)状态。进程已处于准备好运行的状态,即进程已分配到除CPU以外的所有必要资源后,只要再获得CPU,便可立即执行。如果系统中有许多处于就绪状态的进程,通常将它们按一定的策略(如优先级策略)排成一个队列,称该队列为就绪队列。 执行( Running)状态。进程已获得CPU,其程序正在执行的状态。对任何个时刻而言,在单处理机系统中,只有一个进程处于执行状态,而在多处理机系统中,则有多个进程处于执行状态； 阻塞( Block)状态。正在执行的进程由于发生某事件暂时无法继续执行时的状态,亦即进程的执行受到阻塞。此时引起进程调度,OS把处理机分配给另一个就绪进程,而让受阻进程处于暂停状态,一般将这种暂停状态称为阻塞状态,有时也称为等待状态或封锁状态。通常系统将处于阻塞状态的进程也排成一个队列,称该队列为阻塞队列。 2.3.2. 状态转换 2.3.3. 创建状态和终止状态 创建状态：如果进程所需的资源尚不能得到满足,创建工作尚未完成,进程不能被调度运行,于是把此时进程所处的状态称为创建状态。 当其获得了所需的资源以及对其PCB的初始化工作完成后,便可由创建状态转入就绪状态 终止状态：进当一个进程到达了自然结束点,或是出现了无法克服的错误,或是被操作系统所终结,或是被其他有终止权的进程所终结,它将进入终止状态。进入终止状态。 进入终止态的进程以后不能再执行,但在操作系统中依然保留一个记录,其中保存状态码和一些计时统计数据,供其他进程收集。一旦其他进程完成了对其信息的提取之后,操作系统将删除该进程,即将其PCB清零,并将该空白PCB返还系统。 2.4. 挂起操作 ","link":"https://memorykki.github.io/os/"},{"title":"计算机网络","content":"概述、物理层、数据链路层、网络层、运输层、应用层。 70页、5万字，超长总结。 目录 1. 概述 1.1. 互联网概述 1.2. 互联网的组成 1.2.1. 边缘部分 1.2.1.1. 客户-服务器方式 1.2.1.2. 对等连接方式 1.2.2. 核心部分 1.2.2.1. 电路交换 1.2.2.2. 分组交换 1.2.2.3. 报文交换 1.3. 计算机网络的性能指标 1.3.1. 速率 1.3.2. 带宽 1.3.3. 吞吐量 1.3.4. 时延 1.3.5. 时延带宽积 1.3.6. 往返时间RTT 1.3.7. 利用率 1.4. 计算机网络体系结构 1.4.1. OSI七层模型 1.4.1.1. 物理层 1.4.1.2. 数据链路层 1.4.1.3. 网络层 1.4.1.4. 传输层 1.4.1.5. 会话层 1.4.1.6. 表示层 1.4.1.7. 应用层 1.4.2. 总结 2. 物理层 2.1. 数据通信 2.1.1. 数据通信系统 2.1.2. 信道 2.1.3. 信道的极限容量 2.2. 物理层下的传输媒体 2.3. 信道复用技术 2.4. 宽带接入技术 3. 数据链路层 3.1. 使用点对点信道 3.1.1. 三个基本问题 3.1.1.1. 封装成帧 3.1.1.2. 透明传输 3.1.1.3. 差错检测 3.1.2. 点对点协议PPP 3.1.2.1. 协议组成 3.1.2.2. PPP帧格式 3.1.2.3. PPP协议工作状态 3.2. 使用广播信道 3.2.1. 局域网的数据链路层 3.2.1.1. 以太网的两个标准 3.2.1.2. 适配器 3.2.2. CSMA/CD协议 3.2.3. 使用集线器的星形拓扑 3.2.4. 信道利用率 3.2.5. MAC层 3.2.5.1. MAC地址 3.2.5.2. MAC帧的格式 3.2.6. 扩展的以太网 3.2.6.1. 物理层扩展 3.2.6.2. 数据链路层扩展 3.2.6.2.1. 交换机特点 3.2.6.2.2. 路由器与交换机的主要区别 3.2.6.2.3. 以太网的自学习功能 4. 网络层 4.1. 网络层提供的两种服务 4.2. 网际协议IP 4.2.1. 分类的IP地址 4.2.1.1. IP地址的表示 4.2.1.2. 常见的三类IP地址 4.2.2. 私有地址 4.2.3. 网络地址转换NAT 4.2.4. DHCP协议 4.2.5. IP地址与硬件地址 4.2.6. 地址解析协议ARP 4.2.6.1. 同一局域网 4.2.6.2. 不在同一局域网 4.2.7. 逆地址解析协议RARP 4.2.8. IP数据报的格式 4.2.9. IP层转发分组的流程 4.3. 划分子网和构造超网 4.3.1. 划分子网 4.3.1.1. 三级IP地址 4.3.1.2. 子网掩码 4.3.1.3. 使用子网的分组转发 4.3.2. 构造超网 4.3.2.1. 网络前缀 4.4. ICMP网际控制报文协议 4.5. 路由选择协议 4.5.1. 内部网关协议RIP 4.5.1.1. 工作原理 4.5.1.2. 距离向量算法 4.5.2. 内部网关协议OSPF 4.5.3. 外部网关协议BGP 5. 运输层 5.1. 协议概述 5.1.1. 进程之间的通信 5.1.2. 两个主要协议 5.1.3. 端口 5.2. 用户数据报协议UDP 5.3. 传输控制协议TCP概述 5.3.1. TCP的主要特点 5.3.2. TCP的连接 5.4. 可靠传输的工作原理 5.4.1. 停止等待协议 5.4.1.1. 无差错情况 5.4.1.2. 出现差错 5.4.1.3. 确认丢失和确认迟到 5.4.1.4. 信道利用率 5.4.2. 连续ARQ协议 5.5. TCP报文段格式 5.6. TCP可靠传输的实现 5.6.1. 以字节为单位的滑动窗口 5.6.2. 超时重传时间的选择 5.6.3. 选择确认SACK 5.7. TCP的流量控制 5.7.1. 利用滑动窗口实现流量控制 5.7.2. TCP的传输效率 5.8. TCP的拥塞控制 5.8.1. 拥塞控制的一般原理 5.8.2. TCP的拥塞控制方法 5.8.2.1. 慢开始 5.8.2.2. 拥塞避免 5.8.2.3. 快重传 5.8.2.4. 快恢复 5.8.2.5. 总结 5.8.3. 主动队列管理AQM 5.9. TCP的连接管理 5.9.1. TCP的连接建立 5.9.2. TCP的连接释放 5.9.3. TCP的有限状态机 6. 应用层 6.1. 域名系统DNS 6.2. 文件传送协议 6.2.1. FTP 6.2.2. TFTP 6.3. 超文本传送协议HTTP 6.3.1. HTTP基本过程 6.3.2. HTTP报文 6.3.2.1. HTTP请求报文 6.3.2.2. HTTP响应报文报文 6.3.3. HTTP协议的区别 6.3.4. HTTPS 6.3.4.1. HTTPS的工作原理 6.3.4.2. HTTPS和HTTP的区别 6.3.5. HTTP缓存机制 6.3.5.1. 强制缓存 6.3.5.2. 协商缓存 6.3.5.3. 总结 6.3.6. SESSION和COOKIE 6.4. 输入URL之后会发生什么 6.4.1. 大纲 6.4.2. 具体过程 7. 参考链接 1. 概述 1.1. 互联网概述 三大网：电信网、有线电视网、计算机网络。 计算机网络：简称为“网络”，由若干结点和连接这些节点的链路组成。 互连网（internet）：网络通过路由器互连起来构成更大的计算机网络，即“网络的网络”。 主机：与网络相连的计算机。 互联网（Internet）：当前全球最大的、开放的、众多网络互连而成的特定互连网，采用TCP/IP协议族作为通信规则。 互联网服务提供者ISP：商业公司ISP共同拥有整个互联网。 互联网的多层次ISP结构：主干ISP、地区ISP、本地ISP。 互联网交换点IXP：允许两个网络直接相连并交换分组，而不再需要通过更高层的ISP转发分组，从而加快效率。IXP常采用工作在数据链路层的网络交换机，用局域网连接起来。 1.2. 互联网的组成 从工作方式上看，分为： 边缘部分：由所有来凝结在互联网上的主机组成，用户直接使用，用来进行通信和资源共享，即资源子网。 核心部分：由大量网络和连接这些网络的路由器组成，为边缘部分提供连通性和交换，即通信子网。 1.2.1. 边缘部分 连接在互联网上的所有主机又称为“端系统”。端系统之间的通信实质是进程之间的通信。 通信方式有两类：客户服务器方式（C/S）、对等方式（P2P）。 1.2.1.1. 客户-服务器方式 描述的是两个应用进程之间服务与被服务的关系。 客户是服务请求方，服务器是服务提供方。 客户程序 被用户调用后运行，通信时主动想S发起请求，C必须知道S的地址； 不需要特殊的硬件和复杂的OS。 服务器程序 系统启动后自动调用并不断运行，被动地等待并接受C请求，可同时处理多个请求； 一般需要强大的硬件和复杂的OS支持。 1.2.1.2. 对等连接方式 两台主机在通信时不区分哪个是C或S，运行P2P软件即可平等通信，本质上看认识C/S方式。 1.2.2. 核心部分 起特殊作用的是路由器，实现分组交换。 三种交换方式：电路交换、报文交换、分组交换。 1.2.2.1. 电路交换 从通信资源的分配角度来看，交换就是按照某种方式动态地分配传输线路的资源。 在通话之前，必须先拨号请求建立连接，也就是一条专用的物理通路。挂机后，交换机释放刚才使用的这条专用的物理通路。这种必须经过“建立连接（占用通信资源）→通话（一直占用通信资源）→释放连接（归还通信资源）”的交换方式即“电路交换”。 在通话的全部时间内，通话的两个用户始终占用端到端的通信资源。 使用电路交换的线路的传输效率十分低。 1.2.2.2. 分组交换 采用存储转发技术，待发送的在整个数据块称为报文，发送之前将报文划分成等长的数据段，再加上必要的控制信息组成的首部构成分组，又称为“包”。 主机H1向主机H5发送数据。主机H1先将分组逐个地发往与它直接相连的路由器A。此时，除链路H1-A外，其他通信链路并不被目前通信的双方所占用。需要注意的是，即使是链路H1-A，也只是当分组正在此链路上传送时才被占用。在各分组传送之间的空闲时间，链路H1-A仍可为其他主机发送的分组使用。 优点 所采用的手段 高效 在分组传输的过程中动态分配传输带宽，对通信链路是逐段占用 灵活 为每一个分组独立地选择最合适的转发路由 迅速 以分组作为传送单位，可以不先建立连接就能向其他主机发送分组 可靠 保证可靠性的网络协议:分布式多路由的分组交换网，使网络有很好的生存性 问题： 分组在各路由器存储转发时需要排队，造成时延； 各分组携带的控制信息造成开销 1.2.2.3. 报文交换 采用存储转发原理，但每次交换整个报文。 三种交换方式的比较： 1.3. 计算机网络的性能指标 1.3.1. 速率 用的信息量的单位。网络技术中的速率指的是数据的传送速率，它也称为数据率或比特率( bit rate)。速率是计算机网络中最重要的一个性能指标。速率的单位是bit/s。 当提到网络的速率时，往往指的是额定速率或标称速率，而并非网络实际上运行的 1.3.2. 带宽 带宽本来是指某个信号具有的频带宽度。在计算机网络中，带宽用来表示网络中某通道传送数据的能力，因此网络带宽表示在单位时间内网络中的某信道所能通过的“最高数据率”，单位bit/s。 前者为频域称谓，而后者为时域称谓，其本质是相同的。 1.3.3. 吞吐量 吞吐量表示在单位时间内通过某个网络(或信道、接口)的实际的数据量。吞吐量更经常地用于对现实世界中的网络的一种测量，以便知道实际上到底有多少数据量能够通过网络。显然，吞吐量受网络的带宽或网络的额定速率的限制。 1.3.4. 时延 时延包括以下几个部分： 发送时延：主机或路由器发送数据帧所需要的时间 发送时延=数据帧长度bit/发送速率bit/s 传播时延 电磁波在信道中传播一定的距离需要花费的时间。 传播时延=信道长度m/电磁波在信道上的传播速率m/s 发送时延与传输信道的长度没有关系，传播时延与信号的发送速率无关。 处理时延：主机或路由器在收到分组时要花费一定的时间进行处理，例如分析分 组的首部、从分组中提取数据部分、进行差错检验或査找适当的路由等。 排队时延 分组在经过网络传输时，要经过许多路由器。但分组在进入路由器后 要先在输入队列中排队等待处理。排队时延的长短往往取决于网络当时的通信量。 总时延=发送时延+传播时延+处理时延+排队时延 对于高速链路，提高的仅仅是数据的发送速率而非比特在链路上的传播速率，减小了数据的发送时延，所以比特不会传送得更快。 1.3.5. 时延带宽积 时延带宽积=传播时延*带宽 管道中的比特数表示发送端发出的但未到达接收端的比特。 1.3.6. 往返时间RTT 信息双向交互一次所需的时间。 有效数据率=数据长度/发送时间+RTT 1.3.7. 利用率 当前时延=网络空闲时的时延/1-利用率 这里U是网络的利用率，数值在0到1之间。当网络的利用率达到其容量的1/2时 时延就要加倍。 当网络的利用率接近最大值1时，网络的时延就趋于无穷大。因此我们必须有这样的概念:信道或网络的利用率过高会产生非常大的时延。 1.4. 计算机网络体系结构 法律上的国际标准：OSI 事实上的国籍标准：TCP/IP 网络协议：未进行网络中的数据交换而建立的规则、标准或约定。包括： 语法：即数据与控制信息的结构或格式 语义：即需要发出何种控制信息，完成何种动作以及做出何种响应，例如重传或丢弃的时间 同步：即事件实现顺序的详细说明 1.4.1. OSI七层模型 物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。 下4层完成通信子网的功能，上3层完成资源子网的功能。 1.4.1.1. 物理层 利用传输介质为数据链路层提供物理连接，实现比特流的透明传输，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。 考虑多大的电压代表“1”“0”，接收方应如何识别，电缆的插头应有多少根引脚、如何连接等。 1.4.1.2. 数据链路层 在物理层提供的比特流的基础上，通过差错控制、封装成帧、流量控制提供可靠的通过物理介质传输数据的方法。 该层通常又被分为介质访问控制（MAC）和逻辑链路控制（LLC）两个子层。 数据链路层的具体工作是接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层；并且，还负责处理接收端发回的确认帧的信息，以便提供可靠的数据传输。 1.4.1.3. 网络层 将上层的报文段或用户数据报封装成分组，也叫IP数据报。 另一个任务就是要选择合适的路由，使源主机运输层所传下来的分组，能够通过网络中的路由器找到目的主机。 数据链路层是解决同一网络内节点之间的通信，而网络层主要解决不同子网间的通信。 寻址：数据链路层中使用的物理地址（如MAC地址）仅解决网络内部的寻址问题。在不同子网之间通信时，为了识别和找到网络中的设备，每一子网中的设备都会被分配一个唯一的地址。由于各子网使用的物理技术可能不同，因此这个地址应当是逻辑地址（如IP地址）。 交换：规定不同的信息交换方式。常见的交换技术有：线路交换技术和存储转发技术，后者又包括报文交换技术和分组交换技术。 路由算法：当源节点和目的节点之间存在多条路径时，本层可以根据路由算法，通过网络为数据分组选择最佳路径，并将信息从最合适的路径由发送端传送到接收端。 连接服务：与数据链路层流量控制不同的是，前者控制的是网络相邻节点间的流量，后者控制的是从源节点到目的节点间的流量。其目的在于防止阻塞，并进行差错检测。 1.4.1.4. 传输层 OSI下3层的主要任务是数据通信，上3层的任务是数据处理。而传输层是通信子网和资源子网的接口和桥梁，起到承上启下的作用。 传输连接管理：提供建立、维护和拆除传输连接的功能。传输层在网络层的基础上为高层提供“面向连接”和“面向无接连”的两种服务。 处理传输差错：提供可靠的“面向连接”和不太可靠的“面向无连接”的数据传输服务、差错控制和流量控制。在提供“面向连接”服务时，通过这一层传输的数据将由目标设备确认，如果在指定的时间内未收到确认信息，数据将被重发。 1.4.1.5. 会话层 向两个实体的表示层提供建立和使用连接的方法。将不同实体之间的表示层的连接称为会话。因此会话层的任务就是组织和协调两个会话进程之间的通信，并对数据交换进行管理。 会话管理：允许用户在两个实体设备之间建立、维持和终止会话，并支持它们之间的数据交换。例如提供单方向会话或双向同时会话，并管理会话中的发送顺序，以及会话所占用时间的长短。 会话流量控制：提供会话流量控制和交叉会话功能。 寻址：使用远程地址建立会话连接。l 出错控制：从逻辑上讲会话层主要负责数据交换的建立、保持和终止，但实际的工作却是接收来自传输层的数据，并负责纠正错误。会话控制和远程过程调用RPC均属于这一层的功能。但应注意，此层检查的错误不是通信介质的错误，而是磁盘空间、打印机缺纸等类型的高级错误。 1.4.1.6. 表示层 对来自应用层的命令和数据进行解释，对各种语法赋予相应的含义，并按照一定的格式传送给会话层。 其主要功能是“处理用户信息的表示问题，如编码、数据格式转换和加密解密”等。 数据格式处理：协商和建立数据交换的格式，解决各应用程序之间在数据格式表示上的差异。 数据的编码：处理字符集和数字的转换。例如由于用户程序中的数据类型（整型或实型、有符号或无符号等）、用户标识等都可以有不同的表示方式，因此，在设备之间需要具有在不同字符集或格式之间转换的功能。 压缩和解压缩：为了减少数据的传输量，这一层还负责数据的压缩与恢复。 数据的加密和解密：可以提高网络的安全性。 1.4.1.7. 应用层 计算机用户，以及各种应用程序和网络之间的接口，其功能是直接向用户提供服务，通过应用进程间的交互完成特定的网络应用。 用户接口：应用层是用户与网络，以及应用程序与网络间的直接接口，使得用户能够与网络进行交互式联系。 实现各种服务：该层具有的各种应用程序可以完成和实现用户请求的各种服务。 1.4.2. 总结 OSI七层：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。 TCP/IP四层：网络接口层、 网际层、运输层、 应用层。 五层协议：物理层、数据链路层、网络层、运输层、 应用层。 协议 物理层：RJ45、CLOCK、IEEE802.3（网卡，网线，集线器，中继器，调制解调器） 数据链路：PPP、FR、HDLC、VLAN、MAC（网桥，交换机） 网络层：IP、ICMP、ARP、RARP、OSPF、IPX、RIP、IGRP（路由器、网关） 传输层：TCP、UDP、SPX 会话层：NFS、SQL、NETBIOS、RPC 表示层：JPEG、MPEG、ASII 应用层：FTP、DNS、Telnet、SMTP、HTTP、WWW、NFS 作用 物理层：通过媒介传输比特，确定机械及电气规范（比特Bit） 数据链路层：将比特组装成帧和点到点的传递（帧Frame） 网络层：负责数据包从源到宿的传递和网际互连（包PackeT） 传输层：提供端到端的可靠报文传递和错误恢复（段Segment） 会话层：建立、管理和终止会话（会话协议数据单元SPDU） 表示层：对数据进行翻译、加密和压缩（表示协议数据单元PPDU） 应用层：允许访问OSI环境的手段（应用协议数据单元APDU） 协议数据单元PDU：对等层次之间传送的数据单位。 服务数据单元SDU：层间交换的数据单位。 服务访问点SAP：同意系统中相邻两层的实体交换信息的地方。 协议是水平的，服务是垂直的。 交换机、路由器、网关 交换机 在计算机网络系统中，交换机是针对共享工作模式的弱点而推出的。交换机拥有一条高带宽的背部总线和内部交换矩阵。交换机的所有的端口都挂接在这条背 部总线上，当控制电路收到数据包以后，处理端口会查找内存中的地址对照表以确定目的MAC（网卡的硬件地址）的NIC（网卡）挂接在哪个端口上，通过内部 交换矩阵迅速将数据包传送到目的端口。目的MAC若不存在，交换机才广播到所有的端口，接收端口回应后交换机会“学习”新的地址，并把它添加入内部地址表中。 交换机工作于OSI参考模型的第二层，即数据链路层。交换机内部的CPU会在每个端口成功连接时，通过ARP协议学习它的MAC地址，保存成一张 ARP表。在今后的通讯中，发往该MAC地址的数据包将仅送往其对应的端口，而不是所有的端口。因此，交换机可用于划分数据链路层广播，即冲突域；但它不 能划分网络层广播，即广播域。 交换机被广泛应用于二层网络交换，俗称“二层交换机”。 交换机的种类有：二层交换机、三层交换机、四层交换机、七层交换机分别工作在OSI七层模型中的第二层、第三层、第四层盒第七层，并因此而得名。 路由器 路由器（Router）是一种计算机网络设备，提供了路由与转送两种重要机制，可以决定数据包从来源端到目的端所经过 的路由路径（host到host之间的传输路径），这个过程称为路由；将路由器输入端的数据包移送至适当的路由器输出端(在路由器内部进行)，这称为转 送。路由工作在OSI模型的第三层——即网络层，例如网际协议。 路由器的一个作用是连通不同的网络，另一个作用是选择信息传送的线路。 路由器与交换器的差别，路由器是属于OSI第三层的产品，交换器是OSI第二层的产品(这里特指二层交换机)。 网关 网关（Gateway），网关顾名思义就是连接两个网络的设备，区别于路由器（由于历史的原因，许多有关TCP/IP 的文献曾经把网络层使用的路由器（Router）称为网关，在今天很多局域网采用都是路由来接入网络，因此现在通常指的网关就是路由器的IP），经常在家 庭中或者小型企业网络中使用，用于连接局域网和Internet。 网关也经常指把一种协议转成另一种协议的设备，比如语音网关。 在传统TCP/IP术语中，网络设备只分成两种，一种为网关（gateway），另一种为主机（host）。网关能在网络间转递数据包，但主机不能 转送数据包。在主机（又称终端系统，end system）中，数据包需经过TCP/IP四层协议处理，但是在网关（又称中介系 统，intermediate system）只需要到达网际层（Internet layer），决定路径之后就可以转送。在当时，网关 （gateway）与路由器（router）还没有区别。 在现代网络术语中，网关（gateway）与路由器（router）的定义不同。网关（gateway）能在不同协议间移动数据，而路由器（router）是在不同网络间移动数据，相当于传统所说的IP网关（IP gateway）。 网关是连接两个网络的设备，对于语音网关来说，他可以连接PSTN网络和以太网，这就相当于VOIP，把不同电话中的模拟信号通过网关而转换成数字信号，而且加入协议再去传输。在到了接收端的时候再通过网关还原成模拟的电话信号，最后才能在电话机上听到。 对于以太网中的网关只能转发三层以上数据包，这一点和路由是一样的。而不同的是网关中并没有路由表，他只能按照预先设定的不同网段来进行转发。网关最重要的一点就是端口映射，子网内用户在外网看来只是外网的IP地址对应着不同的端口，这样看来就会保护子网内的用户。 2. 物理层 首先要强调指岀，物理层考虑的是怎样才能在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体。现有的计算机网络中的硬件设备和传输媒体的种类非常繁多，而通信手段也有许多不同方式。物理层的作用正是要尽可能地屏蔽掉这些传输媒体和通信手段的差异，使物理层上面的数据链路层感觉不到这些差异。 可以将物理层的主要任务描述为确定与传输媒体的接口有关的一些特性，即 机械特性指明接口所用接线器的形状和尺寸、引脚数目和排列、固定和锁定装置，等。平时常见的各种规格的接插件都有严格的标准化的规定。 电气特性指明在接口电缆的各条线上出现的电压的范围。 功能特性指明某条线上出现的某一电平的电压的意义。 过程特性指明对于不同功能的各种可能事件的出现顺序 物理层还要完成串行-&gt;并行传输的转换。 2.1. 数据通信 2.1.1. 数据通信系统 可划分为三大部分，即源系统(或发送端、发送方)、传输系统(或传输网络)和目的系统(或接收端、接收方)。 模拟信号，或连续信号：代表消息的参数的取值是连续的。 数字信号，或离散信号：代表消息的参数的取值是离散的。代表不同离散数值的基本波形就称为码元。 2.1.2. 信道 从通信的双方信息交互的方式来看，可以有以下三种基本方式： 单向通信：称为单工通信，即只能有一个方向的通信而没有反方向的交互。无 线电广播或有线电广播以及电视广播就属于这种类型 双向交替通信：又称为半双工通信，即通信的双方都可以发送信息，但不能双方 同时发送(当然也就不能同时接收)。 双向同时通信：又称为全双工通信，即通信的双方可以同时发送和接收信息 单向通信只需要一条信道，而双向交替通信或双向同时通信则都需要两条信道(每个 方向各一条)。 显然，双向同时通信的传输效率最高。 来自信源的基带信号往往包含有较多的低频成分，甚至有直流成分，而许多信道并不能传输这种低频分量或直流分量。所以必须对基带信号进行调制，分为两大类： 基带调制：对基带信号的波形进行变换，使它能够与信道特性相适应，变换后的信号仍然是基带信号，也称为编码。 不归零制 归零制 曼彻斯特编码 差分曼彻斯特编码 带通调制：使用载波把基带信号的频率范围搬移到较高的频段，并转换为模拟信号，经过载波调制后的信号称为带通信号，即仅在一段频率范围内能够通过信道。 调幅（AM） 调频（FM） 调相（PM） 2.1.3. 信道的极限容量 码间串扰：接收端收到的信号波形就失去了码元之间的清晰界限。 严重的码间串扰使得本来分得很清楚的一串码元变得模糊而无法识别。 奈氏准则：在任何信道中，码元传输的速率是有上限的，传输速率超过此上限，就会出现严重的码间串扰的问题。 信噪比：所谓信噪比就是信号的平均功率和噪声的平均功率之比，常记为S/N，并用分贝(dB)作为度量单位。 香农公式：信道的极限信息传输速率C是 式中，W为信道的带宽(Hz);S为信道内所传信号的平均功率;N为信道内 部的高斯噪声功率。 香农公式的意义在于:只要信息传输速率低于信道的极限信息传输速率，就一定存在某种办法来实现无差错的传输。 从以上所讲的不难看出，对于频带宽度已确定的信道，可以用编码的方法让每一个码元携带更多比特的信息量。 2.2. 物理层下的传输媒体 导引型 双绞线 同轴线缆 光缆 非导引型 短波通信 无线电微波通信 卫星通信 2.3. 信道复用技术 频分复用（FDM）：所有用户在同样的时间占用不同的带宽资源。 时分复用（TDM）：所有用户在不同的时间占用同样的频带宽度，又称为同步时分复用。 统计时分复用（STDM）：改进的时分复用，缓存用户数据放入STDM帧，又称为异步时分复用 波分复用（WDM）：光的频分复用。 码分复用（CDM）：共享信道。 2.4. 宽带接入技术 ADSL技术：改造电话用户线； 光纤同轴混合网（HFC网）：改造有线电视网； 光纤到户（FTTx）：光信号转电信号。 3. 数据链路层 数据链路层使用的信道： 点对点信道。这种信道使用一对一的点对点通信方式。 广播信道。这种信道使用一对多的广播通信方式。 3.1. 使用点对点信道 3.1.1. 三个基本问题 封装成帧。透明传输、差错检测 3.1.1.1. 封装成帧 封装成帧就是在一段数据的前后分别添加首部和尾部，这样就构成了一个帧， 首部和尾部的一个重要作用就是进行帧定界(即确定帧的界限)。此外，首部和尾部还包括许多必要的控制信息。应当使帧的数据部分长度尽可能地大于首部和尾部的长度。每一种链路层协议都规定了所能传送的帧的数据部分长度上限—最大传送 单元MTU。 帧定界符SOH和EOT标识真的开始与结束，用以判断一个完整的帧。 3.1.1.2. 透明传输 当传送的用文本组成的帧时，不管从键盘上输入什么字符都可以放在这样的帧中传输过去，因此这样的传输就是透明传输。 但当数据部分是非ASCI码的文本文件时(如二进制代码的计算机程序或图像等)，情 况就不同了。如果数据中的某个字节的二进制代码恰好和SOH或EOT这种控制字符一样 ，数据链路层就会错误地“找到帧的边界”，把部分帧收下(误认为是个完整的 帧)，而把剩下的那部分数据丢弃(这部分找不到帧定界控制字符SOH)。 字符填充 为了解决透明传输问题，发送端的数据链路层在数据中出现控制字符 SOH”或“EOT”的前面插入一个转义字符“ESC”。而在接收端的数据链路层在把数据送往网络层之前删除这个插入的转义字符。 3.1.1.3. 差错检测 比特差错：比特在传输过程中可能会产生差错：1可能会变成0，而0也可能变成1。 误码率BER：在一段时间内，传输错误的比特占所传输比特总数的比率。 循环冗余检验CRC 判定这个帧有差错，但无法确定究竟是哪一位或哪几位出现了差错。 在数据链路层若仅仅使用循环冗余检验CRC差错检测技术，则只能做到对帧的无差错接受 即:“凡是接收端数据链路层接受的帧，我们都能以非常接近于1的概率认为这些帧在传输过程中没有产生差错”。接收端丢弃的帧虽然曾收到了，但最终还是因为有差错被丢弃，即没有被接受。 传输差错：帧丢失、帧重复或帧失序。 过去oSI的观点是:必须让数据链路层向上提供可靠传输。因此在CRC检错的基础上，增加了帧编号、确认和重传机制。 3.1.2. 点对点协议PPP 在TCP/P协议族中，可靠传输由运输层的TCP协议负责，因此数据链路层的PPP协 议不需要进行纠错，不需要设置序号，也不需要进行流量控制。PPP协议不支持多点线路，只支持点对点的链路通信。此外，PPP协议只支持全双工链路。 3.1.2.1. 协议组成 PPP协议有三个组成部分： 将P数据报封装到串行链路的方法。PP既支持异步链路(无奇偶检验的8比 特数据)，也支持面向比特的同步链路。IP数据报在PPP帧中就是其信息部分。这个信息部分的长度受最大传送单元MTU的限制。 用来建立、配置和测试数据链路连接的链路控制协议LCP( Link Cont Protoco)。通信的双方可协商一些选项。在RFC1661中定义了11种类型的LCP分组 一套网络控制协议 NCP(Network Control Protocol)°，其中的每一个协议支持不同的网络层协议，如IP、OSI的网络层、 DECnet，以及 AppleTalk等 3.1.2.2. PPP帧格式 F：0x7E 01111110，标志字段，标识帧的开始或结束 A、C：无意义 协议：表示信息部分使用的协议类型 FCS：检验序列 字节填充 使用异步传输时，它把转义符定义为0xD(即ol11101)，并使用字节填充 RFC1662规定了如下所述的填充方法 把信息字段中出现的每一个0x7E字节转变成为2字节序列(0x7D，0x5E) 若信息字段中出现一个0x7D的字节(即出现了和转义字符一样的比特组合)，则 把0x7D转变成为2字节序列(0x7D，0x5D) 若信息字段中出现ASCI码的控制字符(即数值小于0x20的字符)，则在该字符前 面要加入一个αx⑦D字节，同时将该字符的编码加以改变。例如，出现0x03(在控制字符中是“传输结束”ETX)就要把它转变为2字节序列(0x7D，Ox23)。 零比特填充 使用同步传输时，在发送端，只要发现有5个连续1，则立即填入一个0。因此经过这种填充后的数据，就可以保证在信息字段中不会出现6个连续1。接收端在收到一个帧 时，先找到标志字段F以确定一个帧的边界，接着再用硬件对其中的比特流进行扫描。每当发现5个连续1时，就把这5个连续1后的一个0删除，以还原成原来的信息比特流。 3.1.2.3. PPP协议工作状态 当用户拨号接入ISP后，就建立了一条从用户个人电脑到IsP的物理连接。这时，用户个人电脑向ISSP发送一系列的链路控制协议LCP分组(封装成多个PPP帧)，以便建立LCP连接。这些分组及其响应选择了将要使用的一些PPP参数。接着还要进行网络层配置，网络控制协议NCP给新接入的用户个人电脑分配一个临时的IP地址。这样，用户个人电脑就成为互联网上的一个有IP地址的主机 当用户通信完毕时，NCP释放网络层连接，收回原来分配出去的IP地址。接着，LCP释放数据链路层连接。最后释放的是物理层的连接。 在“网络层协议”状态，PPP链路的两端的网络控制协议NCP根据网络层的不同协议互相交换网络层特定的网络控制分组。这个步骤是很重要的，因为现在的路由器都能够同时支持多种网络层协议。总之，PPP协议两端的网络层可以运行不同的网络层协议，但仍然可使用同一个PP协议进行通信 如果在PPP链路上运行的是IP协议，则对PP链路的每一端配置IP协议模块(如分配IP地址)时就要使用NCP中支持IP的协议——IP控制协议IPCP( IP ControlProtoco)IPCP分组也封装成PPP帧(其中的协议字段为0x8021)在PP链路上传送。在低速链路上运行时，双方还可以协商使用压缩的TCP和P首部，以减少在链路上发送的比特数。 PPP协议已不是纯粹的数据链路层的协议，它还包含了物理层和网络层的内容。 3.2. 使用广播信道 3.2.1. 局域网的数据链路层 局域网使用的就是广播信道。按照网络拓扑分为星形网、环形网、总线网。总线网以传统以太网最为著名，之后出现的快速以太网、吉比特以太网在市场中占据绝对优势，以太网成为局域网的同义词。 共享信道技术： 静态划分信道：复用分用技术； 动态媒体接入控制：又称多点接入 随机接入：用户随机发送信息，需要解决碰撞； 受控接入：用户发送信息需要服从控制。轮询。 3.2.1.1. 以太网的两个标准 DIX Ethernet V2 IEEE 802.3 二者差别很小。后来IEEE 802将数据链路层拆分为逻辑链路控制LLC( Logical Link Contro)子层和媒体接入控制MAC( Medium Access Control)子层。与接入到传输媒体有关的内容都放在MAC子层，而LLC子层则与传输媒体无关。 后来市场稳定，LLC子层的作用消失，DIX Ethernet V2市场更大。 3.2.1.2. 适配器 计算机与外界局域网的连接时通过通信适配器进行的，又称为网络接口卡NIC。装有处理器和存储器（包括RAM和ROM） 作用： 串并行的转换 缓存 实现以太网协议 工作： 适配器在接收和发送时不使用计算机的CPU。当适配器收到有差错的帧时，就把这个帧直接丢弃而不必通知计算机。收到正确的帧时，它就使用中断来通知该计算机，并交付协议栈中的网络层。 当计算机要发送IP数据报时，就由协议栈把IP数据报向下交给适配器，组装成帧后发送到局域网。 计算机的硬件地址就在适配器的ROM中，而计算机的软件地址（IP地址）则在计算机的存储器中。 3.2.2. CSMA/CD协议 总线使用。 为了通信简便，采用两个措施： 采用较为灵活的无连接的工作方式，即不必先建立连接就可以直接发送数据 适配器对发送的数据帧不进行编号，也不要求对方发回确认。以太网提供的服务是尽最大努力的交付，即不可靠的交付。 当目的站收到有差错的数据帧时，就把帧丢弃，其他什么也不做。对有差错帧是否需要重传则由高层来决定。 总线上在同一时间只能允许一台计算机发送数据，否则各计算机之间就会互相干扰，因此以太网使用CSMA/CD协议。 以太网发送的数据都使用曼彻斯特( Manchester)编码的信号。 要点 先听后发 在发送前检测信道，是为了获得发送权。如果检测出已经有其他站在发送，则自己就暂时不许发送数据，必须要等到信道变为空闲时才能发送。 边听边发 在发送中检测信道，是为了及时发现有没有其他站的发送和本站发送的碰撞。 冲突停止 当适配器检测到的信号电压变化幅度超过一定的门限值时，就认为总线上至少有两个站同时在发送数据，表明产生了碰撞。适配器就要立即停止发送，免得继续进行无效的 发送，白白浪费网络资源，然后等待一段随机时间后再次发送。 延迟重发 使用截断二进制指数退避算法确定延迟重发时机。 因为传播时延的存在导致先听后发之后人有可能发生碰撞。发送数据后，最迟要经过多长时间才能知道自己发送的数据和其他站发送的数据有没有发生碰撞?这个时间最多是两倍的总线端到端的传播时延(2r)，称为争用期，又叫碰撞窗口。，即经过争用期这段时间还没有检测到碰撞，才能肯定这次发送不会发生碰撞。 因此局域网必须按最坏情况设计，即取总线两端的两个站之间的传播时延(这两个站之间的距离最大)为端到端传播时延。 显然，在使用 CSMA/CD协议时，一个站不可能同时进行发送和接收(但必须边发送 边监听信道)。因此使用 CSMA/CD协议的以太网不可能进行全双工通信而只能进行双向交替通信(半双工通信)。 凡长度小于64字节的帧都是由于冲突而中止的无效帧。 强化碰撞 当发送数据的站一旦发现发生了碰撞时，除了立即停止发送数据外，还要再继续发送3比特或48比特的人为干扰信号，以便让所有用户都知道现在已经发生了碰撞。 帧间最小间隔 以太网还规定了帧间最小间隔为96μs，相当于96比特时间。这样做是为了使刚刚收 到数据帧的站的接收缓存来得及清理，做好接收下一帧的准备。 3.2.3. 使用集线器的星形拓扑 集线器和每个站之间使用两对双绞线，用于发送和接收。 IEEE制定了10BASE-T星形以太网标准802.3i。“10”代表10Mbit/s的数据率，BASE表示连接线上的信号是基带信号，T代表双绞线。每个站到集线器的距离不超过100m。 特点 用集线器的以太网在逻辑上仍是一个总线网，各站共享逻辑上的总线，使用的还是 CSMA/CD协议。 一个集线器有许多接口，每个接口通过RJ-45插头用两对双绞线与一台计算机上的适配器相连。因此，一个集线器很像一个多接口的转发器。 集线器工作在物理层，它的每个接口仅仅简单地转发比特，收到1就转发1，收 到0就转发0，不进行碰撞检测。 3.2.4. 信道利用率 信道利用率a=\\frac{单程端到端时延r}{帧的发送时间T} 因此，以太网的连线的长度受到限制(否则r数值会太大)，同时以太网的帧长不能太短(否则T的值会太小，使a值太大)。 3.2.5. MAC层 3.2.5.1. MAC地址 规定了一种48位的全球地址，是指局域网上的每一台计算机中固化在适配器的ROM中的地址。 适配器从网络上每收到一个MAC帧就先用硬件检査MAC帧中的目的地址。如果是发往本站的帧则收下，然后再进行其他的处理。否则就将此帧丢弃，不再进行其他的处理。 “发往本站的帧”包括以下三种帧： 单播(unicast)帧(一对一)，即收到的帧的MAC地址与本站的硬件地址相同。 广播(broadcast帧(一对全体)，即发送给本局域网上所有站点的帧(全1地址)。 多播(multicas帧(一对多)，即发送给本局域网上一部分站点的帧 所有的适配器都至少应当能够识别前两种帧，即能够识别单播和广播地址。显然，只有目的地址才能使用广播地址和多播地址。 3.2.5.2. MAC帧的格式 DIX Ethernet V2 类型：标注上一层是什么协议。 帧长度 在曼彻斯特编码的每一个码元(不管码元是1或0)的正中间一定有次电压的转换(从高到低或从低到高)。当发送方把一个以太网帧发送完毕后，就不再发送其他码元了(既不发送1，也不发送0)。因此，发送方网络适配器的接口上的电压也就不再变化了。这样，接收方就可以很容易地找到以太网帧的结束位置。在这个位置往前数4字节 (FCS字段长度是4字节)，就能确定数据字段的结束位置。 8字节 第一个字段是7个字节的前同步码(1和0交替码)，它的作用是使接收端的适配器在接收MAC帧时能够迅速调整其时钟频率，使它和发送端的时钟同步 也就是“实现位同步”(位同步就是比特同步的意思)。第二个字段是帧开始定界符，定义为10101011。 定界 以太网在传送帧时，各帧之间还必须有一定的间隙。因此，接收端只要找到帧开始定界符，其后面的连续到达的比特流就都属于同一个MAC帧。可见以太网不需要使用帧结束定界符，也不需要使用字节插入来保证透明传输。 IEEE802.3 IEEE8023规定的MAC帧的第三个字段是“长度/类型”。当这个字段值大于 0x0600时(相当于十进制的1536)，就表示“类型”。这样的帧和以太网V2 MAC帧完全 样。只有当这个字段值小于0x0600时才表示“长度”，即MAC帧的数据部分长度。实际上，前面我们已经讲过，由于以太网采用了曼彻斯特编码，长度字段并无实际意义。 当“长度/类型”字段值小于0x0600时，数据字段必须装入上面的逻辑链路控制 JC子层的LLC帧。 3.2.6. 扩展的以太网 扩展的以太网在网络层看来仍是一个网络。 3.2.6.1. 物理层扩展 使用多级结构的集线器。 缺点 通过集线器互连起来后就把三个碰撞域变成一个碰撞域，而这时的最大吞吐量没有变化。 如果不同的系使用不同的以太网技术，那么就不可能用集线器将们互连起来。 3.2.6.2. 数据链路层扩展 以太网交换机的出现代替了网桥转发帧。 3.2.6.2.1. 交换机特点 实质是一个多接口的网桥，全双工工作，具有并行性，即同时连通多对接口使之同时通信。主机独占传输媒体，无碰撞地传输数据； 接口有存储器帮助缓存； 即插即用，内部的交换表通过自学习算法自动建立； 用户独享带宽，增加了总吞吐量； 多种速率的接口 3.2.6.2.2. 路由器与交换机的主要区别 工作层次不同 最初的的交换机是工作在数据链路层，而路由器一开始就设计工作在网络层。由于交换机工作在数据链路层，所以它的工作原理比较简单，而路由器工作在网络层，可以得到更多的协议信息，路由器可以做出更加智能的转发决策。 数据转发所依据的对象不同 交换机是利用物理地址或者说MAC地址来确定转发数据的目的地址。而路由器则是利用IP地址来确定数据转发的地址。IP地址是在软件中实现的，描述的是设备所在的网络。MAC地址通常是硬件自带的，由网卡生产商来分配的，而且已经固化到了网卡中去，一般来说是不可更改的。而IP地址则通常由网络管理员或系统自动分配。 传统的交换机只能分割冲突域，不能分割广播域；而路由器可以分割广播域 由交换机连接的网段仍属于同一个广播域，广播数据包会在交换机连接的所有网段上传播，在某些情况下会导致通信拥挤和安全漏洞。连接到路由器上的网段会被分配成不同的广播域，广播数据不会穿过路由器。虽然第三层以上交换机具有VLAN功能，也可以分割广播域，但是各子广播域之间是不能通信交流的，它们之间的交流仍然需要路由器。 路由器提供了防火墙的服务 路由器仅仅转发特定地址的数据包，不传送不支持路由协议的数据包传送和未知目标网络数据包的传送，从而可以防止广播风暴。 3.2.6.2.3. 以太网的自学习功能 A先向B发送一帧，从接口1进入到交换机。交换机收到帧后，先查找交换表，没有 查到应从哪个接口转发这个帧(在MAC地址这一列中，找不到目的地址为B的项目)。接 着，交换机把这个帧的源地址A和接口1写入交换表中，并向除接口1以外的所有接口 播这个帧(这个帧就是从接口1进来的，当然不应当把它再从接口1转发出去)。 C和D将丢弃这个帧，因为目的地址不对。只B才收下这个目的地址正确的帧。这也 称为过滤。 从新写入交换表的项目(A，1)可以看出，以后不管从哪一个接口收到帧，只要其目的地 址是A，就应当把收到的帧从接口1转发出去。这样做的依据是:既然A发出的帧是从接 口1进入到交换机的，那么从交换机的接口1转发出的帧也应当可以到达A。 假定接下来B通过接口3向A发送一帧。交换机查找交换表，发现交换表中的MAC地址有A。表明要发送给A的帧(即目的地址为A的帧)应从接口1转发。于是就把这个帧传送到接口1转发给A。显然，现在已经没有必要再广播收到的帧。交换表这时新增加的项目(B，3)，表明今后如有发送给B的帧，就应当从接口3转发出去。 经过一段时间后，只要主机C和D也向其他主机发送帧，以太网交换机中的交换表就会把转发到C或D应当经过的接口号(2或4)写入到交换表中。这样，交换表中的项目就齐全了。要转发给任何一台主机的帧，都能够很快地在交换表中找到相应的转发接口考虑到有时可能要在交换机的接口更换主机，或者主机要更换其网络适配器，这就需要更改交换表中的项目。为此，在交换表中每个项目都设有一定的有效时间。过期的项目就 自动被删除。用这样的方法保证交换表中的数据都符合当前网络的实际状况。 以太网交换机的这种自学习方法使得以太网交换机能够即插即用，不必人工进行配置。 生成树协议STP 假定一开始主机A通过接口交换机#1向主机B发送一帧。交换机#1收到这个帧后就向所有其他接口进行广播发送。现观察其中一个帧的走向:离开交换机#1的 接口3→交换机#2的接口1→接口2→交换机#1的接口4→接口3→交换机#的接口这样就无限制地循环兜圈子下去，白白消耗了网络资源 为了解决这种兜圈子问题，IEE的8021D标准制定了一个生成树协议STP( Spanning Tree Protocol)。其要点就是不改变网络的实际拓扑，但在逻辑上则切断某些链路，使得从台主机到所有其他主机的路径是无环路的树状结构，从而消除了兜圈子现象。 4. 网络层 4.1. 网络层提供的两种服务 面向连接：虚电路 无连接的 网络层向上只提供简单灵活的、无连接的、尽最大努力交付的数据报服。 网络在发送分组时不需要先建立连接。每一个分组(也就是IP数据报)独立发送，与其前后的分组无关(不进行编号)。网络层不提供服务质量的承诺。 虚电路服务与数据报服务的对比： 对比的方面 虚电路服务 数据报服务 思路 靠通信应当由网络来保证 可靠通信应当由用户主机来保证 连接的建立 必须有 不需要 终点地址 仅在连接建立阶段使用，每个分组使用短 每个分组都有终点的完整地址 的虚电路号 分组的转发 属于同一条虚电路的分组均按照同一路由进行转发 每个分组独立选择路由进行转发 当结点出故障时 所有通过出故障的结点的虚电路均不能工作 出故障的结点可能会丢失分组 分组的顺序 总是按发送顺序到达终点 到达终点的时间不一定按发送顺序 端到端的差错处理和流量控制 可以由网络负责，也可以由用户主机负责 由用户主机负责 4.2. 网际协议IP 与IP协议配套使用的还有三个协议： 地址解析协议ARP 网际控制报文协议ICMP 网际组管理协议IGMP 逆地址解析协议RARP：使只知道自己硬件地址的主机能够通过RARP协议找出其IP地址。现在的DHCP协议已经包含RARP协议的功能。 4.2.1. 分类的IP地址 IP编址经历的三个历史阶段： 分类的IP地址 划分子网 构造超网 4.2.1.1. IP地址的表示 一个网络号在整个互联网范围内必须是唯一的，一台主机号在它前面的网络号所指明的网络范围内必须是唯一的，一个IP地址在整个互联网范围内是唯一的。 两级的IP地址可以记为: IP地址:={&lt;网络号&gt;，&lt;主机号&gt;} 4.2.1.2. 常见的三类IP地址 A类地址的网络号字段占1个字节，只有7位可供使用(该字段的第一位已固定为0)，但可指派的网络号是126个(即27-2)。减2的原因是:第一，IP地址中的全0表示“这个this”。网络号字段为全0的IP地址是个保留地址，意思是“本网络”;第二，网络号为127(即01111111作为本地软件环回测试( loopback test)本主机的进程之间的通信之用。目的地址为环回地址的IP数据报永远不会出现在任何网络上，因为网络号为127的地址根本不是一个网络地址。 A类地址的主机号占3个字节，因此每一个A类网络中的最大主机数是24-2，即6770214。这里减2的原因是:全0的主机号字段表示该IP地址是“本主机”所连接到的单个网络地址(例如，一主机的IP地址为567.8，则该主机所在的网络地址就是567.0.0.0)而全1表示“所有的(a)”，因此全1的主机号字段表示该网络上的所有主机 B类地址的网络号字段有2个字节，但前面两位(10)已经固定了，只剩下14位可以进行分配。因为网络号字段后面的14位无论怎样取值也不可能出现使整个2字节的网络号字段成为全0或全1，因此这里不存在网络总数减2的问题。 C类地址有3个字节的网络号字段，最前面的3位是(110)，还有21位可以进行分配。C类网络地址1920.0.0也是不指派的，可以指派的C类最小网络地址是92010 IP地址的指派范围 网络类别 最大可指派的网络数 第一个可指派的网络号 最后一个可指派的网络号 每个网络中的最大主机数 A 2{7}-2 1 126 16777214 B 2{14}-2 128.1 191.255 65534 C 2{21}-2 192.0.1 223.255.255 254 一般不使用的特殊IP地址 网络号 主机号 源地址使用 目的地址使用 代表的意思 0 0 可以 不可 在本网络上的本主机(DHCP) 0 host-id 可以 不可 在本网络上的某台主机 host-id 全1 全1 不可 可以 只在本网络上进行广播(各路由器均不转发) net-id 全1 不可 可以 对net-id上的所有主机进行广播 127 非全0或全1的任何数 可以 可以 用于本地软件环回测试 特点 每一个IP地址都由网络号和主机号两部分组成。所以IP地址是一种 分等级的地址结构。分两个等级的好处是: IP地址管理机构在分配I地址时只分配网络号(第一级)，而剩下的主机号(第二级)则由得到该网络号的单位自行分配。 路由器仅根据目的主机所连接的网络号来转发分组(而不考虑目的主机号)，这样就可以使路由表中的项目数大幅度减少，从而减小了路由表所占的存储空间以及查找路由表的时间。 实际上IP地址是标志一台主机(或路由器)和一条链路的接口。当一台主机同时连 接到两个网络上时，该主机就必须同时具有两个相应的IP地址，其网络号必须是不同的 这种主机称为多归属主机。 用转发器或网桥连接起来的若干个局域网仍为一个网络，因为这些局域网都具有同样的网络号。具有不同网络号的局域网必须使用路由器进行互连。 所有分配到网络号的网络(不管是范围很小的局域网，还是可能覆盖很大地理范围的广域网)都是平等的。所谓平等，是指互联网同等对待每一个IP地址。 4.2.2. 私有地址 由于IP地址的紧缺，一个机构能够申请到的IP地址数往往远小于本机构所拥有的主机数。考虑到互联网并不很安全，一个机构内也并不需要把所有的主机接入到外部的互联网.实际上，在许多情况下，很多主机主要还是和本机构内的其他主机进行通信。假定在一个机构内部的计算机通信也是采用 TCP/IP协议，那么从原则上讲，对于这些仅在机构内部使用的计算机就可以由本机构自行分配其IP地址。这就是说，让这些计算机使用仅在本机构有效的酽地址(这种地址称为私有地址)，而不需要向互联网的管理机构申请全球唯一的IP地址(这种地址称为公有地址)。这样就可以大大节约宝贵的全球IP地址资源。 为了解决这一问题，RFC1918指明了一些专用地址( private address)这些地址只能用 于一个机构的内部通信，而不能用于和互联网上的主机通信。在互联网中的所有路由器，对目的地址是专用地址的数据报 律不进行转发，即 10.0.0.0到10.255.255.255(或记为10.0.0.0/8，它又称为24位块) 172.16.0.0到172.16.255.255(或记为172.16.0.0/12，它又称为20位块) 192.168.0.0到192.168.255.255(或记为192.168.0.0/16，它又称为16位块) 采用这样的专用IP地址的互连网络就叫做专用网。专用IP地址也叫做可重用地址。 4.2.3. 网络地址转换NAT 下面讨论另一种情况，就是在专用网内部的一些主机本来已经分配到了本地IP地址，即仅在本专用网内使用的专用地址)，但现在又想和互联网上的主机通信(并不需要加密)，那么应当采取什么措施呢? 网络地址转换NAT需要在专用网连接到互联网的路由器上安装NAT软件。装有NAT软件的路由器叫做NAT路由器，它至少有一个有效的外部全球IP地址。这样，所有使用本地地址的主机在和外界通信时，都要在NAT路由器上将其本地地址转换成全球IP地址，才能和互联网连接。 4.2.4. DHCP协议 DHCP动态主机设置协议（Dynamic Host Configuration Protocol）是一个局域网的网络协议，使用UDP协议工作，主要有两个用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。 4.2.5. IP地址与硬件地址 从层次的角度看，物理地址是数据链路层和物理层使用的地址，而IP地址是网络层和以上各层使用的地址，是一种逻辑地址(用软件实现的)。 IP地址放在IP数据报的首部，而硬件地址则放在MAC帧的首部。在网络层和网络层以上使用的是IP地址，而数据链路层及以下使用的是硬件地址。当IP数据报放入数据链路层的MAC帧中以后，整个的IP数据报就成为MAC帧的数据，因而在数据链路层看不见数据报的IP地址。 要点 在IP层抽象的互联网上只能看到IP数据报。 虽然在P数据报首部有源站IP地址，但路由器只根据目的站的I地址的网络号进行路由选择 在局域网的链路层，只能看见MAC帧。 IP数据报被封装在MAC帧中。MAC帧在不同网络上传送时，其MAC帧首部中的源地址和目的地址要发生变化。开始在H1到R1间传送时，MAC帧首部中写的是从硬件地址HA1发送到硬件地址HA3，路由器R1收到此MAC帧后，在数据链路层，要丢弃原来的MAC帧的首部和尾部。在转发时在数据链路层，要重新添加上MAC帧的首部和尾部。这时首部中的源地址和目的地址分别便成为HA4和HA5。路由器R2收到此帧后，再次更换MAC帧的首部和尾部，首部中的源地址和目的地址分别变成为HA6和HA2。MAC帧的首部的这种变化，在上面的IP层上是看不见的。 尽管互连在一起的网络的硬件地址体系各不相同，但IP层抽象的互联网却屏蔽了 下层这些很复杂的细节。只要我们在网络层上讨论问题，就能够使用统一的、抽象的IP地址研究主机和主机或路由器之间的通信。 既然在网络链路上传送的帧最终是按照硬件地址找到目的主机的，那么为什么我们还要使用抽象的IP地址，而不直接使用硬件地址进行通信? 由于全世界存在着各式各样的网络，它们使用不同的硬件地址。要使这些异构网络能 够互相通信就必须进行非常复杂的硬件地址转换工作，因此由用户或用户主机来完成这项作几乎是不可能的事。但IP编址把这个复杂问题解决了。连接到互联网的主机只需各自拥有一个唯一的IP地址，它们之间的通信就像连接在同一个网络上那样简单方便，因为上述的调用ARP的复杂过程都是由计算机软件自动进行的，对用户来说是看不见这种调用过程的。 4.2.6. 地址解析协议ARP 网络层使用的是IP地址，但在实际网络的链路上传送数据帧时还是必须使用该网络的硬件地址。但IP地址和下面的网络的硬件地址之间由于格式不同而不存在简单的映射关系。此外，IP或MAC地址可能会改变。地址解析协议ARP解决这个问题的方法是在主机ARP高速缓存中存放一个从IP地址到硬件地址的映射表，并且这个映射表还经常动态更新(新增或超时删除)。 每一台主机都设有一个ARP高速缓存( ARP cache)，里面有本局域网上的各主机和路由器的IP地址到硬件地址的映射表，这些都是该主机目前知道的一些地址。 4.2.6.1. 同一局域网 当主机A要向本局域网上的某台主机B发送IP数据报时，就先在其ARP高速缓存中 查看有无主机B的IP地址。 若有，就在ARP高速缓存中查出其对应的硬件地址，再把这硬件地址写入MAC帧，然后通过局域网把该MAC帧发往此硬件地址； 若没有，主机A就自动运行ARP，然后按以下步骤找出主机B的硬件地址。 ARP进程在本局域网上广播发送一个ARP请求分组； 在本局域网上的所有主机上运行的ARP进程都收到此ARP请求分组 主机B的IP地址与ARP请求分组中要查询的P地址一致，就收下这个ARP请求分组，并向主机A发送ARP响应分组，同时在这个ARP响应分组中写入自己的硬件地址。由于其余的所有主机的IP地址都与ARP请求分组中要查询的IP地址不一致，因此都不理睬这个ARP请求分组。 ARP请求分组是广播发送的，但ARP响应分组是普通的单播。 主机A收到主机B的ARP响应分组后，就在其ARP高速缓存中写入主机B的IP地址到硬件地址的映射。同时主机B记录主机A的映射。 ARP对保存在高速缓存中的每一个映射地址项目都设置生存时间凡超过生存时间的项目就从高速缓存中删除掉。 4.2.6.2. 不在同一局域网 四种情况： 发送方是主机(如H1)，要把IP数据报发送到同一个网络上的另一台主机(如H2)。这时H发送ARP请求分组(在网1上广播)，找到目的主机H2的硬件地址。 发送方是主机(如H1)，要把IP数据报发送到另一个网络上的一台主机(如H3或H4)。这时H1发送ARP请求分组(在网1上广播)，找到网1上的一个路由器R1的硬件地址。剩下的工作由路由器R1来完成。R1要做的事情是下面的(3)或(4)。 发送方是路由器(如R1)，要把P数据报转发到与R1连接在同一个网络(网2)上的主机(如H3)。这时R1发送ARP请求分组(在网2上广播)，找到目的主机H3的硬件地址 发送方是路由器(如R1)，要把IP数据报转发到网3上的一台主机(如H4)。H4与不是连接在同一个网络上。这时R1发送ARP请求分组(在网2上广播)，找到连接在网2上的一个路由器R2的硬件地址。剩下的工作由这个路由器R2来完成 4.2.7. 逆地址解析协议RARP 功能和ARP协议相对，其将局域网中某个主机的物理地址转换为IP地址，比如局域网中有一台主机只知道物理地址而不知道IP地址，那么可以通过RARP协议发出征求自身IP地址的广播请求，然后由RARP服务器负责回答。 工作流程 给主机发送一个本地的RARP广播，在此广播包中，声明自己的MAC地址并且请求任何收到此请求的RARP服务器分配一个IP地址； 本地网段上的RARP服务器收到此请求后，检查其RARP列表，查找该MAC地址对应的IP地址； 如果存在，RARP服务器就给源主机发送一个响应数据包并将此IP地址提供给对方主机使用； 如果不存在，RARP服务器对此不做任何的响应； 源主机收到从RARP服务器的响应信息，就利用得到的IP地址进行通讯；如果一直没有收到RARP服务器的响应信息，表示初始化失败。 4.2.8. IP数据报的格式 版本：占4位，指P协议的版本。通信双方使用的P协议的版本必须一致。 首部长度：占4位，可表示的最大十进制数值是15。 首部长度字段所表示数的单位是32位字(4字节)，因此首部长度字段的最小值是5，最大值15。当IP分组的首部长度不是4字节的整数倍时，必须利用最后的填充字段加以填充。因此IP数据报的数据部分永远在4字节的整数倍时开始，这样在实现IP协议时较为方便。 区分服务：占8位，用来获得更好的服务。旧标准中叫做服务类型，但实际上一直没有被使用过。 总长度：总长度指首部和数据之和的长度，单位为字节。 最常用的以太网规定其MTU值是1500字节。若所传送的数据报长度超过数据链路层的MTU值，就必须把过长的数据报进行分片处理。 虽然使用尽可能长的PP数据报会使传输效率得到提高，但每一个IP数据报越短，路由 转发的速度就越快。为此，IP协议规定，在互联网中所有的主机和路由器，必须能够接 受长度不超过576字节的数据报。这是假定上层交下来的数据长度有512字节，加上最长的IP首部60字节，再加上4字节的富余量，就得到576字节。当主机需要发送长度超过576字节的数据报时，应当先了解一下，目的主机能否接受所要发送的数据报长度。否则，就要进行分片。 在进行分片时(见后面的“片偏移”字段)，数据报首部中的“总长度”字段是指分片 后的每一个分片的首部长度与该分片的数据长度的总和。 标识：占16位。IP软件在存储器中维持一个计数器，每产生一个数据报，计数器就加1，并将此值赋给标识字段。 但这个“标识”并不是序号，因为IP是无连接服务，数据报不存在按序接收的问题。当数据报由于长度超过网络的MTU而必须分片时，这个标识字段的值就被复制到所有的数据报片的标识字段中。相同的标识字段的值使分片后的各数据报片最后能正确地重装成为原来的数据报。 标志：占3位，但目前只有两位有意义 标志字段中的最低位记为MF(More Fragment)。MF=1即表示后面“还有分片”的数据报。MF=0表示这已是若干数据报片中的最后一个。 标志字段中间的一位记为DF(Don' t Fragment)，意思是“不能分片”。只有当DF 0时才允许分片。 片偏移：占13位。片偏移指出:较长的分组在分片后，某片在原分组中的相对位置。也就是说，相对于用户数据字段的起点，该片从何处开始。片偏移以8个字节为偏移单位。这就是说，每个分片的长度一定是8字节(64位)的整数倍。 生存时间：占8位，生存时间字段常用的英文缩写是TTL( Time To live)，表明 这是数据报在网络中的寿命。由发出数据报的源点设置这个字段。其目的是防止无法交付的数据报无限制地在互联网中兜圈子。 然而随着技术的进步，路由器处理数据报所需的时间不断在缩短，一般都远远小于1秒，后来就把TTL字段的功能改为“跳数限制”(但名称不变)。路由器在每次转发数据报之前就把TTL值减1。若TTL值减小到零，就丢弃这个数据报，不再转发。因此，现在TTL的单位不再是秒，而是跳数。TTL的意义是指明数据报在互联网中至多可经过多少路由器。显然，数据报能在互联网中经过的路由器的最大数值是255。若把TTL的初始值设置为1，就表示这个数据报只能在本局域网中传送。因为这个数据报一传送到局域网上的某个路由器，在被转发之前TIL值就减小到零，因而就会被这个路由器丢弃。 协议：占8位，协议字段指出此数据报携带的数据是使用何种协议，以便使目的 主机的IP层知道应将数据部分上交给哪个协议进行处理。 首部检验和占16位。这个字段只检验数据报的首部，但不包括数据部分。非CRC。 源地址：占32位。 目的地址：占32位。 4.2.9. IP层转发分组的流程 在路由表中，对每一条路由最主要的是以下两个信息： (目的网络地址，下一跳地址) 从数据报的首部提取目的主机的IP地址D，得出目的网络地址为N； 若N就是与此路由器直接相连的某个网络地址，则进行直接交付，不需要再经过其 他的路由器，直接把数据报交付目的主机(这里包括把目的主机地址D转换为具体的硬件地址，把数据报封装为MAC帧，再发送此帧);否则就是间接交付，执行3； 若路由表中有目的地址为D的特定主机路由，则把数据报传送给路由表中所指明的 下一跳路由器;否则，执行4； 若路由表中有到达网络N的路由，则把数据报传送给路由表中所指明的下一跳路由 器;否则，执行5； 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器;否 则，执行6； 报告转发分组出错。 当路由器收到一个待转发的数据报，在从路由表得出下一跳路由器的IP地址后，不是 把这个地址填入IP数据报，而是送交数据链路层的网络接口软件。网络接口软件负责把下跳路由器的IP地址转换成硬件地址(必须使用ARP)，并将此硬件地址放在链路层的 MAC帧的首部，然后根据这个硬件地址找到下一跳路由器。由此可见，当发送一连串的数据报时，上述的这种查找路由表、用ARP得到硬件地址、把硬件地址写入MAC帧的首部等过程，将不断地重复进行，造成了一定的开销。 4.3. 划分子网和构造超网 4.3.1. 划分子网 4.3.1.1. 三级IP地址 两级IP地址的缺点 IP地址空间的利用率有时很低。 给每一个物理网络分配一个网络号会使路由表变得太大因而使网络性能变坏。 两级IP地址不够灵活。 基本思路 一个拥有许多物理网络的单位，可将所属的物理网络划分为若干个子网( subne划分子网纯属一个单位内部的事情。本单位以外的网络看不见这个网络是由多少个子网组成，因为这个单位对外仍然表现为一个网络。 划分子网的方法是从网络的主机号借用若干位作为子网号 ibnet-id，当然主机号 也就相应减少了同样的位数。于是两级IP地址在本单位内部就变为三级IP地址:网络号子网号和主机号。也可以用以下记法来表示 IP地址:={&lt;网络号&gt;，&lt;子网号&gt;，&lt;主机号&gt;} 凡是从其他网络发送给本单位某台主机的IP数据报，路由器在收到IP数据报后，再按目的网络号和子网号找到目的子网，把IP数据报交付目的主机。 4.3.1.2. 子网掩码 使用子网掩码的好处:不管网络有没有划分子网，只要把子网掩码和IP地址进行 逐位的“与”运算(AND)，就立即得出网络地址来。这样在路由器处理到来的分组时就可采用同样的算法。 所有的网络都必须使用子网掩码，同时在路由器的路由表中也必须有子网掩码这一栏。如果一个网络不划分子网，那么该网络的子网掩码就使用默认子网掩码。默认子网掩码中1的位置和I地址中的网络号字段net-id正好相对应。因此，若用默认子网掩码和某个不划分子网的IP地址逐位相“与”(AND)，就应当能够得出该IP地址的网络地址来。这样做可以不用査找该地址的类别位就能知道这是哪一类的IP地址。显然， A类地址的默认子网掩码是255.0.0.0，或0xFF000000 B类地址的默认子网掩码是255.255.0.0，或0 XFFFF0000。 C类地址的默认子网掩码是255.255.255.0，或0 XFFFFFF00。 B类地址的子网划分选择(使用固定长度子网)： 子网号的位数 子网掩码 子网数 每个子网的主机数 2 255.255.192.0 2{2次方}-2 2{14次方}-2 2 255.255.224.0 2{3次方}-2 2{13次方}-2 2 255.255.240.0 2{4次方}-2 2{12次方}-2 ... ... ... ... 子网号的位数中没有0，1，15和16这四种情况，因为这没有意义。 划分子网增加了灵活性，但却减少了能够连接在网络上的主机总数。 4.3.1.3. 使用子网的分组转发 我们应当注意到，使用子网划分后，路由表必须包含以下三项内容: 目的网络地址，子网掩码，下一跳地址 在划分子网的情况下，路由器转发分组的算法如下： 从收到的数据报的首部提取目的IP地址D； 先判断是否为直接交付。对路由器直接相连的网络逐个进行检查:用各网络的子网掩码和D逐位相“与”(AND操作)，看结果是否和相应的网络地址匹配。若匹配，则把分组进行直接交付(当然还需要把D转换成物理地址，把数据报封装成帧发送出去)，转发任务结束。否则就是间接交付，执行3； 若路由表中有目的地址为D的特定主机路由，则把数据报传送给路由表中所指明的 下一跳路由器;否则，执行4； 对路由表中的每一行(目的网络地址，子网掩码，下一跳地址)，用其中的子网掩 码和D逐位相“与”(AND操作)，其结果为N。若N与该行的目的网络地址匹配，则把数 据报传送给该行指明的下一跳路由器;否则，执行5； 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器;否 则，执行6； 报告转发分组出错。 4.3.2. 构造超网 4.3.2.1. 网络前缀 因为IPv4的地址即将耗尽，出现无分类域间路由选择CIDR。 特点 CIDR消除了传统的A类、B类和C类地址以及划分子网的概念，32位的IP地址划分为用来指明网络的“网络前缀”和主机号。其记法是： IP地址∷={网络前缀&gt;，&lt;主机号&gt;} CDR还使用“斜线记法”，或称为CIDR记法，即在IP地址后面加上斜线然后写上网络前缀所占的位数。 CIDR使用32位的地址掩码。地址掩码由串1和一串0组成，而1的个数就是网络前缀的长度。斜线记法中，斜线后面的数字就是地址掩码中1的个数。 CIDR把网络前缀都相同的连续的P地址组成一个“CIDR地址块”。 由于一个CIDR地址块中有很多地址，所以在路由表中就利用CIDR地址块来查找目的 网络。这种地址的聚合常称为路由聚合( route aggregation)，它使得路由表中的一个项目可以表示原来传统分类地址的很多个(例如上千个)路由。路由聚合也称为构成超网。 常用的CIDR地址块： CIDR前缀长度 点分十进制 包含的地址数 相当于包含分类的网络数 /13 255.248.0.0 512K 8个B类或2048个C类 /14 255.252.0.0 256K 4个B类或1024个C类 /15 255.254.0.0 128K 2个B类或512个C类 ... ... ... ... 使用CIDR的一个好处就是可以更加有效地分配IPv4的地址空间，可根据客户的需要 分配适当大小的CIDR地址块。然而在分类地址的环境中，向一个部门分配IP地址，就只能以/8，/16或/24为单位来分配，这就很不灵活。 构成超网是将网络前缀缩短。网络前缓越短，其地址块所包含的地址数就越多。而在三级结构的IP地址中，划分子网是使网络前缀变长。 4.4. ICMP网际控制报文协议 为了更有效地转发IP数据报和提高交付成功的机会，在网际层使用了网际控制报文协 议ICMP。ICMP允许主机或路由器报告差错情况和提供有关异常情况的报告。ICMP报文是装在IP数据报中，作为其中的数据部分。 两种常用的ICMP报文类型： ICMP报文种类 类型的值 ICMP报文的类型 差错报告报文 3 终点不可达：不能交付数据报 差错报告报文 11 时间超过：生存时间为零的数据报 差错报告报文 12 参数问题：数据报首部的参数不正确 差错报告报文 5 改变路由：最佳路由 询问报文 8或0 回送请求或回答：测试目的站是否可达以及了解其有关状态 询问报文 13或14 时间戳请求或回答：用于时钟同步和时间测量 4.5. 路由选择协议 倘若从路由算法能否随网络的通信量或拓扑自适应地进行调整变化来划分： 静态路由选择策略：也叫做非自适应路由选择，特点是简单和开销较小，但不能及时适应网络状态的变化。对于很简单的小网络，完全可以采用静态路由选择，用人工配置每一条路由。 动态路由选择策略。也叫做自适应路由选择，其特点是能较好地适应网络状态的变化，但实现起来较为复杂，开销也比较大。适用于较复杂的大网络。 分层次的路由选择协议 把整个互联网划分为许多较小的自治系统(autonomous system)，记为AS。一个AS对其他AS表现出的是一个单一的和一致的路由选择策略。 在目前的互联网中，一个大的ISP就是一个自治系统。这样，互联网就把路由选择协议 划分为两大类： 内部网关协议IGP：即在一个自治系统内部使用的路由选择协议，而这与在互联网中的其他自治系统选用什么路由选择协议无关。如RIP和OSPF协议。 外部网关协议EGP：若源主机和目的主机处在不同的自治系统中，当数据报传到一个自治系统的边界时，就需要使用一种协议将路由选择信息传递到另一个自治系统中。这样的协议就是外部网关协议EGP。目前使用最多的外部网关协议是BGP的版本4(BGP4)。 自治系统之间的路由选择也叫做域间路由选择，而在自治系统内部的路由选择叫做域内路由选择。 4.5.1. 内部网关协议RIP 4.5.1.1. 工作原理 一种分布式的基于距离向量的路由选择协议。 RIP协议要求网络中的每一个路由器都要维护从它自己到其他每一个目的网络的距离记 录。RIP协议将“距离”定义为从一路由器到直接连接的网络的距离定义为1。从一路由器到非直接连接的网络的距离定义为所经过的路由器数加1。 特点 仅和相邻路由器交换信息。不相邻的路由器不交换信息 路由器交换的信息是当前本路由器所知道的全部信息，即自己现在的路由表。 按固定的时间间隔交换路由信息。当网络拓扑发生变化时，路由器也及时向相邻路由器通告拓扑变化后的路由信息。 4.5.1.2. 距离向量算法 对每一个相邻路由器发送过来的RIP报文，进行以下步骤： 对地址为ⅹ的相邻路由器发来的RIP报文，先修改此报文中的所有项目:把“下一跳”字段中的地址都改为X，并把所有的“距离”字段的值加1。每一个项目都有三个关键数据，即:到目的网络N，距离是d，下一跳路由器是X 对修改后的RP报文中的每一个项目，进行以下步骤: 若原来的路由表中没有目的网络N，则把该项目添加到路由表中 否则(即在路由表中有目的网络N，这时就再査看下一跳路由器地址 若下一跳路由器地址是Ⅹ，则把收到的项目替换原路由表中的项目 否则(即这个项目是:到目的网络N，但下一跳路由器不是X 若收到的项目中的距离d小于路由表中的距离，则进行更新 否则什么也不做。 若3分钟还没有收到相邻路由器的更新路由表，则把此相邻路由器记为不可达的路 由器，即把距离置为16(距离为16表示不可达)。 返回。 RIP协议使得从每一个路由器到每一个目的网络的路由都是最短的。虽然所有的路由器最终都拥有了整个自治系统的全局路由信息，但由于每一个路由器的位置不同，它们的路由表当然也应当是不同的。 优点 实现简单、开销较小。 好消息传播得快，但是坏消息传播得慢。 缺点n RIP限制了网络的规模，它能使用的最大距离为15(16表示不可达)。 路由器之间交换的路由信息是路由器中的完整路由表，因而随着网络规模的扩大，开销也就增加。最后，“坏消息传播得慢”，使更新过程的收敛时间过长。 4.5.2. 内部网关协议OSPF 开放最短路径优先OSPF，使用最短路径算法SPF，使用分布式的链路状态协议。它是为克服RIP的缺点而开发。 特点 向本自治系统中所有路由器发送信息。这里使用的方法是洪泛法，路由器通过所有输出端口向所有相邻的路由器发送信息。而每一个相邻路由器又再将此信息发往其所有的相邻路由器(但不再发送给刚刚发来信息的那个路由器)。这样，最终整个区域中所有的路由器都得到了这个信息的一个副本。RIP协议是仅仅向自己相邻的几个路由器发送信息。 发送的信息就是与本路由器相邻的所有路由器的链路状态，但这只是路由器所知道的部分信息。所谓“链路状态”就是说明本路由器都和哪些路由器相邻，以及该链路的度量”，称这个度量为“代价”。RIP协议发送的信息是到所有网络的距离和下一跳路由器。 只有当链路状态发生变化时，路由器才向所有路由器用洪泛法发送此信息。而不像 RIP那样，不管网络拓扑有无发生变化，路由器之间都要定期交换路由表的信息。 优点 所有的路由器最终都能建立一个链路状态数据库，实际上就是全网的拓扑结构图。这个拓扑结构图在全网范围内是一致的。RIP协议的每一个路由器虽然知道到所有的网络的距离以及下一跳路由器，但却不知道全网的拓扑结构(只有到了下一跳路由器，才能知道再下一跳应当怎样走。 OSPF的链路状态数据库能较快地进行更新，使各个路由器能及时更新其路由表，更新过程收敛得快是其重要优点。 OSPF不用UDP而是直接用IP数据报传送。 4.5.3. 外部网关协议BGP 内部网关协议(如RIP或OSPF)主要是设法使数据报在一个AS中尽可能有效地从源站传送到目的站。在一个AS内部也不需要考虑其他方面的策略。然而BGP使用的环境却不同。这主要是因为互联网的规模太大，使得自治系统AS之间路由选择非常困难，并且自治系统AS之间的路由选择必须考虑有关策略（自愿连接、安全等）。 所以边界网关协议BGP只能是力求寻找一条能够到达目的网络且比较好的 路由(不能兜圈子)，而并非要寻找一条最佳路由。 BGP采用了路径向量路由选择协议。 每一个自治系统的管理员要选择至少一个路由器作为该自治系统的“BGP发言人”。一般两个BGP发言人都是通过一个共享网络连接在一起的，而BGP发言人往往就是BGP边界路由器，也可以不是。 BGP发言人与其他AS的BGP发言人要交换路由信息，就要先建立TCP连接，彼此成为对方的邻站( neighbor〕或对等站(peer)。 边界网关协议BGP所交换的网终可达性的信息就是要到达某个网络所要经过的一系列自治系统。发言人就根据所采用的策略从收到的路由信息中找出到达各自治系统的较好路由。 5. 运输层 5.1. 协议概述 5.1.1. 进程之间的通信 从通信和信息处理的角度看，运输层向它上面的应用层提供通信服务，它属于面向通信部分的最高层，同时也是用户功能中的最低层。当网络的边缘部分中的两台主机使用网络的核心部分的功能进行端到端的通信时，只有主机的协议栈才有运输层，而网络核心部分中的路由器在转发分组时都只用到下三层的功能。 运输层有一个很重要的功能—复用和分用。这里的“复用是指在发送方不同的应用进程都可以使用同一个运输层协议传送数据，而“分用”是指接收方的运输层在剥去报文的首部后能够把这些数据正确交付目的应用进程。 网络层为主机之间提供逻辑通信，而运输层为应用进程之间提供端到端的逻辑通信。向高层用户屏蔽了下面网络核心的细节。 5.1.2. 两个主要协议 用户数据报协议UDP：UDP用户数据报，无连接，不可靠，尽最大努力交付的。 传输控制协议TCP：TCP报文段，面向连接，可靠的 5.1.3. 端口 为了使运行不同操作系统的计算机的应用进程能够互相通信，就必须用统一的方法(而这种方法必须与特定操作系统无关)对TCP体系的应用进程进行标志。 解决这个问题的方法就是在运输层使用协议端口号，常简称为端口(port)。这就是说，虽然通信的终点是应用进程，但只要把所传送的报文交到目的主机的某个合适的目的端口，剩下的工作(即最后交付目的进程)就由TCP或UDP来完成。 这种在协议栈层间的抽象的协议端口是软件端口。 用一个16位端口号来标志一个端口。但端口号只具有本地意义，它只是为了标志本计算机应用层中的各个进程在和运输层交互时的层间接口。在互联网不同计算机中，相同的端口号是没有关联的。16位的端口号可允许有65535个不同的端口。分为： 服务器端使用的端口号 熟知端口号 0~1023 等级端口号 1024~49151 客户端使用的端口号 49152~65535 常见的熟知端口号 FTP TELNET SMTP DNS TFTP HTTP SNMP HTTPS 21 23 25 53 69 80 161 443 POP3 SSH MySQL Oracle SQLServer Sockets Tomcat 110 22 3306 1521 1433 1080 8080 5.2. 用户数据报协议UDP 无连接的：减小开销和时延 尽最大努力：交付不可靠的 面向报文：保留上层报文的边界，一次交付一个完整的报文 没有拥塞控制：实时应用允许丢失，但需要低时延 支持一对一、一对多、多对一和多对多的交互通信 首部开销小：8字节 源端口：源端口号。在需要对方回信时选用。不需要时可用全0。 目的端口：目的端口号。这在终点交付报文时必须使用 长度：UDP用户数据报的长度，其最小值是8(仅有首部)。 检验和：检测UDP用户数据报在传输中是否有错。有错就丢弃，并由ICMP发送“终点不可达”。IP数据报只检验首部，UDP把首部和数据一起检验。 5.3. 传输控制协议TCP概述 5.3.1. TCP的主要特点 面向连接：建立连接、释放连接 每条TCP连接只能有两个端点：一对一 可靠交付 全双工通信：发送、接收缓存 面向字节流：然应用程序和TCP的交互是一次一个数据块(大小不等)，但TCP把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。TCP并不知道所传送的字节流的含义。 5.3.2. TCP的连接 TCP的连接有两个断电，断电指的是套接字，即端口号拼接到IP地址： 套接字 socket=(IP地址:端口号) 每一条TCP连接唯一地被通信两端的两个端点(即两个套接字)所确定。即 TCP连接:(socket1， socket2)=((IP1:port1)，(IP2:port2) 5.4. 可靠传输的工作原理 5.4.1. 停止等待协议 “停止等待”就是每发送完一个分组就停止发送，等待对方的确认。在收到确认后再发送下一个分组。 5.4.1.1. 无差错情况 A发送分组M1，发完就暂停发送，等待B的确认。B收到了M1就向A发送确认。A在收到了对M1的确认后，就再发送下一个分组M2。同样，在收到B对M2的确认后，再发送M3。 5.4.1.2. 出现差错 B接收M1时检测出了差错，就丢弃M1，其他什么也不做(不通知A收到有差错的分组)。A只要超过了一段时间仍然没有收到确认，就认为刚才发送的分组丢失了，因而重传前面发送过的分组，。这就叫做超时重传。要实现超时重传，就要在每发送完一个分组时设置一个超时计时器。如果在超时计时器到期之前收到了对方的确认，就撤销已设置的超时计时器。 因此要注意： A在发送完一个分组后，必须暂时保留已发送的分组的副本； 分组和确认分组都必须进行编号。 超时计时器设置的重传时间应当比数据在分组传输的平均往返时间更长一些。 5.4.1.3. 确认丢失和确认迟到 确认丢失 B所发送的对M1的确认丢失了。A在设定的超时重传时间内没有收到确认，并无法知道是自己发送的分组出错、丢失，或者是B发送的确认丢失了。因此A在超时计时器到期后就要重传M1。现在应注意B的动作。假定B又收到了重传的分组M1。这时应采取两个行动。 丢弃这个重复的分组M1，不向上层交付。 向A发送确认。不能认为已经发送过确认就不再发送，因为A之所以重传M 确认迟到 传输过程中没有出现差错，但B对分组M1的确认迟到了。A会收到重复的确认。对重复的确认的处理很简单:收下后就丢弃。B仍然会收到重复的M1，并且同样要丢弃重复的M1，并重传确认分组。 像上述的这种可靠传输协议常称为自动重传请求ARQ( Automatic Repeat reQuest)。意思 是重传的请求是自动进行的。接收方不需要请求发送方重传某个出错的分组。 5.4.1.4. 信道利用率 停止等待协议的优点是简单，但缺点是信道利用率太低。 当往返时间RTT远大于分组发送时间TD时，信道的利用率就会非常低。若出现重传，信道的利用率就还要降低。 为了提高传输效率，发送方可以采用流水线传输。流水线传输就是发送方可连续发送多个分组，不必每发完一个分组就停顿下来等待对方的确认。这样可使信道上一直有数据不间断地在传送。显然，这种传输方式可以获得很高的信道利用率。 即使用连续ARQ协议和滑动窗口协议。 5.4.2. 连续ARQ协议 发送窗口的意义是:位于发送窗口内的5个分组都可连续发送出去，而不需要等待对方的确认。这样，信道利用率就提高了。 发送方每收到一个确认，就把发送窗口向前滑动一个分组的位置。接收方一般都是采用累积确认的方式。接收方不必对收到的分组逐个发送确认，而是在收到几个分组后，对按序到达的最后一个分组发送确认，这就表示:到这个分组为止的所有分组都已正确收到了。 优缺点 累积确认有优点也有缺点。优点是:容易实现，即使确认丢失也不必重传。但缺点是 不能向发送方反映出接收方已经正确收到的所有分组的信息，在通信质量不好时会多次重传。 5.5. TCP报文段格式 TCP虽然是面向字节流的，但TCP传送的数据单元却是报文段。 源端口和目的端口：各占2个字节，分别写入源端口号和目的端口号。 序号：占4字节。本报文段所发送的数据的第一个字节的序号。在一个TCP连接中传送的字节流中的每一个字节都按顺序编号。整个要传送的字节流的起始序号必须在连接建立时设置。 确认号：占4字节，是期望收到对方下一个报文段的第一个数据字节的序号。若确认号=N，则表明:到序号N-1为止的所有数据都已正确收到。 数据偏移：占4位，实际上是指出TCP报文段的首部长度，单位是32位字(即以4字节）。 保留占6位，保留为今后使用，但目前应置为0。 紧急URG：当URG=1时，表明紧急指针字段有效。它告诉系统此报文段中有紧急数据，应尽快传送(高优先级的数据)，而不要按原来的排队顺序来传送，例如Ctrl + C。TCP就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据，这时要与首部中紧急指针字段配合使用。 确认ACK：仅当ACK=1时确认号字段才有效。当ACK=0时，确认号无效。TCP规定，在连接建立后所有传送的报文段都必须把ACK置1。 推送PSH：当两个应用进程进行交互式的通信时，一端的应用进程希望在键入一个命令后立即就能够收到对方的响应。发送方TCP把PSH置1，并立即创建一个报文段发送出去。接收方TCP收到PSH=1的报文段，就尽快地交付接收应用进程，而不再等到整个缓存都填满了后再向上交付。 复位RST：当RST=1时，表明TCP连接中出现严重差错(如由于主机崩溃或其他原因)，必须释放连接，然后再重新建立运输连接。RST置1还用来拒绝一个非法的报文段或拒绝打开一个连接。 同步SYN：在连接建立时用来同步序号。当SYN=1而ACK=0时，表明这是一个连接请求报文段。对方若同意建立连接，则应在响应的报文段中使SYN=1和ACK=1。因此，SYN置为1就表示这是一个连接请求或连接接受报文。 终止FIN：用来释放一个连接。 窗口：占2字节。窗口指的是发送本报文段的的接收窗口(而不是自己的发送窗口)。窗口值代表目前允许对方发送的数据量(以字节为单位)。作为接收方让发送方设置其发送窗口的依据。 检验和：占2字节。检验和字段检验的范围包括首部和数据这两部分。 紧急指针：占2字节。紧急指针仅在URG=1时才有意义，它指出本报文段中的紧急数据的字节数，即紧急数据的末尾在报文段中的位置。 选项：长度可变，最长可达40字节。 窗口扩大：高吞吐量下增大窗口数值的位数 时间戳：计算RTT；处理序号过大，防止序号取模绕回PAWS 选择确认 最大报文段长度MSS：每一个TCP报文段中的数据字段的最大长度。MSS太小，网络利用率降低；太大，IP层需要分片。 5.6. TCP可靠传输的实现 5.6.1. 以字节为单位的滑动窗口 发送窗口里面的序号表示允许发送的序号。发送窗口后沿的后面部分表示己发送且己收到了确认。这些数据显然不需要再保留，而发送窗口前沿的前面部分表示不允许发送的。 发送窗口的位置由窗口前沿和后沿的位置共同确定。发送窗口后沿的变化情况有两种可能，即不动(没有收到新的确认)和前移(收到了新的确认)。发送窗口后沿不可能向后移动，因为不能撤销掉已收到的确认。发送窗口前沿通常是不断向前移动，但也有可能不动。 B只能对按序收到的数据中的最高序号给出确认，因此B发送的确认报文段中的确认号仍然是31，而不能是32或33。 现在假定B收到了序号为31的数据，并把序号为31-33的数据交付主机，然后B删除这些数据。接着把接收窗口向前移动3个序号，同时给A发送确认，其中窗口值仍为20，但确认号是34。这表明B已经收到了到序号33为止的数据。我们注意到，B还收到了序号为37，38和40的数据，但这些都没有按序到达，只能先暂存在接收窗口中A收到B的确认后，就可以把发送窗口向前滑动3个序号，但指针P2不动。可以看出，现在A的可用窗口增大了，可发送的序号范围42-53。 TCP的缓存与窗口的关系 发送缓存用来暂时存放: 发送应用程序传送给发送方TCP准备发送的数据; TCP已发送出但尚未收到确认的数据。 接收缓存用来暂时存放: 按序到达的、但尚未被接收应用程序读取的数据 未按序到达的数据。 注意 在同一时刻，A的发送窗口并不总是和B的接收窗口一样大。因为通过网络传送窗口值需要经历一定的时间滞后； 对于不按序到达的数据应如何处理，TCP标准并无明确规定。如果接收方一律丢弃，对网络资源的利用不利，因此通常对不按序到达的数据是先临时存放在接收窗口中，等到字节流中所缺少的字节收到后，再按序交付上层的应用进程； TCP要求接收方必须有累积确认的功能，这样可以减小传输开销。接收方可以在合适的时候发送确认，也可以在自己有数据要发送时把确认信息顺便捎带上。 5.6.2. 超时重传时间的选择 如果把超时重传时间设置得太短，就会引起很多报文段的不必要的重传，使网络负荷增大。但若把超时重传时间设置得过长，则又使网络的空闲时间增大，降低了传输效率。 TCP采用了一种自适应算法，它记录报文段的往返时间RTT。TCP保留了RTT的一个加权平均往返时间RTTs(这又称为平滑的往返时间）因为进行的是加权平均，因此得出的结果更加平滑)。每当第一次测量到RTT样本时，RTTs值就取为所测量到的RTT样本值。但以后每测量到一个新的RTT样本，就按下式重新计算一次RTTs： 新的RTTs=(1-α)×(旧的RTTs)+α*(新的RTT样本) 在上式中，0≤α&lt;1。若α很接近于零，表示新的RTTs值和旧的RTTs值相比变化不大，而对新的RTT样本影响不大(RTT值更新较慢)。若选择α接近于1，则表示新的RTTs值受新的RTT样本的影响较大(RT值更新较快)。 显然，超时计时器设置的超时重传时间RTO应略大于RTTs。RFC6298建议使用下式计算RTO RTO=RTTs+4×RTTD 而RTTD是RTT的偏差的加权平均值，它与RTTs和新的RTT样本之差有关。 RFC6298建议这样计算RTTD。当第一次测量时，RTTD值取为测量到的RTT样本值的一半。在以后的测量中，则使用下式计算加权平均的RTT 新的RTTD=(1-β)×(旧的RTTD+β*|RTTs-新的RTT样本| 若收到的确认是对重传报文段的确认，但却被源主机当成是对原来的报文段的确认则这样计算出的RTTs和超时重传时间RTO就会偏大。若后面再发送的报文段又是经过重传后才收到确认报文段，则按此方法得出的超时重传时间RTO就越来越长。 Karn提出了一个算法:在计算加权平均RTTS时，只要报文段重传了，就不采用其往返时间样本，这样得出的加权平均RTTS和RTO就较准确。但是如果报文段的时延突然增大了很多，因 此在原来得出的重传时间内，不会收到确认报文段，于是就重传报文段。但根据Karn算 法，不考虑重传的报文段的往返时间样本，这样，超时重传时间就无法更新。 因此要对Karn算法进行修正。方法是:报文段每重传一次，就把超时重传时间RTO增大一些。典型的做法是取新的重传时间为旧的重传时间的2倍。当不再发生报文段的重传时，才根据上面给出的2式计算超时重传时间。实践证明，这种策略较为合理。 5.6.3. 选择确认SACK 若收到的报文段无差错，只是未按序号，中间还缺少一些序号的数据，那么能否设法只传送缺少的数据而不重传已经正确到达接收方的数据?选择确认就是一种可行的处理方法。 和前后字节不连续的每一个字节块都有两个边界:左边界和右边界。因此在图中用四个指针标记这些边界。左边界指出字节块的第一个字节的序号，但右边界减1才是字节块中的最后一个序号。 然而，SACK文档并没有指明发送方应当怎样响应SACK。因此大多数的实现还是重传所有未被确认的数据块。 5.7. TCP的流量控制 5.7.1. 利用滑动窗口实现流量控制 所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。 现在我们考虑一种情况。B向A发送了零窗口的报文段后不久，B的接收缓存又有了一些存储空间。于是B向A发送了rwnd=400的报文段。然而这个报文段在传送过程中丢失了。A一直等待收到B发送的非零窗口的通知，而B也一直等待A发送的数据。如果没有其他措施，这种互相等待的死锁局面将一直延续下去。 为了解决这个问题，TCP为每一个连接设有一个持续计时器。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口探测报文段(仅携带1字节的数据)，而对方就在确认这个探测报文段时给出了现在的窗口值。如果窗口仍然是零，那么收到这个报文段的一方就重新设置持续计时器。如果窗口不是零，那么死锁的僵局就可以打破了。 5.7.2. TCP的传输效率 不同的机制来控制TCP报文段的发送时机 TCP维持一个变量，它等于最大报文段长度MSS。只要缓存中存放的数据达到MSS字节时，就组装成一个TCP报文段发送出去。 由发送方的应用进程指明要求发送报文段，即TCP支持的推送(push)操作。第三种机制是 发送方的一个计时器期限到了，这时就把当前已有的缓存数据装入报文段(但长度不能超过MSS)发送出去。 在TCP的实现中广泛使用Nage算法。 算法如下:若发送应用进程把要发送的数据逐个字节地送到TCP的发送缓存，则发送方就把第一个数据字节先发送出去，把后面到达的数据字节都缓存起来。当发送方收到对第一个数据字符的确认后，再把发送缓存中的所有数据组装成一个报文段发送出去，同时继续对随后到达的数据进行缓存。只有在收到对前一个报文段的确认后才继续发送下一个报文段。当数据到达较快而网络速率较慢时，用这样的方法可明显地减少所用的网络带宽。 Nagle算法还规定，当到达的数据已达到发送窗口大小的半或已达到报文段的最大长度时，就立即发送一个报文段。这样做，就可以有效地提高网络的吞吐量。 糊涂窗口综合征 设想一种情况:TCP接收方的缓存已满，而交互式的应用进程一次只从接收缓存中读取1个字节(这样就使接收缓存空间仅腾出1个字节)，然后向发送方发送确认，并把窗口设置为1个字节(但发送的数据报是40字节长)。接着，发送方又发来1个字节的数据(请注意，发送方发送的IP数据报是41字节长)。接收方发回确认，仍然将窗口设置为1个字节。这样进行下去，使网络的效率很低。 要解决这个问题，可以让接收方等待一段时间，使得或者接收缓存已有足够空间容纳最长的报文段，或者等到接收缓存已有一半空闲的空间。只要出现这两种情况之一，接收方就发出确认报文，并向发送方通知当前的窗口大小。此外，发送方也不要发送太小的报文段，而是把数据积累成足够大的报文段，或达到接收方缓存的空间的一半大小。 5.8. TCP的拥塞控制 5.8.1. 拥塞控制的一般原理 在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫做拥塞。若网络中有许多资源同时呈现供应不足，网络的性能就要明显变坏，整个网络的吞吐量将随输入负荷的增大而下降。 网络拥塞往往是由许多因素引起的。简单地扩大缓存的存储空间同样会造成网络资源的严重浪费，因而解决不了网络拥塞的问题。简单地将处理机的速率提高，可能会使上述情况缓解一些，但往往又会将瓶颈转移到其他地方。问题的实质往往是整个系统的各个部分不匹配。只有所有的部分都平衡了，问题才会得到解决。 拥塞控制与流量控制的关系密切，它们之间也存在着一些差别。所谓拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程。相反，流量控制往往是指点对点通信量的控制，是个端到端的问题。流量控制所要做的就是抑制发送端发送数据的速率，以便使接收端来得及接收。 从控制理论的角度来看有两种方法： 开环控制：在设计网络时事先将有关发生拥塞的因素考虑周到，力求网络在工作时不产生拥塞。 闭环控制是基于反馈环路的概念，主要有以下几种措施 监测网络系统以便检测到拥塞在何时、何处发生。 把拥塞发生的信息传送到可采取行动的地方 调整网络系统的运行以解决出现的问题。 监测网络的拥塞： 指标：由于缺少缓存空间而被丢弃的分组的百分数、平均队列长度、超时重传的分组数、平均分组时延、分组时延的标准差，等等。上述这些指标的上升都标志着拥塞的增长 般在监测到拥塞发生时，要将拥塞发生的信息传送到产生分组的源站。当然，通知 拥塞发生的分组同样会使网络更加拥塞 比特位：在路由器转发的分组中保留一个比特或字段，用该比特或字段的值表示网络没有拥塞或产生了拥塞。也可以由一些主机或路由器周期性地发出探测分组，以询问拥塞是否发生。 5.8.2. TCP的拥塞控制方法 TCP进行拥塞控制的算法有四种：慢开始(sow-start)、拥塞避免(congestion avoidance)、快重传(fast retransmit)和快恢复(fast recovery)。 5.8.2.1. 慢开始 发送方维持一个叫做拥塞窗口cwnd(congestion window)的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口。 发送方控制拥塞窗口的原则是:只要网络没有出现拥塞，拥塞窗口就可以再增大些，以便把更多的分组发送出去。但只要网络出现拥塞或有可能出现拥塞，就必须把拥塞窗口减小一些。 判断网络拥塞的依据就是出现了超时。 慢开始算法的思路是这样的:当主机开始发送数据时，是先探测一下，由小到大逐渐增大发送窗口，即由小到大逐渐增大拥塞窗口数值。 慢开始规定，在每收到一个对新的报文段的确认后，可以把拥塞窗口増加最多一个SMSS的数值。 拥塞窗口cwnd每次的增加量=min(N，SMSS) 请注意，实际上TCP是用字节数作为窗口大小的单位。但为叙述方便起见用报文段的个数作为窗口大小的单位。 发送方每收到一个对新报文段的确认就使发送方的拥塞窗口加1每经过一个传输轮次，拥塞窗口cwnd就加倍。 一个传输轮次时间其实就是往返时间RTT(请注意，RTT并非是恒定的数值)。强调把拥塞窗口cwnd所允许发送的报文段都连续发送出去，并收到了对已发送的最后个字节的确认。 为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量。慢开始门限ssthresh的用法如下: 当cwnd &lt; ssthresh时，使用上述的慢开始算法。 当cwnd &gt; ssthresh时，停止使用慢开始算法而改用拥塞避免算法。 当cwnd = ssthresh时，既可使用慢开始算法，也可使用拥塞避免算法。 5.8.2.2. 拥塞避免 拥塞避免算法的思路是让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是像慢开始阶段那样加倍增长。因此在拥塞避免阶段就有“加法増大”的特点。这表明在拥塞避免阶段，拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。 按线性规律增长。但请注意，“拥塞避免”并非完全能够避免了拥塞。“拥塞避免”是说托拥塞窗口控制为按线性规律增长，使网络比较不容易出现拥塞。 当拥塞窗口cwnd=24时，网络出现了超时(图中的点2)，发送方判断为网络拥塞。于是调整门限值 ssthresh=cwnd/2=12，同时设置拥塞窗口cwnd=1，进入慢开始阶段 按照慢开始算法，发送方每收到一个对新报文段的确认ACK，就把拥塞窗口值加1当拥塞窗口cwnd= ssthresh=12时(图中的点3，这是新的 ssthresh值)，改为执行拥塞避免算法，拥塞窗口按线性规律增大。 当拥塞窗口cwnd=16时(图中的点4)，出现了一个新的情况，就是发送方一连收到3 个对同一个报文段的重复确认(图中记为3-ACK)。 3-ACK解释： 有时，个别报文段会在网络中丢失，但实际上网络并未发生拥塞。如果发送方迟迟收不到确认，就会产生超时，就会误认为网络发生了拥塞。这就导致发送方错误地启动慢开始，把拥塞窗口cwnd又设置为1，因而降低了传输效率。 采用快重传算法可以让发送方尽早知道发生了个别报文段的丢失。 5.8.2.3. 快重传 快重传算法首先要求接收方不要等待自己发送数据时才进行捎带确认，而是要立即发送确认，即使收到了失序的报文段也要立即发出对已收到的报文段的重复确认。如图5-26所示，接收方收到了M1和M2后都分别及时发出了确认。现假定接收方没有收到M3但却收到了M4本来接收方可以什么都不做。但按照快重传算法，接收方必须立即发送对M2的重复确认，以便让发送方及早知道接收方没有收到报文段M3。发送方接着发送M5和M6。接收方收到后也仍要再次分别发出对M2的重复确认。这样，发送方共收到了接收方的4个对M2的确认，其中后3个都是重复确认。 快重传算法规定，发送方只要一连收到3个重复确认，就知道接收方确实没有收到报文段M3，因而应当立即进行重传(即“快重传”)，这样就不会出现超时，发送方也不就会误认为出现了网络拥塞。使用快重传可以使整个网络的吞吐量提高约20%。 因此，在图中的点4，发送方知道现在只是丢失了个别的报文段。于是不启动慢开始，而是执行快恢复算法。这时，发送方调整门限值 ssthresh=cwnd/2=8，同时设置拥塞窗口cwnd=ssthresh=8(图中的点6)，并开始执行拥塞避免算法。 5.8.2.4. 快恢复 也有的快恢复实现是把快恢复开始时的拥塞窗口cwd值再增大一些(增大报文段的长度)，即等于新的 ssthresh+3×MSS。这样做的理由是:既然发送方收到3个重复的确认，就表明有3个分组已经离开了网络。这3个分组不再消耗网络的资源而是停留在接收方的缓存中(接收方发送出3个重复的确认就证明了这个事实)。可见现在网络中并不是堆积了分组而是减少了3个分组。因此可以适当把拥塞窗口扩大些。 5.8.2.5. 总结 在拥塞避免阶段，拥塞窗口是按照线性规律増大的，这常称为加法增大AI，而一旦出现超时或3个重复的确认，就要把门限值设置为当前拥塞窗口值的一半，并大大减小拥塞窗口的数值。这常称为“乘法减小”MD，二者合在一起就是所谓的AIMD算法。 总之，发送方的窗口的上限值应当取为接收方窗口rwnd和拥塞窗口cwnd这两个变量中较小的： 发送方窗口的上限值 = Min[ rwnd ， cwnd ] 当rwnd &lt; cwnd时，是接收方的接收能力限制发送方窗口的最大值 当cwnd &lt; rwnd时，则是网络的拥塞程度限制发送方窗口的最大值。 5.8.3. 主动队列管理AQM 网络层路由器的队列通常都是按照“先进先出”FIFO的规则处理到来的分组。当队列已满时，以后再到达的所有分组将都被丢弃。这就叫做尾部丢弃策略。这会同时影响到很多条TCP连接，结果使这许多TCP连接在同一时间突然都进入到慢开始状态。这在TCP的术语中称为全局同步。全局同步使得全网的通信量突然下降了很多，而在网络恢复正常后，其通 信量又突然增大很多。 为了避免发生网络中的全局同步现象，提出了主动队列管理AQM。AQM可以有不同实现方法，其中曾流行多年的就是随机早期检测RED。 实现RED时需要使路由器维持两个参数，即队列长度最小门限和最大门限。当每一个分组到达时，RED就按照规定的算法先计算当前的平均队列长度： 若平均队列长度小于最小门限，则把新到达的分组放入队列进行排队 若平均队列长度超过最大门限，则把新到达的分组丢弃。 若平均队列长度在最小门限和最大门限之间，则按照某一丢弃概率p把新到达的分组丢弃(这就体现了丢弃分组的随机性) 由此可见，RED不是等到已经发生网络拥塞后才把所有在队列尾部的分组全部丢弃， 而是在检测到网络拥塞的早期征兆时(即路由器的平均队列长度达到一定数值时)，就以概 率p丢弃个别的分组，让拥塞控制只在个别的TCP连接上进行，因而避免发生全局性的拥 塞控制。 5.9. TCP的连接管理 5.9.1. TCP的连接建立 开始，B的TCP服务器进程先创建传输控制块TCB，准备接受客户进程的连接请求。然后服务器进程就处于 LISTEN(收听)状态，等待客户的连接请求。如有，即作出响应。 A的TCP客户进程也是首先创建传输控制模块TCB。然后，在打算建立TCP连接时，向B发出连接请求报文段，这时首部中的同步位SYN=1，同时选择一个初始序号seq=ⅹ。TCP规定，SYN报文段(即SYN=1的报文段)不能携带数据，但要消耗掉一个序号这时，TCP客户进程进入 SYN-SENT(同步已发送)状态。 B收到连接请求报文段后，如同意建立连接，则向A发送确认。在确认报文段中应把SYN位和ACK位都置1，确认号是ack=x+1，同时也为自己选择一个初始序号seq=y。请注意，这个报文段也不能携带数据，但同样要消耗掉一个序号。这时TCP服务器进程进入 SYN-RCVD(同步收到)状态。 TCP客户进程收到B的确认后，还要向B给出确认。确认报文段的ACK置1，确认号ack=y+1，而自己的序号seq=x+1。TCP的标准规定，ACK报文段可以携带数据。但如果不携带数据则不消耗序号，在这种情况下，下一个数据报文段的序号仍是seq=x。 这时，TCP连接已经建立，A进入 ESTABLISHED(已建立连接)状态。当B收到A的确认后，也进入 ESTABLISHED状态 为什么A最后还要发送一次确认呢?这主要是为了防止已失效的连接请求报文段突然又传送到了B，因而产生错误。 所谓“已失效的连接请求报文段”是这样产生的。考虑一种正常情况，A发出连接请求，但因连接请求报文丢失而未收到确认。于是A再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接。A共发送了两个连接请求报文段，其中第一个丢失，第二个到达了B，没有“已失效的连接请求报文段”。现假定出现一种异常情况，即A发出的第一个连接请求报文段并没有丢失，而是在某些网络结点长时间滞留了，以致延误到连接释放以后的某个时间才到达B。本来这是一个早已失效的报文段。但B收到此失效的连接请求报文段后，就误认为是A又发出一次新的连接请求。于是就向A发出确认报文段，同意建立连接。假定不采用报文握手，那么只要B发出确认，新的连接就建立了由于现在A并没有发出建立连接的请求，因此不会理睬B的确认，也不会向B发送数据。但B却以为新的运输连接已经建立了，并一直等待A发来数据。B的许多资源就这样白白浪费了。 5.9.2. TCP的连接释放 数据传输结束后，通信的双方都可释放连接。现在A和B都处于 ESTABLISHED状态。A的应用进程先向其TCP发出连接释放报文段，并停止再发送数据，主动关闭TCP连接。A把连接释放报文段首部的终止控制位FN置1，其序号seq=u，它等于前面已传送过的数据的最后一个字节的序号加1。这时A进入FⅠ N-WAIT-1(终止等待1)状态，等待B的确认。请注意，TCP规定，FIN报文段即使不携带数据，它也消耗掉序号。 B收到连接释放报文段后即发出确认，确认号是ack=u+1，而这个报文段自己的序号是v，等于B前面已传送过的数据的最后一个字节的序号加1。然后B就进入 CLOSEWAT(关闭等待)状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的连接就释放了，这时的TCP连接处于半关闭状态，即A已经没有数据要发送了，但B若发送数据，A仍要接收。也就是说，从B到A这个方向的连接并未关闭，这个状态可能会持续一段时间。 A收到来自B的确认后，就进入 FIN-WAIT2(终止等待2)状态，等待B发出的连接释放报文段。 若B已经没有要向A发送的数据，其应用进程就通知TCP释放连接。这时B发出的连接释放报文段必须使FIN=1。现假定B的序号为w(在半关闭状态B可能又发送了一些数据)。B还必须重复上次已发送过的确认号ack=u+1。这时B就进入 LAST-ACK(最后确认)状态，等待A的确认。 A在收到B的连接释放报文段后，必须对此发出确认。在确认报文段中把ACK置1，确认号ack=w+1，而自己的序号是seq=u+1(根据TCP标准，前面发送过的FIN报文段要消耗一个序号)。然后进入到 TIME-WAIT(时间等待)状态。请注意，现在TCP连接还没有释放掉。必须经过时间等待计时器( TIME-WAIT timer)设置的时间2MSL后，A才进入到 CLOSED状态。 MSL：最长报文段寿命。 为什么A在 TMME-WAIT状态必须等待2MSL的时间呢? 为了保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，因而使处在 LAST-ACK状态的B收不到对已发送的FN+ACK报文段的确认。B会超时重传这个FN+ACK报文段，而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。接着A重传一次确认，重新启动2MSL计时器。最后，A和B都正常进入到CLOSED状态。如果A在 TIME-WAIT状态不等待一段时间，而是在发送完ACK报文段后立即释放连接，那么就无法收到B重传的FN+ACK报文段，因而也不会再发送一次确认报文段。这样，B就无法按照正常步骤进入 CLOSED状态。1MSL时间留给最后的ACK确认报文段到达服务器端，1MSL时间留给服务器端再次发送的FIN。 防止上一节提到的“已失效的连接请求报文段”出现在本连接中。A在发送完最后一个ACK报文段后，再经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。B只要收到了A发出的确认，就进入 CLOSED状态。同样，B在撤销相应的传输控制块TCB后，就结束了这次的TCP连接。我们注意到，B结束TCP连接的时间要比A早。 保活计时器 设想有这样的情况:客户已主动与服务器建立了TCP连接。但后来客户端的主机突然出故障。显然，服务器以后就不能再收到客户发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就是使用保活计时器。服务器每收到一次客户的数据，就重新设置保活计时器，时间的设置通常是两小时。若两小时没有收到客户的数据，服务器就发送一个探测报文段，以后则每隔75秒钟发送一次。若一连发送10个探测报文段后仍无客户的响应，服务器就认为客户端出了故障，接着就关闭这个连接。 5.9.3. TCP的有限状态机 6. 应用层 6.1. 域名系统DNS 顶级域名： 国家顶级域名：.cn .us 通用顶级域名：.com .org 基础结构域名：.arpa 互联网的域名结构： 域名服务器： 根域名服务器：最高层次的、最重要的域名服务器 顶级域名服务器：负责管理在该顶级域名服务器下注册的所有二级域名 权限域名服务器：负责一个区的域名服务器 本地域名服务器：默认域名服务器 解析过程： 主机向本地域名服务器的查询一般都是采用递归查询。 所谓递归查询就是:如果主机所询问的本地域名服务器不知道被査询域名的IP地址，那么本地域名服务器就以DNS客户的身份，向其他根域名服务器继续发出查询请求报文(即替该主机继续查询)，而不是让该主机自己进行下一步的查询。因此，递归查询返回的查询结果或者是所要查询的IP地址，或者是报错，表示无法查询到所需的IP地址。 本地域名服务器向根域名服务器的查询通常是采用迭代查询 迭代査询的特点是这样的:当根域名服务器收到本地域名服务器发出的迭代查询请求报文时要么给出所要查询的IP地址，要么告诉本地域名服务器:“你下一步应当向哪一个域名服务器进行查询”。然后让本地域名服务器进行后续的查询(而不是替本地域名服务器进行后续的查询)。根域名服务器通常是把自己知道的顶级域名服务器的IP地址告诉本地域名服务器，让本地域名服务器再向顶级域名服务器查询。顶级域名服务器在收到本地域名服务器的查询请求后，要么给出所要查询的IP地址，要么告诉本地域名服务器下一步应当向哪一个权限域名服务器进行查询，本地域名服务器就这样进行迭代査询。最后，知道了所要解析的域名的IP地址，然后把这个结果返回给发起查询的主机。 为了提高DNS查询效率，并减轻根域名服务器的负荷和减少互联网上的DNs查询报文数量，在域名服务器中广泛地使用了高速缓存(有时也称为高速缓存域名服务器)。由于名字到地址的绑定并不经常改变，为保持高速缓存中的内容正确，域名服务器应为每项内容设置计时器并处理超过合理时间的项。 6.2. 文件传送协议 6.2.1. FTP 文件传送协议FTP是互联网上使用得最广泛的文件送协议，提供交互式的访问，允许客户指明文件的类型与格式，并允许文件具有存取权限，FTP屏蔽了各计算机系统的细节，因而适合于在异构网络中任意计算机之间传送文件。 基于TCP的FTP和基于UDP的简单文件传送协议TFTP，它们都是文件共享协议中的一大类，即复制整个文件。而网络文件系统NFS则采用另一种思路，允许应用进程打开一个远地文件，并能在该文件的某一个特定的位置上开始读写数据。在网络上传送的只是少量的修改数据。 FTP的服务器进程由两大部分组成:一个主进程，负责接受新的请求;另外有若干个从属进程， 负责处理单个请求 主进程的工作步骤 打开熟知端口(端口号为21)，使客户进程能够连接上 等待客户进程发出连接请求 启动从属进程处理客户进程发来的请求。从属进程对客户进程的请求处理完毕后即终止，但从属进程在运行期间根据需要还可能创建其他一些子进程发之()回到等待状态，继续接受其他客户进程发来的请求。主进程与从属进程的处理是并行的 两个从属进程:控制进程和数据传送进程。为简单起见，服务器端的主进程没有画上 在进行文件传输时，FTP的客户和服务器之间要建立两个并行的TCP连接:“控制连 接（21号端口）”和“数据连接（20号端口）”。控制连接在整个会话期间一直保持打开，FTP客户所发出的传送请求，通过控制连接发送给服务器端的控制进程，但控制连接并不用来传送文件。实际用于传输文件的是“数据连接”。 6.2.2. TFTP 简单文件传送协议TFTP，它是个很小且易于实现的文件传送协议。也使用客户服务器方式，但它使用UDP数据报，因此TFTP需要有自己的差错改正措施。TFTP只支持文件传输而不支持交互。TFTP没有一个庞大的命令集，没有列目录的功能，也不能对用户进行身份鉴别。 每次传送的数据报文中有512字节的数据，但最后一次可不足512字节，这正好可作为文件结束的标志。 优点： TFTP可用于UDP环境 TFTP代码所占的内存较小 6.3. 超文本传送协议HTTP 6.3.1. HTTP基本过程 统一资源定位符URL：&lt;协议&gt;://&lt;主机&gt;:&lt;端口&gt;/路径&gt; URL里面的协议和主机不分大小写，路径有时区分。 HTTP使用了面向连接的TCP作为运输层协议，保证了数据的可靠传输。但是HTTP协议本身是无连接的。 HTTP协议是无状态的。也就是说，同一个客户第二次访问同一个服务器上的页面时，服务器的响应与第一次被访问时的相同。 万维网客户就把HTTP请求报文作为建立TCP连接的三报文握手中的第三个报文的数据，发送给万维网服务器。服务器收到HTTP请求报文后，就把所请求的文档作为响应报文返回给客户。 所以，请求一个万维网文档所需的时间是该文档的传输时间加上两倍往返时间RTT。 HTTP/1.0的主要缺点 每请求一个文档就要有两倍RTT的开销。 万维网客户和服务器每一次建立新的TCP连接都要分配缓存和变量。特别是同时服务于大量客户的请求，使用并行TCP连接可以缩短响应时间。 HTTP/1.1协议较好地解决了这个问题，它使用了持续连接( persistent connection)。所谓持续连接就是万维网服务器在发送响应后仍然在一段时间内保持这条连接，使同一个客户(浏览器)和该服务器可以继续在这条连接上传送后续的HTTP请求报文和响应报文。 持续连接的工作方式： 非流水线方式：客户在收到前一个响应后才能发出下一个请求。因此客户每访问一次对象都要用去一个往返时间RTT，比1.0少一个。 流水线方式：客户在收到HTTP的响应报文之前就能够接着发送新的请求报文。因此客户访问所有的对象只需花费一个RTT时间。 代理服务器 代理服务器(proxy server)是一种网络实体，它又称为万维网高速缓存( Web cache)。代理服务器把最近的一些请求和响应暂存在本地磁盘中。当新请求到达时，若代理服务器发现这个请求与暂时存放的请求相同，就返回暂存的响应，而不需要按URL的地址再次去互联网 访问该资源。代理服务器可在客户端或服务器端工作，也可在中间系统上工作。 6.3.2. HTTP报文 请求报文 响应报文 由于HTTP是面向文本的，因此在报文中的每一个字段都是一些ASCI码串，因而各个字段的长度都是不确定的。 HTTP请求报文和响应报文都是由三个部分组成的，区别仅开始行不同。 6.3.2.1. HTTP请求报文 请求报文的第一行有方法，请求资源的URL，以及HTTP的版本。 方法 描述 Get 请求资源 Post 向服务端发送资源 Delete 用于删除资源 Put 用于资源的更新，若资源不存在则新建一个 Option 请求一些选项信息 Head 只请求资源的头部，该请求方法的一个使用场景是在下载一个大文件前先获取其大小再决定是否要下载， 以此可以节约带宽资源 Trace 用于环回测试的请求报文 Connect 用于代理服务器 Get/Post的区别 Get请求的参数放在URL里，有长度限制；Post请求的参数放在实体里，没有限制。 Get请求比起Post请求更加不安全，因为参数放在URL中，不能用来传递敏感信息。 GET用于信息获取，而且应该是安全和幂等的（对同一URL的多个请求应该返回同样的结果）；POST请求表示可能修改服务器上资源的请求 6.3.2.2. HTTP响应报文报文 状态行包括三项内容，即HTTP的版本，状态码，以及解释状态码的简单短语 状态码都是三位数字的，分为5大类： 1xx：通知信息 2xx：成功 3xx：重定向，中间态，如要完成请求还必须采取进一步的行动 4xx：客户的差错，如请求中有错误的语法或不能完成。 5xx：服务器错误，这些状态代码表示服务器在尝试处理请求时发生内部错误。 具体的： 200 OK：表示从客户端发送给服务器的请求被正常处理并返回； 204 No Content：表示客户端发送给客户端的请求得到了成功处理，但在返回的响应报文中没有资源可以返回； 206 Patial Content：表示客户端进行了范围请求，并且服务器成功执行了这部分的GET请求。 301 Moved Permanently：永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL； 302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL； 永久是指原来访问的资源已经永久删除啦，客户端应该根据新的URI访问重定向。 临时是指访问的资源可能暂时先用location的URI访问，但旧资源还在的，下次你再来访问的时候可能就不用重定向了。 303 See Other：表示请求的资源被分配了新的URL，客户端应使用GET方法定向获取请求的资源； 304 Not Modified：该状态码表示客户端发送附带条件的请求时，服务器端允许请求访问资源，但是从缓存获取资源 该状态码表示客户端发送附带条件的请求时，服务器端允许请求访问资源，但是从缓存获取资源客户端在请求一个文件的时候，发现自己有该请求之前的缓存的文件，并且记录了 Last Modified ，那么在请求头中会包含 If Modified Since ，这个时间就是缓存文件的 Last Modified 。因此，如果请求中包含 If Modified Since，就说明已经有缓存在客户端。服务端只要判断这个时间和当前请求的文件的修改时间就可以确定是返回 304 还是 200 。在服务端的返回的响应头中通常有Last Modified 如果此值与请求头中的If Modified Since时间一致那么返回就是304，否则就是200。 对于静态文件，例如：CSS、图片，服务器会自动完成 Last Modified 和 If Modified Since 的比较，完成缓存或者更新。但是对于动态页面，就是动态产生的页面，往往没有包含 Last Modified 信息，这样浏览器、网关等都不会做缓存，也就是在每次请求的时候都完成一个 200 的请求。 因此，对于动态页面做缓存加速，首先要在 Response 的 HTTP Header 中增加 Last Modified 定义，其次根据 Request 中的 If Modified Since 和被请求内容的更新时间来返回 200 或者 304 。虽然在返回 304 的时候已经做了一次数据库查询，但是可以避免接下来更多的数据库查询，并且没有返回页面内容而只是一个 HTTP Header，从而大大的降低带宽的消耗，对于用户的感觉也是提高。 307 Temporary Redirect：临时重定向，与303有着相同的含义，307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）； 400 Bad Request：表示请求报文中存在语法错误； 401 Unauthorized：未经许可，需要通过HTTP认证； 403 Forbidden：服务器拒绝该次访问（访问权限出现问题）； 404 Not Found：表示服务器上无法找到请求的资源，除此之外，也可以在服务器拒绝请求但不想给拒绝原因时使用； 500 Inter Server Error：表示服务器在执行请求时发生了错误，也有可能是web应用存在的bug或某些临时的错误时； 503 Server Unavailable：表示服务器暂时处于超负载或正在进行停机维护，无法处理请求； 重定向 所谓重定向就是，当浏览器向服务端发送url 请求的时候返回状态码为30x时表示请求被重定向了，例如请求时http://www.a.com 返回的请求头中location:https://www.a.com 那么客户端就要重新根据location 提供的信息重新发起新的请求。例如在外网的ingress 配置http 强制跳转https 的时候就是通过配置301强制跳转的。 请求报文： GET /pet-products.txt HTTP/1.1 Host: www.joes-hardware.com Accept: * 服务端响应报文重定向到另一个地址： HTTP/1.1 301 OK Location: http://www.gentle-grooming.com/ Content-length: 56 Content-type: text/plain 客户端浏览器收到重定向响应报文后，将发起一个向新地址的请求： GET / HTTP/1.1 Host: www.gentle-grooming.com Accept: * 新地址的响应报文： HTTP/1.1 200 OK Content-type: text/html Content-length: 3307 6.3.3. HTTP协议的区别 HTTP1.1比起1.0 缓存处理。在HTTP1.0中主要使用header里的If-Modified-Since、Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since， If-Match， If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用。HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 Host头处理，为虚拟化准备。在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。 长连接。HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。 HTTP2.0比起1.1 新的二进制格式（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（MultiPlexing），即连接共享，即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的id将request再归属到各自不同的服务端请求里面。 header压缩、缓存。如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。 服务端推送（server push），同SPDY一样，HTTP2.0也具有server push功能。 6.3.4. HTTPS HTTPS：是以安全为目标的HTTP通道，简单讲：HTTPS = HTTP + TLS/SSL（TLS的前身是SSL）。 HTTPS协议的主要作用： 加密数据，建立一个信息安全通道，来保证数据传输的安全；另一种就是 对网站服务器进行真实身份认证。 6.3.4.1. HTTPS的工作原理 Client发起一个HTTPS的请求，并将自己支持的加密规则发给Server。 Server从中选出一资加密算法和HASH算法，把自己的身份信息以公钥证书形式返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的对称密钥，然后用证书的公钥加密这个对称密钥。 Client使用约定好的HASH算法计算握手消息，并用生成的随机数对消息加密，发给Server。 Server使用自己的私钥解密，得到对称密钥。至此，Client和Server双方都持有了相同的对称密钥。使用密码解析Client发来的握手消息，并验证HASH是否一致。 Server使用对称密钥加密一段内容，发送给Client。 Client使用对称密钥解密响应的密文，并计算HASH，得到明文内容。 Server与Client互相发送加密的握手消息并验证，目的是保证双方获得一致的密码，并且可以正常地加解密，做一次测试。 非对称加密算法用于在握手过程中加密生成的密码，对称加密算法用于对真正传输的数据加密，HASH算法用于验证数据完整性。 SSL握手过程中有任何错误都会是加密连接断开，从而阻止隐私信息的传输，非常安全。攻击者常采用假证书欺骗客户端。 6.3.4.2. HTTPS和HTTP的区别 https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。 http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。 6.3.5. HTTP缓存机制 浏览器的缓存机制也就是我们说的HTTP缓存机制，其机制是根据HTTP报文的缓存标识进行的，所以在分析浏览器缓存机制之前，我们先使用图文简单介绍一下HTTP报文，HTTP报文分为两种： HTTP请求(Request)报文，报文格式为：请求行 – HTTP头(通用信息头，请求头，实体头) – 请求报文主体(只有POST才有报文主体) HTTP响应(Response)报文，报文格式为：状态行 – HTTP头(通用信息头，响应头，实体头) – 响应报文主体 通用信息头指的是请求和响应报文都支持的头域，分别为Cache-Control、Connection、Date、Pragma、Transfer-Encoding、Upgrade、Via； 实体头则是实体信息的实体头域，分别为Allow、Content-Base、Content-Encoding、Content-Language、Content-Length、Content-Location、Content-MD5、Content-Range、Content-Type、Etag、Expires、Last-Modified、extension-header。 浏览器与服务器通信的方式为应答模式，即：浏览器发起HTTP请求 – 服务器响应该请求。 浏览器每次发起请求，都会先在浏览器缓存中查找该请求的结果以及缓存标识 浏览器每次拿到返回的请求结果都会将该结果和缓存标识存入浏览器缓存中 根据是否需要向服务器重新发起HTTP请求将缓存过程分为两个部分，分别是强制缓存和协商缓存。 6.3.5.1. 强制缓存 强制缓存就是向浏览器缓存查找该请求结果，并根据该结果的缓存规则来决定是否使用该缓存结果的过程，强制缓存的情况主要有三种： 不存在该缓存结果和缓存标识，强制缓存失效，则直接向服务器发起请求（跟第一次发起请求一致）： 存在该缓存结果和缓存标识，但是结果已经失效，强制缓存失效，则使用协商缓存： （3）存在该缓存结果和缓存标识，且该结果没有还没有失效，强制缓存生效，直接返回该结果： 强制缓存的缓存规则 当浏览器向服务器发送请求的时候，服务器会将缓存规则（放入HTTP响应的报文的HTTP头中和请求结果一起）返回给浏览器，控制强制缓存的字段分别是Expires和Cache-Control，其中Cache-Conctrol的优先级比Expires高。 Expires Expires是HTTP/1.0控制网页缓存的字段，其值为服务器返回该请求的结果缓存的到期时间（绝对时间），即再次发送请求时，如果客户端的时间小于Expires的值时，直接使用缓存结果。 到了HTTP/1.1，Expires已经被Cache-Control替代，原因在于Expires控制缓存的原理是使用客户端的时间与服务端返回的时间做对比，如果客户端与服务端的时间由于某些原因（时区不同；客户端和服务端有一方的时间不准确）发生误差，那么强制缓存直接失效，那么强制缓存存在的意义就毫无意义。 Cache-Control 取值为： public：所有内容都将被缓存（客户端和代理服务器都可缓存） private：所有内容只有客户端可以缓存，Cache-Control的默认取值 no-cache：客户端缓存内容，但是是否使用缓存则需要经过协商缓存来验证决定 no-store：所有内容都不会被缓存，即不使用强制缓存，也不使用协商缓存 max-age=xxx (xxx is numeric)：缓存内容将在xxx秒后失效（相对时间） 浏览器的缓存存放在哪里，如何在浏览器中判断强制缓存是否生效？ 内存缓存(from memory cache)和硬盘缓存(from disk cache): 内存缓存(from memory cache)：内存缓存具有两个特点，分别是快速读取和时效性： 快速读取：内存缓存会将编译解析后的文件，直接存入该进程的内存中，占据该进程一定的内存资源，以方便下次运行使用时的快速读取。 时效性：一旦该进程关闭，则该进程的内存则会清空。 硬盘缓存(from disk cache)：硬盘缓存则是直接将缓存写入硬盘文件中，读取缓存需要对该缓存存放的硬盘文件进行I/O操作，然后重新解析该缓存内容，读取复杂，速度比内存缓存慢。 在浏览器中，浏览器会在js和图片等文件解析执行后直接存入内存缓存中，那么当刷新页面时只需直接从内存缓存中读取(from memory cache)；而css文件则会存入硬盘文件中，所以每次渲染页面都需要从硬盘读取缓存(from disk cache)。 行为分析： 访问某个网站 –&gt; 200 –&gt; 关闭博客的标签页 –&gt; 重新打开网站（渲染） –&gt; 200(from disk cache) –&gt; 刷新 –&gt; 200(from memory cache)。ctrl+F5 强制刷新就会200 发起新的请求而忽略缓存。 6.3.5.2. 协商缓存 协商缓存就是强制缓存失效后，浏览器携带缓存标识向服务器发起请求，由服务器根据缓存标识决定是否使用缓存的过程，主要有以下两种情况： 协商缓存生效，返回304： 协商缓存失败，返回200和请求结果： 同样，协商缓存的标识也是在响应报文的HTTP头中和请求结果一起返回给浏览器的，控制协商缓存的字段分别有： Last-Modified / If-Modified-Since Etag / If-None-Match 其中Etag / If-None-Match的优先级比Last-Modified / If-Modified-Since高。 Last-Modified / If-Modified-Since Last-Modified是服务器响应请求时，返回该资源文件在服务器最后被修改的时间，时间值。 If-Modified-Since则是客户端再次发起该请求时，携带上次请求返回的Last-Modified值，告诉服务器上次请求该资源时返回的最后被修改时间。服务器收到该请求，发现请求头含有If-Modified-Since字段，则会根据If-Modified-Since的字段值与该资源在服务器的最后被修改时间做对比： 若服务器的资源最后被修改时间大于If-Modified-Since的字段值，则重新返回资源，状态码为200； 否则则返回304，代表资源无更新，可继续使用缓存文件。 Etag / If-None-Match Etag是服务器响应请求时，返回当前资源文件的一个唯一标识(由服务器生成) If-None-Match是客户端再次发起该请求时，携带上次请求返回的唯一标识Etag值，通过此字段值告诉服务器该资源上次请求返回的唯一标识值。服务器收到该请求后，发现该请求头中含有If-None-Match，则会根据If-None-Match的字段值与该资源在服务器的Etag值做对比： 一致则返回304，代表资源无更新，继续使用缓存文件； 不一致则重新返回资源文件，状态码为200。 6.3.5.3. 总结 强制缓存优先于协商缓存进行，若强制缓存(Expires和Cache-Control)生效则直接使用缓存，若不生效则进行协商缓存(Last-Modified / If-Modified-Since和Etag / If-None-Match) 协商缓存由服务器决定是否使用缓存，若协商缓存失效，那么代表该请求的缓存失效，重新获取请求结果，再存入浏览器缓存中；生效则返回304，继续使用缓存，主要过程如下： 6.3.6. session和cookie session与cookie的区别 session：Session 是存放在服务器端的，类似于Session结构来存放用户数据，当浏览器 第一次发送请求时，服务器自动生成了一个Session和一个Session ID用来唯一标识这个Session，并将其通过响应发送到浏览器。当浏览器第二次发送请求，会将前一次服务器响应中的Session ID放在请求中一并发送到服务器上，服务器从请求中提取出Session ID，并和保存的所有Session ID进行对比，找到这个用户对应的Session。由于Session存放在服务器端，所以随着时间的推移或者用户访问的增多，会给服务器增加负担。使用的时候要考虑下服务器的性能。 cookie：Cookie实际上是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端会把Cookie保存起来。由于Cookie是存放在客户端，是可见的，安全性就会很低。因此不建议将一些重要的信息放在cookie中。 区别 cookie存放在客户端，session存放在服务器端 cookie不是很安全，别人可以分析你本地的cookie信息进行cookie欺骗，因此重要信息应考虑保存在服务器端 session一定时间内回报存在服务器端，当访问量增大时，会影响服务器性能，从性能方面考虑应使用cookie 不同浏览器对cookie的数据大小限制不同，个数限制也不同 可以考虑将登陆信息等重要信息存放为session，不重要的信息可以放在cookie中 联系 都是用来记录用户的信息，以便让服务器分辨不同的用户 可以搭配使用，但都有自己的使用局限，要考虑到安全和性能的问题 如果客户端禁止 cookie，session 还能用吗？ 如果浏览器禁止cookie，那么客户端访问服务端时无法携带sessionid，服务端无法识别用户身份，便无法进行会话控制，session失效。但可以通过以下几种方法： URL重写：URL重写要求将站点中的所有超链接都进行改造，在超链接后用一个特殊的参数JSESSIONID保存当前浏览器对应session的编号，这样一来，当用户点击超链接访问服务器时，服务器可以从URL后的参数中分析出JSESSIONID，从而找到对应的sesison使用. 用文件、数据库等形式保存Session ID，在跨页过程中手动调用 6.4. 输入URL之后会发生什么 6.4.1. 大纲 浏览器解析URL 查找资源缓存 DNS解析 建立TCP连接（三次握手） HTTP/HTTPS发起请求 对TCP报文打包，加入源IP地址和目标IP地址 网络层查询下一跳路由，ARP查询下一跳路由的MAC地址 数据链路层对IP报文打包并附上MAC地址 物理层发送数据 服务器处理请求，响应，浏览器接收HTTP响应 选择关闭TCP连接（四次挥手） 编码、解析构建规则树 渲染树 6.4.2. 具体过程 浏览器解析URL获取协议、域名、端口、路径； 查看浏览器是否有资源的缓存 有。判断是否过期 没过期。直接读取缓存 过期。 Etag和If-None-Match Last-Modify和lf-Modified-Since 文件修改了则把新资源发给浏览器（状态码200），没修改则告诉浏览器读取缓存（状态码304） 没有则进行下一步 DNS解析 寻找浏览器是否存在缓存，若没有 寻找操作系统是否存在缓存，若没有 寻找hosts文件中是否有域名和ip的对应关系，若没有 查找路由器中是否有缓存 寻找DNS服务器是否没缓存，若没有 向本地域名服务器递归查询 本地域名服务器想根域名服务器迭代查询 生成HTTP请求 建立TCP连接，三次握手 客户端发送SYN=1，Seq=X 服务端发回SYN=1，ACK=X+1，Seq=Y 客户端发送ACK=Y+1，Seq=Y + 1 如果是HTTP请求，对HTTP报文进行报文分割并标记序号和端口号 如果是HTTPS请求 Client发起一个HTTPS的请求，并将自己支持的加密规则发给Server。 Server从中选出一资加密算法和HASH算法，把自己的身份信息以公钥证书形式返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的对称密钥，然后用证书的公钥加密这个对称密钥。 Client使用约定好的HASH算法计算握手消息，并用生成的随机数对消息加密，发给Server。 Server使用自己的私钥解密，得到对称密钥。至此，Client和Server双方都持有了相同的对称密钥。使用密码解析Client发来的握手消息，并验证HASH是否一致。 Server使用对称密钥加密一段内容，发送给Client。 Client使用对称密钥解密响应的密文，并计算HASH，得到明文内容。 对TCP报文打包，加入源IP地址和目标IP地址。 网络层根据目标IP地址和路由表，查询下一跳路由。使用ARP查询下一跳路由的MAC地址。 数据链路层对IP报文打包并附上MAC地址。 物理层发送数据。 服务器在收到浏览器发送的HTTP请求之后，会将收到的HTTP报文封装成HTTP的Request对象，并通过不同的Web服务器进行处理，处理完的结果以HTTP的Response对象返回，主要包括状态码，响应头，响应报文三个部分。 浏览器接收到HTTP响应，关闭TCP连接或保持复用，四次握手。 如果说响应的内容是HTML文档的话，就需要浏览器进行解析渲染呈现给用户。 根据响应头的字符集进行解码如果响应头没有字符集，则浏览器会默认用一套解码规则，当解析html解析到meta标签中的编码规则时，则替换成新的解码方式重新解码。 浏览器通过解析HTML，生成DOM树，解析CSS，生成CSS规则树。 然后通过DOM树和CSS规则树生成渲染树。渲染树与DOM树不同，渲染树中并没有head、display为none等不必显示的节点。repaint（重画颜色等，不影响布局）和reflow（组件的几何尺寸变了，重新验证并计算渲染树）。 7. 参考链接 《计算机网络（第7版）》谢希仁，电子工业出版社 计算机网络基础知识总结 OSI七层模型详解 HTTP协议以及HTTP2.0/1.1/1.0区别 HTTP1.0、HTTP1.1 和 HTTP2.0 的区别 HTTP常见状态码（14种） 菜鸟向前端指南 HTTP与HTTPS的区别 计算机网络常见面试题 计算机网络常见面试题 计算机网络之面试常考 HTTP请求和响应3：状态码（status） http 状态码之3xx 彻底理解浏览器的缓存机制(htp绶存机制) 这样看 B 站，可以更快！ session与cookie的区别？ 如果客户端禁止 cookie session 还能用吗？ ","link":"https://memorykki.github.io/network-basic/"},{"title":"网络爬虫基础","content":"待写 正文 ","link":"https://memorykki.github.io/webCrawler-basic/"},{"title":"《鸟哥的Linux私房菜》第0-3章","content":"基础篇第 0-3 章学习笔记 目录 计算机概论 计算机硬件 CPU 内存 显卡 硬盘与存储设备 PCI 适配卡 主板 电源 文字编码系统 操作系统 应用程序 总结 Linux是什么 历史 Linux的内核版本 Linux distribution Linux的特色 主机规划与磁盘分区 磁盘连接的方式与设备文件名的关系 IDE设备 SATA设备 磁盘的组成 磁盘分区表 开机流程与主引导分区MBR 开机流程 BootLoader的功能 磁盘分区的选择 主机硬盘的主要规划 安装CentOS 5.x与多重引导 安装规划 安装步骤 计算机概论 计算机硬件 CPU CPU 的架构： 精简指令集 RISC：单指令工作简单，执行快。arm 复杂指令集 CISC。单指令工作复杂，执行慢。intel(x86)、amd(x86_64) 1Byte=8bit 原本的单核 CPU 仅有一个运算单元,所谓的多核则是在一个 CPU 封装当中嵌入了两个以上的运算内核,简单地说,就是一个实际的 CPU 外壳中含有两个以上的 CPU 单元。 amd 内存与 cpu 直连，intel 内存通过北桥与 cpu 连接 CPU 频率=外频*倍频 外频：cpu 与外部传输速率 倍频：cpu 内部运算的加速功能（出厂固定） 超频：通过主板的设定功能更改成较高频率的一种方式。因为频率并非正常速度,故可能会造成死机等。 总线宽度：32/64 CPU 有向下兼容的能力 内存 个人计算机的内存主要组件为动态随机访问内存 DRAM，挥发性 SDRAM DDR SDRAM：双倍数据传输速度 - DDR - DDRⅡ - DDRⅢ 双通道设计：两根 64 位总线宽度-&gt;128 位 CPU 外频=内存外频 第二层缓存：常用的数据或程序放置在 CPU 内部，不在通过北桥从内存加载 DRAM 频率速度无法达到 L2=CPU 外频，因此使用静态随机访问内存 SRAM 只读存储器 ROM 主板上组件参数存储在 CMOS 中，BIOS 是一套写死在 ROM 上的程序，非挥发性，系统开机时首先读取 BIOS；固件很多采用 ROM 进行软件写入；ROM 无法改写，因此 BIOS 通常写入 Flash Memory 或 EEPROM 中，以更新。 显卡 一般对于图形影像的显示重点在于分辨率与色彩深度，因为每个图像显示的颜色会占用内存，因此显卡上面会有一个内存的容量，这个显存容量将会影响到最终你的屏幕分辨率与色彩深度。越大越好。 在显卡上嵌入 3D 加速芯片提升运算能力，即 GPU。 北桥通信 规格：AGP-&gt;PCI-&gt;PCIe 硬盘与存储设备 硬盘：盘片、机械手臂、磁头、主轴马达。 盘片上的数据： 磁盘上最小存储单位：扇区 512bytes；扇区组成圆：磁道；所有片上面的同一磁道：柱面（分割硬盘的最小单位）。 磁盘容量=header 数量每个 header 负责的柱面数量每个柱面所含有的扇区数量*扇区的容量 传输接口： - IDE 接口：连接两个设备，理论 133MB/s - SATA 接口：PC，一个设备，利于散热，理论 300MB/s 缓冲存储器：加速读取 转速：主轴马达转动盘片 运转：正常关机使机械手臂归回原位；避免移动主机使硬盘抖动 PCI适配卡 用户额外的功能卡安插在 PCI 插槽上，很多组件（声卡、网卡）采用 PCI 接口传输，现多 PCIe 主板 主板上面负责通信各个组件的就是芯片组，芯片组一般分为北桥与南桥，需要散热。北桥负责 CPU/RAM/VGA 等的连接,南桥则负责 PCI 接口与速度较慢的 I/O 设备。 芯片组：集成（显卡、网卡）型、独立型 I/O 地址：标识不同的设备；IRQ 中断信道：告知 CPU 该设备工作情况方便分配工作。sharing IRQ 技术。 CMOS 与 BIOS 电源 能源转换率 接口 文字编码系统 英文：ASCII，一个字符占用 1B 中文：big5（简体 gb2312），占用 2B ISO/IEC 制订了 Unicode 编码系统，即 UTF8 操作系统 内核、接口以及相关应用 操作系统内核（kernel）：管理计算机的所有活动及驱动系统中的所有硬件，分配资源。内核所放置到内存中的区块受保护，开机后常驻内存。参考硬件规格，硬件不同则内核不同。 系统调用（System Call）：一套开发接口，开发软件参考内核功能相关的开发接口。 内核功能： 系统调用接口：与内核通信，软件开发 程序管理：多任务环境下有效分配资源 内存管理：虚拟内存功能提供在内存不足时的内存交换 swap 文件系统管理 设备驱动 操作系统与驱动程序：OS 提供开发接口，让硬件开发商制作驱动，安装。 应用程序 程序与 OS 有关系 总结 计算机的定义为：”接受用户输入指令与数据，经由中央处理器的数据与逻辑单元运算处理后，以产生或存储成有用的信息“。 计算机的五大单元包括输入单元、输出单元、CP∪ 内部的控制单元、算术逻辑单元与内存五大部分。 数据会流进流出内存是 CPU 所发布的控制命令，而 CPU 实际要处理的数据则完全来自于内存。 CPU 依设计理念主要分为精简指令集(RSC)与复杂指令集(CSC)系统。 关于 CPU 的频率部分，外频指的是 cPU 与外部组件进行数据传输时的速度,倍频则是 CPU 内部用来加速工作性能的一个倍数，两者相乘才是 cPU 的频率速度。 一般主板芯片组分为北桥与南桥，北桥的总线称为系统总线,因为是内存传输的主要信道所以速度较快。南桥就是所谓的输入/输出(I/O)总线,主要在于连系硬盘、USB、网卡等接口设备。 北桥所支持的频率我们称为前端总线速度( Front Side Bus,FSB),而每次传送的位数则是总线宽度。 CPU 每次能够处理的数据量称为字组大小( word size),字组大小依据 cPU 的设计而有 32 位与 64 位之分。我们现在所称的计算机是 32 或 64 位主要是依据这个 CPU 解析的字组大小而来的! 个人计算机的内存主要组件为动态随机访问内存( Dynamic Random Access Memory,DRAM），至于 CPU 内部的第二层缓存则使用静态随机访问内存（Static Random Access Memory, SRAM）。 BIOS(Basic Input Output System)是一套程序,这套程序是写死到主板上面的一个内存芯片中,这个内存芯片在没有通电时也能够将数据记录下来,那就是只读存储器(Read Only Memory,ROM)。 显卡的规格有 PCI/AGP/PCe,目前的主流为 Pcle 接口。 硬盘是由盘片、机械手臂、磁头与主轴马达所组成的,其中盘片的组成为扇区、磁道与柱面。 操作系统( Operating System,Os)其实也是一组程序,这组程序的重点在于管理计算机的所有活动以及驱动系统中的所有硬件。 计算机主要以二进制作为单位,常用的磁盘容量单位为 Byte,其单位换算为 1Byte=8bt 操作系统仅在于驱动与管理硬件,而要使用硬件时,就得需要通过应用软件或者是 she 的功能,来调用操作系统操纵硬件工作。目前,操作系统除了上述功能外,通常已经包含了日常工作所需要的应用软件在内了。 Linux是什么 Linux 是 OS 内核，包含内核及工具，沿袭 Unix，提供了一个完整的 OS 中最底层的硬件控制与资源管理的完整架构。 历史 Bell 实验室开发 Multics 分时系统无果，Ken Thompson 用汇编语言编写内核程序 Unics，之后为了移植，改用 C 语言编写内核发性 UNIX 正式版本，强调多用户多任务的兼容分时系统。后来 AT&amp;T 收回 UNIX 源码版权，谭宁邦自主编写 Minix，用于教学，公开源码，付费发行。 史托曼为了创建一个自由开放的 UNIX 操作系统，开始 GNU 项目，成立自由软件基金会 FSF，合作之下完成 GNU C Compiler，撰写通用公共许可证 GPL（CopyLeft），1991 年，芬兰大学生 Linus Torvalds 宣称 LInux 内核程序可以在 Intel 386 上运行。 GNU：发布软件的同时公布源码。 GNU GPL：GPL 代表具有自由度的软件，用户可以自由执行、复制、再发行、学习、修改与强化，但不能取消 GPL 授权，软件是自由的，但软件相关的“服务”是可以商业的。Open Source 的好处即修改程序代码去适合的工作环境。 Linux的内核版本 主版本.次版本.释出版本-修改版本 次版本为奇数：开发中版本，内核工程师使用； 次版本为偶数：稳定版本，家庭计算机以及企业版本。 释出版本：主次版本不变情况下，新增功能累积到一定程度之后释出。 Linux 内核版本与 distibution 版本并不相同 Linux distribution Linux 是 kernel，GNU GPL 授权模式，参考 POSIX 设计规范（POSIX 即可携式操作系统接口，重点在于规范内核与应用程序之间的接口），兼容 UNIX 操作系统，即 UNIX Like。 为了让用户能够接触到 Linux，于是很多的商业公司或非营利团体就将 Kernel+ Softwares+ Tools 集成起来，加上自己具有创意的工具程序,这个工具程序可以让用户以光盘、DvD 或者通过网络直接安装管理 Lnux 系统。称为 dLinux distribution。 Linux standard Base(LsSB)等标准来规范开发者,以及目录架构的 Fle system Hierarchy Standard(FHS)标准规范，它们的唯一差别可能就是该开发者自家所开发出来的管理工具以及套件管理的模式。所以说，基本上，每个 Linux distributions 除了架构的严谨度与选择的套件内容外，其实差异并不太大，最大的差异在于软件的安装模式。 团队版适合个人（更新快），商业版适合服务器（稳定）。 事实上鸟哥认为 distributions 主要分为两大系统，一种是使用 RPM 方式安装软件的系统,包括 Red Hat， Fedora，SusE，Centos 等都是这类；一种则是使用 Debian 的 dpkg 方式安装软件的系统，包括 Debian，Ubuntu，B2D 等。 Linux的特色 稳定的系统 免费或少许费用 安全性、漏洞的快速修补 多任务、多用户 用户、用户组 相对不耗资源 需要小内核的嵌入式系统 整合度佳且多样的用户界面（GUI） 主机规划与磁盘分区 各个硬件设备在 linux 中当成一个文件来对待，几乎所有的硬件设备文件在/dev 目录里。 Windows 下不管有多少个 IDE 设备都是顺序地分配盘符，而在 Linux 下是认真区分对待每一个硬盘的。 （来自https://blog.csdn.net/weixin_32482133/article/details/112946362） 磁盘连接的方式与设备文件名的关系 接口 命名 IDE 设备 /dev/hd[a-d] SCSI/SATA/USB 硬盘 /dev/sd[a-p] IDE设备 一个 IDE 扁平线缆可以连接两个 IDE 设备，通常主机会提供两个接口 IDE1（primary）和 IDE2（secondary），故可以连接四个 IDE 设备，每个扁平线缆上的 IDE 设备分为 Master/Slave。 IDE Master Slave IDE1（primary） /dev/hda /dev/hdb IDE2（secondary） /dev/hdc /dev/hdd SATA设备 SATA/SCSI/USB 等磁盘接口使用 SCSI 模块驱动，使用/dev/sd[a-p]格式，SATA/USB 接口的磁盘根本没有顺序，文件名由 Linux 内核检测到磁盘的顺序排列。 磁盘的组成 磁盘的第一个扇区（512bytes）记录重要信息： 主引导分区 MBR（446bytes）：安装引导加载程序的地方。系统开机时主动读取，系统知道程序存放位置和如何开机。 分区表（64bytes）：记录整块硬盘分区的状态。硬盘必须切割才能使用。 磁盘分区表 分区的最小单位是柱面，参考柱面号码切割硬盘，在分区表所在的 64bytes 容量中，分为四组记录区，每组记录区记录了该区段的起始与结束的柱面号码。这四个分区称为主或扩展分区。 系统写入磁盘时，一定会参考分区表。分区的目的：安全性与性能。 利用额外的（第一个扇区之外的）扇区记录更多的分区信息，扩展分区本身不能被格式化，主分区与逻辑分区可以。 这五个由扩展分区切割的分区成为逻辑分区（5个），设备名从主分区保留之后开始。 主分区+扩展分区≤4（硬盘限制）； 扩展分区≤1（操作系统限制）。 P主分区、E扩展分区、L逻辑分区。主分区和扩展分区之外的空间将被浪费。考虑到连续性，E分配在最后。分区&gt;4时，一定要有E。 例如：在第二块SATA硬盘下分6个分区，有方法： 开机流程与主引导分区MBR 开机流程 BIOS：开机主动执行韧体 BIOS，寻找开机设备（根据用户设置的能开机的硬盘） MBR：分析存储设备（如硬盘）第一个扇区的 MBR 位置，内置引导加载程序 引导加载程序 Boot Loader：加载内核 内核文件：OS 工作。 BIOS 和 MBR 由硬件支持，BootLoader 由 OS 支持。 BootLoader的功能 提供菜单：多重引导 载入内核：指向可开机的程序区段 转交其他 Loader（一个硬盘上包含多个系统，如双系统） 硬盘的每个分区都有自己的启动扇区，内核文件实际放置在各分区内，Loader 只认识自己分区的内核以及其他 Loader。 磁盘分区的选择 Linux 所有数据以文件形态呈现，使用目录树结构，实际存放在磁盘分区中。 挂载：文件系统与目录树的关系 将一个目录当作磁盘的入口，进入目录即可读取分区。即“挂载”与“挂载点”，最重要的根目录一定需要挂载到某分区。 （磁盘上有/file，挂载到/mnt 下，即访问路径/mnt/file） 安装 distribution，自定义安装 Custom 模式，建议仅区分主分区/和交换分区 swap（补充内存）。更详细的分区方案参考网络。 主机硬盘的主要规划 主机硬盘的需求和主机开放的服务有关，还需要注意数据分类和安全。 最简单的的分区方法 / &amp; swap &amp; 未分区 根据用途： 容量大且读写频繁的目录：/ /usr /home /var swap 大硬盘配合旧主机造成无法开机的问题 旧主板找不到大容量磁盘，导致误判，但是 Linux 内核顺利开机后会重新检测硬盘而不例会 BIOS，所以 Linux 能够安装。 但是安装完毕无法开机，BIOS 不能正确识别硬盘，开机错误。BIOS 只能读取前面的扇区，故将启动扇区所在分区规范在 ≤1024 个柱面以内。 分区/boot &amp; / &amp; swap，/boot 100MB 左右，放在硬盘最前面。 安装CentOS 5.x与多重引导 安装规划 Linux主机的角色定位 选择distribution 计算机硬件配置 磁盘分区配置：/boot&amp;/&amp;/home&amp;swap PPPL 引导装载程序BootLoader：安装到MBR 选择软件：默认/最小/全部/自定义 安装步骤 调整启动媒体(BIOS):使用Linux安装光盘，调整BIOS的开机启动顺序为光盘；（无法以DVD开机时很可能是由于计算机硬件不支持、光驱会挑片、光盘问题、硬件超频等） 选择安装结构与开机:包括图形界面命令行界面等，也可加入特殊参数（硬件检测、安装媒体、内存测试）来开机进入安装界面； 选择语系数据:由于不同地区的键盘按键不同，此时需要调整语系/键盘/鼠标等配置； 磁盘分区:最重要的项目之一。提前规划。 四种分区结构： 整块硬盘：删除所有分区并创建默认分区结构 Linux分区：删除Linux分区并创建默认分区 选定驱动器的空闲空间：未分区空间 建立自定义分区结构 Linux支持的文件系统类型： （二三版书上介绍有差别） ext2/ext3：Linux使用的文件系统类型，ext3/ext4比ext2多日志记录，恢复系统较为迅速 引导装载程序、网络、时区设置与root密码:一些需要的系统基础设置! 软件选择:需要什么样的软件?全部安装还是默认安装即可? 安装后的首次设置:安装完毕后还有一些事项要处理,包括用户、 SELinux与防火墙等 ","link":"https://memorykki.github.io/linux-vbird-0-3/"},{"title":"记一次使用Github登录Gitalk跳至首页的解决过程","content":"使用自定义域名之后的 Github page 在使用 Github 登录 Github 时跳转首页，无限循环 问题 博客使用的是Hexo+Github+Gitalk，使用 Github 登录 Gitalk 之后跳转至首页，无限循环，地址栏URL显示为： https://memorykk.cn/?error=redirect_uri_mismatch&amp;error_description=The+redirect_uri+MUST+match+the+registered+callback+URL+for+this+application.&amp;error_uri=https%3A%2F%2Fdocs.github.com%2Fapps%2Fmanaging-oauth-apps%2Ftroubleshooting-authorization-request-errors%2F%23redirect-uri-mismatch 解决过程 为了更清楚地检查，将此 URL 中的 Unicode 转为中文： https://memorykk.cn/? error=redirect_uri_mismatch&amp; error_description=The+redirect_uri+MUST+match+the+registered+callback+URL+for+this+application.&amp; error_uri=https://docs.github.com/apps/managing-oauth-apps/troubleshooting-authorization-request-errors/#redirect-uri-mismatch 根据描述，redirect error registered callback URL不匹配。 打开F12查看Network： this application我使用的是OAuth Apps，查看其Authorization callback URL，果不其然： http和https的差别还是很大的！修改Authorization callback URL为https之后成功了。 点击使用GitHub登录之后，又出现 405 问题， 打开F12查看Network： 转换为中文： Request URL: https://memorykk.cn/TeamSpeak/%7B%7B%20theme.gitalk.proxy%20%7D%7D Request URL: https://memorykk.cn/TeamSpeak/{{ theme.gitalk.proxy }} 好家伙，这还是源代码形式啊，坑定是后台有什么问题！ 检查theme\\layout\\components\\comment.ejs，这行代码果然是从这里出来的： var gitalk = new Gitalk({ clientID: '&lt;%= theme.gitalk_client_id %&gt;', clientSecret: '&lt;%= theme.gitalk_client_secret %&gt;', repo: '&lt;%= theme.gitalk_repo %&gt;', owner: '&lt;%= theme.gitalk_owner %&gt;', admin: ['&lt;%- theme.gitalk_admin.join(&quot;', '&quot;) %&gt;'], id: location.pathname, distractionFreeMode: false, createIssueManually: true, proxy: '{{ theme.gitalk.proxy }}', }) 之前对node.js也不了解（虽然现在也一样，允悲），没注意格式。仿照上面几行代码改成： proxy: '&lt;%= theme.gitalk.proxy %&gt;', 还是报 405 错误，打开F12查看： 总感觉之前白嫖的 CorsAnywhere Proxy 链接压根没有传递过去，直接手动写死在里面： proxy: 'https://netnr-proxy.cloudno.de/https://github.com/login/oauth/access_token', 好家伙，成功了，ohhhhhhhhhhh，虽然这种做法极不优雅！ 总结 网页出错先用F12看看哪部分的问题，顺藤摸瓜，缩小范围。 ","link":"https://memorykki.github.io/gitalk-405/"},{"title":"TeamSpeak","content":"一个简易的TeamSpeak使用教程 更新于2021-02-19 14:22:14： 旧地址memorykk.cn已不可用，为了您后续的正常使用，请使用新地址ts.memorykk.cn 更新于2020/5/22 15:30： 根据 @洛杉矶湖人名宿孙悦 ​的提醒，不推荐下载ts1.cn的盗版客户端，也不推荐俱乐部或者各位玩家租用ts1.cn的盗版服务器。具体可查看原帖，所以希望大家去官网下载正版软件，如果官网或github下载太慢，可以点正文中转存的链接，都是一样的。 去年自己搭建了一个TeamSpeak服务器，闲着也是闲着，分享出来给大家使用，市面上的服务器要么收费、要么音质权限不给，本服务器免费共享，合法合规，不涉及隐私信息等，请放心使用！ 什么是TeamSpeak TeamSpeak (简称TS) 是一套专有的VoIP软件，使用者可以用耳机和麦克风，通过客户端软件连线到指定的服务器，与在服务器内频道的其他使用者进行通话。是一种很像电话会议的方式。 通常 TeamSpeak 的使用者大多为多人连线游戏的玩家，与同队伍的玩家进行通讯。在游戏的对战方面，语音对话通讯具有竞争优势。 简言之，TeamSpeak是一款非常优秀的语音软件，类似于国内的YY语音、TT语音等，PUBG、CSGO等项目职业选手都在使用，在和小伙伴激情上分时，你该不会还在使用QQ微信电话吧？那么TS优点在哪里呢？有几点： 界面简洁，没有广告，没有花里胡哨的付费内容； 极高的音乐级音质，超低的延迟； 独有的麦克风降噪、键盘声衰减、回声消除； 自动感应麦真的很好用。 怎么使用 下载TeamSpeak客户端（最新版本为3.5.6） 官网下载 转存下载：（文件和官网一样，解决下载过慢问题） WINDOWS CLIENT 32-BIT 3.5.6 WINDOWS CLIENT 64-BIT 3.5.6 MACOS CLIENT UNIVERSAL BINARY 3.5.6 LINUX CLIENT 32-BIT 3.5.6 LINUX CLIENT 64-BIT 3.5.6 下载中文语音包&amp;语言包 安装客户端和中文包 默认“下一步”就完事了，打开如下： 注册登录你的TS账号（跳过这一步也可以） 安装软件后会弹出来，没弹出来可以依次点击 工具-&gt;选项-&gt;myTeamSpeak​ 继续操作。 设置语音输入方式 依次点击 工具-&gt;选项-&gt;输入 建议设置如下： 语音输入方式： Automatic：自动感应麦 Volume Gate：自定义麦克风的收音敏感度，点“开始测试”，比最高的分贝低一点就可以了 Hybrid：不懂，没用过 连接服务器 依次点击左上角 连接-&gt;连接-&gt;输入服务器地址（ts.memorykk.cn）和昵称，无密码，直接点击连接，自动进入大厅。 开房间 空白处右键单击“创建频道”，设置房间名，有需要的可以设置密码，点击确定，房间开好了，双击房间名即可进入，这时候你就可以让你的小伙伴直接进入你的房间就完事了。 注意 创建频道如果选择临时，当频道最后一人离开时，频道会自动消失。 可以设置频道音质，右键单击频道-&gt;声音-&gt;拖到质量条。 不推荐设置密码，方便大家组队。 频道可以设置图标，标注自己玩的游戏，有需要的下方留言哦。 最后，有问题到下方留言或是查看联系方式，希望大家玩得开心。 ","link":"https://memorykki.github.io/TeamSpeak/"},{"title":"Git","content":"Git是目前世界上最先进的分布式版本控制系统（没有之一）。 目录 1. 版本控制 2. 安装GIT 2.1. LINUX 2.2. LINUX 2.2.1. DEBIAN/UBUNTU 2.2.2. CENTOS/REDHAT 2.3. WINDOWS 3. 配置GIT 3.1. 配置用户名和EMAIL 3.2. 增删配置项 3.2.1. 添加配置项 3.2.2. 删除配置项 3.3. 更多配置项 4. GIT基本概念 5. GIT工作流程 6. GIT操作 6.1. GIT创建版本库 6.1.1. 在工作目录中初始化 6.1.2. 从现有仓库克隆 6.2. 文件更新 6.2.1. 文件状态 6.2.2. 检查文件状态 6.2.3. 跟踪文件 6.2.4. 暂存已修改文件 6.2.5. 忽略某些文件 6.2.6. 查看未暂存的更新 6.2.7. 查看已暂存的更新 6.2.8. 提交更新 6.2.9. 跳过使用暂存区域 6.2.10. 移除文件 6.2.11. 移动文件 6.3. 查看提交历史 6.3.1. 限制输出长度 6.3.2. 使用图形化工具查阅提交历史 6.4. 撤销 6.4.1. 修改最后一次提交 6.4.2. 取消已经暂存的文件 6.4.3. 取消对文件的修改 6.4.4. 文件操作总结 7. GIT远程仓库 7.1. 查看远程仓库 7.2. 添加远程仓库 7.3. 从远程仓库抓取数据 7.4. 推送数据到远程仓库 7.5. 查看远程仓库信息 7.6. 远程仓库的删除和重命名 7.6.1. 重命名 7.6.2. 删除 8. 版本标签 8.1. 显示已有标签 8.2. 新建标签 8.2.1. 含附注的标签 8.2.2. 轻量级标签 8.2.3. 签署标签 8.2.4. 验证标签 8.3. 后期加注标签 8.4. 分享标签 9. GIT分支 9.1. 分支简介 9.2. 新建分支 9.3. HEAD 9.4. 切换分支 9.5. 合并分支 9.6. 遇到冲突时的分支合并 9.7. 删除分支 9.8. 案例 9.9. 分支管理 9.10. 远程分支 9.11. 推送本地分支 9.12. 跟踪远程分支 9.13. 删除远程分支 9.14. 分支衍合 10. 参考链接 Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。最初由 Linus Torvalds 为了帮助管理 Linux 内核开发而花了两周时间自己用C写出来的。（理那厮·掏袜子真的太绝了⊙ｏ⊙） 1. 版本控制 版本控制系统分为本地式、集中式和分布式。 本地式：适合个人使用，用于记录文件更新。代表：RCS 集中式：必须联网，断网后可编辑无法commit和回滚。版本库都存在中央服务器上，用户的本地只有自己以前所同步的版本，没有完整的版本库。代表：SVN、CVS、VSS。 假设SVN服务器没了，那你丢掉了所有历史信息，因为你的本地只有当前版本以及部分历史信息。 分布式：只有在push、pull时需要联网，断网后可正常commit。中央服务器只为了修改起来方便，可有可无，每个用户本地都是一个完整的版本库。代表：Git。 每个人都同时对同一版本修改，commit操作提交到本地，多人协作时需要push操作和别人同步，但是团队人数多会导致非常麻烦，所以出现所谓的（便于修改的）中央服务器。 假设GitHub服务器没了，你不会丢掉任何git历史信息，因为你的本地有完整的版本库信息，你可以把本地的git库重新上传到另外的git服务商。 Git的特点 直接记录快照，而非差异比较 Git 只关心文件数据的整体是否发生变化，而大多数其他系统则只关心文件内容的具体差异。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。有变化则保存，无则链接。 近乎所有操作都是本地执行 时刻保持数据完整性 保存在 Git 数据库中的东西都是用此哈希值来作索引的，而不是靠文件名。 多数操作仅添加数据 2. 安装Git 2.1. Linux Git 的工作需要调用 curl，zlib，openssl，expat，libiconv 等库的代码，所以需要先安装这些依赖工具。 yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel 或 $ apt-get install libcurl4-gnutls-dev libexpat1-dev gettext libz-dev libssl-dev 下载源码：http://git-scm.com/download 编译安装 $ tar -zxf git-1.7.2.2.tar.gz $ cd git-1.7.2.2 $ make prefix=/usr/local all $ sudo make prefix=/usr/local install 2.2. Linux 2.2.1. Debian/Ubuntu $ sudo apt-get install git 2.2.2. Centos/RedHat $ yum install git-core 2.3. Windows 官网下载 镜像下载 默认安装。 {% img /images/Git/winGit.PNG %} Git Bash：Unix与Linux风格的命令行（推荐） Git CMD：DOS风格的命令 Git GUI：图形界面（新手不推荐） 打开Git Bash检查一下： $ git --version git version 2.29.2.windows.2 3. 配置Git 3.1. 配置用户名和Email 信息将会嵌入你的每次提交中。 配置级别 系统级--system：系统中所有用户适用的配置 Linux：/etc/gitconfig Windows：Git/mingw64/etc/gitconfig 用户级--global：当前用户适用的配置 Linux：~/.gitconfig Windows：C:/Users/Administrator/.gitconfig 项目级--local：特定项目适用的配置 Linux：gitProject/.gitconfig Windows：gitProject/.gitconfig 低级覆盖高级：local&lt;globall&lt;system $ git config --global user.name &quot;Your Name&quot; $ git config --global user.email &quot;Your Email&quot; 如果用了--global选项，所有项目都会默认使用这个全局配置，不加则为某个特定的项目信息。 3.2. 增删配置项 3.2.1. 添加配置项 git config [--local|--global|--system] section.key value 例如： 51139@DESKTOP MINGW64 /e/blog $ git config --global Student.number 2018000001 51139@DESKTOP MINGW64 /e/blog $ cat C:/Users/51139/.gitconfig [user] name = memorykkk email = 511390937@qq.com [Student] number = 2018000001 3.2.2. 删除配置项 git config [--local|--global|--system] --unset section.key 3.3. 更多配置项 git config -l, --list #list all 列出所有 git config -e, --edit #open an editor 打开一个编辑器 git config --global color.ui true #打开所有的默认终端着色 git config --global alias.ci commit #别名 ci 是commit的别名 user.name #用户名 user.email #邮箱 core.editor #文本编辑器 merge.tool #差异分析工具 core.paper &quot;less -N&quot; #配置显示方式 color.diff true #diff颜色配置 alias.co checkout #设置别名 git config user.name #获得用户名 git config core.filemode false #忽略修改权限的文件 学习config命令，运行 $ git help config 查看手册 git config 4. Git基本概念 Git本地有三个工作区域：工作区（Workspace）、暂存区（Index/Stage）、仓库（Repository），加上远程仓库（Remote）共四个工作区域。 Git 各个命令可以理解为在各个仓库间转移数据，各个命令对应对每个仓库输入输出。 Workspace：工作区，平时能看到的存放项目代码的地方 Index / Stage：暂存区，临时存放改动，保存即将提交到文件列表信息的文件，有时也叫作索引 Repository：仓库区（或本地仓库），安全存放数据的位置，含所有版本的数据，HEAD指向最新版本 Remote：远程仓库，托管代码的服务器，用于远程数据交换 对于本地三个区域的关系如下： Directory：使用Git管理的仓库，包含我们的工作空间和Git的管理空间 WorkSpace：需要通过Git进行版本控制的目录和文件组成工作空间 .git：存放Git管理信息的目录，初始化仓库的时候自动创建 Index/Stage：暂存区，或者叫待提交更新区，在提交进入repo之前存放所有的更新 Local Repo：本地仓库，HEAD会只是当前的开发分支（branch）。 Stash：隐藏，是一个工作状态保存栈，用于保存/恢复WorkSpace中的临时状态。 观察命令对各区域的影响： master 是 master 分支所代表的目录树，此时 HEAD 实际是指向 master 分支的一个&quot;游标&quot;，objects 标识的区域为 Git 的对象库，实际位于 &quot;.git/objects&quot; 目录下，里面包含了创建的各种对象及内容。 当对工作区修改（或新增）的文件执行 git add 命令时，暂存区的目录树被更新，同时工作区修改（或新增）的文件内容被写入到对象库中的一个新的对象中，而该对象的ID被记录在暂存区的文件索引中。 当执行提交操作 git commit时，暂存区的目录树写到版本库（对象库）中，master 分支会做相应的更新。即 master 指向的目录树就是提交时暂存区的目录树。 当执行 git reset HEAD 命令时，暂存区的目录树会被重写，被 master 分支指向的目录树所替换，但是工作区不受影响。 当执行 git rm --cached &lt;file&gt; 命令时，会直接从暂存区删除文件，工作区则不做出改变。 当执行 git checkout . 或者 git checkout -- &lt;file&gt; 命令时，会用暂存区全部或指定的文件替换工作区的文件。这个操作很危险，会清除工作区中未添加到暂存区的改动。 当执行 git checkout HEAD . 或者 git checkout HEAD &lt;file&gt; 命令时，会用 HEAD 指向的 master 分支中的全部或者部分文件替换暂存区和以及工作区中的文件。这个命令也是极具危险性的，因为不但会清除工作区中未提交的改动，也会清除暂存区中未提交的改动。 若对四个区域分级：workspace &lt; index &lt; repository &lt; remote，基本的转移如下表，参数和选项决定数据来源。 低等级输入 高等级输入 workspace 手动 git checkout/git stash index/stage git add git reset repository git commit git pull remote git push - 四个区域转换关系如下： 5. Git工作流程 一般工作流程如下： 在工作目录中修改某些文件。 对修改后的文件进行快照，然后保存到暂存区域。 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中。 6. Git操作 6.1. Git创建版本库 在工作目录中初始化新仓库 从现有仓库克隆 6.1.1. 在工作目录中初始化 $ git init 在新建空目录或已有目录中，使用命令 git init 初始化一个新仓库。初始化后该目录下自动生成.git目录，用于生成所有 Git 需要的数据和资源。 文件加入版本库 将文件添加到仓库（workspace -&gt; stage） $ git add &lt;file&gt; 执行后没有任何信息则代表执行成功。 将文件提交到仓库（stage -&gt; repository） $ git commit -m &lt;message&gt; 多次add不同的文件，但只需一次commit就可以提交。 6.1.2. 从现有仓库克隆 $ git clone &lt;url&gt; &lt;newName&gt; Git 收取的是项目历史的完整数据（每一个文件的每一个版本），而不是某个特定版本，服务器上有的数据克隆之后本地也都有了。 url 支持 git 协议或 SSH 传输协议。通常来说，Git协议下载速度最快，SSH协议用于需要用户认证的场合。 克隆之后的目录名为 url 中 .git 前的名称，通过 &lt;newName&gt; 可以自定义。目录中包含.git目录，用于保存下载下来的所有版本记录，然后从中取出最新版本的文件拷贝。 6.2. 文件更新 6.2.1. 文件状态 文件状态主要分两种，已跟踪（tracked）即本来就被纳入版本控制管理的文件，其它文件即未跟踪（untracked）。 Untracked：未跟踪，此文件没有加入到git库，不参与版本控制。通过 git add 状态变为staged Unmodify：文件已经入库，未修改，即版本库中的文件快照内容与文件夹中完全一致。这种文件被修改成为Modified，通过git rm移出版本库成为Untracked Modified：文件已修改。这种文件通过git add可变成staged，通过git checkout则丢弃修改回到unmodify Staged：暂存状态。通过git commit则将修改同步到库中，这时库中的文件和本地文件又变为一致，变为Unmodify状态。执行git reset HEAD &lt;filename&gt;取消暂存，变为Modified 初次克隆某个仓库时，工作目录中的所有文件都属于已跟踪文件，且状态为未修改（unmodified）。 文件状态变化周期： 修改后的文件状态是 modified，逐步放在暂存区域，最后一次性提交。 6.2.2. 检查文件状态 #查看指定文件状态 $ git status &lt;filename&gt; #查看所有文件状态 $ git status 例如创建README之后： $ vim README $ git status # On branch master # Untracked files: # (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) # # README nothing added to commit but untracked files present (use &quot;git add&quot; to track) 表示当前处在master分支，未跟踪文件里包含 README，即 Git 之前的快照中没有这个文件，未纳入管理。 6.2.3. 跟踪文件 （workspace -&gt; stage；untracked -&gt; staged） # 添加指定文件到暂存区 $ git add &lt;file1&gt; &lt;file2&gt; ... # 添加指定目录到暂存区，包括子目录 $ git add &lt;dir&gt; # 添加当前目录的所有文件到暂存区 $ git add . 例如： $ git add README $ git status # On branch master # Changes to be committed: # (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) # # new file: README # “Changes to be committed:” 表示其下的文件处于暂存区 6.2.4. 暂存已修改文件 （untracked -&gt; staged） 修改一个已跟踪的文件后 $ git status # On branch master # Changes not staged for commit: # (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) # # modified: benchmarks.rb # “Changes not staged for commit:” 表示该文件内容发生变化但还未放到暂存区，需要重新运行 git add 命令。 也就是说，文件修改之后必须再次 git add，否则 git commit 提交的是修改前版本。 $ git add benchmarks.rb $ git status # On branch master # Changes to be committed: # (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) # # new file: README # modified: benchmarks.rb # git add 根据目标文件的状态产生不同的效果： 开始跟踪新文件 把已跟踪的文件放到暂存区 合并时把有冲突的文件标记为已解决状态 6.2.5. 忽略某些文件 有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。有以下三种方式： 创建.gitignore文件 项目设置中指定排除文件 临时指定。需要编辑当前项目中的 .git/info/exclude 文件 定义Git全局的 .gitignore 文件 设置全局的.gitignore文件来管理所有Git项目的行为。创建.gitignore文件，存放在任意位置，然后使用命令 # git config --global core.excludesfile ~/.gitignore 配置。 格式规范如下： 每行一个，空格不匹配任意文件，可作为分隔符，可用反斜杠转义 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略 可以使用标准的 glob 模式匹配：shell 所使用的简化了的正则表达式 匹配模式最后跟反斜杠/说明要忽略的是目录下所有文件 使用两个星号&quot;**&quot; 表示匹配任意中间目录 /结束的模式只匹配文件夹以及在该文件夹路径下的内容，但是不匹配该文件；/开始的模式匹配项目根目录 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号!取反。 关于优先级等更多语法查看Git忽略提交规则 - .gitignore配置运维总结 一般来说每个Git项目中都需要一个“.gitignore”文件，告诉Git哪些文件不需要添加到版本管理中，实际项目中很多文件都是不需要版本管理的，例如日志、缩略图、敏感信息等。 例如： # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt doc/*.txt 6.2.6. 查看未暂存的更新 $ git diff 查看工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变化内容。 例如： $ git diff diff --git a/benchmarks.rb b/benchmarks.rb index 3cb747f..da65585 100644 --- a/benchmarks.rb +++ b/benchmarks.rb @@ -36,6 +36,10 @@ def main @commit.parents[0].parents[0].parents[0] end + run_code(x, 'commits 1') do + git.commits.size + end + run_code(x, 'commits 2') do log = git.commits('master', 15) log.size 6.2.7. 查看已暂存的更新 $ git diff --cached 查看已经暂存起来的文件和上次提交时的快照之间的差异 6.2.8. 提交更新 （staged -&gt; commited；stage -&gt; repository） # 提交暂存区到仓库区 $ git commit -m &lt;file&gt; # 提交暂存区的指定文件到仓库区 $ git commit &lt;file1&gt; &lt;file2&gt; ... -m &lt;message&gt; # 提交工作区自上次commit之后的变化，直接到仓库区，跳过add,对新文件无效 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 如果我们提交后发现有个文件改错了，将修改过的文件通过&quot;git add&quot;后执行 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m &lt;message&gt; # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend &lt;file1&gt; &lt;file2&gt; ... 使用 git status 查看暂存区域状态，准备妥当之后就可以提交了。 这样看来暂存区的意义在于精心准备每次提交。 直接使用不加 -m 的 git commit 提交会打开编辑器（通过 $ git config --global core.editor emacs 配置）以便输入本次提交的说明。 使用-m选项比较方便。 例如： $ git commit -m &quot;Story 182: Fix benchmarks for speed&quot; [master]: created 463dc4f: &quot;Fix benchmarks for speed&quot; 2 files changed, 3 insertions(+), 0 deletions(-) create mode 100644 README 显示了提交的分支（master）、SHA-1校验和（463dc4f）、修改过的文件数（2）和增（3）删行数。 因为修改后但未暂存的处于已修改态（modified），只能纳入下一次版本。 每次提交都是对项目的一次快照，以后可以回退 6.2.9. 跳过使用暂存区域 $ git commit -a -m &lt;message&gt; Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤，对新文件无效 6.2.10. 移除文件 从 Git 中移除文件 未跟踪文件对 Git 来说不存在，手动rm即可，已跟踪文件就必须要从已跟踪文件清单（stage）中移除，然后提交。可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 $ git rm &lt;file&gt; 如果只是简单地从工作目录中手工删除文件，运行 git status 时就会在 “Changes not staged for commit” 部分（未暂存清单）看到。（判断为“修改”操作？） 从 Git 中移除，但仍然希望保留在当前工作目录 （stage -&gt; workspace；tracked:staged -&gt; untracked） 即不小心纳入仓库后，想要移除跟踪但不删除文件。 $ git rm --cached &lt;file&gt; 6.2.11. 移动文件 $ git mv &lt;oldfileName&gt; &lt;newFileName&gt; Git 并不跟踪文件移动操作。如果在 Git 中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。要在 Git 中对文件改名，执行git mv实际上相当于 $ mv old new $ git rm old $ git add new 6.3. 查看提交历史 $ git log 默认不用任何参数，git log 会按提交时间列出所有的更新信息。 例如： $ git log commit ca82a6dff817ec66f44342007202690a93763949 #SHA-1校验和 Author: Scott Chacon &lt;schacon@gee-mail.com&gt; #作者&lt;邮箱&gt; Date: Mon Mar 17 21:52:11 2008 -0700 #时间 changed the version number #提交说明 选项 $ git log &lt;option&gt; -p #开显示每次提交的内容差异 --stat #仅显示简要的增改行数统计 --shortstat #只显示 --stat 中最后的行数修改添加移除统计。 --name-only #仅在提交信息后显示已修改的文件清单。 --name-status #显示新增、修改、删除的文件清单。 --abbrev-commit #仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 --relative-date #使用较短的相对时间显示（比如，“2 weeks ago”）。 --graph 显示 ASCII #图形表示的分支合并历史。 --pretty #使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。 6.3.1. 限制输出长度 $ git log &lt;option&gt; -(n) #仅显示最近的 n 条提交 --since, --after #仅显示指定时间之后的提交。 --until, --before #仅显示指定时间之前的提交。 --author #仅显示指定作者相关的提交。 --committer #仅显示指定提交者相关的提交。 6.3.2. 使用图形化工具查阅提交历史 随 Git 一同发布的 gitk 相当于 git log 命令的可视化版本。 6.4. 撤销 任何已经提交到 Git 的都可以被恢复。你可能失去的数据，仅限于没有提交过的，对 Git 来说它们就像从未存在过一样。 6.4.1. 修改最后一次提交 # 如果我们提交后发现有个文件改错或没有加，将修改过的文件通过&quot;git add&quot;后执行 # 修改上一次commit的提交信息 $ git commit --amend -m &lt;message&gt; # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend &lt;file1&gt; &lt;file2&gt; ... 也就是说，在执行git commit之后暂存区为空，此时若再次执行git commit --amend判断为修改上次的提交信息；此时若git add新文件到暂存区，执行此命令相当于添加到上次提交。 6.4.2. 取消已经暂存的文件 （staged -&gt; modified） $ git reset HEAD &lt;file&gt; 文件从暂存区回到已修改未暂存状态。 6.4.3. 取消对文件的修改 （modified -&gt; unmodified） $ git checkout -- &lt;file&gt; 6.4.4. 文件操作总结 7. Git远程仓库 Git 要求每个远程主机都必须指定一个主机名。 7.1. 查看远程仓库 $ git remote 查看每个远程库的简短名字。Git 默认使用“origin”这个名字来标识你所克隆的原始仓库。 $ git remote -v 加上-v选项，显示对应的克隆地址。 7.2. 添加远程仓库 $ git remote add &lt;shortname&gt; &lt;url&gt; &lt;shortname&gt;即自定义远程主机名，在git remote中显示， 7.3. 从远程仓库抓取数据 git fetch $ git fetch add &lt;remote-name&gt; $ git fetch add &lt;remote-name&gt; &lt;branch-name&gt; #指定分支 抓取所有本地仓库没有的数据。但只是将远端的数据拉到本地仓库，并不自动合并到当前工作分支。 通常用来查看其他人的进程，因为它取回的代码对本地代码没有影响。 指定分支所取回的更新，在本地主机上要用&quot;远程主机名/分支名&quot;的形式读取。 类似的，git fetch origin会抓取上次clone以来的别人提交的更新。 如果当前分支与远程分支存在追踪关系，git pull就可以省略远程分支名。 git pull $ git pull &lt;remote-name&gt; &lt;remote-branch-name&gt;:&lt;local-branch-name&gt; 取回远程主机某个分支的更新，再与本地的指定分支合并。 如果当前分支与远程分支存在追踪关系，git pull就可以省略远程分支名。 如果是与当前分支合并，则冒号后面的部分可以省略。 等同于先git fetch再git merge。 如果远程主机删除了某个分支，默认情况下，git pull 不会在拉取远程分支的时删除对应的本地分支。这是为了防止由于其他人操作了远程主机，导致git pull不知不觉删除了本地分支。除非加上参数 -p 就会在本地删除远程已经删除的分支。 7.4. 推送数据到远程仓库 $ git push &lt;remote-name&gt; &lt;local-branch-name&gt;:&lt;remote-branch-name&gt; 如果省略远程分支名，则表示将本地分支推送与之存在&quot;追踪关系&quot;的远程分支（通常两者同名），如果该远程分支不存在，则会被新建。 如果省略本地分支名，则表示删除指定的远程分支，因为这等同于推送一个空的本地分支到远程分支。 将本地仓库中的数据推送到远程仓库，只有在所克隆的服务器上有写权限，或者同一时刻没有其他人在推数据，这条命令才会如期完成任务。 如果当前分支与远程分支之间存在追踪关系，则本地分支和远程分支都可以省略。 如果是新建分支第一次git push，会提示： fatal: The current branch dev1 has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin dev1 输入这行命令，然后输入用户名和密码就成功了，以后的push就只需要输入git push origin。 如果当前分支只有一个追踪分支，那么主机名都可以省略。 如果当前分支与多个主机存在追踪关系，则可以使用-u选项指定一个默认主机，这样后面就可以不加任何参数使用git push。 如果不管是否存在对应的远程分支，将本地的所有分支都推送到远程主机，这时需要使用--all选项。 如果远程主机的版本比本地版本更新，推送时Git会报错，要求先在本地做git pull合并差异，然后再推送到远程主机。这时如果你一定要推送，可以使用--force选项。 结果导致远程主机上更新的版本被覆盖。 7.5. 查看远程仓库信息 $ git remote show &lt;remote-name&gt; 例如： $ git remote show origin * remote origin URL: git@github.com:defunkt/github.git Remote branch merged with 'git pull' while on branch issues # git pull 时将自动合并的分支 issues Remote branch merged with 'git pull' while on branch master # git pull 时将自动合并的分支 master New remote branches (next fetch will store in remotes/origin) # 还没有同步到本地的远端分支 caching Stale tracking branches (use 'git remote prune') #已同步到本地的远端分支在远端服务器上已被删除 libwalker walker2 Tracked remote branches acl apiv2 dashboard2 issues master postgres Local branch pushed with 'git push' # git push 缺省推送的分支 master:master 7.6. 远程仓库的删除和重命名 7.6.1. 重命名 $ git remote rename pb paul 修改的是某个远程仓库在本地的简称，分支名也会发生变化。 7.6.2. 删除 $ git remote rm &lt;remote-name&gt; 8. 版本标签 8.1. 显示已有标签 $ git tag $ $ git tag -l '&lt;tag-name&gt;' #显示指定版本 按照字母顺序排列。 8.2. 新建标签 Git 使用的标签有两种类型：轻量级的（lightweight）和含附注的（annotated）。轻量级标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用；而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息。 建议一般使用含附注型的标签，临时性加注标签用轻量级标签。 8.2.1. 含附注的标签 $ git tag -a &lt;tag-name&gt; -m &lt;message&gt; $ git show '&lt;tag-name&gt;' #查看相应标签的版本信息 -a指定标签名，-m指定对应的标签说明，git show可以显示出。 8.2.2. 轻量级标签 $ git tag &lt;tag-name&gt; 没有选项。 8.2.3. 签署标签 如果你有自己的私钥，还可以用 GPG 来签署标签，只需要把之前的 -a 改为 -s 即可。 $ git tag -s &lt;tag-name&gt; -m &lt;message&gt; 8.2.4. 验证标签 $ git tag -v &lt;tag-name&gt; 验证已经签署的标签，会调用 GPG 来验证签名，所以需要有签署者的公钥，存放在 keyring 中，才能验证，否则报错gpg: Can't check signature: public key not found。 8.3. 后期加注标签 $ git tag -a &lt;tag-name&gt; &lt;SHA-1&gt; 只要在打标签的时候跟上对应提交对象的校验和（或前几位字符）即可。 例如： $ git log --pretty=oneline 9fceb02d0ae598e95dc970b74767f19372d61af8 updated rakefile $ git tag -a v1.2 9fceb02 $ git show v1.2 version 1.2 commit 9fceb02d0ae598e95dc970b74767f19372d61af8 Author: Magnus Chacon &lt;mchacon@gee-mail.com&gt; Date: Sun Apr 27 20:43:35 2008 -0700 updated rakefile 8.4. 分享标签 默认情况下，git push 并不会把标签传送到远端服务器上 使用--tags选项一次推送所有本地新增的标签上去 通过显式命令才能分享标签到远端仓库。 $ git push origin --tags $ git push origin [tag-name] 9. Git分支 使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。 9.1. 分支简介 在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针、作者等附属信息、零个或多个指向该提交对象的父对象指针。首次提交是没有直接祖先的，普通提交有一个祖先，由两个或多个分支合并产生的提交则有多个祖先。 举例：对于三个文件暂存后，Git 嫁给你当前版本的快照（blob类型）连同每个文件的额SHA-1保存至仓库。 $ git add README test.rb LICENSE $ git commit -m 'initial commit of my project' 现在仓库中有五个对象： 三个 blob 对象：表示文件快照内容 tree 对象：记录着目录树内容及其中各个文件对应的 blob 对象索引的 commit 对象：指向 tree 对象（根目录）的索引和其他提交信息元数据。在需要的时候重现此次快照。 作些修改后再次提交，那么这次的提交对象会包含一个指向上次提交对象的指针（parent 对象）。两次提交后： Git 中的分支，其实本质上仅仅是个指向 commit 对象的可变指针。Git 会使用 master 作为分支的默认名字。在若干次提交后，你其实已经有了一个指向最后一次提交对象的 master 分支，它在每次提交的时候都会自动向前移动。 9.2. 新建分支 $ git branch &lt;branch-name&gt; 使用git branch testing在当前 commit 对象上新建一个分支指针testing： 9.3. HEAD HEAD 是一个指向你正在工作中的本地分支的指针，即当前分支。 运行 git branch 命令，仅仅是建立了一个新的分支，但不会自动切换到这个分支中去，所以依然还在 master 分支里工作： 9.4. 切换分支 $ git checkout &lt;branch-name&gt; $ git checkout -b &lt;branch-name&gt; 使用git checkout testing切换分支testing： 或者运行 git checkout 并加上 -b 参数，新建并切换到该分支。 提交之后： $ vim test.rb $ git commit -a -m 'made a change' 提交后 HEAD 随testing分支一起向前移动，，而 master 分支仍然指向原先 git checkout 时所在的 commit 对象。 如果此时执行git checkout master，切回master分支： 做了两件事： HEAD 指针移回到 master 分支 工作目录中的文件换成了 master 分支所指向的快照内容（较旧的进度） 它的主要作用是将 testing 分支里作出的修改暂时取消，这样你就可以向另一个方向进行开发。 此时如果提交新文件，即： $ vim test.rb $ git commit -a -m 'made other changes' Git 中的分支实际上仅是一个包含所指对象校验和（40 个字符长度 SHA-1 字串）的文件，Git 的实现与项目复杂度无关，并且每次提交都记录了 parent 对象，所以 Git 分支操作非常廉价。 当我们创建新的分支，例如dev时，Git新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev上： 你看，Git创建一个分支很快，因为除了增加一个dev指针，改改HEAD的指向，工作区的文件都没有任何变化！ 不过，从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变： 假如我们在dev上的工作完成了，就可以把dev合并到master上。Git怎么合并呢？最简单的方法，就是直接把master指向dev的当前提交，就完成了合并： 所以Git合并分支也很快！就改改指针，工作区内容也不变！ 合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支。 9.5. 合并分支 $ git checkout master $ git merge iss53 回到 master 分支，运行 git merge 命令指定要合并进来的分支iss53 9.6. 遇到冲突时的分支合并 如果在不同的分支中都修改了同一个文件的同一部分，Git 就无法干净地把两者合到一起（逻辑上说，这种问题只能由人来裁决）。结果如下： $ git merge iss53 Auto-merging index.html CONFLICT (content): Merge conflict in index.html Automatic merge failed; fix conflicts and then commit the result. Git 做了合并但没有提交，等待解决冲突。可以用 git status 查看合并时发生冲突的文件： ... # unmerged: index.html 任何包含未解决冲突的文件都会以未合并unmerged的状态列出。Git 会在有冲突的文件里加入标准的冲突解决标记： &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html &lt;div id=&quot;footer&quot;&gt;contact : email.support@github.com&lt;/div&gt; ======= &lt;div id=&quot;footer&quot;&gt; please contact us at support@github.com &lt;/div&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html 可以看到 ======= 隔开的上半部分，是 HEAD（当前分支）中的内容，下半部分是在 iss53 分支中的内容。 解决办法是删除上面的内容（包括'=''&lt;''&gt;'），自行决定怎么写。之后运行git add和git status： ... # modified: index.html # 确认冲突已解决，标记为modified，再git commit提交。 也可以使用相关可视化工具解决冲突，运行git mergetool。 9.7. 删除分支 $ git branch -d &lt;branch-name&gt; #强制删除 $ git branch -D &lt;branch-name&gt; # 删除远程分支，分支必须完全合并在其上游分支，或者在HEAD上没有设置上游 $ git push origin --delete &lt;branch-name&gt; $ git branch -dr &lt;remote-name/branch-name&gt; 9.8. 案例 工作流程 开发某个网站为 实现某个新的需求，创建一个分支。 在这个分支上开展工作。 假设此时突然有个很严重的问题需要紧急修补，那么： 返回到原先已经发布到生产服务器上的分支。 为这次紧急修补建立一个新分支，并在其中修复问题。 通过测试后，回到生产服务器所在的分支，将修补分支合并进来，然后再推送到生产服务器上。 切换到之前实现新需求的分支，继续工作。 # 新建iss53并切换 $ git checkout -b iss53 # 遇问题切回master $ git checkout master # 此时工作区和之前 master 提交时完全一样 # 创建紧急修补分支 hotfix $ git checkout -b 'hotfix' Git 的好处： 不需要同时发布这个补丁和 iss53 里作出的修改 不需要在创建和发布该补丁到服务器之前花费大力气来复原这些修改 切换分支时需要留心你的暂存区或者工作目录里还没有提交的修改，它会产生冲突从而阻止切换，最好保持一个清洁的工作区域（也可以通过 stashing 和 commit amending 绕过这种问题）。 修改、测试之后回到 master 分支并把 hotfix 合并： $ git checkout master $ git merge hotfix 请注意，合并时出现了“Fast forward”的提示。由于当前 master 分支所在的提交对象是要并入的 hotfix 分支的直接上游，Git 只需把 master 分支指针直接右移。换句话说，如果顺着一个分支走下去可以到达另一个分支的话，那么 Git 在合并两者时，只会简单地把指针右移，因为这种单线的历史分支不存在任何需要解决的分歧，所以这种合并过程可以称为快进（Fast forward）。 hotfix分支完成历史使命之后可以删掉，回到正常的 iss53 正常工作： $ git branch -d hotfix $ git checkout iss53 需求#53开发完之后，合并master和iss53分支： $ git checkout master $ git merge iss53 这次合并操作的底层实现，并不同于之前 hotfix 的并入方式。因为这次你的开发历史是从更早的地方开始分叉的。由于当前 master 分支所指向的提交对象（C4）并不是 iss53 分支的直接祖先，Git 不得不进行一些额外处理。就此例而言，Git 会用两个分支的末端（C4 和 C5）以及它们的共同祖先（C2）进行一次简单的三方合并计算： 这次，Git 没有简单地把分支指针右移，而是对三方合并后的结果重新做一个新的快照，并自动创建一个指向它的提交对象（C6）（见图 3-17）。这个提交对象比较特殊，它有两个祖先（C4 和 C5）。 值得一提的是 Git 可以自己裁决哪个共同祖先才是最佳合并基础；这和 CVS 或 Subversion（1.5 以后的版本）不同，它们需要开发者手工指定合并基础。所以此特性让 Git 的合并操作比其他系统都要简单不少。 此时iss53没用了，可以删除。 9.9. 分支管理 查看所有分支 $ git branch iss53 * master testing git branch 命令不加参数列出所有分支。 master 分支前的 * 字符表示当前所在的分支。也就是说如果现在提交更新，master 分支将随着开发进度前移。 查看各分支最后一个 commit 对象的信息 $ git branch -v 查看与HEAD已合并/或未合并分支 # 查看已并入当前分支的==哪些分支是当前分支的直接上游 $ git branch --merged # 查看尚未合并的 $ git branch --no-merged 对于已合并的，可以用git branch -d直接删除，不会有损失。 对于已合并的，用git branch -d删除会报错，因为这样做会丢失数据，除非-D强制删除。 9.10. 远程分支 用remote-name/remote-branch-name描述远程分支，区别于本地分支。 在第一次git clone之后，下载的数据命名为origin/master，无法修改。之后 Git 创建属于本地的 master 分支，二者都指向远程origin的master分支。 如果在本地 master 分支做了些改动，在本地的提交历史正朝向不同方向发展，与此同时，其他人向 git.ourcompany.com 推送了他们的更新，服务器上的 master 分支就会向前推进。不过只要不和服务器通讯，本地的 origin/master 指针仍然保持原位不会移动。 可以运行 git fetch origin 来同步远程服务器上的数据到本地。从 origin 上获取尚未拥有的数据，更新你本地的数据库，然后把 origin/master 的指针移到最新位置。 假设，还有另一个仅供你的敏捷开发小组使用的内部服务器 git.team1.ourcompany.com。加为当前项目的远程分支之一，并命名为 teamone。 现在可以用 git fetch teamone 来获取小组服务器上你还没有的数据了。由于当前该服务器上的内容是你 origin 服务器上的子集，Git 不会下载任何数据，而只是简单地创建一个名为 teamone/master 的远程分支，指向 teamone 服务器上 master 分支所在的提交对象 31b8e。 master并不是多么神秘复杂的东西，分清楚远程的别人的origin/master、本地的别人的origin/master、本地的自己的master就可以了。 9.11. 推送本地分支 $ git push &lt;remote-name&gt; &lt;branch-name&gt; 要想和其他人分享某个本地分支，你需要把它推送到一个你拥有写权限的远程仓库。你创建的本地分支不会因为你的写入操作而被自动同步到你引入的远程服务器上，你需要明确地执行推送分支的操作。 git push origin serverfix:serverfix意思是“上传我本地的 serverfix 分支到远程仓库中去，仍旧称它为 serverfix 分支”。通过此语法，你可以把本地分支推送到某个命名不同的远程分支：若想把远程分支叫作 awesomebranch，可以用 git push origin serverfix:awesomebranch 来推送数据。 9.12. 跟踪远程分支 $ git checkout -b &lt;branch-name&gt; &lt;remote-name/branch-name&gt; 从远程分支 checkout 出来的本地分支，称为跟踪分支 (tracking branch)。跟踪分支是一种和某个远程分支有直接联系的本地分支。在跟踪分支里输入 git push，Git 会自行推断应该向哪个服务器的哪个分支推送数据。 9.13. 删除远程分支 $ git push &lt;remote-name&gt; :&lt;branch-name&gt; 对于git push &lt;remote-name&gt; &lt;local-branch-name&gt;:&lt;remote-branch-name&gt; 语法，如果省略 ，那就等于是在说“在这里提取空白然后把它变成”。 9.14. 分支衍合 $ git checkout &lt;rebase-branch-name&gt; $ git rebase &lt;master-branch-name&gt; 把一个分支中的修改整合到另一个分支有两种办法：merge 和 rebase。 merge rebase 还有可以把在 C3 里产生的变化补丁在 C4 的基础上重新打一遍，这种操作叫做衍合（rebase）。即把在一个分支里提交的改变移到另一个分支里重放一遍。 原理是回到两个分支最近的共同祖先，根据当前分支（也就是要进行衍合的分支 experiment）后续的历次提交对象（这里只有一个 C3），生成一系列文件补丁，然后以基底分支（也就是主干分支 master）最后一个提交对象（C4）为新的出发点，逐个应用之前准备好的补丁文件，最后会生成一个新的合并提交对象（C3'），从而改写 experiment 的提交历史，使它成为 master 分支的直接下游： 再进行快进合并： 现在的 C3' 对应的快照，其实和普通的三方合并，即 C5 对应的快照内容一模一样，结果没有任何区别，只不过提交历史不同。但衍合能产生一个更为整洁的提交历史，仿佛所有修改都是在一根线上先后进行的。 一般我们使用衍合的目的，是想要得到一个能在远程分支上干净应用的补丁。项目志愿开发者通过衍合提交，维护者就不需要做任何整合工作。实际上是把解决分支补丁同最新主干代码之间冲突的责任，化转为由提交补丁的人来解决。 衍合是按照每行的修改次序重演一遍修改，而合并是把最终结果合在一起。 10. 参考链接 Git官方参考文档 Pro Git（中文版） 深入浅出Git教程 Git图解剖析 廖雪峰的官方网站 菜鸟教程 Git忽略提交规则 - .gitignore配置运维总结 ","link":"https://memorykki.github.io/Git/"},{"title":"记一次博客接入Gitalk出现403问题的解决过程","content":"小站的评论系统始终无法接入 Gitalk，提示 403 Error 目录 问题 我的解决过程 其他解决过程 起因 思路 经验 部署自己的 CORSproxy 参考链接 问题 最近博客接入 Gitalk 评论系统时，始终报错 403，不知所以，无奈转至 LeanCloud 的 valine comment，但 valine 的风格总让人感觉不够优雅 ╯︿╰。 报错界面：{% img /images/gitalk-403/no-issue.PNG '&quot;no-issue&quot; &quot;no-issue&quot;' %} 点击 使用 GitHub 登录 之后，显示 Error：{% img /images/gitalk-403/403.PNG '&quot;403&quot; &quot;403&quot;' %} 我的解决过程 打开 F12 控制台，刷新页面，查看出错的 Header：{% img /images/gitalk-403/f12.PNG '&quot;f12&quot; &quot;f12&quot;' %} 触及到知识盲区了。这个 URL 尽管没见过，但两个地址套起来的形式很有特点，习惯性地打开前面的链接，展示如下：{% img /images/gitalk-403/core.PNG '&quot;core&quot; &quot;core&quot;' %} 大意是：CORS Anywhere 这是个实现跨域请求的 Demo，单击按钮临时解锁对演示的访问。 So，我被加锁了？ 打开提示的参考链接，这个 CORS Anywhere 的作者说： Public demo server (cors-anywhere.herokuapp.com) will be very limited by January 2021, 31st #301 The user (developer) must visit a page at cors-anywhere.herokuapp.com to temporarily unlock the demo for their browser. This allows developers to try out the functionality, to help with deciding on self-hosting or looking for alternatives. 意思是 CORS Anywhere 这个 Demo 被滥用，从 2021/1/31 开始限制，用户必须访问页面临时解锁。 虽然这只是个临时解决方案，但当时没想太多，能用就行。{% img /images/gitalk-403/requestTemp.PNG '&quot;requestTemp&quot; &quot;requestTemp&quot;' %} Gitalk 也确实正常工作了。 其他解决过程 在 GitHub 上寻找相关 Issues 时看见这位大神的解决过程，自愧不如，经原作者同意之后搬运过来，特别是解决思路让我很受启发。以下是原文： 起因 我的博客最近刚换上了 hexo 框架，于是评论功能就换成了 Gitalk。但最近发现登录功能不好用了，点击使用 GitHub 登录总是失败。 就点击这个按钮的时候，始终登录不上去： 于是我就想着手解决一下这个问题。 思路 这里就记录一下我在排查过程中碰到的一些坑和解决思路。 首先，登录失败的问题，第一时间应该去排查的就是网络请求，打开控制台，查看 Network 面板，出现类似的结果： 网络请求直接 403 了，拿不到 token 了，于是就登录不上了。 观察下，这个链接 cors-anywhere 似乎是用来解决跨域限制的，后面还跟了一个 GitHub 的 Access Token 获取地址，那没跑了，前面这个就是一个反向代理，后面是真实的请求 URL。 OK，看着这个也没啥思路啊，然后接着怎么办？ 那就接着去搜这个 cors-anywhere.herokuapp.com，因为 herokuapp 很眼熟嘛，就是一个公用的网址 Host 平台，类似于 AWS、Azure 之类的，那么前面这个可能包含某些信息。万一是开源的那就好办了。 接着搜，cors-anywhere，然后就搜到了这个：https://github.com/Rob--W/cors-anywhere 介绍如下： CORS Anywhere is a NodeJS proxy which adds CORS headers to the proxied request. The url to proxy is literally taken from the path, validated and proxied. The protocol part of the proxied URI is optional, and defaults to “http”. If port 443 is specified, the protocol defaults to “https”. This package does not put any restrictions on the http methods or headers, except for cookies. Requesting user credentials is disallowed. The app can be configured to require a header for proxying a request, for example to avoid a direct visit from the browser. 真是一个开源框架，和我猜的一样，就是一个解决跨域问题而生的反向代理。 然后我就在它的 README 中看到了这个： 好家伙，这不就是我刚才用到的链接吗？ 那肯定是这个玩意出了什么毛病。 咋看呢？这个果断就是找 Issue 了： 一看，太明显了： PSA: Public demo server (cors-anywhere.herokuapp.com) will be very limited by January 2021, 31st 意思就是从今年 1.31 开始这个网站的访问会受限，点进去看看： The demo server of CORS Anywhere (cors-anywhere.herokuapp.com) is meant to be a demo of this project. But abuse has become so common that the platform where the demo is hosted (Heroku) has asked me to shut down the server, despite efforts to counter the abuse (rate limits in #45 and #164, and blocking other forms of requests). Downtime becomes increasingly frequent (e.g. recently #300, #299, #295, #294, #287) due to abuse and its popularity. To counter this, I will make the following changes: The rate limit will decrease from 200 (#164) per hour to 50 per hour. By January 31st, 2021, cors-anywhere.herokuapp.com will stop serving as an open proxy. From February 1st. 2021, cors-anywhere.herokuapp.com will only serve requests after the visitor has completed a challenge: The user (developer) must visit a page at cors-anywhere.herokuapp.com to temporarily unlock the demo for their browser. This allows developers to try out the functionality, to help with deciding on self-hosting or looking for alternatives. 好吧，意思就是说这个网站本来是演示用的，但是现在已经被滥用了，然后从 1.31 开始用户手动必须手动先访问这个网站获取临时的访问权限，然后才能使用。另外推荐开发者自己来维护一个网站。 接着下面的评论第一个就更滑稽了： 这个人直接艾特了 gitalk，哈哈哈，因为 Gitalk 就如刚才所说的那样，也用了这个。 那就顺便去 Gitalk https://github.com/gitalk/gitalk，逛一下 issue，看看是不是也有人遇到了同样的问题，果不其然了： 最近几个 issue 都是关于 403 的，真热闹。 点进去看看，有个大收获，里面有个好心人说： 这次直接去嫖了一个 CORS proxy,把 gitalk.js 的 6794 行改为 proxy: ‘https://netnr-proxy.cloudno.de/https://github.com/login/oauth/access_token‘, 就可以了。具体能用多久我也没普，且用且珍惜。 真是得来全不费功夫，本来还想着自己部署着，这次那就换了就行了。 然而，这样不行，得需要改 gitalk.js 的源码，并不太好吧。 好，这时候就遇到了一个问题，要修改某些开源软件的源码应该怎么办？ 首选的思路当然不是硬改，改了之后还要自己 host 一个新的 js 文件，那显然是很费精力的。 其实一半程序在编写的时候应该是预留一些接口和配置的，我们应该能很轻易地通过某些配置就能实现某些配置的复写。 那就接着看看吧，既然要改，那就得先看看 Gitalk 是怎么用的吧。 看文档，Gitalk 调用方式如下： const gitalk = new Gitalk({ clientID: 'GitHub Application Client ID', clientSecret: 'GitHub Application Client Secret', repo: 'GitHub repo', // The repository of store comments, owner: 'GitHub repo owner', admin: \\['GitHub repo owner and collaborators, only these guys can initialize github issues'\\], id: location.pathname, // Ensure uniqueness and length less than 50 distractionFreeMode: false // Facebook-like distraction free mode }) gitalk.render('gitalk-container') 看来这个在声明的时候是有参数的，那刚才 URL 配置没看到在哪里配啊，既然如此，那就看看 Gitalk 这个对象支持多少参数吧。 接着就去找 Gitalk 的构造参数说明，找到这么一个： proxy：String Default: https://cors-anywhere.herokuapp.com/https://github.com/login/oauth/access_token. 果然找到了，所以这里如果我们要修改，那就改 proxy 参数就行了，初始化 Gitalk 的时候复写掉 proxy 就行。 OK，基本思路有了，那我怎么改到我的源码里呢？ 我的博客是基于 Hexo 的 Next 主题的，根据经验，Gitalk 是 Next 主题自带的，所以 Gitalk 的声明应该就在 Next 主题源码里面。 那怎么找呢？ 这时候就需要借助于一些搜索技巧了，搜什么？既然要用 Gitalk，那一定有刚才初始化的调用，那就搜 Gitalk 这个关键字就行了。另外还需要缩小一下搜索范围。 于是我就把范围限定到了 next 主题目录，搜索 Gitalk。 简直不要太舒服，一搜就有了，文件是 themes/next/layout/_third-party/comments/gitalk.swig。 这里我们只需要把 proxy 参数加上不就行了，值是什么呢？仿照写就行了，配置风格保持统一，那就加一条： 1 proxy : '{{ theme.gitalk.proxy }}', OK，那这个配置的值很明显是主题配置文件，那就把配置文件里面加上 proxy 这个参数就好了。 找到 themes/next/_config.yml，添加行： 1 proxy: https://netnr-proxy.cloudno.de/https://github.com/login/oauth/access_token 结果如下： 好了，大功告成！ 重新部署 Hexo，现在评论又能重新使用了，问题就解决了！ 测试地址：https://cuiqingcai.com/message/，大家来给我留言吧～ 好了，这就是我排查问题的整个过程，做一下记录。 经验 另外，其实这篇文章的用意不仅仅是单纯解决这个问题，因为这个问题大家可能并没有遇到过，因此这个解决方案仅仅是给极少数遇到这个问题的朋友提供的。 但是，这并不代表这篇文章没有价值，因为其中有的思路是通用的，在这里稍微做一下总结，希望对大家有帮助： 当遇到网页功能异常的时候，排查问题就主要看两个——控制台、网络请求，这里面往往能找到主要问题。 结合一些基本知识进行合理的推断，比如刚才我就推断了 cors- anywhere 的作用并结合 herokuapp 推断这个可能还会是个公用的服务。 当碰到没有思路或者不确定的时候，去谷歌它！不要百度，另外还可以在 GitHub 或者 Gitee 上搜。 如果找到对应的 GitHub 仓库，Issue 区往往能找到一些有效答案，比如刚才我就在 Issue 区找到了一个可替代的 cors 网站。 修改代码功能的时候要想着尽量复写，也就是 overwrite，而不是直接改，前者更具有灵活性，而且某些情况下会省去一些麻烦。 复写的时候去找一些参数配置，比如找一些初始化参数、默认参数配置，看看能否实现改写的需求。 找不到入口的时候善用全局搜索功能，比如刚才 Gitalk 找哪里调用的时候，就直接全局搜索。 根据功能限制某些搜索范围，比如刚才我就知道 Gitalk 这个功能是 Next 主题提供的，所以我就直接限制搜索范围是 Next 主题的源码。 以上步骤多尝试，熟能生巧。 部署自己的 CORSproxy 就在我以为一切都要结束的时候，我看到了这个评论，兴趣来了！ {% img /images/gitalk-403/baipiao.PNG '&quot;baipiao&quot; &quot;baipiao&quot;' %} 参考步骤： 注册 CloudFlare 账号 building Worker 上传代码 然而我卡在了第一步，DNS 设置 CF 的名称服务器一直验证不通过。。。 {% img /images/gitalk-403/cfCheck.PNG 50 50 '&quot;cfCheck&quot; &quot;cfCheck&quot;' %} 参考链接 Gitalk 评论登录 403 问题解决 精觅-崔庆才的个人站点. PSA: Public demo server (cors-anywhere.herokuapp.com) will be very limited by January 2021, 31st #301 使用 Github 登录后，提示 403 错误 #428 cloudflare-cors-anywhere 一分钟教程：利用 cloudflare worker 搭建在线代理—-jsproxy ","link":"https://memorykki.github.io/gitalk-403/"},{"title":"Hexo中Markdown语法失效问题","content":"在Hexo站上用Markdown写博时发现好多语法失效问题，记录一下 目录 1.表格直接以源码显示无渲染 2.&lt;kbd&gt;无样式 3.插入&lt;table&gt;不渲染 4.插入HTML标签产生大量换行 5.目录跳转无反应 总结 1.表格直接以源码显示无渲染 问题 | 左对齐 | 右对齐 | 居中对齐 | | :----| ----: | :------: | | 单元格aaaaaaaaaa | 单aaaaaaaaaaaaaa元格 | 单元aaaaaaaaaaa格 | | 单元格 | 单元格 | 单元格 | | 单 | 单 | 单 | 解决 表格和正文之间空两行。 Markdown中生成的表格十分简洁，这倒也符合快速的特点，如果想自定义样式，通过嵌入css实现，例如本例。 我使用的Hacker主题，通用样式文件存放在themes\\Hacker\\source\\css\\base\\normalize.css，对这种通用性的追加即可，其他主题同理。 居中对齐 对齐 居中对齐 单元格 单元格 单元格 单元格 单元格 单元格 单 单 单 待解决 表格正常显示之后，对齐方式却不生效，网上没找到和我一样问题的，不过这个需求也不大，或许只能通过css解决。 2.&lt;kbd&gt;无样式 问题 比如这个kbd标签，Ctrl ，看起来很不明显有没有，大概率本来就这样，但是我一开始希望它是这样：{% img /images/Hexo-Markdown-nonsupport/ctrl.PNG 50 50 '&quot;ctrl&quot; &quot;ctrl&quot;' %} 提示: &lt;kbd&gt; 标签已废弃，不推荐使用，但是可以通过CSS实现丰富的效果。 ------HTML&lt;kbd&gt;标签 解决 因为我感觉这个会很常用，但我给它加了css样式，还不错。Ctrl 3.插入&lt;table&gt;不渲染 问题 我在HTML里面测试好table，再将整个&lt;table&gt;&lt;/table&gt;粘贴到VSC的md文件中，但是这时HTML标签不渲染，直接以源码显示。 解决 先在md中手写&lt;table&gt;&lt;/table&gt;这两个标签，再将table里面的内容粘过去，不要开始和结束标签。 或者可能是HTML代码中有空行！ 4.插入HTML标签产生大量换行 问题 插入HTML标签产生大量换行 解决 将HTML标签之间的空格删除，也就是写到一行中。 5.目录跳转无反应 问题 因为本主题（或是别的问题）不会生成文章目录结构，所以我一般写完后用MarkdownAllinOne生成目录，但是点击无反应，不跳转。 解决 首先标题里不能有符号 如果标题为&quot;&amp;总结&quot;，但MarkdownAllinOne生成目录自动链接到&quot;#总结&quot;，Hexo渲染出的HTML源码却是： &lt;h2 id=&quot;amp-总结&quot;&gt;&lt;a href=&quot;#amp-总结&quot; class=&quot;headerlink&quot; title=&quot;&amp;amp;总结&quot;&gt;&lt;/a&gt;&amp;amp;总结&lt;/h2&gt; 这样看来，&quot;#amp-总结&quot;才是正确跳转。 若大小写原因，关闭MarkdownAllinOne扩展的自动转换小写选项 去扩展设置里关闭。 （！这不是必须的，只是为了方便，顺便关闭自动更新目录） 总结 关键还得搞清楚Hexo生成的是静态页面这个大方向，然后根据自己想法发挥。 ","link":"https://memorykki.github.io/Hexo-Markdown-nonsupport/"},{"title":"Markdown","content":"Markdown是一种轻量级标记语言，以纯文本格式快速编写文档。 Markdown编写的文档后缀为 .md、.markdown，可以导出HTML、Word、图像、PDF、Epub等多种格式的文档。 Hexo、github等支持使用markdown编辑器编写文章，刚好边学边练。 可以通过css的方式，美化Hexo元素，嵌入style 标签，在生成html时，style标签也随之写入到html页面中，产生自定义效果。 目录 Markdown语法 标题 字体 换行 分隔线 脚注 列表 区块 代码 链接 图片 表格 支持的HTML元素 转义 公式 画图 Markdown工具 推荐 参考资料 Markdown语法 标题 使用#标记标题，1-6个#标识1-6级标题。 （markdown语法中的关键字之后一般要加一个空格） 示例： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 效果如下： 一级标题 二级标题 三级标题 四级标题 五级标题 六级标题 字体 斜体 使用一个*包起来 加粗 使用两个*包起来 斜体加粗 使用三个*包起来 删除线 使用两个~包起来 下划线 使用HTML的&lt;u&gt;&lt;/u&gt;包起来 示例： *这是倾斜的文字* **这是加粗的文字** ***这是斜体加粗的文字*** ~~这是加删除线的文字~~ &lt;u&gt;这是加下划线的文字&lt;/u&gt; 效果如下： 这是倾斜的文字 这是加粗的文字 这是斜体加粗的文字 这是加删除线的文字 这是加下划线的文字 换行 markdown源文件中的回车不会换行，必须使用两个空格+回车或者两个回车。 示例 （注意第一段之后的空格）： 第一段 第二段 第三段 效果如下： 第一段 第二段 第三段 分隔线 在一行中用三个以上的*、-、_来建分隔线，显示效果一致，行内不能有其他东西，但可以在行内插入空格。 示例： *** --- ___ 效果如下： 脚注 ！貌似无法使用 脚注是对文本的补充说明，鼠标悬停即可查看，使用[要注明的文本]&lt;/kbd&gt;，在之后任意位置重复&lt;kbd&gt;[要注明的文本]:补充内容。 示例： Welcome To [^Memorykk's Blog] [^Memorykk's Blog]: https://memorykk.cn 效果如下： Welcome To [^ Memorykk ] [^ Memorykk ]: https://memorykk.cn 参考： 列表 无序列表 无序列表使用*、+、-标记，追加列表内容，三者无差异。 示例： * 第一项 + 第二项 - 第三项 效果如下： 第一项 第二项 第三项 有序列表 有序列表使用数字+.标记。 示例： 1. 第一项 2. 第二项 效果如下： 第一项 第二项 嵌套列表 子列表的选项前加四个空格即可。 示例： 1. 第一项 * 元素1 * 元素2 2. 第二项 - 元素3 - 元素4 效果如下： 第一项 元素1 元素2 第二项 元素3 元素4 区块 区块使用&gt;标记，追加区块内容。 区块支持嵌套，一个&gt;为最外层，两个&gt;为第一层嵌套，以此类推。 另外，区块可与列表等随意嵌套。 &gt; 1. 第一项 &gt; * 元素1 &gt; * 元素2 2. 第二项 &gt; - 元素3 &gt; - 元素4 效果如下： 第一项 元素1 元素2 第二项 元素3 元素4 代码 单行代码 使用一个`将代码包起来。 `System.out.println(&quot;Hello World!&quot;);` 效果如下： System.out.println(&quot;Hello World!&quot;); 多行代码 使用六个`将代码包起来，前三个`接代码语言，也可不指定。 示例：（为了防止编辑器转义，在前后加了前后括号，实际使用中不能写哦~） （```java System.out.println(&quot;Hello&quot;); System.out.println(&quot;World&quot;); System.out.println(&quot;!&quot;); ```） 效果如下： System.out.println(&quot;Hello&quot;); System.out.println(&quot;World&quot;); System.out.println(&quot;!&quot;); 链接 普通链接 使用[链接名称](链接地址)或者直接&lt;链接地址&gt;标记。 [Memorykk's Blog](https://memorykk.cn) &lt;https://www.memorykkk.cn&gt; 效果如下： Memorykk's Blog https://www.memorykkk.cn 高级链接 通过变量来设置一个链接，变量赋值在文档末尾。 这个链接赋给变量1：[Memorykk's Blog][1] 这个链接赋给变量google：[GOOGLE][google] [1]: https://memorykk.cn [google]: https://www.google.com 效果如下： 这个链接赋给变量1：Memorykk's Blog 这个链接赋给变量google：GOOGLE 图片 使用![alt文本](图片链接 &quot;可选标题&quot;)标记。 markdown不支持指定宽高，可使用HTML &lt;img&gt;标签标记。 可以制作图片链接 示例： ![Github.ico](https://github.githubassets.com/favicons/favicon.svg) 效果如下： 表格 使用|标记单元格，使用-分隔表头和其他行，还可设置对其方式： -:设置内容和标题栏居右对齐。 :-设置内容和标题栏居左对齐。 :-:设置内容和标题栏居中对齐。 :之间多余的空格会被忽略 的数量至少一个 内容和|之间多余的空格会被忽略 示例：（但是本页并未生效，Hexo的问题？） | 左对齐 | 右对齐 | 居中对齐 | | :------| ----: | :----: | | 单元格 | 单元格 | 单元格 | | 单元格 | 单元格 | 单元格 | | 单 | 单 | 单 | 效果如下： 支持的HTML元素 直接在文档里面用 HTML 撰写。 目前支持的 HTML 元素有：&lt;kbd&gt; &lt;b&gt; &lt;i&gt; &lt;em&gt; &lt;sup&gt; &lt;sub&gt; &lt;br&gt;等。 转义 Markdown 使用了很多特殊符号来表示特定的意义，如果需要显示特定的符号则需要使用转义字符反斜杠\\。 目前支持的特殊字符： \\、`、*、_、{}、[]、()、#、+、-、.、!、HTML元素。 公式 使用两个$包裹 TeX 或 LaTeX 格式的数学公式。 提交后会根据需要加载 Mathjax 对数学公式进行渲染。 示例： $$ \\lim_{x \\to 0} \\frac{3x^2 +7x^3}{x^2 +5x^4} = 3 $$ 效果如下： lim⁡x→03x2+7x3x2+5x4=3\\lim_{x \\to 0} \\frac{3x^2 +7x^3}{x^2 +5x^4} = 3 x→0lim​x2+5x43x2+7x3​=3 画图 参考 菜鸟教程 Markdown工具 目前我使用的是Visual Studio Code，默认支持markdown编辑以及即时预览，配合插件十分好用： Markdown Preview Github Styling：预览您的Markdown在Github上呈现的样子 Markdown All in OneMarkdown所需的全部功能（键盘快捷键、目录、自动预览等） 推荐 Hexo中Markdown语法失效问题 参考资料 Markdown教程 Markdown基本语法 markdown简明使用方法 ","link":"https://memorykki.github.io/Markdown/"},{"title":"Hello Gridea","content":" ","link":"https://memorykki.github.io/hello-gridea/"}]}